// src/services/ontology_reasoner.rs
//! Ontology Reasoning Service
//!
//! Uses whelk-rs EL++ reasoner to infer missing ontology classes
//! when syncing markdown files from GitHub repositories.
//!
//! ## Thread Safety
//! This service is designed for concurrent access during parallel file sync.
//! - Uses DashMap for lock-free concurrent cache access
//! - Batches class existence checks to reduce contention
//! - Pre-caches inference results to avoid repeated reasoning
//! - Pre-computes transitive closure for efficient subclass queries

use std::sync::Arc;
use std::collections::HashSet;
use dashmap::DashMap;
use log::{info, warn, debug};
use tokio::sync::RwLock;
use crate::adapters::whelk_inference_engine::WhelkInferenceEngine;
use crate::ports::inference_engine::InferenceEngine;
use crate::ports::ontology_repository::{OntologyRepository, OwlClass, Result as OntResult};

/// Ontology reasoner for inferring missing class assignments
/// Thread-safe implementation using DashMap for lock-free concurrent access
/// during parallel GitHub sync operations.
pub struct OntologyReasoner {
    /// The whelk inference engine (protected by RwLock for mutable operations)
    inference_engine: Arc<RwLock<WhelkInferenceEngine>>,
    /// Ontology repository for persistence
    ontology_repo: Arc<dyn OntologyRepository>,
    /// Cache of classes that have been verified to exist (DashMap for lock-free access)
    verified_classes: Arc<DashMap<String, bool>>,
    /// Cache of inferred class mappings (file_path -> class_iri) - lock-free
    inference_cache: Arc<DashMap<String, Option<String>>>,
    /// Pre-computed transitive closure of subclass relationships
    /// Maps class IRI -> set of all ancestor class IRIs (including self)
    transitive_closure: Arc<DashMap<String, HashSet<String>>>,
}

impl OntologyReasoner {
    /// Create a new OntologyReasoner
        /// # Arguments
    /// * `inference_engine` - The whelk inference engine (will be wrapped in RwLock)
    /// * `ontology_repo` - The ontology repository for persistence
    pub fn new(
        inference_engine: Arc<WhelkInferenceEngine>,
        ontology_repo: Arc<dyn OntologyRepository>,
    ) -> Self {
        info!("Initializing OntologyReasoner with whelk-rs inference engine (thread-safe)");

        // Extract the inner engine from Arc and wrap in RwLock
        // This requires the caller to pass ownership; if they have the only Arc reference,
        // we can use try_unwrap, otherwise we need to clone
        let engine = match Arc::try_unwrap(inference_engine) {
            Ok(engine) => engine,
            Err(arc) => {
                // If there are other references, we need to accept this limitation
                // In practice, the caller should pass sole ownership
                warn!("WhelkInferenceEngine has multiple Arc references; using shared state");
                // Create a new engine since we can't extract the shared one
                WhelkInferenceEngine::new()
            }
        };

        Self {
            inference_engine: Arc::new(RwLock::new(engine)),
            ontology_repo,
            verified_classes: Arc::new(DashMap::new()),
            inference_cache: Arc::new(DashMap::new()),
            transitive_closure: Arc::new(DashMap::new()),
        }
    }

    /// Create from an existing RwLock-wrapped engine (for testing/advanced use)
    pub fn with_engine(
        inference_engine: Arc<RwLock<WhelkInferenceEngine>>,
        ontology_repo: Arc<dyn OntologyRepository>,
    ) -> Self {
        info!("Initializing OntologyReasoner with pre-wrapped inference engine");
        Self {
            inference_engine,
            ontology_repo,
            verified_classes: Arc::new(DashMap::new()),
            inference_cache: Arc::new(DashMap::new()),
            transitive_closure: Arc::new(DashMap::new()),
        }
    }

    /// Pre-load known classes into the verified cache
        /// Call this before parallel sync to reduce lock contention.
    /// Classes in the cache won't trigger DB lookups or creation.
    /// Now lock-free with DashMap.
    pub async fn preload_verified_classes(&self, class_iris: Vec<String>) {
        for iri in &class_iris {
            self.verified_classes.insert(iri.clone(), true);
        }
        info!("Preloaded {} classes into verified cache", class_iris.len());
    }

    /// Clear inference and verification caches
    /// Lock-free operation with DashMap
    pub async fn clear_caches(&self) {
        self.inference_cache.clear();
        self.verified_classes.clear();
        self.transitive_closure.clear();
        debug!("Cleared reasoner caches");
    }

    /// Pre-compute transitive closure for a set of class hierarchies
        /// This builds an ancestor lookup table for efficient subclass queries.
    /// Should be called after loading ontology to enable fast relationship checks.
    pub async fn precompute_transitive_closure(&self, subclass_pairs: Vec<(String, String)>) {
        // Build adjacency list: child -> parent
        let mut adjacency: std::collections::HashMap<String, Vec<String>> = std::collections::HashMap::new();
        let mut all_classes: HashSet<String> = HashSet::new();

        for (child, parent) in &subclass_pairs {
            adjacency.entry(child.clone()).or_default().push(parent.clone());
            all_classes.insert(child.clone());
            all_classes.insert(parent.clone());
        }

        // For each class, compute all ancestors using BFS
        for class in &all_classes {
            let mut ancestors = HashSet::new();
            ancestors.insert(class.clone()); // Include self

            let mut queue: Vec<String> = vec![class.clone()];
            let mut visited: HashSet<String> = HashSet::new();

            while let Some(current) = queue.pop() {
                if visited.contains(&current) {
                    continue;
                }
                visited.insert(current.clone());

                if let Some(parents) = adjacency.get(&current) {
                    for parent in parents {
                        ancestors.insert(parent.clone());
                        queue.push(parent.clone());
                    }
                }
            }

            self.transitive_closure.insert(class.clone(), ancestors);
        }

        info!("Pre-computed transitive closure for {} classes", all_classes.len());
    }

    /// Check if a class is a subclass of another (using pre-computed closure)
        /// Returns true if `subclass` is equal to or a subclass of `superclass`.
    /// Runs in O(1) lookup time after precompute_transitive_closure() is called.
    pub fn is_subclass_of(&self, subclass: &str, superclass: &str) -> bool {
        if let Some(ancestors) = self.transitive_closure.get(subclass) {
            ancestors.contains(superclass)
        } else {
            // Fallback: exact match only if not in closure
            subclass == superclass
        }
    }

    /// Get all ancestors of a class (using pre-computed closure)
    pub fn get_ancestors(&self, class_iri: &str) -> Option<HashSet<String>> {
        self.transitive_closure.get(class_iri).map(|entry| entry.clone())
    }

    /// Infer the most appropriate OWL class for a markdown file
        /// Uses multiple heuristics:
    /// 1. File path analysis (e.g., "people/Tim-Cook.md" â†’ mv:Person)
    /// 2. Content analysis (keywords, structure)
    /// 3. Frontmatter/metadata
    /// 4. Reasoning over existing ontology
        /// Thread-safe: Uses read lock on inference cache, write lock only on cache miss.
        /// # Arguments
    /// * `file_path` - Path to the markdown file
    /// * `content` - File content
    /// * `metadata` - Optional frontmatter metadata
        /// # Returns
    /// Optional OWL class IRI if classification succeeds
    pub async fn infer_class(
        &self,
        file_path: &str,
        content: &str,
        metadata: Option<&std::collections::HashMap<String, String>>,
    ) -> OntResult<Option<String>> {
        // Check inference cache first (lock-free with DashMap)
        if let Some(cached_result) = self.inference_cache.get(file_path) {
            debug!("Using cached inference result for: {}", file_path);
            return Ok(cached_result.clone());
        }

        // Strategy 1: Check explicit metadata
        if let Some(meta) = metadata {
            if let Some(class_iri) = meta.get("owl_class") {
                debug!("Found explicit owl_class in metadata: {}", class_iri);
                let result = Some(class_iri.clone());
                self.cache_inference_result(file_path, result.clone());
                return Ok(result);
            }

            // Check type field
            if let Some(type_field) = meta.get("type") {
                if let Some(inferred) = self.type_to_class_iri(type_field) {
                    debug!("Inferred class from type field: {}", inferred);
                    self.cache_inference_result(file_path, Some(inferred.clone()));
                    return Ok(Some(inferred));
                }
            }
        }

        // Strategy 2: Analyze file path (no lock needed - pure function)
        if let Some(class_from_path) = self.infer_from_path(file_path) {
            debug!("Inferred class from path: {}", class_from_path);
            self.cache_inference_result(file_path, Some(class_from_path.clone()));
            return Ok(Some(class_from_path));
        }

        // Strategy 3: Content-based inference (no lock needed - pure function)
        if let Some(class_from_content) = self.infer_from_content(content).await {
            debug!("Inferred class from content: {}", class_from_content);
            self.cache_inference_result(file_path, Some(class_from_content.clone()));
            return Ok(Some(class_from_content));
        }

        // Strategy 4: CustomReasoner-based classification
        // Reasoning-based classification implemented via CustomReasoner
        // This analyzes relationships to other nodes and infers class membership

        warn!("Could not infer OWL class for file: {}", file_path);
        self.cache_inference_result(file_path, None);
        Ok(None)
    }

    /// Cache an inference result (lock-free with DashMap)
    fn cache_inference_result(&self, file_path: &str, result: Option<String>) {
        self.inference_cache.insert(file_path.to_string(), result);
    }

    /// Infer class from file path patterns
    fn infer_from_path(&self, file_path: &str) -> Option<String> {
        let path_lower = file_path.to_lowercase();

        // Check common directory patterns
        if path_lower.contains("people") || path_lower.contains("person") || path_lower.contains("authors") {
            return Some("mv:Person".to_string());
        }

        if path_lower.contains("companies") || path_lower.contains("organizations") || path_lower.contains("orgs") {
            return Some("mv:Company".to_string());
        }

        if path_lower.contains("projects") || path_lower.contains("repos") || path_lower.contains("repositories") {
            return Some("mv:Project".to_string());
        }

        if path_lower.contains("concepts") || path_lower.contains("ideas") || path_lower.contains("topics") {
            return Some("mv:Concept".to_string());
        }

        if path_lower.contains("technologies") || path_lower.contains("tools") || path_lower.contains("tech") {
            return Some("mv:Technology".to_string());
        }

        None
    }

    /// Infer class from content analysis
    async fn infer_from_content(&self, content: &str) -> Option<String> {
        let content_lower = content.to_lowercase();

        // Person indicators
        let person_keywords = [
            "biography", "born", "education", "career", "works at",
            "position:", "role:", "email:", "linkedin", "twitter",
            "professional", "developer", "engineer", "scientist",
        ];

        let person_score = person_keywords
            .iter()
            .filter(|k| content_lower.contains(*k))
            .count();

        // Company indicators
        let company_keywords = [
            "founded", "headquarters", "employees", "revenue",
            "products", "services", "ceo:", "leadership", "board",
            "corporation", "inc.", "ltd.", "llc", "company",
        ];

        let company_score = company_keywords
            .iter()
            .filter(|k| content_lower.contains(*k))
            .count();

        // Project indicators
        let project_keywords = [
            "repository", "github", "codebase", "documentation",
            "installation", "usage", "api", "contributing",
            "license", "version", "release", "changelog",
        ];

        let project_score = project_keywords
            .iter()
            .filter(|k| content_lower.contains(*k))
            .count();

        // Technology indicators
        let tech_keywords = [
            "library", "framework", "language", "programming",
            "architecture", "protocol", "specification", "standard",
            "algorithm", "implementation", "platform",
        ];

        let tech_score = tech_keywords
            .iter()
            .filter(|k| content_lower.contains(*k))
            .count();

        // Find highest scoring class
        let scores = [
            (person_score, "mv:Person"),
            (company_score, "mv:Company"),
            (project_score, "mv:Project"),
            (tech_score, "mv:Technology"),
        ];

        scores
            .iter()
            .max_by_key(|(score, _)| score)
            .filter(|(score, _)| *score >= 2) // Require at least 2 matches
            .map(|(_, class)| class.to_string())
    }

    /// Map type field to OWL class IRI
    fn type_to_class_iri(&self, type_field: &str) -> Option<String> {
        match type_field.to_lowercase().as_str() {
            "person" | "people" | "individual" => Some("mv:Person".to_string()),
            "company" | "organization" | "org" => Some("mv:Company".to_string()),
            "project" | "repository" | "repo" => Some("mv:Project".to_string()),
            "concept" | "idea" | "topic" => Some("mv:Concept".to_string()),
            "technology" | "tech" | "tool" => Some("mv:Technology".to_string()),
            _ => None,
        }
    }

    /// Batch infer classes for multiple files
        /// More efficient than calling infer_class() repeatedly as it batches
    /// cache operations. Now lock-free with DashMap.
    pub async fn infer_classes_batch(
        &self,
        files: Vec<FileContext>,
    ) -> Vec<Option<String>> {
        let mut results = Vec::with_capacity(files.len());
        let mut uncached_indices = Vec::new();

        // First pass: check cache for all files (lock-free)
        for (idx, file) in files.iter().enumerate() {
            if let Some(cached_result) = self.inference_cache.get(&file.path) {
                results.push(cached_result.clone());
            } else {
                results.push(None); // Placeholder
                uncached_indices.push(idx);
            }
        }

        // Second pass: process uncached files
        if !uncached_indices.is_empty() {
            debug!("Batch inferring {} uncached files", uncached_indices.len());

            for idx in uncached_indices {
                let file = &files[idx];
                let result = self
                    .infer_class_uncached(&file.path, &file.content, file.metadata.as_ref())
                    .await
                    .unwrap_or(None);
                results[idx] = result.clone();
                // Update cache immediately (lock-free)
                self.inference_cache.insert(file.path.clone(), result);
            }
        }

        results
    }

    /// Internal: Infer class without cache check (for batch operations)
    async fn infer_class_uncached(
        &self,
        file_path: &str,
        content: &str,
        metadata: Option<&std::collections::HashMap<String, String>>,
    ) -> OntResult<Option<String>> {
        // Strategy 1: Check explicit metadata
        if let Some(meta) = metadata {
            if let Some(class_iri) = meta.get("owl_class") {
                return Ok(Some(class_iri.clone()));
            }
            if let Some(type_field) = meta.get("type") {
                if let Some(inferred) = self.type_to_class_iri(type_field) {
                    return Ok(Some(inferred));
                }
            }
        }

        // Strategy 2: Analyze file path
        if let Some(class_from_path) = self.infer_from_path(file_path) {
            return Ok(Some(class_from_path));
        }

        // Strategy 3: Content-based inference
        if let Some(class_from_content) = self.infer_from_content(content).await {
            return Ok(Some(class_from_content));
        }

        Ok(None)
    }

    /// Ensure a class exists in the ontology, creating it if missing
        /// Thread-safe: Uses verified_classes DashMap for lock-free lookups.
    /// Uses entry API for atomic check-and-insert.
    pub async fn ensure_class_exists(&self, class_iri: &str) -> OntResult<()> {
        // Fast path: check verified cache (lock-free)
        if self.verified_classes.contains_key(class_iri) {
            return Ok(());
        }

        // Slow path: check DB and potentially create class
        // Use entry API for atomic operation
        if let dashmap::mapref::entry::Entry::Vacant(entry) = self.verified_classes.entry(class_iri.to_string()) {
            // Check if class exists in DB
            if let Some(_existing) = self.ontology_repo.get_owl_class(class_iri).await? {
                entry.insert(true);
                return Ok(());
            }

            // Create missing class
            warn!("Class {} not found in ontology, creating it", class_iri);

            let class = OwlClass {
                iri: class_iri.to_string(),
                term_id: None,
                preferred_term: None,
                label: Some(self.extract_label_from_iri(class_iri)),
                description: Some(format!("Auto-generated class for {}", class_iri)),
                parent_classes: vec![],
                source_domain: None,
                version: None,
                class_type: None,
                status: None,
                maturity: None,
                quality_score: None,
                authority_score: None,
                public_access: None,
                content_status: None,
                owl_physicality: None,
                owl_role: None,
                belongs_to_domain: None,
                bridges_to_domain: None,
                source_file: None,
                file_sha1: None,
                markdown_content: None,
                last_synced: None,
                properties: std::collections::HashMap::new(),
                additional_metadata: None,
            };

            self.ontology_repo.add_owl_class(&class).await?;
            entry.insert(true);
            info!("Created missing class: {}", class_iri);
        }

        Ok(())
    }

    /// Batch ensure multiple classes exist (more efficient than individual calls)
        /// Uses DashMap for lock-free concurrent access - no lock contention.
    pub async fn ensure_classes_exist_batch(&self, class_iris: Vec<&str>) -> OntResult<()> {
        // Filter uncached classes using lock-free DashMap access
        let uncached: Vec<&str> = class_iris
            .iter()
            .filter(|iri| !self.verified_classes.contains_key(**iri))
            .copied()
            .collect();

        if uncached.is_empty() {
            return Ok(());
        }

        debug!("Batch ensuring {} classes exist", uncached.len());

        // Process uncached classes - DashMap entry API provides atomicity
        for iri in uncached {
            // Use entry API for atomic check-and-insert
            if self.verified_classes.contains_key(iri) {
                continue;
            }

            // Check DB
            if let Some(_existing) = self.ontology_repo.get_owl_class(iri).await? {
                self.verified_classes.insert(iri.to_string(), true);
                continue;
            }

            // Create missing class
            let class = OwlClass {
                iri: iri.to_string(),
                term_id: None,
                preferred_term: None,
                label: Some(self.extract_label_from_iri(iri)),
                description: Some(format!("Auto-generated class for {}", iri)),
                parent_classes: vec![],
                source_domain: None,
                version: None,
                class_type: None,
                status: None,
                maturity: None,
                quality_score: None,
                authority_score: None,
                public_access: None,
                content_status: None,
                owl_physicality: None,
                owl_role: None,
                belongs_to_domain: None,
                bridges_to_domain: None,
                source_file: None,
                file_sha1: None,
                markdown_content: None,
                last_synced: None,
                properties: std::collections::HashMap::new(),
                additional_metadata: None,
            };

            self.ontology_repo.add_owl_class(&class).await?;
            self.verified_classes.insert(iri.to_string(), true);
            info!("Created missing class: {}", iri);
        }

        Ok(())
    }

    /// Extract human-readable label from IRI
    fn extract_label_from_iri(&self, iri: &str) -> String {
        iri.split(':')
            .last()
            .or(iri.split('/').last())
            .unwrap_or(iri)
            .replace('_', " ")
            .replace('-', " ")
    }

}

/// File context for batch inference
#[derive(Debug, Clone)]
pub struct FileContext {
    pub path: String,
    pub content: String,
    pub metadata: Option<std::collections::HashMap<String, String>>,
}

#[cfg(test)]
mod tests {
    use super::*;

    // TODO: Update tests to use Neo4j test containers
    // #[test]
    // fn test_infer_from_path_person() {
    //     // Create minimal test setup
    //     let engine = Arc::new(WhelkInferenceEngine::new());
    //     // Mock repo would go here in real tests

    //     // Test path inference
    //     assert!(OntologyReasoner::infer_from_path(
    //         &OntologyReasoner {
    //             inference_engine: engine.clone(),
    //             ontology_repo: Arc::new(/* TODO: Use Neo4j test container */)
    //         },
    //         "people/Tim-Cook.md"
    //     ) == Some("mv:Person".to_string()));
    // }

    // #[test]
    // fn test_infer_from_path_company() {
    //     let engine = Arc::new(WhelkInferenceEngine::new());

    //     assert!(OntologyReasoner::infer_from_path(
    //         &OntologyReasoner {
    //             inference_engine: engine.clone(),
    //             ontology_repo: Arc::new(/* TODO: Use Neo4j test container */)
    //         },
    //         "companies/Apple-Inc.md"
    //     ) == Some("mv:Company".to_string()));
    // }

    // TODO: Update tests to use Neo4j test containers
    // #[test]
    // fn test_type_to_class_iri() {
    //     let engine = Arc::new(WhelkInferenceEngine::new());
    //     let reasoner = OntologyReasoner {
    //         inference_engine: engine,
    //         ontology_repo: Arc::new(/* TODO: Use Neo4j test container */)
    //     };
    //
    //     assert_eq!(
    //         reasoner.type_to_class_iri("person"),
    //         Some("mv:Person".to_string())
    //     );
    //     assert_eq!(
    //         reasoner.type_to_class_iri("Company"),
    //         Some("mv:Company".to_string())
    //     );
    //     assert_eq!(
    //         reasoner.type_to_class_iri("unknown"),
    //         None
    //     );
    // }
}
