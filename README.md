<div align="center">

# 🌌 VisionFlow

[![License](https://img.shields.io/badge/License-Mozilla%202.0-blue.svg)](LICENSE)
[![CUDA](https://img.shields.io/badge/CUDA-40%20Kernels-green.svg)](docs/)
[![Agents](https://img.shields.io/badge/AI%20Agents-50%2B%20Concurrent-orange.svg)](docs/)
[![Performance](https://img.shields.io/badge/Performance-60%20FPS%20@%20100k%20nodes-red.svg)](docs/)

</div>

<div align="center">
  <table>
    <tr>
      <td align="center">
        <strong>👥 Collaborative Team AI</strong><br/>
        <sub>Research Environment</sub>
      </td>
      <td align="center">
        <strong>🤖 Continuous Background</strong><br/>
        <sub>Agent Intelligence</sub>
      </td>
      <td align="center">
        <strong>🔐 Self-Sovereign</strong><br/>
        <sub>Multi-Modal Interface</sub>
      </td>
    </tr>
    <tr>
      <td align="center">
        <strong>📊 Massive Scale</strong><br/>
        <sub>Dozens of Users • Hundreds of Agents • Thousands of Nodes</sub>
      </td>
      <td align="center">
        <strong>🛡️ Enterprise Security</strong><br/>
        <sub>Thin Client • Secure Server • W3C DID</sub>
      </td>
      <td align="center">
        <strong>🎙️ Voice-First</strong><br/>
        <sub>Natural Human-AI Conversation</sub>
      </td>
    </tr>
  </table>
</div>

---

<div align="center">
  <img src="./visionflow.gif" alt="VisionFlow Visualisation" style="width:90%; max-width:800px; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">

  <br/><br/>

  <img src="./jarvisOld.png" alt="Runtime Screenshot" style="width:90%; max-width:800px; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
</div>

<br/>

<div align="center">
  <h3>🚀 Like ChatGPT Pulse for Your Private Knowledge</h3>
  <p><strong>VisionFlow deploys self-sovereign AI agents that continuously research, analyse, and surface insights<br/>from your data corpus - all visualised in stunning real-time 3D.</strong></p>

  <sub>Inspired by the innovative work of <a href="https://github.com/trebornipsa">Prof. Rob Aspin</a></sub>
</div>

---

<div align="center">
  <table>
    <tr>
      <th>🏆 Enterprise Value</th>
      <th>⚡ Performance</th>
      <th>🔧 Technology</th>
      <th>👥 Collaboration</th>
    </tr>
    <tr>
      <td align="center">
        <strong>$24-75M</strong><br/>
        <sub>Market Valuation</sub>
      </td>
      <td align="center">
        <strong>60 FPS</strong><br/>
        <sub>@ 100k+ nodes</sub>
      </td>
      <td align="center">
        <strong>40 CUDA</strong><br/>
        <sub>GPU Kernels</sub>
      </td>
      <td align="center">
        <strong>50+ Agents</strong><br/>
        <sub>Concurrent Swarms</sub>
      </td>
    </tr>
    <tr>
      <td align="center">
        <strong>$7.5-15M</strong><br/>
        <sub>Annual Revenue</sub>
      </td>
      <td align="center">
        <strong><10ms</strong><br/>
        <sub>WebSocket Latency</sub>
      </td>
      <td align="center">
        <strong>34-byte</strong><br/>
        <sub>Binary Protocol</sub>
      </td>
      <td align="center">
        <strong>Real-time</strong><br/>
        <sub>Voice-to-Voice AI</sub>
      </td>
    </tr>
  </table>
</div>

---

![Group In Octave](./groupOctave.jpg)

![Chloe In Octave](./ChloeOctave.jpg)

---

## 🧠 The ChatGPT Pulse of Private Knowledge

Just as ChatGPT Pulse performs asynchronous research on your behalf, **VisionFlow orchestrates swarms of AI agents** that continuously work in the background to:

### 🔄 Continuous Background Intelligence
- **Autonomous Research**: Like Pulse's overnight research, VisionFlow agents continuously analyse your private corpus
- **Proactive Discovery**: Surface new connections and insights without being prompted
- **Living Knowledge Graph**: Your data evolves and updates in real-time as agents discover relationships
- **Multi-Agent Collaboration**: Specialised agents (Researcher, Analyst, Coder, Reviewer) work together like Pulse's research engine

### 🎯 Key Similarities with ChatGPT Pulse

| ChatGPT Pulse | VisionFlow |
|---------------|------------|
| Asynchronous daily research | Continuous real-time agent research |
| Surfaces insights from past chats | Discovers patterns in your knowledge corpus |
| Visual summaries you can expand | 3D visualisation you can explore |
| Proactive morning delivery | Real-time insight streaming |
| Based on your memories & history | Based on your private data & documents |
| Curated topics from interactions | Agent-discovered knowledge connections |

### 🚀 Beyond Pulse: Multi-User Human-AI Collaboration Universe

While ChatGPT Pulse works with your OpenAI conversations, VisionFlow creates an **immersive collaborative space** where human experts and AI agents work together:

- **👥 Multi-User Collaboration**: Multiple human experts collaborate with AI agents in real-time
- **🤝 Human-AI Symbiosis**: Agents learn from human expertise while augmenting human capabilities
- **🔐 Private & Secure**: Your data never leaves your infrastructure
- **👁️ Immersive 3D Workspace**: Watch humans and AI agents collaborate in shared virtual space
- **🌐 Unlimited Corpus**: Connect GitHub repos, documents, databases, APIs
- **⚡ Real-time Synchronisation**: All participants see updates instantly
- **🎮 Interactive Control**: Humans direct agent swarms while agents suggest new research directions

---

## 🚀 Quick Start

```bash
# Clone and deploy your private research assistant
git clone https://github.com/your-org/VisionsFlow
cd VisionsFlow

# Configure environment
cp .env.example .env

# Deploy with Docker
docker-compose up -d

# Access your AI research universe
open http://localhost:3001
```

**[📚 Full Documentation](docs/)** | **[🎯 Quick Start Guide](docs/getting-started/02-quick-start.md)** | **[🔧 Installation](docs/getting-started/01-installation.md)**

---

## ✨ Core Capabilities: Human-AI Collaborative Intelligence

### 🤝 Immersive Multi-User Collaboration Platform
Unlike ChatGPT Pulse's single-user experience, VisionFlow enables **team-based human-AI research with voice**:

- **🎙️ Voice-to-Voice AI Interaction**: Natural conversation with AI agents using advanced TTS/STT
- **👁️ Independent Specialist Views**: Each user maintains their own perspective while staying synchronised
- **🌐 Shared Virtual Workspace**: Multiple experts and AI agents in the same 3D environment
- **📊 Personalised Data Lenses**: Individual filtering and visualisation preferences per user
- **🔊 Spatial Audio**: Voice communication positioned in 3D space for natural collaboration
- **Real-time Presence**: See where team members and agents are focusing
- **Collaborative Discovery**: Humans guide agents through voice while agents respond verbally
- **Knowledge Handoffs**: Seamless transfer of findings between humans and AI
- **Expertise Amplification**: AI agents learn from human domain knowledge through conversation

### 🤖 Autonomous Agent Research System
Just like ChatGPT Pulse researches on your behalf, VisionFlow deploys intelligent agents that:

- **Background Processing**: Agents work 24/7 analysing your data
- **Human-Guided Learning**: Agents adapt based on expert feedback
- **Pattern Recognition**: Automatically identify trends and anomalies
- **Collaborative Discovery**: Surface findings to human experts for validation
- **Knowledge Evolution**: Continuously update understanding through human-AI interaction

### 📊 Visual Intelligence Dashboard
Instead of Pulse's card-based summaries, VisionFlow provides:

- **3D Knowledge Universe**: Navigate your data like a galaxy of interconnected insights
- **Real-time Agent Visualisation**: Watch AI agents collaborate and discover
- **GPU-Accelerated Rendering**: Smooth interaction with 100,000+ data points
- **Interactive Exploration**: Dive deep into any insight cluster
- **Time-Travel Analysis**: Replay how knowledge evolved over time

### 🧠 Multi-Agent Orchestration
While Pulse uses a single research engine, VisionFlow orchestrates specialised swarms:

- **Researcher Agents**: Deep-dive into specific topics
- **Analyst Agents**: Identify patterns and correlations
- **Coder Agents**: Understand and document code relationships
- **Reviewer Agents**: Validate and cross-reference findings
- **Planner Agents**: Coordinate research strategies

---

## 🎯 Use Cases: Collaborative Human-AI Research Teams

### 🎙️ Voice-First Collaborative Research
- **Natural Voice Conversations**: Speak directly to AI agents, receive voice responses
- **Hands-Free Operation**: Control entire system through voice commands
- **Multi-Language Support**: Teams collaborate in their preferred languages
- **Voice Transcription**: Automatic documentation of verbal insights
- **Audio Spatial Awareness**: Voices positioned in 3D space matching visual presence

### 👁️ Independent Specialist Views with Team Sync
- **Personalised Perspectives**: Each expert sees data through their specialist lens
- **Synchronous Exploration**: Individual views while maintaining team awareness
- **Custom Visualisation Filters**: Data scientists see metrics, developers see code structure
- **Focus Independence**: Zoom into details without affecting teammates' views
- **Instant View Sharing**: "Show my view" voice command for knowledge transfer

### 👥 Team-Based Knowledge Discovery
- **Collaborative Research Sessions**: Multiple experts work with AI agents simultaneously
- **Voice-Guided AI Training**: Train agents through natural conversation
- **Real-time Insight Sharing**: Discoveries instantly visible with voice announcements
- **Focus Coordination**: See and hear where colleagues and agents are investigating
- **Expertise Routing**: AI agents route findings to the right expert with voice alerts

### 🏢 Enterprise Collaboration Scenarios
- **Cross-Functional Teams**: Data scientists, developers, and domain experts in one space
- **Global Collaboration**: Teams across time zones with persistent AI agents
- **Knowledge Handoffs**: Agents continue research when humans go offline
- **Audit Trails**: Complete history of human decisions and AI discoveries
- **Training & Onboarding**: New team members learn from AI-captured expertise

### 📚 Knowledge Management with Human Oversight
- **Human-Validated Insights**: AI discoveries reviewed by domain experts
- **Guided Exploration**: Experts direct agent focus areas
- **Quality Control**: Human verification of AI-generated connections
- **Iterative Refinement**: Continuous improvement through human feedback

### 💻 Collaborative Code Intelligence
- **Pair Programming with AI**: Developers and coder agents work together
- **Code Review Sessions**: Multiple reviewers and AI agents analyse code
- **Architecture Discussions**: System architects guide AI analysis
- **Knowledge Transfer**: Senior developers' expertise captured by AI

### 🔬 Research & Development Teams
- **Hypothesis Validation**: Scientists validate AI-generated hypotheses
- **Experimental Design**: Researchers and AI co-create experiments
- **Data Interpretation**: Collaborative analysis of complex results
- **Publication Support**: AI agents assist with literature review while experts write

---

## 🏗️ Cutting-Edge Architecture: Beyond ChatGPT Pulse

### 🎯 TransitionalGraphSupervisor Pattern (Unique to VisionFlow)

```mermaid
graph TB
    subgraph "Transitional Architecture - Bridge Pattern"
        TransitionalSupervisor[TransitionalGraphSupervisor<br/>Bridge Pattern Wrapper]
        GraphActor[GraphServiceActor<br/>35,193 lines - Being Refactored]

        subgraph "Extracted Actor Services"
            GraphStateActor[GraphStateActor<br/>State Management]
            PhysicsOrchestrator[PhysicsOrchestratorActor<br/>GPU Physics]
            SemanticProcessor[SemanticProcessorActor<br/>AI Analysis]
            ClientCoordinator[ClientCoordinatorActor<br/>WebSocket Management]
        end

        TransitionalSupervisor -->|Manages| GraphActor
        TransitionalSupervisor -->|Supervises| GraphStateActor
        TransitionalSupervisor -->|Orchestrates| PhysicsOrchestrator
        TransitionalSupervisor -->|Coordinates| SemanticProcessor
        TransitionalSupervisor -->|Routes| ClientCoordinator
    end

    style TransitionalSupervisor fill:#ff9800
    style GraphActor fill:#ffd54f
```

### 🚀 40 CUDA Kernels for GPU Acceleration

```mermaid
graph LR
    subgraph "GPU Computation Layer - 40 Production CUDA Kernels"
        subgraph "visionflow_unified.cu (28 kernels)"
            Physics[Force-Directed Layout<br/>Spring-Mass Physics]
            Clustering1[K-means++ Clustering<br/>Spectral Analysis]
            Anomaly1[Local Outlier Factor<br/>Statistical Z-score]
        end

        subgraph "gpu_clustering_kernels.cu (8 kernels)"
            Louvain[Louvain Modularity<br/>Community Detection]
            LabelProp[Label Propagation<br/>Graph Partitioning]
        end

        subgraph "Specialised Kernels (4)"
            Stability[Stability Gates<br/>2 kernels]
            SSSP[Shortest Path<br/>2 kernels]
        end
    end

    style Physics fill:#4caf50
    style Clustering1 fill:#2196f3
    style Anomaly1 fill:#ff5722
```

### 📡 Binary Protocol: 95% Bandwidth Reduction

```mermaid
graph TD
    subgraph "34-Byte Wire Protocol (Actual Implementation)"
        WireFormat["Wire Packet Structure<br/>34 bytes total"]

        subgraph "Packet Layout"
            NodeID["node_id: u16 (2 bytes)"]
            Position["position: [f32; 3] (12 bytes)"]
            Velocity["velocity: [f32; 3] (12 bytes)"]
            Distance["sssp_distance: f32 (4 bytes)"]
            Parent["sssp_parent: i32 (4 bytes)"]
        end

        WireFormat --> NodeID
        WireFormat --> Position
        WireFormat --> Velocity
        WireFormat --> Distance
        WireFormat --> Parent
    end

    Comparison["JSON: 680 bytes → Binary: 34 bytes<br/>95% reduction"]

    style WireFormat fill:#673ab7
    style Comparison fill:#4caf50
```

---

### 👥 Multi-User Voice-Enabled Collaboration Architecture

```mermaid
graph TB
    subgraph "Immersive Voice-Enabled Collaboration"
        subgraph "Human Experts with Independent Views"
            Expert1[Research Lead<br/>🎙️ Voice + Custom View]
            Expert2[Data Scientist<br/>🎙️ Voice + Analytics View]
            Expert3[Developer<br/>🎙️ Voice + Code View]
        end

        subgraph "Voice-Responsive AI Swarms"
            ResearchSwarm[Research Agents<br/>🔊 Voice Response]
            CoderSwarm[Coder Agents<br/>🔊 Code Narration]
            AnalystSwarm[Analyst Agents<br/>🔊 Data Insights]
        end

        subgraph "Shared Knowledge + Individual Lenses"
            KnowledgeGraph[Living Knowledge Graph<br/>Common Data Layer]

            subgraph "Personalised Views"
                View1[Research View<br/>Publications Focus]
                View2[Analytics View<br/>Metrics Focus]
                View3[Code View<br/>Architecture Focus]
            end

            HolographicVis[200x Holographic Sphere<br/>Synchronised Perspectives]
        end

        Expert1 <-->|🎙️ Voice Commands| ResearchSwarm
        Expert2 <-->|🎙️ Voice Queries| AnalystSwarm
        Expert3 <-->|🎙️ Voice Reviews| CoderSwarm

        KnowledgeGraph --> View1 & View2 & View3
        View1 --> Expert1
        View2 --> Expert2
        View3 --> Expert3
    end

    subgraph "Voice & Sync Infrastructure"
        VoiceSystem[Dual Voice System<br/>Legacy + Centralised]
        SpatialAudio[3D Spatial Audio<br/>Positioned Voices]
        WebSocketSync[Binary WebSocket<br/>34-byte packets]
        IndependentState[Per-User State<br/>Custom Settings]
    end

    VoiceSystem --> Expert1 & Expert2 & Expert3
    SpatialAudio --> HolographicVis
    WebSocketSync --> KnowledgeGraph

    style Expert1 fill:#4caf50
    style VoiceSystem fill:#ff5722
    style View1 fill:#e3f2fd
```

### 🔬 Unified Client Architecture with HolographicDataSphere

```mermaid
graph TB
    subgraph "React Three Fiber Visualisation Pipeline"
        subgraph "Core Rendering (60 FPS @ 100k nodes)"
            GraphCanvas["GraphCanvas.tsx<br/>R3F Main Canvas"]
            GraphManager["GraphManager<br/>Scene Orchestration"]
            HolographicDataSphere["HolographicDataSphere<br/>200x Scale Hologram System"]
        end

        subgraph "Binary WebSocket (34-byte protocol)"
            UnifiedApiClient["UnifiedApiClient<br/>31 References Across Codebase"]
            BinaryProtocol["Binary Protocol<br/>85% Bandwidth Reduction"]
            WebSocketService["WebSocket Service<br/><10ms Latency"]
        end

        subgraph "Multi-User Synchronisation"
            ClientCoordinator["ClientCoordinatorActor<br/>User Session Management"]
            PresenceSystem["Presence Tracking<br/>Real-time Locations"]
            CollaborationLayer["Collaboration Layer<br/>Shared State Sync"]
        end
    end

    style GraphCanvas fill:#e1f5fe
    style HolographicDataSphere fill:#fff3e0
    style CollaborationLayer fill:#c8e6c9
```

## 📊 Performance Metrics: Production-Ready at Scale

| Component | Specification | Performance |
|-----------|--------------|-------------|
| **GPU Kernels** | 40 CUDA kernels | 100x CPU speedup |
| **Binary Protocol** | 34-byte packets | 95% bandwidth saving |
| **Actor System** | 20 Actix actors | 1000+ req/min |
| **WebSocket Latency** | Binary streaming | <10ms updates |
| **3D Rendering** | Three.js + WebGL | 60 FPS @ 100k nodes |
| **Agent Swarms** | MCP orchestration | 50+ concurrent agents |
| **Memory Efficiency** | Per-node overhead | 34 bytes only |
| **Hologram Scale** | HolographicDataSphere | 200x visual scale |
| **API Architecture** | UnifiedApiClient | 31 optimised endpoints |

---

## 🎙️ Voice-to-Voice Architecture: Natural Human-AI Conversation

```mermaid
graph LR
    subgraph "Voice Input/Output Pipeline"
        subgraph "Human Voice Input"
            Mic[Microphone<br/>Voice Capture]
            STT[Speech-to-Text<br/>OpenAI Whisper]
            Intent[Intent Recognition<br/>Context Analysis]
        end

        subgraph "AI Voice Response"
            TTS[Text-to-Speech<br/>OpenAI/Kokoro]
            Spatial[3D Spatial Audio<br/>Positioned Output]
            Speaker[Voice Output<br/>Natural Response]
        end

        subgraph "Dual Voice System"
            Legacy[useVoiceInteraction<br/>196 lines - Active]
            Central[useVoiceInteractionCentralised<br/>856 lines - Available]
            Hooks[9 Specialised Hooks<br/>Domain-Specific]
        end
    end

    Mic --> STT --> Intent
    Intent --> Legacy & Central
    Legacy & Central --> TTS
    TTS --> Spatial --> Speaker

    style Mic fill:#4caf50
    style TTS fill:#2196f3
    style Central fill:#ff9800
```

## 🛠️ Technology Stack: State-of-the-Art Implementation

### 🧠 Intelligence Layer - Advanced AI Orchestration
```
┌─────────────────────────────────────────────────────────────┐
│ MCP Protocol Stack (Model Context Protocol)                 │
├─────────────────────────────────────────────────────────────┤
│ • ClaudeFlowActor → TcpConnectionActor → MCP Server :9500  │
│ • DockerHiveMind: Container orchestration (948 lines)      │
│ • JsonRpcClient: Protocol correlation layer                │
│ • 50+ concurrent agent swarms with unique routing          │
└─────────────────────────────────────────────────────────────┘
```

### ⚡ GPU Acceleration - 40 Production CUDA Kernels
```
┌─────────────────────────────────────────────────────────────┐
│ CUDA Kernel Distribution                                    │
├─────────────────────────────────────────────────────────────┤
│ • visionflow_unified.cu: 28 kernels (physics, clustering)  │
│ • gpu_clustering_kernels.cu: 8 kernels (Louvain, K-means) │
│ • visionflow_unified_stability.cu: 2 stability kernels     │
│ • sssp_compact.cu: 2 shortest path kernels                │
│ • Dynamic grid sizing, shared memory optimisation          │
└─────────────────────────────────────────────────────────────┘
```

### 🎨 Visualisation Layer - React Three Fiber Pipeline
```
┌─────────────────────────────────────────────────────────────┐
│ 3D Rendering Architecture                                   │
├─────────────────────────────────────────────────────────────┤
│ • HolographicDataSphere: 200x scale hologram system        │
│ • SelectiveBloom: Layer-based post-processing              │
│ • Binary WebSocket: 34-byte protocol, <10ms latency        │
│ • UnifiedApiClient: 31 references, centralised HTTP        │
└─────────────────────────────────────────────────────────────┘
```

### 🏗️ Infrastructure - Production-Grade Architecture
```
┌─────────────────────────────────────────────────────────────┐
│ Actor System Architecture (Rust + Actix)                    │
├─────────────────────────────────────────────────────────────┤
│ • TransitionalGraphSupervisor: Bridge pattern wrapper      │
│ • 20 specialised actors with supervision strategies        │
│ • GraphServiceActor: 35,193 lines (being refactored)      │
│ • Binary protocol: 95% bandwidth reduction vs JSON         │
└─────────────────────────────────────────────────────────────┘
```

---

## 🚀 Getting Started: Deploy Your Research Assistant

### Prerequisites
- Docker 20.10+ with Docker Compose
- 8GB RAM (16GB recommended)
- NVIDIA GPU (optional, for acceleration)

### Quick Installation

1. **Clone Repository**
   ```bash
   git clone https://github.com/your-org/VisionsFlow
   cd VisionsFlow
   ```

2. **Configure Your Knowledge Sources**
   ```bash
   cp .env.example .env
   # Add your data sources, API keys, and agent configuration
   ```

3. **Deploy Your Research Universe**
   ```bash
   docker-compose up -d
   ```

4. **Access Your Intelligence Dashboard**
   - Research Interface: http://localhost:3001
   - Agent Monitor: http://localhost:3001/agents
   - API Docs: http://localhost:3001/api/docs

---

## 👁️ Independent Specialist Views: Your Data, Your Perspective

### Synchronous Yet Independent
Each team member experiences the same data through their own specialist lens:

```
┌──────────────────────────────────────────────────────────────────┐
│ Data Scientist View          │ Developer View                    │
├──────────────────────────────┼───────────────────────────────────┤
│ • Statistical overlays        │ • Code structure graphs          │
│ • Correlation matrices        │ • Dependency trees                │
│ • Anomaly highlights         │ • Performance hotspots            │
│ • Predictive models          │ • Architecture diagrams           │
│                              │                                   │
│ Research Analyst View        │ Domain Expert View                │
├──────────────────────────────┼───────────────────────────────────┤
│ • Citation networks          │ • Business process flows         │
│ • Literature connections     │ • Risk assessments                │
│ • Hypothesis tracking        │ • Compliance mappings             │
│ • Evidence trails            │ • Strategic insights              │
└──────────────────────────────────────────────────────────────────┘
          All views share the same underlying knowledge graph
               while maintaining individual perspectives
```

## 🔮 Roadmap: Evolution of Collaborative Intelligence

### Current Capabilities (v2.0)
- ✅ Voice-to-voice AI interaction with TTS/STT
- ✅ Multi-user synchronous collaboration
- ✅ Independent specialist views per user
- ✅ Multi-agent orchestration with 50+ concurrent agents
- ✅ Real-time 3D visualisation of knowledge graphs
- ✅ GPU-accelerated processing and rendering
- ✅ Binary protocol for efficient communication
- ✅ MCP integration for tool extensibility

### Coming Soon
- 🔄 **Enhanced Voice Features**: Multi-language support, voice cloning
- 🔄 **AR Collaboration**: Quest 3 shared workspace with hand tracking
- 🔄 **Scheduled Insights**: Pulse-style daily summaries option
- 🔄 **Mobile Companion**: iOS/Android apps with voice control
- 🔄 **Plugin Marketplace**: Community-built specialist views

### Future Vision
- 🔮 **Predictive Intelligence**: Anticipate information needs
- 🔮 **Cross-Corpus Learning**: Insights from multiple organisations
- 🔮 **AR Knowledge Space**: Spatial computing interface
- 🔮 **Autonomous Workflows**: Agents that take action on insights

---

## 📚 Documentation

Our comprehensive documentation covers everything from basic setup to advanced agent orchestration:

- **[Getting Started](docs/getting-started/00-index.md)** - Installation and first steps
- **[Agent Orchestration](docs/guides/04-orchestrating-agents.md)** - Configure your research swarms
- **[System Architecture](docs/concepts/01-system-overview.md)** - Technical deep-dive
- **[API Reference](docs/reference/api/)** - Integration documentation

---

## 🤝 Community & Support

- **GitHub Issues**: [Report bugs or request features](https://github.com/your-org/VisionsFlow/issues)
- **Discord**: [Join our community](https://discord.gg/ar-ai-kg)
- **Documentation**: [Full Documentation Hub](docs/)

---

## 🙏 Acknowledgements

- **Prof Rob Aspin**: For inspiring unified knowledge visualisation
- **Anthropic**: For the Model Context Protocol enabling agent orchestration
- **OpenAI**: For ChatGPT Pulse inspiration and GPT models
- **Open Source Community**: For the incredible tools that power VisionFlow

---

## 📄 Licence

Mozilla Public License 2.0 - See [LICENSE](LICENSE) for details.

---

**VisionFlow: Where ChatGPT Pulse meets private knowledge sovereignty** 🚀

*Continuous research. Real-time insights. Your data, your control.*