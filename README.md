<div align="center">

# ğŸŒŒ VisionFlow

[![License](https://img.shields.io/badge/License-Mozilla%202.0-blue.svg)](LICENSE)
[![CUDA](https://img.shields.io/badge/CUDA-40%20Kernels-green.svg)](docs/)
[![Agents](https://img.shields.io/badge/AI%20Agents-50%2B%20Concurrent-orange.svg)](docs/)
[![Performance](https://img.shields.io/badge/Performance-60%20FPS%20@%20100k%20nodes-red.svg)](docs/)

</div>

<div align="center">
  <table>
    <tr>
      <td align="center">
        <strong>ğŸ‘¥ Collaborative Team AI</strong><br/>
        <sub>Action on 100k + complex nodes</sub>
      </td>
      <td align="center">
        <strong>ğŸ¤– Continuous Background Agents</strong><br/>
        <sub>Agent Intelligence â€¢ Dip In Dip Out</sub>
      </td>
      <td align="center">
        <strong>ğŸ” Private & Self-Sovereign</strong><br/>
        <sub>Multi-Modal Immersive Interface</sub>
      </td>
    </tr>
    <tr>
      <td align="center">
        <strong>ğŸ“Š Massive Scale</strong><br/>
        <sub>Dozens of Users â€¢ Hundreds of Agents</sub>
      </td>
      <td align="center">
        <strong>ğŸ›¡ï¸ Enterprise Security</strong><br/>
        <sub>Thin Client â€¢ Secure Server â€¢ W3C DID</sub>
      </td>
      <td align="center">
        <strong>ğŸ™ï¸ Voice-First & Human First</strong><br/>
        <sub>Natural Human-AI Intreractions</sub>
      </td>
    </tr>
  </table>
</div>

---

<div align="center">
  <table>
    <tr>
      <td><img src="./visionflow.gif" alt="VisionFlow Visualisation" style="width:100%; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);"></td>
      <td><img src="./jarvisSept.gif" alt="Runtime Screenshot" style="width:100%; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);"></td>
    </tr>
  </table>
</div>

<br/>

<div align="center">
  <h3>ğŸš€ Like ChatGPT Pulse for Your Private Knowledge</h3>
  <p><strong>VisionFlow deploys self-sovereign AI agents that continuously research, analyse, and surface insights<br/>from your data corpus - all visualised for collaborative teams, in stunning real-time 3D.</strong></p>

  <sub>Inspired by the innovative work of <a href="https://github.com/trebornipsa">Prof. Rob Aspin</a></sub>
</div>

---

<div align="center">
  <table>
    <tr>
      <th>ğŸ† Enterprise Value</th>
      <th>âš¡ Performance</th>
      <th>ğŸ”§ Technology</th>
      <th>ğŸ‘¥ Collaboration</th>
    </tr>
    <tr>
      <td align="center">
        <strong>Completely Open Source</strong><br/>
        <sub>Support Contracts on Request</sub>
      </td>
      <td align="center">
        <strong>60 FPS</strong><br/>
        <sub>@ 100k+ nodes</sub>
      </td>
      <td align="center">
        <strong>40 CUDA</strong><br/>
        <sub>GPU Kernels</sub>
      </td>
      <td align="center">
        <strong>50+ Agents</strong><br/>
        <sub>Concurrent Swarms</sub>
      </td>
    </tr>
    <tr>
      <td align="center">
        <strong>Cost Savings though Flow</strong><br/>
        <sub>Based on Decades of Immersive Research</sub>
      </td>
      <td align="center">
        <strong><10ms</strong><br/>
        <sub>WebSocket Latency</sub>
      </td>
      <td align="center">
        <strong>34-byte</strong><br/>
        <sub>Binary Protocol</sub>
      </td>
      <td align="center">
        <strong>Real-time</strong><br/>
        <sub>Voice-to-Voice AI</sub>
      </td>
    </tr>
  </table>
</div>

---

<div align="center">
  <table>
    <tr>
      <td><img src="./groupOctave.jpg" alt="Group In Octave" style="width:100%; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);"></td>
      <td><img src="./ChloeOctave.jpg" alt="Chloe In Octave" style="width:100%; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);"></td>
    </tr>
  </table>
</div>

---

### ğŸ“ Powerful Markdown Based Data Management with Logseq

VisionFlow integrates seamlessly with [Logseq](https://logseq.com/), a privacy-first, open-source platform for knowledge management. Logseq enables you to maintain your research data in plain Markdown files while providing advanced features like:

- **Block-based organization** for structured knowledge capture
- **Bidirectional linking** to connect related concepts
- **Knowledge graphs** to visualize relationships in your data
- **Local storage** ensuring complete data ownership and privacy
- **Plugin ecosystem** for extending functionality
- **Multi Platform & Mobile** any of your users or team, anywhere

<div align="center">
  <table>
    <tr>
      <td><img src="./logseq1.png" alt="Logseq View 1" style="width:100%; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);"></td>
      <td><img src="./logseq2.png" alt="Logseq View 2" style="width:100%; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);"></td>
      <td><img src="./logseq3.png" alt="Logseq View 3" style="width:100%; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);"></td>
      <td><img src="./logseq4.png" alt="Logseq View 4" style="width:100%; border-radius:10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);"></td>
    </tr>
  </table>
</div>


# Private Knowledge with Powerful Data Analysis

VisionFlow combines cutting-edge graph analysis techniques with version-controlled data management to deliver unparalleled insights from your private knowledge corpus.

### ğŸ” Advanced Knowledge Graph Architecture

- **Microsoft GraphRAG Integration**: Unlike traditional RAG systems that rely on simple vector similarity, VisionFlow leverages [Microsoft's GraphRAG](https://github.com/microsoft/graphrag) to build hierarchical knowledge structures. This creates subject-object-predicate relationships (e.g., "Project Alpha - requires - Machine Learning") that capture the deep semantic meaning within your data.

- **Leiden Clustering Algorithm**: Automatically organises your knowledge into well-connected communities using the state-of-the-art Leiden algorithm. This ensures:
  - **Hierarchical Organisation**: Knowledge is structured from high-level domains down to specific details
  - **Implicit Connection Discovery**: Reveals hidden relationships between disparate pieces of information
  - **Scalable Analysis**: Efficiently handles massive knowledge bases with hundreds of thousands of nodes

- **Cutting Edge Shortest Path Analysis**: Cutting edge ([new research](https://arxiv.org/abs/2504.17033)) shortest path analysis enables multi-hop reasoning across your knowledge graph to:
  - Connect concepts that aren't directly linked
  - Visually trace the flow of information through your organisation
  - Discover optimal knowledge paths for complex queries


# ğŸ”„ Continuous Background Intelligence

### ğŸ¤– Intelligent Agent Orchestration

VisionFlow deploys specialised AI agents that work continuously in the background:

- **Researcher Agents**: Deep-dive into specific topics using GraphRAG's local search capabilities
- **Analyst Agents**: Identify patterns and correlations using clustering algorithms
- **Coder Agents**: Parse and understand code relationships, documentation, and dependencies
- **Reviewer Agents**: Validate findings and cross-reference information across the knowledge graph
- **Planner Agents**: Coordinate research strategies using global search for holistic understanding

- **Autonomous Research**: Unlike ChatGPT Pulse's daily summaries, VisionFlow agents continuously analyse your private corpus in real-time, using GraphRAG's dual query modes (local for detail, global for synthesis)

- **Proactive Discovery**: Surface new connections through:
  - Community detection that groups related concepts
  - Path analysis that reveals indirect relationships
  - Hierarchical summarisation that provides context at every level

- **Living Knowledge Graph with Git Integration**:
  - Your data evolves in real-time as agents discover relationships
  - All changes are tracked through Git version control
  - Updates are submitted as **merge requests** for human oversight
  - Complete transparency with detailed commit history
  - Rollback capabilities for any knowledge state

### ğŸ” Enterprise-Grade Data Governance
- Your data is stored in (private) **Git-tracked structures** with comprehensive version history, enabling:

- **Version Control**: Every knowledge update is tracked, attributed, and reversible
- **Audit Trail**: Complete history of how your knowledge graph evolved
- **Human-in-the-Loop**: AI discoveries require human approval via merge requests
- **Data Sovereignty**: All processing happens on your infrastructure
- **Time Travel**: Wind back and forth through the history of your data visually in the immersive graph

### ğŸš€ Multi-User Human-AI Collaboration Universe

VisionFlow creates an **immersive collaborative space** where human experts and AI agents work together:

- **ğŸ‘¥ Multi-User Collaboration**: Multiple human experts collaborate with AI agents in real-time
- **ğŸ¤ Human-AI Symbiosis**: Agents learn from human expertise while augmenting human capabilities
- **ğŸ” Private & Secure**: Your data never leaves your infrastructure
- **ğŸ‘ï¸ Immersive 3D Workspace**: Watch humans and AI agents collaborate in shared virtual space
- **ğŸŒ Unlimited Corpus**: Connect GitHub repos, documents, databases, APIs
- **âš¡ Real-time Synchronisation**: All participants see updates instantly
- **ğŸ® Interactive Control**: Humans direct agent swarms while agents suggest new research directions


# VisionFlow Vs ChatGPT Pulse

| ChatGPT Pulse | VisionFlow |
|---------------|------------|
| Asynchronous daily research | Continuous real-time agent research |
| Surfaces insights from past chats | Discovers patterns in your knowledge corpus |
| Visual summaries you can expand | 3D visualisation you can explore |
| Proactive morning delivery | Real-time insight streaming |
| Based on your memories & history | Based on your private data & documents |
| Curated topics from interactions | Agent-discovered knowledge connections |



---

## ğŸš€ Quick Start

```bash
# Clone and deploy your private research assistant
git clone https://github.com/your-org/VisionsFlow
cd VisionsFlow

# Configure environment
cp .env.example .env

# Deploy with Docker
docker-compose up -d

# Access your AI research universe
open http://localhost:3001
```

**[ğŸ“š Full Documentation](docs/)** | **[ğŸ¯ Quick Start Guide](docs/getting-started/02-quick-start.md)** | **[ğŸ”§ Installation](docs/getting-started/01-installation.md)**

---

## âœ¨ Core Capabilities: Human-AI Collaborative Intelligence

### ğŸ¤ Immersive Multi-User Collaboration Platform
VisionFlow enables **team-based human-AI research with voice**:

- **ğŸ™ï¸ Voice-to-Voice AI Interaction**: Natural conversation with AI agents using advanced TTS/STT
- **ğŸ‘ï¸ Independent Specialist Views**: Each user maintains their own perspective while staying synchronised
- **ğŸŒ Shared Virtual Workspace**: Multiple experts and AI agents in the same 3D environment
- **ğŸ“Š Personalised Data Lenses**: Individual filtering and visualisation preferences per user
- **ğŸ”Š Spatial Audio**: Voice communication positioned in 3D space for natural collaboration
- **Real-time Presence**: See where team members and agents are focusing
- **Collaborative Discovery**: Humans guide agents through voice while agents respond verbally
- **Knowledge Handoffs**: Seamless transfer of findings between humans and AI
- **Expertise Amplification**: AI agents learn from human domain knowledge through conversation

### ğŸ¤– Autonomous Agent Research System
Just like ChatGPT Pulse researches on your behalf, VisionFlow deploys intelligent agents that:

- **Background Processing**: Agents work 24/7 analysing your data
- **Human-Guided Learning**: Agents adapt based on expert feedback
- **Pattern Recognition**: Automatically identify trends and anomalies
- **Collaborative Discovery**: Surface findings to human experts for validation
- **Knowledge Evolution**: Continuously update understanding through human-AI interaction

### ğŸ“Š Visual Intelligence Dashboard
VisionFlow provides:

- **3D Knowledge Universe**: Navigate your data like a galaxy of interconnected insights
- **Real-time Agent Visualisation**: Watch AI agents collaborate and discover
- **GPU-Accelerated Rendering**: Smooth interaction with 100,000+ data points
- **Interactive Exploration**: Dive deep into any insight cluster
- **Time-Travel Analysis**: Replay how knowledge evolved over time

### ğŸ§  Multi-Agent Orchestration
VisionFlow orchestrates specialised swarms:

- **Researcher Agents**: Deep-dive into specific topics
- **Analyst Agents**: Identify patterns and correlations
- **Coder Agents**: Understand and document code relationships
- **Reviewer Agents**: Validate and cross-reference findings
- **Planner Agents**: Coordinate research strategies

---

## ğŸ¯ Use Cases: Collaborative Human-AI Research Teams

### ğŸ™ï¸ Voice-First Collaborative Research
- **Natural Voice Conversations**: Speak directly to AI agents, receive voice responses
- **Hands-Free Operation**: Control entire system through voice commands
- **Multi-Language Support**: Teams collaborate in their preferred languages
- **Voice Transcription**: Automatic documentation of verbal insights
- **Audio Spatial Awareness**: Voices positioned in 3D space matching visual presence

### ğŸ‘ï¸ Independent Specialist Views with Team Sync
- **Personalised Perspectives**: Each expert sees data through their specialist lens
- **Synchronous Exploration**: Individual views while maintaining team awareness
- **Custom Visualisation Filters**: Data scientists see metrics, developers see code structure
- **Focus Independence**: Zoom into details without affecting teammates' views
- **Instant View Sharing**: "Show my view" voice command for knowledge transfer

### ğŸ‘¥ Team-Based Knowledge Discovery
- **Collaborative Research Sessions**: Multiple experts work with AI agents simultaneously
- **Voice-Guided AI Training**: Train agents through natural conversation
- **Real-time Insight Sharing**: Discoveries instantly visible with voice announcements
- **Focus Coordination**: See and hear where colleagues and agents are investigating
- **Expertise Routing**: AI agents route findings to the right expert with voice alerts

### ğŸ¢ Enterprise Collaboration Scenarios
- **Cross-Functional Teams**: Data scientists, developers, and domain experts in one space
- **Global Collaboration**: Teams across time zones with persistent AI agents
- **Knowledge Handoffs**: Agents continue research when humans go offline
- **Audit Trails**: Complete history of human decisions and AI discoveries
- **Training & Onboarding**: New team members learn from AI-captured expertise

### ğŸ“š Knowledge Management with Human Oversight
- **Human-Validated Insights**: AI discoveries reviewed by domain experts
- **Guided Exploration**: Experts direct agent focus areas
- **Quality Control**: Human verification of AI-generated connections
- **Iterative Refinement**: Continuous improvement through human feedback

### ğŸ’» Collaborative Code Intelligence
- **Pair Programming with AI**: Developers and coder agents work together
- **Code Review Sessions**: Multiple reviewers and AI agents analyse code
- **Architecture Discussions**: System architects guide AI analysis
- **Knowledge Transfer**: Senior developers' expertise captured by AI

### ğŸ”¬ Research & Development Teams
- **Hypothesis Validation**: Scientists validate AI-generated hypotheses
- **Experimental Design**: Researchers and AI co-create experiments
- **Data Interpretation**: Collaborative analysis of complex results
- **Publication Support**: AI agents assist with literature review while experts write

---

## ğŸ—ï¸ Cutting-Edge Architecture:

### ğŸ¯ TransitionalGraphSupervisor Pattern (Unique to VisionFlow)

```mermaid
graph TB
    subgraph "Transitional Architecture - Bridge Pattern"
        TransitionalSupervisor[TransitionalGraphSupervisor<br/>Bridge Pattern Wrapper]
        GraphActor[GraphServiceActor<br/>35,193 lines - Being Refactored]

        subgraph "Extracted Actor Services"
            GraphStateActor[GraphStateActor<br/>State Management]
            PhysicsOrchestrator[PhysicsOrchestratorActor<br/>GPU Physics]
            SemanticProcessor[SemanticProcessorActor<br/>AI Analysis]
            ClientCoordinator[ClientCoordinatorActor<br/>WebSocket Management]
        end

        TransitionalSupervisor -->|Manages| GraphActor
        TransitionalSupervisor -->|Supervises| GraphStateActor
        TransitionalSupervisor -->|Orchestrates| PhysicsOrchestrator
        TransitionalSupervisor -->|Coordinates| SemanticProcessor
        TransitionalSupervisor -->|Routes| ClientCoordinator
    end

    style TransitionalSupervisor fill:#ff9800
    style GraphActor fill:#ffd54f
```

### ğŸš€ 40 CUDA Kernels for GPU Acceleration

```mermaid
graph LR
    subgraph "GPU Computation Layer - 40 Production CUDA Kernels"
        subgraph "visionflow_unified.cu (28 kernels)"
            Physics[Force-Directed Layout<br/>Spring-Mass Physics]
            Clustering1[K-means++ Clustering<br/>Spectral Analysis]
            Anomaly1[Local Outlier Factor<br/>Statistical Z-score]
        end

        subgraph "gpu_clustering_kernels.cu (8 kernels)"
            Louvain[Louvain Modularity<br/>Community Detection]
            LabelProp[Label Propagation<br/>Graph Partitioning]
        end

        subgraph "Specialised Kernels (4)"
            Stability[Stability Gates<br/>2 kernels]
            SSSP[Shortest Path<br/>2 kernels]
        end
    end

    style Physics fill:#4caf50
    style Clustering1 fill:#2196f3
    style Anomaly1 fill:#ff5722
```

### ğŸ“¡ Binary Protocol: 95% Bandwidth Reduction

```mermaid
graph TD
    subgraph "34-Byte Wire Protocol (Actual Implementation)"
        WireFormat["Wire Packet Structure<br/>34 bytes total"]

        subgraph "Packet Layout"
            NodeID["node_id: u16 (2 bytes)"]
            Position["position: [f32; 3] (12 bytes)"]
            Velocity["velocity: [f32; 3] (12 bytes)"]
            Distance["sssp_distance: f32 (4 bytes)"]
            Parent["sssp_parent: i32 (4 bytes)"]
        end

        WireFormat --> NodeID
        WireFormat --> Position
        WireFormat --> Velocity
        WireFormat --> Distance
        WireFormat --> Parent
    end

    Comparison["JSON: 680 bytes â†’ Binary: 34 bytes<br/>95% reduction"]

    style WireFormat fill:#673ab7
    style Comparison fill:#4caf50
```

---

### ğŸ‘¥ Multi-User Voice-Enabled Collaboration Architecture

```mermaid
graph TB
    subgraph "Immersive Voice-Enabled Collaboration"
        subgraph "Human Experts with Independent Views"
            Expert1[Research Lead<br/>ğŸ™ï¸ Voice + Custom View]
            Expert2[Data Scientist<br/>ğŸ™ï¸ Voice + Analytics View]
            Expert3[Developer<br/>ğŸ™ï¸ Voice + Code View]
        end

        subgraph "Voice-Responsive AI Swarms"
            ResearchSwarm[Research Agents<br/>ğŸ”Š Voice Response]
            CoderSwarm[Coder Agents<br/>ğŸ”Š Code Narration]
            AnalystSwarm[Analyst Agents<br/>ğŸ”Š Data Insights]
        end

        subgraph "Shared Knowledge + Individual Lenses"
            KnowledgeGraph[Living Knowledge Graph<br/>Common Data Layer]

            subgraph "Personalised Views"
                View1[Research View<br/>Publications Focus]
                View2[Analytics View<br/>Metrics Focus]
                View3[Code View<br/>Architecture Focus]
            end

            HolographicVis[200x Holographic Sphere<br/>Synchronised Perspectives]
        end

        Expert1 <-->|ğŸ™ï¸ Voice Commands| ResearchSwarm
        Expert2 <-->|ğŸ™ï¸ Voice Queries| AnalystSwarm
        Expert3 <-->|ğŸ™ï¸ Voice Reviews| CoderSwarm

        KnowledgeGraph --> View1 & View2 & View3
        View1 --> Expert1
        View2 --> Expert2
        View3 --> Expert3
    end

    subgraph "Voice & Sync Infrastructure"
        VoiceSystem[Dual Voice System<br/>Legacy + Centralised]
        SpatialAudio[3D Spatial Audio<br/>Positioned Voices]
        WebSocketSync[Binary WebSocket<br/>34-byte packets]
        IndependentState[Per-User State<br/>Custom Settings]
    end

    VoiceSystem --> Expert1 & Expert2 & Expert3
    SpatialAudio --> HolographicVis
    WebSocketSync --> KnowledgeGraph

    style Expert1 fill:#4caf50
    style VoiceSystem fill:#ff5722
    style View1 fill:#e3f2fd
```

### ğŸ”¬ Unified Client Architecture with HolographicDataSphere

```mermaid
graph TB
    subgraph "React Three Fiber Visualisation Pipeline"
        subgraph "Core Rendering (60 FPS @ 100k nodes)"
            GraphCanvas["GraphCanvas.tsx<br/>R3F Main Canvas"]
            GraphManager["GraphManager<br/>Scene Orchestration"]
            HolographicDataSphere["HolographicDataSphere<br/>200x Scale Hologram System"]
        end

        subgraph "Binary WebSocket (34-byte protocol)"
            UnifiedApiClient["UnifiedApiClient<br/>31 References Across Codebase"]
            BinaryProtocol["Binary Protocol<br/>85% Bandwidth Reduction"]
            WebSocketService["WebSocket Service<br/><10ms Latency"]
        end

        subgraph "Multi-User Synchronisation"
            ClientCoordinator["ClientCoordinatorActor<br/>User Session Management"]
            PresenceSystem["Presence Tracking<br/>Real-time Locations"]
            CollaborationLayer["Collaboration Layer<br/>Shared State Sync"]
        end
    end

    style GraphCanvas fill:#e1f5fe
    style HolographicDataSphere fill:#fff3e0
    style CollaborationLayer fill:#c8e6c9
```

## ğŸ“Š Performance Metrics: Production-Ready at Scale

| Component | Specification | Performance |
|-----------|--------------|-------------|
| **GPU Kernels** | 40 CUDA kernels | 100x CPU speedup |
| **Binary Protocol** | 34-byte packets | 95% bandwidth saving |
| **Actor System** | 20 Actix actors | 1000+ req/min |
| **WebSocket Latency** | Binary streaming | <10ms updates |
| **3D Rendering** | Three.js + WebGL | 60 FPS @ 100k nodes |
| **Agent Swarms** | MCP orchestration | 50+ concurrent agents |
| **Memory Efficiency** | Per-node overhead | 34 bytes only |
| **Hologram Scale** | HolographicDataSphere | 200x visual scale |
| **API Architecture** | UnifiedApiClient | 31 optimised endpoints |

---

## ğŸ™ï¸ Voice-to-Voice Architecture: Natural Human-AI Conversation

```mermaid
graph LR
    subgraph "Voice Input/Output Pipeline"
        subgraph "Human Voice Input"
            Mic[Microphone<br/>Voice Capture]
            STT[Speech-to-Text<br/>OpenAI Whisper]
            Intent[Intent Recognition<br/>Context Analysis]
        end

        subgraph "AI Voice Response"
            TTS[Text-to-Speech<br/>OpenAI/Kokoro]
            Spatial[3D Spatial Audio<br/>Positioned Output]
            Speaker[Voice Output<br/>Natural Response]
        end

        subgraph "Dual Voice System"
            Legacy[useVoiceInteraction<br/>196 lines - Active]
            Central[useVoiceInteractionCentralised<br/>856 lines - Available]
            Hooks[9 Specialised Hooks<br/>Domain-Specific]
        end
    end

    Mic --> STT --> Intent
    Intent --> Legacy & Central
    Legacy & Central --> TTS
    TTS --> Spatial --> Speaker

    style Mic fill:#4caf50
    style TTS fill:#2196f3
    style Central fill:#ff9800
```

## ğŸ› ï¸ Technology Stack: State-of-the-Art Implementation

### ğŸ§  Intelligence Layer - Advanced AI Orchestration
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Protocol Stack (Model Context Protocol)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ ClaudeFlowActor â†’ TcpConnectionActor â†’ MCP Server :9500  â”‚
â”‚ â€¢ DockerHiveMind: Container orchestration (948 lines)      â”‚
â”‚ â€¢ JsonRpcClient: Protocol correlation layer                â”‚
â”‚ â€¢ 50+ concurrent agent swarms with unique routing          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš¡ GPU Acceleration - 40 Production CUDA Kernels
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CUDA Kernel Distribution                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ visionflow_unified.cu: 28 kernels (physics, clustering)  â”‚
â”‚ â€¢ gpu_clustering_kernels.cu: 8 kernels (Louvain, K-means) â”‚
â”‚ â€¢ visionflow_unified_stability.cu: 2 stability kernels     â”‚
â”‚ â€¢ sssp_compact.cu: 2 shortest path kernels                â”‚
â”‚ â€¢ Dynamic grid sizing, shared memory optimisation          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¨ Visualisation Layer - React Three Fiber Pipeline
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3D Rendering Architecture                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ HolographicDataSphere: 200x scale hologram system        â”‚
â”‚ â€¢ SelectiveBloom: Layer-based post-processing              â”‚
â”‚ â€¢ Binary WebSocket: 34-byte protocol, <10ms latency        â”‚
â”‚ â€¢ UnifiedApiClient: 31 references, centralised HTTP        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ—ï¸ Infrastructure - Production-Grade Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Actor System Architecture (Rust + Actix)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ TransitionalGraphSupervisor: Bridge pattern wrapper      â”‚
â”‚ â€¢ 20 specialised actors with supervision strategies        â”‚
â”‚ â€¢ GraphServiceActor: 35,193 lines (being refactored)      â”‚
â”‚ â€¢ Binary protocol: 95% bandwidth reduction vs JSON         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Getting Started: Deploy Your Research Assistant

### Prerequisites
- Docker 20.10+ with Docker Compose
- 8GB RAM (16GB recommended)
- NVIDIA GPU (optional, for acceleration)

### Quick Installation

1. **Clone Repository**
   ```bash
   git clone https://github.com/your-org/VisionsFlow
   cd VisionsFlow
   ```

2. **Configure Your Knowledge Sources**
   ```bash
   cp .env.example .env
   # Add your data sources, API keys, and agent configuration
   ```

3. **Deploy Your Research Universe**
   ```bash
   docker-compose up -d
   ```

4. **Access Your Intelligence Dashboard**
   - Research Interface: http://localhost:3001
   - Agent Monitor: http://localhost:3001/agents
   - API Docs: http://localhost:3001/api/docs

---

## ğŸ‘ï¸ Independent Specialist Views: Your Data, Your Perspective

### Synchronous Yet Independent
Each team member experiences the same data through their own specialist lens:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Scientist View          â”‚ Developer View                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Statistical overlays        â”‚ â€¢ Code structure graphs          â”‚
â”‚ â€¢ Correlation matrices        â”‚ â€¢ Dependency trees                â”‚
â”‚ â€¢ Anomaly highlights         â”‚ â€¢ Performance hotspots            â”‚
â”‚ â€¢ Predictive models          â”‚ â€¢ Architecture diagrams           â”‚
â”‚                              â”‚                                   â”‚
â”‚ Research Analyst View        â”‚ Domain Expert View                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Citation networks          â”‚ â€¢ Business process flows         â”‚
â”‚ â€¢ Literature connections     â”‚ â€¢ Risk assessments                â”‚
â”‚ â€¢ Hypothesis tracking        â”‚ â€¢ Compliance mappings             â”‚
â”‚ â€¢ Evidence trails            â”‚ â€¢ Strategic insights              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          All views share the same underlying knowledge graph
               while maintaining individual perspectives
```

## ğŸ”® Roadmap: Evolution of Collaborative Intelligence

### Current Capabilities (v2.0)
- âœ… Voice-to-voice AI interaction with TTS/STT
- âœ… Multi-user synchronous collaboration
- âœ… Independent specialist views per user
- âœ… Multi-agent orchestration with 50+ concurrent agents
- âœ… Real-time 3D visualisation of knowledge graphs
- âœ… GPU-accelerated processing and rendering
- âœ… Binary protocol for efficient communication
- âœ… MCP integration for tool extensibility

### Coming Soon
- ğŸ”„ **AR Knowledge Space**: Spatial computing interface
- ğŸ”„ **Enhanced Voice Features**: Multi-language support, voice cloning
- ğŸ”„ **AR Collaboration**: Quest 3 shared workspace with hand tracking
- ğŸ”„ **Scheduled Insights**: Daily summaries option
- ğŸ”„ **Email Integration**: Search and integrate across huge legacy email stores
- ğŸ”„ **Mobile Companion**: iOS/Android apps with voice control
- ğŸ”„ **Plugin Marketplace**: Community-built specialist views

### Future Vision
- ğŸ”® **Predictive Intelligence**: Anticipate information needs
- ğŸ”® **Cross-Corpus Learning**: Insights from multiple organisations
- ğŸ”® **Autonomous Workflows**: Agents that take action on insights

---

## ğŸ“š Documentation

Our comprehensive documentation covers everything from basic setup to advanced agent orchestration:

- **[Getting Started](docs/getting-started/00-index.md)** - Installation and first steps
- **[Agent Orchestration](docs/guides/04-orchestrating-agents.md)** - Configure your research swarms
- **[System Architecture](docs/concepts/01-system-overview.md)** - Technical deep-dive
- **[API Reference](docs/reference/api/)** - Integration documentation

---

## ğŸ¤ Community & Support

- **GitHub Issues**: [Report bugs or request features](https://github.com/your-org/VisionsFlow/issues)
- **Discord**: [Join our community](https://discord.gg/ar-ai-kg)
- **Documentation**: [Full Documentation Hub](docs/)

---

## ğŸ™ Acknowledgements

- **Prof Rob Aspin**: For inspiring unified knowledge visualisation
- **Anthropic**: For the Model Context Protocol enabling agent orchestration
- **OpenAI**: For ChatGPT Pulse inspiration and GPT models
- **Open Source Community**: For the incredible tools that power VisionFlow

---

## ğŸ“„ Licence

Mozilla Public License 2.0 - See [LICENSE](LICENSE) for details.

---

*Continuous research. Real-time insights. Your data, your control.*