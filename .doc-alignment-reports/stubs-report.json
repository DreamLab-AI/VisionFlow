{
  "files_scanned": 3860,
  "total_markers": 458,
  "by_type": {
    "NOTE": 132,
    "STUB": 3,
    "TODO": 146,
    "PLACEHOLDER": 101,
    "BUG": 4,
    "TEMP": 26,
    "FIXME": 5,
    "HACK": 19,
    "IDEA": 12,
    "REVIEW": 9,
    "XXX": 1
  },
  "by_severity": {
    "info": 255,
    "warning": 193,
    "error": 10
  },
  "top_files": [
    [
      "tests/neo4j_settings_integration_tests.rs",
      29
    ],
    [
      "data/pages/house.md",
      14
    ],
    [
      "tests/api/reasoning_api_tests.rs",
      9
    ],
    [
      "tests/ontology_api_test.rs",
      8
    ],
    [
      "docs/reference/semantic-physics-implementation.md",
      8
    ],
    [
      "docs/guides/ontology-reasoning-integration.md",
      8
    ],
    [
      "archive/archive/working-documents-2025-11-05/STUB_AND_DISCONNECTED_AUDIT.md",
      8
    ],
    [
      "src/adapters/neo4j_ontology_repository.rs",
      7
    ],
    [
      "data/markdown/AI User.md",
      7
    ],
    [
      "archive/working-notes-2025-11-06/SESSION_SUMMARY_CONTINUED_UPGRADES.md",
      7
    ]
  ],
  "todos": [
    {
      "file": "tests/ontology_api_test.rs",
      "line_number": 446,
      "marker_type": "TODO",
      "content": "Implement when endpoint is available",
      "context": "444:     #[actix_rt::test]\n445:     async fn test_load_ontology_endpoint() {\n446:         // TODO: Implement when endpoint is available\n447:         println!(\"Load ontology endpoint test - placeholder\");\n448:         assert!(true);",
      "severity": "warning"
    },
    {
      "file": "tests/ontology_api_test.rs",
      "line_number": 454,
      "marker_type": "TODO",
      "content": "Implement when endpoint is available",
      "context": "452:     #[actix_rt::test]\n453:     async fn test_get_cached_ontologies_endpoint() {\n454:         // TODO: Implement when endpoint is available\n455:         println!(\"Get cached ontologies endpoint test - placeholder\");\n456:         assert!(true);",
      "severity": "warning"
    },
    {
      "file": "tests/ontology_api_test.rs",
      "line_number": 462,
      "marker_type": "TODO",
      "content": "Implement when endpoint is available",
      "context": "460:     #[actix_rt::test]\n461:     async fn test_clear_cache_endpoint() {\n462:         // TODO: Implement when endpoint is available\n463:         println!(\"Clear cache endpoint test - placeholder\");\n464:         assert!(true);",
      "severity": "warning"
    },
    {
      "file": "tests/ontology_api_test.rs",
      "line_number": 470,
      "marker_type": "TODO",
      "content": "Implement when endpoint is available",
      "context": "468:     #[actix_rt::test]\n469:     async fn test_get_health_endpoint() {\n470:         // TODO: Implement when endpoint is available\n471:         println!(\"Get health endpoint test - placeholder\");\n472:         assert!(true);",
      "severity": "warning"
    },
    {
      "file": "tests/ontology_api_test.rs",
      "line_number": 478,
      "marker_type": "TODO",
      "content": "Implement when endpoint is available",
      "context": "476:     #[actix_rt::test]\n477:     async fn test_apply_inferences_endpoint() {\n478:         // TODO: Implement when endpoint is available\n479:         println!(\"Apply inferences endpoint test - placeholder\");\n480:         assert!(true);",
      "severity": "warning"
    },
    {
      "file": "tests/ontology_api_test.rs",
      "line_number": 486,
      "marker_type": "TODO",
      "content": "Implement when endpoint is available",
      "context": "484:     #[actix_rt::test]\n485:     async fn test_update_mapping_endpoint() {\n486:         // TODO: Implement when endpoint is available\n487:         println!(\"Update mapping endpoint test - placeholder\");\n488:         assert!(true);",
      "severity": "warning"
    },
    {
      "file": "tests/ontology_api_test.rs",
      "line_number": 494,
      "marker_type": "TODO",
      "content": "Implement when endpoint is available",
      "context": "492:     #[actix_rt::test]\n493:     async fn test_get_violations_endpoint() {\n494:         // TODO: Implement when endpoint is available\n495:         println!(\"Get violations endpoint test - placeholder\");\n496:         assert!(true);",
      "severity": "warning"
    },
    {
      "file": "tests/CRITICAL_github_sync_regression_test.rs",
      "line_number": 48,
      "marker_type": "TODO",
      "content": "Implement these with real services after hexagonal migration",
      "context": "46: }\n47: \n48: // TODO: Implement these with real services after hexagonal migration\n49: async fn mock_github_sync_service(config: GitHubSyncConfig) -> Result<GitHubSyncResult, String> {\n50:     // This will be replaced with:",
      "severity": "warning"
    },
    {
      "file": "tests/CRITICAL_github_sync_regression_test.rs",
      "line_number": 298,
      "marker_type": "TODO",
      "content": "Wait for event to propagate and cache to invalidate",
      "context": "296:         .expect(\"GitHub sync should succeed\");\n297: \n298:     // TODO: Wait for event to propagate and cache to invalidate\n299:     // tokio::time::sleep(Duration::from_millis(100)).await;\n300: ",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 21,
      "marker_type": "TODO",
      "content": "Uncomment when compilation is fixed",
      "context": "19:     /// once the compilation issues are resolved.\n20:     async fn setup_test_repository() -> Result<(), Box<dyn std::error::Error>> {\n21:         // TODO: Uncomment when compilation is fixed\n22:         // use crate::adapters::neo4j_settings_repository::Neo4jSettingsRepository;\n23:         // use crate::models::settings::SettingsConfig;",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 44,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "42:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n43:     async fn test_create_and_get_settings() {\n44:         // TODO: Implement once compilation is fixed\n45:         // let repo = setup_test_repository().await.unwrap();\n46:         //",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 67,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "65:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n66:     async fn test_update_settings() {\n67:         // TODO: Implement once compilation is fixed\n68:         // let repo = setup_test_repository().await.unwrap();\n69:         //",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 87,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "85:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n86:     async fn test_delete_settings() {\n87:         // TODO: Implement once compilation is fixed\n88:         // let repo = setup_test_repository().await.unwrap();\n89:         //",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 106,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "104:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n105:     async fn test_clustering_settings_crud() {\n106:         // TODO: Implement once compilation is fixed\n107:         // Test create, read, update, delete for clustering settings\n108:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 114,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "112:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n113:     async fn test_display_settings_crud() {\n114:         // TODO: Implement once compilation is fixed\n115:         // Test create, read, update, delete for display settings\n116:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 122,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "120:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n121:     async fn test_graph_settings_crud() {\n122:         // TODO: Implement once compilation is fixed\n123:         // Test create, read, update, delete for graph settings\n124:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 130,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "128:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n129:     async fn test_gpu_settings_crud() {\n130:         // TODO: Implement once compilation is fixed\n131:         // Test create, read, update, delete for GPU settings\n132:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 138,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "136:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n137:     async fn test_layout_settings_crud() {\n138:         // TODO: Implement once compilation is fixed\n139:         // Test create, read, update, delete for layout settings\n140:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 146,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "144:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n145:     async fn test_connection_success() {\n146:         // TODO: Implement once compilation is fixed\n147:         // Verify successful connection to Neo4j\n148:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 154,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "152:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n153:     async fn test_connection_failure() {\n154:         // TODO: Implement once compilation is fixed\n155:         // Test behavior when Neo4j is unavailable\n156:         // Should return appropriate error",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 163,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "161:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n162:     async fn test_authentication_failure() {\n163:         // TODO: Implement once compilation is fixed\n164:         // Test behavior with invalid credentials\n165:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 171,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "169:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n170:     async fn test_reconnection() {\n171:         // TODO: Implement once compilation is fixed\n172:         // Test automatic reconnection after connection loss\n173:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 179,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "177:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n178:     async fn test_invalid_data_handling() {\n179:         // TODO: Implement once compilation is fixed\n180:         // Test handling of malformed or invalid settings data\n181:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 187,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "185:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n186:     async fn test_query_failure_handling() {\n187:         // TODO: Implement once compilation is fixed\n188:         // Test handling of Cypher query failures\n189:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 195,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "193:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n194:     async fn test_constraint_violation_handling() {\n195:         // TODO: Implement once compilation is fixed\n196:         // Test handling of database constraint violations\n197:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 203,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "201:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n202:     async fn test_data_persistence() {\n203:         // TODO: Implement once compilation is fixed\n204:         // Save data, close connection, reconnect, verify data persists\n205:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 211,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "209:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n210:     async fn test_serialization_round_trip() {\n211:         // TODO: Implement once compilation is fixed\n212:         // Verify data integrity through save/load cycle\n213:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 219,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "217:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n218:     async fn test_large_dataset() {\n219:         // TODO: Implement once compilation is fixed\n220:         // Test performance with large settings objects\n221:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 227,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "225:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n226:     async fn test_concurrent_reads() {\n227:         // TODO: Implement once compilation is fixed\n228:         // Spawn multiple tasks reading settings concurrently\n229:         // Verify no data corruption or errors",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 236,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "234:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n235:     async fn test_concurrent_writes() {\n236:         // TODO: Implement once compilation is fixed\n237:         // Spawn multiple tasks writing settings concurrently\n238:         // Verify proper synchronization and data integrity",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 245,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "243:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n244:     async fn test_concurrent_read_write() {\n245:         // TODO: Implement once compilation is fixed\n246:         // Mix concurrent reads and writes\n247:         // Verify eventual consistency",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 254,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "252:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n253:     async fn test_transaction_rollback() {\n254:         // TODO: Implement once compilation is fixed\n255:         // Test that failed operations don't leave partial state\n256:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 262,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "260:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n261:     async fn test_query_performance_simple() {\n262:         // TODO: Implement once compilation is fixed\n263:         // Benchmark simple CRUD operations\n264:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 270,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "268:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n269:     async fn test_query_performance_complex() {\n270:         // TODO: Implement once compilation is fixed\n271:         // Benchmark complex queries with multiple joins\n272:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 278,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "276:     #[ignore = \"Requires Neo4j instance and fixed compilation errors\"]\n277:     async fn test_batch_operations() {\n278:         // TODO: Implement once compilation is fixed\n279:         // Test bulk insert/update/delete operations\n280:     }",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 285,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "283:     #[allow(dead_code)]\n284:     fn create_test_settings() -> () {\n285:         // TODO: Implement once compilation is fixed\n286:         // SettingsConfig {\n287:         //     clustering: ClusteringSettings::default(),",
      "severity": "warning"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 302,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed",
      "context": "300:     #[allow(dead_code)]\n301:     async fn cleanup_test_data() -> Result<(), Box<dyn std::error::Error>> {\n302:         // TODO: Implement once compilation is fixed\n303:         // Delete all test settings from database\n304:         Ok(())",
      "severity": "warning"
    },
    {
      "file": "src/actors/client_coordinator_actor.rs",
      "line_number": 1325,
      "marker_type": "TODO",
      "content": "Load saved filter from Neo4j using msg.pubkey",
      "context": "1323:             );\n1324: \n1325:             // TODO: Load saved filter from Neo4j using msg.pubkey\n1326:             // For now, if filter is enabled, recompute with current graph data\n1327:             if client.filter.enabled {",
      "severity": "warning"
    },
    {
      "file": "src/adapters/neo4j_adapter.rs",
      "line_number": 351,
      "marker_type": "TODO",
      "content": "Apply query timeout from config",
      "context": "349:         }\n350: \n351:         // TODO: Apply query timeout from config\n352:         // Note: neo4rs doesn't currently support query timeouts directly\n353:         // Consider implementing timeout at the application level",
      "severity": "warning"
    },
    {
      "file": "src/adapters/neo4j_ontology_repository.rs",
      "line_number": 813,
      "marker_type": "TODO",
      "content": "Calculate from hierarchy traversal",
      "context": "811:             property_count: property_count as usize,\n812:             axiom_count: axiom_count as usize,\n813:             max_depth: 0, // TODO: Calculate from hierarchy traversal\n814:             average_branching_factor: 0.0, // TODO: Calculate branching factor\n815:         })",
      "severity": "warning"
    },
    {
      "file": "src/adapters/neo4j_ontology_repository.rs",
      "line_number": 814,
      "marker_type": "TODO",
      "content": "Calculate branching factor",
      "context": "812:             axiom_count: axiom_count as usize,\n813:             max_depth: 0, // TODO: Calculate from hierarchy traversal\n814:             average_branching_factor: 0.0, // TODO: Calculate branching factor\n815:         })\n816:     }",
      "severity": "warning"
    },
    {
      "file": "src/adapters/neo4j_ontology_repository.rs",
      "line_number": 855,
      "marker_type": "TODO",
      "content": "Implement pathfinding cache if needed",
      "context": "853:     #[instrument(skip(self))]\n854:     async fn cache_sssp_result(&self, _entry: &PathfindingCacheEntry) -> RepoResult<()> {\n855:         // TODO: Implement pathfinding cache if needed\n856:         Ok(())\n857:     }",
      "severity": "warning"
    },
    {
      "file": "src/adapters/neo4j_ontology_repository.rs",
      "line_number": 861,
      "marker_type": "TODO",
      "content": "Implement pathfinding cache if needed",
      "context": "859:     #[instrument(skip(self))]\n860:     async fn get_cached_sssp(&self, _source_node_id: u32) -> RepoResult<Option<PathfindingCacheEntry>> {\n861:         // TODO: Implement pathfinding cache if needed\n862:         Ok(None)\n863:     }",
      "severity": "warning"
    },
    {
      "file": "src/adapters/neo4j_ontology_repository.rs",
      "line_number": 867,
      "marker_type": "TODO",
      "content": "Implement pathfinding cache if needed",
      "context": "865:     #[instrument(skip(self))]\n866:     async fn cache_apsp_result(&self, _distance_matrix: &Vec<Vec<f32>>) -> RepoResult<()> {\n867:         // TODO: Implement pathfinding cache if needed\n868:         Ok(())\n869:     }",
      "severity": "warning"
    },
    {
      "file": "src/adapters/neo4j_ontology_repository.rs",
      "line_number": 873,
      "marker_type": "TODO",
      "content": "Implement pathfinding cache if needed",
      "context": "871:     #[instrument(skip(self))]\n872:     async fn get_cached_apsp(&self) -> RepoResult<Option<Vec<Vec<f32>>>> {\n873:         // TODO: Implement pathfinding cache if needed\n874:         Ok(None)\n875:     }",
      "severity": "warning"
    },
    {
      "file": "src/adapters/neo4j_ontology_repository.rs",
      "line_number": 880,
      "marker_type": "TODO",
      "content": "Implement pathfinding cache if needed",
      "context": "878:     async fn invalidate_pathfinding_caches(&self) -> RepoResult<()> {\n879:         info!(\"Clearing pathfinding cache\");\n880:         // TODO: Implement pathfinding cache if needed\n881:         Ok(())\n882:     }",
      "severity": "warning"
    },
    {
      "file": "src/handlers/socket_flow_handler.rs",
      "line_number": 1252,
      "marker_type": "TODO",
      "content": "Save to Neo4j via settings repository",
      "context": "1250:                                             // Also persist to Neo4j if authenticated\n1251:                                             if let Some(pubkey) = pubkey_opt {\n1252:                                                 // TODO: Save to Neo4j via settings repository\n1253:                                                 info!(\"Filter updated for pubkey {}: enabled={}, max_nodes={:?}\",\n1254:                                                       pubkey, update.enabled, update.max_nodes);",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoner.rs",
      "line_number": 295,
      "marker_type": "TODO",
      "content": "Update tests to use Neo4j test containers",
      "context": "293:     use super::*;\n294: \n295:     // TODO: Update tests to use Neo4j test containers\n296:     // #[test]\n297:     // fn test_infer_from_path_person() {",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoner.rs",
      "line_number": 306,
      "marker_type": "TODO",
      "content": "Use Neo4j test container */)",
      "context": "304:     //         &OntologyReasoner {\n305:     //             inference_engine: engine.clone(),\n306:     //             ontology_repo: Arc::new(/* TODO: Use Neo4j test container */)\n307:     //         },\n308:     //         \"people/Tim-Cook.md\"",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoner.rs",
      "line_number": 319,
      "marker_type": "TODO",
      "content": "Use Neo4j test container */)",
      "context": "317:     //         &OntologyReasoner {\n318:     //             inference_engine: engine.clone(),\n319:     //             ontology_repo: Arc::new(/* TODO: Use Neo4j test container */)\n320:     //         },\n321:     //         \"companies/Apple-Inc.md\"",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoner.rs",
      "line_number": 325,
      "marker_type": "TODO",
      "content": "Update tests to use Neo4j test containers",
      "context": "323:     // }\n324: \n325:     // TODO: Update tests to use Neo4j test containers\n326:     // #[test]\n327:     // fn test_type_to_class_iri() {",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoner.rs",
      "line_number": 331,
      "marker_type": "TODO",
      "content": "Use Neo4j test container */)",
      "context": "329:     //     let reasoner = OntologyReasoner {\n330:     //         inference_engine: engine,\n331:     //         ontology_repo: Arc::new(/* TODO: Use Neo4j test container */)\n332:     //     };\n333:     //",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoning_service.rs",
      "line_number": 459,
      "marker_type": "TODO",
      "content": "Update all tests to use Neo4j test containers",
      "context": "457: }\n458: \n459: // TODO: Update all tests to use Neo4j test containers\n460: #[cfg(test)]\n461: mod tests {",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoning_service.rs",
      "line_number": 465,
      "marker_type": "TODO",
      "content": "Update tests to use Neo4j test containers",
      "context": "463:     use std::collections::HashMap;\n464: \n465:     // TODO: Update tests to use Neo4j test containers\n466:     // #[tokio::test]\n467:     // async fn test_create_service() {",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoning_service.rs",
      "line_number": 469,
      "marker_type": "TODO",
      "content": "Use Neo4j test container */);",
      "context": "467:     // async fn test_create_service() {\n468:     //     let engine = Arc::new(WhelkInferenceEngine::new());\n469:     //     let repo = Arc::new(/* TODO: Use Neo4j test container */);\n470:     //\n471:     //     let service = OntologyReasoningService::new(engine, repo);",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoning_service.rs",
      "line_number": 480,
      "marker_type": "TODO",
      "content": "Use Neo4j test container */);",
      "context": "478:     // async fn test_hierarchy_depth_calculation() {\n479:     //     let engine = Arc::new(WhelkInferenceEngine::new());\n480:     //     let repo = Arc::new(/* TODO: Use Neo4j test container */);\n481:     //\n482:     //     let service = OntologyReasoningService::new(engine, repo);",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoning_service.rs",
      "line_number": 496,
      "marker_type": "TODO",
      "content": "Use Neo4j test container */);",
      "context": "494:     //     // Test disabled until Neo4j test containers are available\n495:     //     let engine = Arc::new(WhelkInferenceEngine::new());\n496:     //     let repo = Arc::new(/* TODO: Use Neo4j test container */);\n497:     //\n498:     //     let service = OntologyReasoningService::new(engine, repo);",
      "severity": "warning"
    },
    {
      "file": "src/services/local_file_sync_service.rs",
      "line_number": 464,
      "marker_type": "TODO",
      "content": "Implement save_ontology_data",
      "context": "462: \n463:                     // Save ontology data immediately\n464:                     // TODO: Implement save_ontology_data\n465:                     stats.ontology_files_processed += 1;\n466:                 }",
      "severity": "warning"
    },
    {
      "file": "src/services/github_sync_service.rs",
      "line_number": 460,
      "marker_type": "TODO",
      "content": "File metadata tracking removed after Neo4j migration",
      "context": "458:         &self,\n459:     ) -> Result<std::collections::HashMap<String, String>, String> {\n460:         // TODO: File metadata tracking removed after Neo4j migration\n461:         // Return empty map to process all files\n462:         info!(\"[GitHubSync][SHA1] File metadata tracking disabled (Neo4j migration)\");",
      "severity": "warning"
    },
    {
      "file": "src/services/github_sync_service.rs",
      "line_number": 470,
      "marker_type": "TODO",
      "content": "File metadata tracking removed after Neo4j migration",
      "context": "468:         files: &[GitHubFileBasicMetadata],\n469:     ) -> Result<(), String> {\n470:         // TODO: File metadata tracking removed after Neo4j migration\n471:         // This function is now a no-op\n472:         info!(\"[GitHubSync] File metadata update skipped (Neo4j migration) - {} files\", files.len());",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_enrichment_service.rs",
      "line_number": 268,
      "marker_type": "TODO",
      "content": "Update tests to use Neo4j test containers",
      "context": "266: }\n267: \n268: // TODO: Update tests to use Neo4j test containers\n269: // Tests temporarily disabled - need to be updated to use Neo4j instead of SQLite\n270: // #[cfg(test)]",
      "severity": "warning"
    },
    {
      "file": "src/handlers/api_handler/semantic_forces.rs",
      "line_number": 228,
      "marker_type": "TODO",
      "content": "Send GetHierarchyLevels message to SemanticForcesActor via GPU manager",
      "context": "226: \n227:     // For now, return mock data\n228:     // TODO: Send GetHierarchyLevels message to SemanticForcesActor via GPU manager\n229:     ok_json!(json!({\n230:         \"status\": \"success\",",
      "severity": "warning"
    },
    {
      "file": "src/handlers/api_handler/semantic_forces.rs",
      "line_number": 295,
      "marker_type": "TODO",
      "content": "Send RecalculateHierarchy message to SemanticForcesActor",
      "context": "293:     };\n294: \n295:     // TODO: Send RecalculateHierarchy message to SemanticForcesActor\n296:     ok_json!(json!({\n297:         \"status\": \"success\",",
      "severity": "warning"
    },
    {
      "file": "src/actors/gpu/connected_components_actor.rs",
      "line_number": 268,
      "marker_type": "TODO",
      "content": "Replace with GPU kernel call when available",
      "context": "266:         drop(unified_compute); // Release lock before CPU computation\n267: \n268:         // TODO: Replace with GPU kernel call when available\n269:         // For now, use CPU-based label propagation\n270:         let max_iterations = msg.max_iterations.unwrap_or(100);",
      "severity": "warning"
    },
    {
      "file": "scripts/whelk-rs/src/whelk/reasoner.rs",
      "line_number": 129,
      "marker_type": "TODO",
      "content": "negative_self_restrictions",
      "context": "127:         })\n128:         .collect();\n129:     //TODO negative_self_restrictions\n130:     let mut assertions_queue: Vec<Rc<ConceptInclusion>> = vec![];\n131:     let mut todo: Vec<QueueExpression> = vec![];",
      "severity": "warning"
    },
    {
      "file": "scripts/whelk-rs/src/whelk/reasoner.rs",
      "line_number": 201,
      "marker_type": "TODO",
      "content": "maybe this can be done in one step with the contains check",
      "context": "199:             }\n200:         }\n201:         //TODO maybe this can be done in one step with the contains check\n202:         state.inits.insert(Rc::clone(concept));\n203:         rule_0(concept, state, todo);",
      "severity": "warning"
    },
    {
      "file": "scripts/whelk-rs/src/whelk/reasoner.rs",
      "line_number": 230,
      "marker_type": "TODO",
      "content": "",
      "context": "228:         rule_bottom_left(ci, state, todo);\n229:         rule_subclass_right(ci, state, todo);\n230:         //TODO\n231:         rule_plus_and_right(ci, state, todo);\n232:         rule_plus_and_left(ci, state, todo);",
      "severity": "warning"
    },
    {
      "file": "scripts/whelk-rs/src/whelk/reasoner.rs",
      "line_number": 286,
      "marker_type": "TODO",
      "content": "",
      "context": "284:         rule_ring_left(subject, role, target, state, todo);\n285:         rule_squiggle(subject, role, target, state, todo);\n286:         //rule_plus_self_nominal() //TODO\n287:     }\n288: }",
      "severity": "warning"
    },
    {
      "file": "scripts/whelk-rs/src/whelk/owl.rs",
      "line_number": 197,
      "marker_type": "TODO",
      "content": "",
      "context": "195:                 .collect()\n196:         }\n197:         // hm::Component::ObjectPropertyRange(_) => {} //TODO\n198:         // hm::Component::DisjointObjectProperties(_) => {}\n199:         // hm::Component::InverseObjectProperties(_) => {}",
      "severity": "warning"
    },
    {
      "file": "scripts/whelk-rs/src/whelk/owl.rs",
      "line_number": 213,
      "marker_type": "TODO",
      "content": "",
      "context": "211:             globals,\n212:         ),\n213:         // hm::Component::SameIndividual(_) => {} //TODO\n214:         // hm::Component::DifferentIndividuals(_) => {} //TODO\n215:         hm::Component::ClassAssertion(hm::ClassAssertion { ce: cls, i: hm::Individual::Named(hm::NamedIndividual(ind)) }) => convert_expression(cls)",
      "severity": "warning"
    },
    {
      "file": "scripts/whelk-rs/src/whelk/owl.rs",
      "line_number": 214,
      "marker_type": "TODO",
      "content": "",
      "context": "212:         ),\n213:         // hm::Component::SameIndividual(_) => {} //TODO\n214:         // hm::Component::DifferentIndividuals(_) => {} //TODO\n215:         hm::Component::ClassAssertion(hm::ClassAssertion { ce: cls, i: hm::Individual::Named(hm::NamedIndividual(ind)) }) => convert_expression(cls)\n216:             .iter()",
      "severity": "warning"
    },
    {
      "file": "scripts/whelk-rs/src/whelk/owl.rs",
      "line_number": 238,
      "marker_type": "TODO",
      "content": "",
      "context": "236:             HashSet::unit(concept_inclusion(&subclass, &superclass))\n237:         }\n238:         // hm::Component::NegativeObjectPropertyAssertion(_) => {} //TODO\n239:         // hm::Component::SubDataPropertyOf(_) => {}\n240:         // hm::Component::EquivalentDataProperties(_) => {}",
      "severity": "warning"
    },
    {
      "file": "tests/ports/test_inference_engine.rs",
      "line_number": 4,
      "marker_type": "TODO",
      "content": "Implement when InferenceEngine adapter is ready",
      "context": "2: //! Placeholder for InferenceEngine contract tests\n3: \n4: // TODO: Implement when InferenceEngine adapter is ready\n5: ",
      "severity": "warning"
    },
    {
      "file": "tests/ports/test_gpu_semantic_analyzer.rs",
      "line_number": 4,
      "marker_type": "TODO",
      "content": "Implement when CUDA analyzer is ready",
      "context": "2: //! Placeholder for GpuSemanticAnalyzer contract tests\n3: \n4: // TODO: Implement when CUDA analyzer is ready\n5: ",
      "severity": "warning"
    },
    {
      "file": "tests/ports/test_gpu_physics_adapter.rs",
      "line_number": 4,
      "marker_type": "TODO",
      "content": "Implement when CUDA adapter is ready",
      "context": "2: //! Placeholder for GpuPhysicsAdapter contract tests\n3: \n4: // TODO: Implement when CUDA adapter is ready\n5: ",
      "severity": "warning"
    },
    {
      "file": "tests/api/reasoning_api_tests.rs",
      "line_number": 18,
      "marker_type": "TODO",
      "content": "Add actual endpoint tests when API is exposed",
      "context": "16: \n17:         println!(\"API health check test placeholder\");\n18:         // TODO: Add actual endpoint tests when API is exposed\n19:     }\n20: ",
      "severity": "warning"
    },
    {
      "file": "tests/api/reasoning_api_tests.rs",
      "line_number": 29,
      "marker_type": "TODO",
      "content": "Add actual endpoint implementation",
      "context": "27: \n28:         println!(\"Inference request test placeholder\");\n29:         // TODO: Add actual endpoint implementation\n30:     }\n31: ",
      "severity": "warning"
    },
    {
      "file": "tests/api/reasoning_api_tests.rs",
      "line_number": 38,
      "marker_type": "TODO",
      "content": "Add actual endpoint implementation",
      "context": "36: \n37:         println!(\"Cache invalidation test placeholder\");\n38:         // TODO: Add actual endpoint implementation\n39:     }\n40: ",
      "severity": "warning"
    },
    {
      "file": "tests/api/reasoning_api_tests.rs",
      "line_number": 48,
      "marker_type": "TODO",
      "content": "Add actual endpoint implementation",
      "context": "46: \n47:         println!(\"Constraint generation test placeholder\");\n48:         // TODO: Add actual endpoint implementation\n49:     }\n50: }",
      "severity": "warning"
    },
    {
      "file": "tests/api/reasoning_api_tests.rs",
      "line_number": 60,
      "marker_type": "TODO",
      "content": "Add WebSocket protocol tests",
      "context": "58:         // Test WebSocket connection establishment\n59:         println!(\"WebSocket connection test placeholder\");\n60:         // TODO: Add WebSocket protocol tests\n61:     }\n62: ",
      "severity": "warning"
    },
    {
      "file": "tests/api/reasoning_api_tests.rs",
      "line_number": 67,
      "marker_type": "TODO",
      "content": "Add streaming tests",
      "context": "65:         // Test streaming inference results via WebSocket\n66:         println!(\"WebSocket inference stream test placeholder\");\n67:         // TODO: Add streaming tests\n68:     }\n69: ",
      "severity": "warning"
    },
    {
      "file": "tests/api/reasoning_api_tests.rs",
      "line_number": 74,
      "marker_type": "TODO",
      "content": "Add error handling tests",
      "context": "72:         // Test error handling in WebSocket protocol\n73:         println!(\"WebSocket error handling test placeholder\");\n74:         // TODO: Add error handling tests\n75:     }\n76: }",
      "severity": "warning"
    },
    {
      "file": "whelk-rs/src/whelk/reasoner.rs",
      "line_number": 129,
      "marker_type": "TODO",
      "content": "negative_self_restrictions",
      "context": "127:         })\n128:         .collect();\n129:     //TODO negative_self_restrictions\n130:     let mut assertions_queue: Vec<Rc<ConceptInclusion>> = vec![];\n131:     let mut todo: Vec<QueueExpression> = vec![];",
      "severity": "warning"
    },
    {
      "file": "whelk-rs/src/whelk/reasoner.rs",
      "line_number": 201,
      "marker_type": "TODO",
      "content": "maybe this can be done in one step with the contains check",
      "context": "199:             }\n200:         }\n201:         //TODO maybe this can be done in one step with the contains check\n202:         state.inits.insert(Rc::clone(concept));\n203:         rule_0(concept, state, todo);",
      "severity": "warning"
    },
    {
      "file": "whelk-rs/src/whelk/reasoner.rs",
      "line_number": 230,
      "marker_type": "TODO",
      "content": "",
      "context": "228:         rule_bottom_left(ci, state, todo);\n229:         rule_subclass_right(ci, state, todo);\n230:         //TODO\n231:         rule_plus_and_right(ci, state, todo);\n232:         rule_plus_and_left(ci, state, todo);",
      "severity": "warning"
    },
    {
      "file": "whelk-rs/src/whelk/reasoner.rs",
      "line_number": 286,
      "marker_type": "TODO",
      "content": "",
      "context": "284:         rule_ring_left(subject, role, target, state, todo);\n285:         rule_squiggle(subject, role, target, state, todo);\n286:         //rule_plus_self_nominal() //TODO\n287:     }\n288: }",
      "severity": "warning"
    },
    {
      "file": "whelk-rs/src/whelk/owl.rs",
      "line_number": 197,
      "marker_type": "TODO",
      "content": "",
      "context": "195:                 .collect()\n196:         }\n197:         // hm::Component::ObjectPropertyRange(_) => {} //TODO\n198:         // hm::Component::DisjointObjectProperties(_) => {}\n199:         // hm::Component::InverseObjectProperties(_) => {}",
      "severity": "warning"
    },
    {
      "file": "whelk-rs/src/whelk/owl.rs",
      "line_number": 213,
      "marker_type": "TODO",
      "content": "",
      "context": "211:             globals,\n212:         ),\n213:         // hm::Component::SameIndividual(_) => {} //TODO\n214:         // hm::Component::DifferentIndividuals(_) => {} //TODO\n215:         hm::Component::ClassAssertion(hm::ClassAssertion { ce: cls, i: hm::Individual::Named(hm::NamedIndividual(ind)) }) => convert_expression(cls)",
      "severity": "warning"
    },
    {
      "file": "whelk-rs/src/whelk/owl.rs",
      "line_number": 214,
      "marker_type": "TODO",
      "content": "",
      "context": "212:         ),\n213:         // hm::Component::SameIndividual(_) => {} //TODO\n214:         // hm::Component::DifferentIndividuals(_) => {} //TODO\n215:         hm::Component::ClassAssertion(hm::ClassAssertion { ce: cls, i: hm::Individual::Named(hm::NamedIndividual(ind)) }) => convert_expression(cls)\n216:             .iter()",
      "severity": "warning"
    },
    {
      "file": "whelk-rs/src/whelk/owl.rs",
      "line_number": 238,
      "marker_type": "TODO",
      "content": "",
      "context": "236:             HashSet::unit(concept_inclusion(&subclass, &superclass))\n237:         }\n238:         // hm::Component::NegativeObjectPropertyAssertion(_) => {} //TODO\n239:         // hm::Component::SubDataPropertyOf(_) => {}\n240:         // hm::Component::EquivalentDataProperties(_) => {}",
      "severity": "warning"
    },
    {
      "file": "sdk/vircadia-world-sdk-ts/bun/src/module/vircadia.common.bun.postgres.module.ts",
      "line_number": 4,
      "marker_type": "TODO",
      "content": "Use Bun native .sql client and use pooling to reduce latency issues.",
      "context": "2: import { BunLogModule } from \"./vircadia.common.bun.log.module\";\n3: import { serverConfiguration } from \"../config/vircadia.server.config\";\n4: // TODO: Use Bun native .sql client and use pooling to reduce latency issues.\n5: \n6: const IDLE_TIMEOUT_S = 86400; // 24 hours",
      "severity": "warning"
    },
    {
      "file": "client/src/features/visualisation/components/HolographicDataSphere.tsx",
      "line_number": 27,
      "marker_type": "TODO",
      "content": "Map these hardcoded values to settings system",
      "context": "25: import { useSettingsStore } from '../../../store/settingsStore';\n26: \n27: // TODO: Map these hardcoded values to settings system\n28: // Note: Settings system is brittle - don't update UX names yet\n29: // These values should eventually come from visualisation.hologram settings",
      "severity": "warning"
    },
    {
      "file": "client/src/features/visualisation/components/ControlPanel/SemanticZoomControls.tsx",
      "line_number": 46,
      "marker_type": "TODO",
      "content": "Implement auto-zoom logic based on camera distance",
      "context": "44:     logger.info('Auto-zoom toggled', { enabled: !autoZoom });\n45: \n46:     // TODO: Implement auto-zoom logic based on camera distance\n47:     if (!autoZoom) {\n48:       // Enable auto-zoom",
      "severity": "warning"
    },
    {
      "file": "multi-agent-docker/skills/docs-alignment/scripts/generate_report.py",
      "line_number": 275,
      "marker_type": "TODO",
      "content": "s",
      "context": "273:             sections.append(self._format_table(['File', 'Line', 'Type', 'Content'], rows))\n274: \n275:         # TODOs\n276:         todos = stubs_report.get('todos', [])\n277:         if todos:",
      "severity": "warning"
    },
    {
      "file": "multi-agent-docker/skills/docs-alignment/scripts/generate_report.py",
      "line_number": 278,
      "marker_type": "TODO",
      "content": "s\\n')",
      "context": "276:         todos = stubs_report.get('todos', [])\n277:         if todos:\n278:             sections.append('### TODOs\\n')\n279:             rows = [[\n280:                 t.get('file', 'N/A'),",
      "severity": "warning"
    },
    {
      "file": "multi-agent-docker/skills/import-to-ontology/import-engine.js",
      "line_number": 449,
      "marker_type": "TODO",
      "content": "Remove added content from target file",
      "context": "447:     }\n448: \n449:     // TODO: Remove added content from target file\n450:     // This would require tracking what was added to properly remove it\n451: ",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Helpfulness.md",
      "line_number": 106,
      "marker_type": "TODO",
      "content": "nostr IoT",
      "context": "104: - # Useful papers\n105: \n106: \t\t- ### TODO nostr IoT\n107: \t\t\t- [[MUST]]\n108: \t\t\t-",
      "severity": "warning"
    },
    {
      "file": "data/markdown/AI User.md",
      "line_number": 3740,
      "marker_type": "TODO",
      "content": "nostr IoT",
      "context": "3738: \t\t\t  * [BakedAvatar](https://buaavrcg.github.io/BakedAvatar/): A project focused on avatar creation.\n3739: \n3740: \t\t- ### TODO nostr IoT\n3741: \t\t\t- [[MUST]]\n3742: \t\t\t-",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Data Poisoning.md",
      "line_number": 280,
      "marker_type": "TODO",
      "content": "",
      "context": "278: \t\t\t\t\t\t- `value`: (`xsd:string`) The value of the tag.\n279: \n280: \t\t\t- ## TODO\n281: \t\t\t\t- **AI Agent Capabilities:** Expand the capabilities property of `AIAgent` to include specific actions and functions related to Bitcoin, RGB, and Nostr, such as \"create_digital_asset\", \"transfer_ownership\", \"publish_nostr_event\", etc.\n282: \t\t\t\t- **Event Logging and Attestation:** Consider adding mechanisms for logging significant events and generating cryptographic attestations, which could be used for dispute resolution or auditing purposes. This would operate on an automated threshold trigger system mediated by LLM, and would wrap the recent interactions between parties in pubkey encrypted data blobs, sending them to both parties alongside a report of the trigger event. This would potentially allow action by the parties in their jurisdictions. The data would then be deleted from the metaverse.",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Data Poisoning.md",
      "line_number": 457,
      "marker_type": "TODO",
      "content": "",
      "context": "455: \t\t\t\t\t\t- `value`: (`xsd:string`) The value of the tag.\n456: \n457: \t\t\t- ## TODO\n458: \t\t\t\t- **AI Agent Capabilities:** Expand the capabilities property of `AIAgent` to include specific actions and functions related to Bitcoin, RGB, and Nostr, such as \"create_digital_asset\", \"transfer_ownership\", \"publish_nostr_event\", etc.\n459: \t\t\t\t- **Event Logging and Attestation:** Consider adding mechanisms for logging significant events and generating cryptographic attestations, which could be used for dispute resolution or auditing purposes. This would operate on an automated threshold trigger system mediated by LLM, and would wrap the recent interactions between parties in pubkey encrypted data blobs, sending them to both parties alongside a report of the trigger event. This would potentially allow action by the parties in their jurisdictions. The data would then be deleted from the metaverse.",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Data Poisoning.md",
      "line_number": 576,
      "marker_type": "TODO",
      "content": "",
      "context": "574: \t\t\t\t\t\t- `value`: (`xsd:string`) The value of the tag.\n575: \n576: \t\t\t- ## TODO\n577: \t\t\t\t- **AI Agent Capabilities:** Expand the capabilities property of `AIAgent` to include specific actions and functions related to Bitcoin, RGB, and Nostr, such as \"create_digital_asset\", \"transfer_ownership\", \"publish_nostr_event\", etc.\n578: \t\t\t\t- **Event Logging and Attestation:** Consider adding mechanisms for logging significant events and generating cryptographic attestations, which could be used for dispute resolution or auditing purposes. This would operate on an automated threshold trigger system mediated by LLM, and would wrap the recent interactions between parties in pubkey encrypted data blobs, sending them to both parties alongside a report of the trigger event. This would potentially allow action by the parties in their jurisdictions. The data would then be deleted from the metaverse.",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Transformer.md",
      "line_number": 197,
      "marker_type": "TODO",
      "content": "Vectorworks Plan Visualisation",
      "context": "195: \t\t\t\t\t- The paper reveals a key difference between SSMs and Transformers in how they implement multi-head computations. Mamba2, like its predecessor, adopts a \"multi-value attention\" (MVA) pattern, where the expansion and contraction matrices (B and C) are shared across all heads. This pattern, a natural outcome of the SSM formulation, stands in contrast to the multi-query attention (MQA) pattern commonly used in Transformers. Empirical evidence suggests that the MVA pattern, unique to SSMs, might contribute to Mamba2's strong performance.\n196: \n197: - # TODO Vectorworks Plan Visualisation\n198: \t- Modern AI can transform 2D Vectorworks plans into client-ready visuals in five key formats.\n199: ",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Transformer.md",
      "line_number": 242,
      "marker_type": "TODO",
      "content": "Vectorworks Plan Visualisation",
      "context": "240: \t\t\t\t\t- The paper reveals a key difference between SSMs and Transformers in how they implement multi-head computations. Mamba2, like its predecessor, adopts a \"multi-value attention\" (MVA) pattern, where the expansion and contraction matrices (B and C) are shared across all heads. This pattern, a natural outcome of the SSM formulation, stands in contrast to the multi-query attention (MQA) pattern commonly used in Transformers. Empirical evidence suggests that the MVA pattern, unique to SSMs, might contribute to Mamba2's strong performance.\n241: \n242: - # TODO Vectorworks Plan Visualisation\n243: \t- Modern AI can transform 2D Vectorworks plans into client-ready visuals in five key formats.\n244: ",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 12,
      "marker_type": "TODO",
      "content": "Energy Performance Certificate (EPC)",
      "context": "10: \t  ---\n11: \t- ## Phase 1: Pre-Tenancy Compliance Setup\n12: \t- ### TODO Energy Performance Certificate (EPC)\n13: \t- [ ] **Priority**: HIGH\n14: \t- [ ] Verify property has valid EPC with minimum Band E rating",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 19,
      "marker_type": "TODO",
      "content": "Gas Safety Certificate (CP12)",
      "context": "17: \t- [ ] **Deadline**: Before tenancy starts\n18: \t- [ ] **Valid for**: 10 years\n19: \t- ### TODO Gas Safety Certificate (CP12)\n20: \t- [ ] **Priority**: CRITICAL\n21: \t- [ ] Schedule gas safety check with Gas Safe registered engineer",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 26,
      "marker_type": "TODO",
      "content": "Electrical Installation Condition Report (EICR)",
      "context": "24: \t- [ ] Confirm OpenRent provides copy to tenants within 28 days\n25: \t- [ ] **Renewal**: Annual requirement\n26: \t- ### TODO Electrical Installation Condition Report (EICR)\n27: \t- [ ] **Priority**: HIGH\n28: \t- [ ] Schedule electrical inspection with qualified electrician",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 33,
      "marker_type": "TODO",
      "content": "Alarm Systems Installation & Testing",
      "context": "31: \t- [ ] Verify OpenRent provides copy to tenants within 28 days\n32: \t- [ ] **Renewal**: Every 5 years maximum\n33: \t- ### TODO Alarm Systems Installation & Testing\n34: \t- [ ] **Priority**: CRITICAL\n35: \t- [ ] Install working smoke alarm on each storey used as living accommodation",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 40,
      "marker_type": "TODO",
      "content": "Right to Rent Checks",
      "context": "38: \t- [ ] Document testing results\n39: \t- [ ] **Note**: OpenRent cannot do this testing for you\n40: \t- ### TODO Right to Rent Checks\n41: \t- [ ] **Priority**: CRITICAL - PERSONAL RESPONSIBILITY\n42: \t- [ ] Conduct checks for all adult tenants (18+)",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 47,
      "marker_type": "TODO",
      "content": "Documentation & Legal Setup",
      "context": "45: \t- [ ] Keep clear, dated copies/screenshots for 12+ months after tenancy ends\n46: \t- [ ] **Deadline**: Before tenancy starts\n47: \t- ### TODO Documentation & Legal Setup\n48: \t- [ ] Verify tenants receive current \"How to Rent\" guide via OpenRent\n49: \t- [ ] Customize and sign AST agreement through OpenRent platform",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 56,
      "marker_type": "TODO",
      "content": "Monthly/Quarterly Maintenance",
      "context": "54: \t  ---\n55: \t- ## Phase 2: Ongoing Tenancy Management\n56: \t- ### TODO Monthly/Quarterly Maintenance\n57: \t- [ ] Set up system for tenant repair requests (use OpenRent messaging)\n58: \t- [ ] Maintain 24-hour notice policy for property access",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 61,
      "marker_type": "TODO",
      "content": "Annual Compliance Renewals",
      "context": "59: \t- [ ] Monitor and maintain property structure, water, gas, electricity, heating\n60: \t- [ ] **Ongoing**: Respond promptly to repair requests\n61: \t- ### TODO Annual Compliance Renewals\n62: \t- [ ] **Every 12 months**: Re-order gas safety checks through OpenRent\n63: \t- [ ] Confirm new certificates provided to tenants within 28 days",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 66,
      "marker_type": "TODO",
      "content": "Risk Assessments & Health Standards",
      "context": "64: \t- [ ] **Every 5 years**: Schedule EICR renewals\n65: \t- [ ] Monitor smoke/CO alarm functionality (replace if faulty)\n66: \t- ### TODO Risk Assessments & Health Standards\n67: \t- [ ] **PERSONAL RESPONSIBILITY**: Conduct Legionella risk assessment\n68: \t- [ ] Ensure property remains fit for human habitation (HHSRS compliance)",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 73,
      "marker_type": "TODO",
      "content": "Record Keeping System Setup",
      "context": "71: \t  ---\n72: \t- ## Phase 3: Financial & Tax Optimization Strategy\n73: \t- ### TODO Record Keeping System Setup\n74: \t- [ ] **Priority**: CRITICAL\n75: \t- [ ] Implement meticulous expense tracking system",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 79,
      "marker_type": "TODO",
      "content": "Annual Tax Planning",
      "context": "77: \t- [ ] Set up digital filing for receipts, invoices, bank statements\n78: \t- [ ] Maintain records for minimum 6 years (HMRC requirement)\n79: \t- ### TODO Annual Tax Planning\n80: \t- [ ] **Every January**: Prepare self-assessment tax return\n81: \t- [ ] Calculate and record rental losses for carry-forward",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 85,
      "marker_type": "TODO",
      "content": "Allowable Expense Optimization",
      "context": "83: \t- [ ] Apply for 20% tax credit on mortgage interest payments\n84: \t- [ ] **Action**: Consult qualified tax advisor/accountant\n85: \t- ### TODO Allowable Expense Optimization\n86: \t- #### Immediate Deductible Expenses:\n87: \t- [ ] Track general maintenance and repairs",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 102,
      "marker_type": "TODO",
      "content": "Insurance & Professional Support",
      "context": "100: \t  ---\n101: \t- ## Phase 4: Additional Protections & Best Practices\n102: \t- ### TODO Insurance & Professional Support\n103: \t- [ ] **Priority**: HIGH\n104: \t- [ ] Obtain comprehensive landlord insurance (not provided by OpenRent)",
      "severity": "warning"
    },
    {
      "file": "data/pages/house.md",
      "line_number": 108,
      "marker_type": "TODO",
      "content": "Monitoring & Updates",
      "context": "106: \t- [ ] Establish relationship with qualified tax advisor\n107: \t- [ ] Join landlord association (NRLA) for ongoing support\n108: \t- ### TODO Monitoring & Updates\n109: \t- [ ] **Monthly**: Review OpenRent account for compliance confirmations\n110: \t- [ ] **Quarterly**: Check for landlord legislation updates",
      "severity": "warning"
    },
    {
      "file": "data/pages/Metaverse Ontology.md",
      "line_number": 380,
      "marker_type": "TODO",
      "content": "",
      "context": "378: \t\t\t\t\t  ```\n379: \t\t\t\t\t\t- In this example, the custom property `metaverse:minCardinality` is used to indicate that a `MetaverseScene` must have at least one `SceneObject`.\n380: \t\t\t- ## TODO\n381: \t\t\t\t- **AI Agent Capabilities:** Expand the capabilities property of `AIAgent` to include specific actions and functions related to Bitcoin, RGB, and Nostr, such as \"create_digital_asset\", \"transfer_ownership\", \"publish_nostr_event\", etc.\n382: \t\t\t\t- **Event Logging and Attestation:** Consider adding mechanisms for logging significant events and generating cryptographic attestations, which could be used for dispute resolution or auditing purposes. This would operate on an automated threshold trigger system mediated by LLM, and would wrap the recent interactions between parties in pubkey encrypted data blobs, sending them to both parties alongside a report of the trigger event. This would potentially allow action by the parties in their jurisdictions. The data would then be deleted from the metaverse.",
      "severity": "warning"
    },
    {
      "file": "data/pages/Training for Design Practitioners.md",
      "line_number": 127,
      "marker_type": "TODO",
      "content": "Vectorworks Plan Visualisation",
      "context": "125: \t\t\t- [AI QR Code Generator - Create Free QR in 2025](https://qrcode-ai.com/)\n126: - co-working engine (not quite sure what that one means)\n127: - # TODO Vectorworks Plan Visualisation\n128: \t- Modern AI can transform 2D Vectorworks plans into client-ready visuals in five key formats.\n129: \t- ## Renderings from Plan Drawings",
      "severity": "warning"
    },
    {
      "file": "data/pages/Nostr protocol.md",
      "line_number": 65,
      "marker_type": "TODO",
      "content": "nostr IoT",
      "context": "63: \t\t- nostr implicitly enables all of the features and functionality of PKI infrastructure. Nostr keys can be used for many purposes\n64: \t\t- [kind-0/nsecbunkerd: nsecbunker daemon (github.com)](https://github.com/kind-0/nsecbunkerd)\n65: \t\t- ### TODO nostr IoT\n66: \t\t\t- [[MUST]]\n67: \t\t\t-",
      "severity": "warning"
    },
    {
      "file": "data/pages/Transformer.md",
      "line_number": 198,
      "marker_type": "TODO",
      "content": "Vectorworks Plan Visualisation",
      "context": "196: \t\t\t\t\t- The paper reveals a key difference between SSMs and Transformers in how they implement multi-head computations. Mamba2, like its predecessor, adopts a \"multi-value attention\" (MVA) pattern, where the expansion and contraction matrices (B and C) are shared across all heads. This pattern, a natural outcome of the SSM formulation, stands in contrast to the multi-query attention (MQA) pattern commonly used in Transformers. Empirical evidence suggests that the MVA pattern, unique to SSMs, might contribute to Mamba2's strong performance.\n197: \n198: - # TODO Vectorworks Plan Visualisation\n199: \t- Modern AI can transform 2D Vectorworks plans into client-ready visuals in five key formats.\n200: ",
      "severity": "warning"
    },
    {
      "file": "data/pages/Transformer.md",
      "line_number": 243,
      "marker_type": "TODO",
      "content": "Vectorworks Plan Visualisation",
      "context": "241: \t\t\t\t\t- The paper reveals a key difference between SSMs and Transformers in how they implement multi-head computations. Mamba2, like its predecessor, adopts a \"multi-value attention\" (MVA) pattern, where the expansion and contraction matrices (B and C) are shared across all heads. This pattern, a natural outcome of the SSM formulation, stands in contrast to the multi-query attention (MQA) pattern commonly used in Transformers. Empirical evidence suggests that the MVA pattern, unique to SSMs, might contribute to Mamba2's strong performance.\n242: \n243: - # TODO Vectorworks Plan Visualisation\n244: \t- Modern AI can transform 2D Vectorworks plans into client-ready visuals in five key formats.\n245: ",
      "severity": "warning"
    },
    {
      "file": "docs/fixes/actor-handlers.md",
      "line_number": 91,
      "marker_type": "TODO",
      "content": "Replace with actual GPU kernel call",
      "context": "89:             // Power iteration method\n90:             while iterations < max_iterations && !converged {\n91:                 // TODO: Replace with actual GPU kernel call\n92:                 // For now, use CPU fallback\n93:                 std::mem::swap(&mut pagerank_values, &mut prev_values);",
      "severity": "warning"
    },
    {
      "file": "docs/reference/code-quality-status.md",
      "line_number": 257,
      "marker_type": "TODO",
      "content": "Implement auto-zoom logic based on camera distance",
      "context": "255: **File:** `client/src/features/visualisation/components/ControlPanel/SemanticZoomControls.tsx:46`\n256: ```typescript\n257: // TODO: Implement auto-zoom logic based on camera distance\n258: ```\n259: **Impact:** LOW",
      "severity": "warning"
    },
    {
      "file": "docs/reference/code-quality-status.md",
      "line_number": 267,
      "marker_type": "TODO",
      "content": "Map these hardcoded values to settings system",
      "context": "265: **File:** `client/src/features/visualisation/components/HolographicDataSphere.tsx:27`\n266: ```typescript\n267: // TODO: Map these hardcoded values to settings system\n268: ```\n269: **Impact:** LOW",
      "severity": "warning"
    },
    {
      "file": "docs/analysis/gpu-implementation-audit.md",
      "line_number": 840,
      "marker_type": "TODO",
      "content": "Map advanced params to actual kernel parameters",
      "context": "838: \n839: ```rust\n840: // TODO: Map advanced params to actual kernel parameters\n841: if msg.params.semantic_force_weight > 0.0 {\n842:     self.unified_params.temperature *= msg.params.semantic_force_weight;",
      "severity": "warning"
    },
    {
      "file": "docs/analysis/gpu-implementation-audit.md",
      "line_number": 1052,
      "marker_type": "TODO",
      "content": "s Found:",
      "context": "1050: 1. **Stress Majorization** - Two CUDA files (stress_majorization.cu, unified_stress_majorization.cu)\n1051: \n1052: ### TODOs Found:\n1053: 1. `semantic_forces.rs` line 366 - \"In production, this would call CUDA kernels\"\n1054: 2. `neo4j_ontology_repository.rs` line 657 - \"Calculate from hierarchy traversal\"",
      "severity": "warning"
    },
    {
      "file": "docs/features/client-side-filtering.md",
      "line_number": 210,
      "marker_type": "TODO",
      "content": "Load saved filter from Neo4j",
      "context": "208:         // ... set client.pubkey, client.is_power_user\n209: \n210:         // TODO: Load saved filter from Neo4j\n211: \n212:         // Recompute if filter enabled",
      "severity": "warning"
    },
    {
      "file": "docs/features/client-side-filtering.md",
      "line_number": 234,
      "marker_type": "TODO",
      "content": "Save to Neo4j",
      "context": "232:         recompute_filtered_nodes(&mut client.filter, &graph_data);\n233: \n234:         // TODO: Save to Neo4j\n235:     }\n236: }",
      "severity": "warning"
    },
    {
      "file": "docs/features/client-side-filtering.md",
      "line_number": 271,
      "marker_type": "TODO",
      "content": "",
      "context": "269: This ensures nodes without explicit scores are treated neutrally.\n270: \n271: ## TODO\n272: \n273: ### Neo4j Persistence",
      "severity": "warning"
    },
    {
      "file": "docs/guides/ontology-reasoning-integration.md",
      "line_number": 65,
      "marker_type": "TODO",
      "content": "Add OntologyReasoningService to OntologyActor state",
      "context": "63: ```rust\n64: impl Handler<TriggerReasoning> for OntologyActor {\n65:     // TODO: Add OntologyReasoningService to OntologyActor state\n66:     // TODO: Call reasoning-service.infer-axioms(&ontology-id).await\n67:     // TODO: Broadcast OntologyUpdated event to EventBus",
      "severity": "warning"
    },
    {
      "file": "docs/guides/ontology-reasoning-integration.md",
      "line_number": 66,
      "marker_type": "TODO",
      "content": "Call reasoning-service.infer-axioms(&ontology-id).await",
      "context": "64: impl Handler<TriggerReasoning> for OntologyActor {\n65:     // TODO: Add OntologyReasoningService to OntologyActor state\n66:     // TODO: Call reasoning-service.infer-axioms(&ontology-id).await\n67:     // TODO: Broadcast OntologyUpdated event to EventBus\n68:     // TODO: Store inferred axioms with user-defined=false",
      "severity": "warning"
    },
    {
      "file": "docs/guides/ontology-reasoning-integration.md",
      "line_number": 67,
      "marker_type": "TODO",
      "content": "Broadcast OntologyUpdated event to EventBus",
      "context": "65:     // TODO: Add OntologyReasoningService to OntologyActor state\n66:     // TODO: Call reasoning-service.infer-axioms(&ontology-id).await\n67:     // TODO: Broadcast OntologyUpdated event to EventBus\n68:     // TODO: Store inferred axioms with user-defined=false\n69: }",
      "severity": "warning"
    },
    {
      "file": "docs/guides/ontology-reasoning-integration.md",
      "line_number": 68,
      "marker_type": "TODO",
      "content": "Store inferred axioms with user-defined=false",
      "context": "66:     // TODO: Call reasoning-service.infer-axioms(&ontology-id).await\n67:     // TODO: Broadcast OntologyUpdated event to EventBus\n68:     // TODO: Store inferred axioms with user-defined=false\n69: }\n70: ```",
      "severity": "warning"
    },
    {
      "file": "docs/guides/ontology-reasoning-integration.md",
      "line_number": 110,
      "marker_type": "TODO",
      "content": "Broadcast OntologyUpdated event",
      "context": "108:                 info!(\"Inferred {} new axioms\", inferred.len());\n109: \n110:                 // TODO: Broadcast OntologyUpdated event\n111: \n112:                 Ok(format!(\"Inferred {} axioms\", inferred.len()))",
      "severity": "warning"
    },
    {
      "file": "docs/concepts/architecture/services-architecture.md",
      "line_number": 3518,
      "marker_type": "TODO",
      "content": "Actually send notification (email, webhook, etc.)",
      "context": "3516:                 sync_event.total_nodes);\n3517: \n3518:             // TODO: Actually send notification (email, webhook, etc.)\n3519: \n3520:             Ok(())",
      "severity": "warning"
    },
    {
      "file": "archive/phase-5-reports-2025-11-06/PHASE-5-VALIDATION-REPORT.md",
      "line_number": 1278,
      "marker_type": "TODO",
      "content": "marker detection",
      "context": "1276: find docs -name \"*.md\" -exec head -5 {} \\; | grep -c \"^---$\"\n1277: \n1278: # TODO marker detection\n1279: find docs -name \"*.md\" -exec grep -l \"TODO\\|FIXME\" {} \\; | wc -l\n1280: ```",
      "severity": "warning"
    },
    {
      "file": "archive/archive/task-documents-2025-11-05/task-neo4j.md",
      "line_number": 1,
      "marker_type": "TODO",
      "content": "List: Full Migration to Neo4j",
      "context": "1: # Todo List: Full Migration to Neo4j\n2: \n3: This document outlines the tasks required to migrate the application's persistence layer entirely to Neo4j, deprecating all SQLite-based components. The tasks are designed to be executed in parallel where possible by a swarm of coding agents.",
      "severity": "warning"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/STUB_AND_DISCONNECTED_AUDIT.md",
      "line_number": 74,
      "marker_type": "TODO",
      "content": "Calculate actual components",
      "context": "72: \n73: ```rust\n74: connected_components: 1, // TODO: Calculate actual components\n75: ```\n76: ",
      "severity": "warning"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/STUB_AND_DISCONNECTED_AUDIT.md",
      "line_number": 109,
      "marker_type": "TODO",
      "content": "Implement once compilation is fixed`",
      "context": "107: **File:** `tests/neo4j_settings_integration_tests.rs`\n108: \n109: **Status:** All 28 tests are stubbed with `// TODO: Implement once compilation is fixed`\n110: \n111: **Test Categories:**",
      "severity": "warning"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/STUB_AND_DISCONNECTED_AUDIT.md",
      "line_number": 141,
      "marker_type": "TODO",
      "content": "Implement when endpoint is available`",
      "context": "139: \n140: **Impact:** MEDIUM\n141: **Status:** All marked with `// TODO: Implement when endpoint is available`\n142: **Recommendation:** Implement after ontology handler endpoints are finalized.\n143: ",
      "severity": "warning"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/STUB_AND_DISCONNECTED_AUDIT.md",
      "line_number": 168,
      "marker_type": "TODO",
      "content": "Implement when CUDA analyzer is ready`",
      "context": "166: \n167: **Files:**\n168: - `tests/ports/test_gpu_semantic_analyzer.rs` - 4 lines, `// TODO: Implement when CUDA analyzer is ready`\n169: - `tests/ports/test_gpu_physics_adapter.rs` - 4 lines, `// TODO: Implement when CUDA adapter is ready`\n170: - `tests/ports/test_inference_engine.rs` - 4 lines, `// TODO: Implement when InferenceEngine adapter is ready`",
      "severity": "warning"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/STUB_AND_DISCONNECTED_AUDIT.md",
      "line_number": 169,
      "marker_type": "TODO",
      "content": "Implement when CUDA adapter is ready`",
      "context": "167: **Files:**\n168: - `tests/ports/test_gpu_semantic_analyzer.rs` - 4 lines, `// TODO: Implement when CUDA analyzer is ready`\n169: - `tests/ports/test_gpu_physics_adapter.rs` - 4 lines, `// TODO: Implement when CUDA adapter is ready`\n170: - `tests/ports/test_inference_engine.rs` - 4 lines, `// TODO: Implement when InferenceEngine adapter is ready`\n171: ",
      "severity": "warning"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/STUB_AND_DISCONNECTED_AUDIT.md",
      "line_number": 170,
      "marker_type": "TODO",
      "content": "Implement when InferenceEngine adapter is ready`",
      "context": "168: - `tests/ports/test_gpu_semantic_analyzer.rs` - 4 lines, `// TODO: Implement when CUDA analyzer is ready`\n169: - `tests/ports/test_gpu_physics_adapter.rs` - 4 lines, `// TODO: Implement when CUDA adapter is ready`\n170: - `tests/ports/test_inference_engine.rs` - 4 lines, `// TODO: Implement when InferenceEngine adapter is ready`\n171: \n172: **Impact:** LOW",
      "severity": "warning"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/STUB_AND_DISCONNECTED_AUDIT.md",
      "line_number": 290,
      "marker_type": "TODO",
      "content": "Implement auto-zoom logic based on camera distance",
      "context": "288: \n289: ```typescript\n290: // TODO: Implement auto-zoom logic based on camera distance\n291: ```\n292: ",
      "severity": "warning"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/STUB_AND_DISCONNECTED_AUDIT.md",
      "line_number": 303,
      "marker_type": "TODO",
      "content": "Map these hardcoded values to settings system",
      "context": "301: \n302: ```typescript\n303: // TODO: Map these hardcoded values to settings system\n304: ```\n305: ",
      "severity": "warning"
    }
  ],
  "fixmes": [
    {
      "file": "src/utils/binary_protocol.rs",
      "line_number": 28,
      "marker_type": "BUG",
      "content": "These constants truncate node IDs > 16383, causing collisions",
      "context": "26: \n27: // Node type flag constants for u16 (wire format v1 - DEPRECATED)\n28: // BUG: These constants truncate node IDs > 16383, causing collisions\n29: // FIXED: Use PROTOCOL_V2 with full u32 IDs for node_id > 16383\n30: const WIRE_V1_AGENT_FLAG: u16 = 0x8000; ",
      "severity": "error"
    },
    {
      "file": "src/utils/binary_protocol.rs",
      "line_number": 165,
      "marker_type": "BUG",
      "content": "Only supports node IDs 0-16383 (14 bits). IDs > 16383 get truncated!",
      "context": "163: //   - SSSP Parent: 4 bytes (i32)\n164: // Total: 34 bytes per node\n165: // BUG: Only supports node IDs 0-16383 (14 bits). IDs > 16383 get truncated!\n166: //\n167: // - Server format (BinaryNodeData - 28 bytes total):",
      "severity": "error"
    },
    {
      "file": "src/actors/gpu/stress_majorization_actor.rs",
      "line_number": 383,
      "marker_type": "FIXME",
      "content": "Type conflict - commented for compilation",
      "context": "381: }\n382: \n383: // FIXME: Type conflict - commented for compilation\n384: \n385: ",
      "severity": "error"
    },
    {
      "file": "scripts/whelk-rs/src/whelk/owl.rs",
      "line_number": 300,
      "marker_type": "FIXME",
      "content": "return placeholder identity class expression",
      "context": "298:         // ClassExpression::DataMaxCardinality { .. } => Default::default(),\n299:         // ClassExpression::DataExactCardinality { .. } => Default::default(),\n300:         _ => Default::default(), //FIXME return placeholder identity class expression\n301:     }\n302: }",
      "severity": "error"
    },
    {
      "file": "whelk-rs/src/whelk/owl.rs",
      "line_number": 300,
      "marker_type": "FIXME",
      "content": "return placeholder identity class expression",
      "context": "298:         // ClassExpression::DataMaxCardinality { .. } => Default::default(),\n299:         // ClassExpression::DataExactCardinality { .. } => Default::default(),\n300:         _ => Default::default(), //FIXME return placeholder identity class expression\n301:     }\n302: }",
      "severity": "error"
    },
    {
      "file": "multi-agent-docker/skills/docs-alignment/scripts/generate_report.py",
      "line_number": 263,
      "marker_type": "FIXME",
      "content": "s",
      "context": "261:             sections.append(self._format_table(['File', 'Line', 'Type', 'Content'], rows))\n262: \n263:         # FIXMEs\n264:         fixmes = stubs_report.get('fixmes', [])\n265:         if fixmes:",
      "severity": "error"
    },
    {
      "file": "multi-agent-docker/skills/docs-alignment/scripts/generate_report.py",
      "line_number": 266,
      "marker_type": "FIXME",
      "content": "s and Bugs\\n')",
      "context": "264:         fixmes = stubs_report.get('fixmes', [])\n265:         if fixmes:\n266:             sections.append('### FIXMEs and Bugs\\n')\n267:             rows = [[\n268:                 f.get('file', 'N/A'),",
      "severity": "error"
    },
    {
      "file": "docs/guides/development-workflow.md",
      "line_number": 185,
      "marker_type": "BUG",
      "content": "fix",
      "context": "183: # Types\n184: feat:      # New feature\n185: fix:       # Bug fix\n186: docs:      # Documentation changes\n187: style:     # Formatting, missing semicolons, etc",
      "severity": "error"
    },
    {
      "file": "docs/guides/developer/01-development-setup.md",
      "line_number": 415,
      "marker_type": "BUG",
      "content": "fixes",
      "context": "413: git checkout -b feature/add-user-authentication\n414: \n415: # Bug fixes\n416: git checkout -b fix/resolve-upload-timeout\n417: ",
      "severity": "error"
    }
  ],
  "stubs": [
    {
      "file": "tests/cqrs_api_integration_tests.rs",
      "line_number": 237,
      "marker_type": "STUB:todo!(\"...\")",
      "content": "todo!(\"Implement when actor system test harness is available\")",
      "context": "235:     // (Would require full initialization in real tests)\n236:     pub async fn create_minimal_app_state() -> web::Data<AppState> {\n237:         todo!(\"Implement when actor system test harness is available\")\n238:     }\n239: ",
      "severity": "warning"
    },
    {
      "file": "multi-agent-docker/skills/docs-alignment/scripts/scan_stubs.py",
      "line_number": 61,
      "marker_type": "STUB:pass # stub",
      "content": "(re.compile(r'pass\\s*#.*stub'), 'warning', 'pass # stub'),",
      "context": "59:         'python': [\n60:             (re.compile(r'raise\\s+NotImplementedError'), 'error', 'NotImplementedError'),\n61:             (re.compile(r'pass\\s*#.*stub'), 'warning', 'pass # stub'),\n62:             (re.compile(r'\\.{3}\\s*#'), 'info', '... # ellipsis'),\n63:         ],",
      "severity": "warning"
    },
    {
      "file": "multi-agent-docker/skills/docs-alignment/scripts/scan_stubs.py",
      "line_number": 62,
      "marker_type": "STUB:... # ellipsis",
      "content": "(re.compile(r'\\.{3}\\s*#'), 'info', '... # ellipsis'),",
      "context": "60:             (re.compile(r'raise\\s+NotImplementedError'), 'error', 'NotImplementedError'),\n61:             (re.compile(r'pass\\s*#.*stub'), 'warning', 'pass # stub'),\n62:             (re.compile(r'\\.{3}\\s*#'), 'info', '... # ellipsis'),\n63:         ],\n64:         'typescript': [",
      "severity": "info"
    }
  ],
  "placeholders": [
    {
      "file": "examples/ontology_sync_example.rs",
      "line_number": 44,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "let enrichment_service = Arc::new(OntologyEnrichmentService::new(/* ... */));",
      "context": "42:         unimplemented!(\"Provide your KG repository\");\n43:     let onto_repo = Arc::new(Neo4jOntologyRepository::new(\"neo4j://localhost:7687\", \"neo4j\", \"password\"));\n44:     let enrichment_service = Arc::new(OntologyEnrichmentService::new(/* ... */));\n45: \n46:     let sync_service = LocalFileSyncService::new(",
      "severity": "info"
    },
    {
      "file": "src/handlers/schema_handler.rs",
      "line_number": 132,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "/// ...",
      "context": "130: /// - person (150 nodes)\n131: /// - organization (45 nodes)\n132: /// ...\n133: /// ```\n134: pub async fn get_llm_context(",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/docs-alignment/scripts/scan_stubs.py",
      "line_number": 83,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "(re.compile(r'/\\*\\s*\\.\\.\\.\\s*\\*/'), 'info', '/* ... */ placeholder'),",
      "context": "81:     PLACEHOLDER_PATTERNS = [\n82:         (re.compile(r'//\\s*\\.\\.\\.\\s*$'), 'info', '// ... placeholder'),\n83:         (re.compile(r'/\\*\\s*\\.\\.\\.\\s*\\*/'), 'info', '/* ... */ placeholder'),\n84:         (re.compile(r'#\\s*\\.\\.\\.\\s*$'), 'info', '# ... placeholder'),\n85:     ]",
      "severity": "info"
    },
    {
      "file": "task.md",
      "line_number": 158,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "156: \n157: pub struct GraphStateActor {\n158:     // ...\n159:     local_graph_db: GraphDB, // In-memory RuVector graph\n160: }",
      "severity": "info"
    },
    {
      "file": "PAGINATION_BUG_FIX.md",
      "line_number": 72,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "all_markdown_files.push(/* ... */);",
      "context": "70:         for file in files {\n71:             if file[\"type\"] == \"file\" && file[\"name\"].ends_with(\".md\") {\n72:                 all_markdown_files.push(/* ... */);\n73:             }\n74:         }",
      "severity": "info"
    },
    {
      "file": "docs/analytics-visualization-design.md",
      "line_number": 475,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "const bloomPass = new UnrealBloomPass(/* ... */);",
      "context": "473:     bloomComposer.renderToScreen = false;\n474: \n475:     const bloomPass = new UnrealBloomPass(/* ... */);\n476:     bloomComposer.addPass(new RenderPass(scene, camera));\n477:     bloomComposer.addPass(bloomPass);",
      "severity": "info"
    },
    {
      "file": "docs/settings-authentication.md",
      "line_number": 328,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "let app = test::init_service(/* ... */).await;",
      "context": "326:     #[actix_rt::test]\n327:     async fn test_authenticated_write_requires_token() {\n328:         let app = test::init_service(/* ... */).await;\n329: \n330:         // Without auth headers - should fail",
      "severity": "info"
    },
    {
      "file": "docs/settings-authentication.md",
      "line_number": 342,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "let app = test::init_service(/* ... */).await;",
      "context": "340:     #[actix_rt::test]\n341:     async fn test_anonymous_read_allowed() {\n342:         let app = test::init_service(/* ... */).await;\n343: \n344:         let req = test::TestRequest::get()",
      "severity": "info"
    },
    {
      "file": "docs/ruvector-integration-analysis.md",
      "line_number": 314,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "312:     // Generate constraints (clustering, separation, alignment, boundary)\n313:     let clustering_constraints = self.generate_clustering_constraints(&clusters)?;\n314:     // ...\n315: }\n316: ",
      "severity": "info"
    },
    {
      "file": "docs/ruvector-integration-analysis.md",
      "line_number": 387,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "385:                     node_ids: cluster_nodes.clone(),\n386:                     coherence: /* GNN-enhanced coherence */,\n387:                     // ...\n388:                 });\n389:                 processed.extend(cluster_nodes);",
      "severity": "info"
    },
    {
      "file": "docs/ruvector-integration-analysis.md",
      "line_number": 395,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "393:         // 4. Generate constraints with GNN-learned weights\n394:         let clustering_constraints = self.generate_gnn_clustering_constraints(&clusters, &gnn_embeddings)?;\n395:         // ...\n396:     }\n397: ",
      "severity": "info"
    },
    {
      "file": "docs/ruvector-integration-analysis.md",
      "line_number": 461,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "let features = /* ... */;",
      "context": "459:     pub fn analyze_metadata(&mut self, metadata: &Metadata) -> SemanticFeatures {\n460:         // Extract semantic features (same as current implementation)\n461:         let features = /* ... */;\n462: \n463:         // Store in HNSW index",
      "severity": "info"
    },
    {
      "file": "docs/ruvector-integration-analysis.md",
      "line_number": 1008,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "pub struct GraphCacheService { /* ... */ }",
      "context": "1006: ```rust\n1007: #[cfg(feature = \"ruvector\")]\n1008: pub struct GraphCacheService { /* ... */ }\n1009: ```\n1010: ",
      "severity": "info"
    },
    {
      "file": "data/markdown/implementation-examples.md",
      "line_number": 509,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "507: \t\t      // Only treasurers can withdraw\n508: \t\t      function withdraw(uint256 amount) external onlyRole(TREASURER_ROLE) {\n509: \t\t          // ...\n510: \t\t      }\n511: \t\t  ",
      "severity": "info"
    },
    {
      "file": "data/markdown/implementation-examples.md",
      "line_number": 514,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "512: \t\t      // Only auditors can view sensitive data\n513: \t\t      function getAuditLog() external view onlyRole(AUDITOR_ROLE) returns (bytes32[] memory) {\n514: \t\t          // ...\n515: \t\t      }\n516: \t\t  }",
      "severity": "info"
    },
    {
      "file": "data/pages/k8 virtualisation.md",
      "line_number": 676,
      "marker_type": "PLACEHOLDER:# ... placeholder",
      "content": "# ...",
      "context": "674:   \n675:   # Perform maintenance\n676:   # ...\n677:   \n678:   # Restore node",
      "severity": "info"
    },
    {
      "file": "docs/fixes/before-after-comparison.md",
      "line_number": 34,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "32: let gpu_result = unified_compute.execute_physics_step(sim_params);\n33: \n34: // ...\n35: \n36: if let Err(e) = shared_context.update_utilization(gpu_utilization) {  // \u2190 immutable borrow still used",
      "severity": "info"
    },
    {
      "file": "docs/fixes/before-after-comparison.md",
      "line_number": 71,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "69: let gpu_result = unified_compute.execute_physics_step(sim_params);\n70: \n71: // ...\n72: \n73: if let Err(e) = shared_context.update_utilization(gpu_utilization) {  // \u2705 No conflict",
      "severity": "info"
    },
    {
      "file": "docs/fixes/before-after-comparison.md",
      "line_number": 362,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "360: \n361:     let mut unified_compute = match &self.shared_context {  // \u2190 Lock acquired\n362:         // ...\n363:     };\n364: ",
      "severity": "info"
    },
    {
      "file": "docs/fixes/before-after-comparison.md",
      "line_number": 370,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "368:     self.update_stats(false, computation_time);  // \u274c ERROR: lock still held\n369: \n370:     // ...\n371: }  // \u2190 Lock dropped here\n372: ```",
      "severity": "info"
    },
    {
      "file": "docs/fixes/before-after-comparison.md",
      "line_number": 382,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "380:     let (apsp_distances, num_nodes, landmarks, computation_time) = {\n381:         let mut unified_compute = match &self.shared_context {\n382:             // ...\n383:         };\n384: ",
      "severity": "info"
    },
    {
      "file": "docs/fixes/before-after-comparison.md",
      "line_number": 395,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "393:     self.update_stats(false, computation_time);\n394: \n395:     // ...\n396: }\n397: ```",
      "severity": "info"
    },
    {
      "file": "docs/fixes/technical-details.md",
      "line_number": 50,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "48: \n49: // 3. Use shared context\n50: // ...\n51: shared_context.update_utilization(gpu_utilization);\n52: ```",
      "severity": "info"
    },
    {
      "file": "docs/reference/semantic-physics-implementation.md",
      "line_number": 323,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "321:     axioms: &[crate::reasoning::custom-reasoner::InferredAxiom],\n322: ) -> Result<ConstraintSet, String> {\n323:     // ...\n324:     for axiom in axioms {\n325:         match axiom.axiom-type {",
      "severity": "info"
    },
    {
      "file": "docs/reference/semantic-physics-implementation.md",
      "line_number": 546,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "544:         // Step 3: Generate constraints WITH graph data\n545:         match self.generate-constraints-from-axioms(&axioms, &graph-data).await {\n546:             // ...\n547:         }\n548:     }",
      "severity": "info"
    },
    {
      "file": "docs/reference/semantic-physics-implementation.md",
      "line_number": 650,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "648: pub struct AdvancedParams {\n649:     pub semantic-force-weight: f32,  // Global multiplier (default: 0.6)\n650:     // ...\n651: }\n652: ",
      "severity": "info"
    },
    {
      "file": "docs/reference/semantic-physics-implementation.md",
      "line_number": 831,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "void launch-disjoint-classes-kernel(/*...*/);      // Line 427",
      "context": "829: ```c\n830: extern \"C\" {\n831:     void launch-disjoint-classes-kernel(/*...*/);      // Line 427\n832:     void launch-subclass-hierarchy-kernel(/*...*/);    // Line 439\n833:     void launch-sameas-colocate-kernel(/*...*/);       // Line 451",
      "severity": "info"
    },
    {
      "file": "docs/reference/semantic-physics-implementation.md",
      "line_number": 832,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "void launch-subclass-hierarchy-kernel(/*...*/);    // Line 439",
      "context": "830: extern \"C\" {\n831:     void launch-disjoint-classes-kernel(/*...*/);      // Line 427\n832:     void launch-subclass-hierarchy-kernel(/*...*/);    // Line 439\n833:     void launch-sameas-colocate-kernel(/*...*/);       // Line 451\n834:     void launch-inverse-symmetry-kernel(/*...*/);      // Line 463",
      "severity": "info"
    },
    {
      "file": "docs/reference/semantic-physics-implementation.md",
      "line_number": 833,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "void launch-sameas-colocate-kernel(/*...*/);       // Line 451",
      "context": "831:     void launch-disjoint-classes-kernel(/*...*/);      // Line 427\n832:     void launch-subclass-hierarchy-kernel(/*...*/);    // Line 439\n833:     void launch-sameas-colocate-kernel(/*...*/);       // Line 451\n834:     void launch-inverse-symmetry-kernel(/*...*/);      // Line 463\n835:     void launch-functional-cardinality-kernel(/*...*/);// Line 475",
      "severity": "info"
    },
    {
      "file": "docs/reference/semantic-physics-implementation.md",
      "line_number": 834,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "void launch-inverse-symmetry-kernel(/*...*/);      // Line 463",
      "context": "832:     void launch-subclass-hierarchy-kernel(/*...*/);    // Line 439\n833:     void launch-sameas-colocate-kernel(/*...*/);       // Line 451\n834:     void launch-inverse-symmetry-kernel(/*...*/);      // Line 463\n835:     void launch-functional-cardinality-kernel(/*...*/);// Line 475\n836: }",
      "severity": "info"
    },
    {
      "file": "docs/reference/semantic-physics-implementation.md",
      "line_number": 835,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "void launch-functional-cardinality-kernel(/*...*/);// Line 475",
      "context": "833:     void launch-sameas-colocate-kernel(/*...*/);       // Line 451\n834:     void launch-inverse-symmetry-kernel(/*...*/);      // Line 463\n835:     void launch-functional-cardinality-kernel(/*...*/);// Line 475\n836: }\n837: ```",
      "severity": "info"
    },
    {
      "file": "docs/specialized/client-typescript-architecture.md",
      "line_number": 93,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "export const useSettingsStore = create<SettingsState>()(/*...*/);",
      "context": "91: \n92: // Stores use Zustand\n93: export const useSettingsStore = create<SettingsState>()(/*...*/);\n94: ```\n95: ",
      "severity": "info"
    },
    {
      "file": "docs/specialized/client-typescript-architecture.md",
      "line_number": 274,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "class ErrorBoundary extends React.Component {/*...*/}",
      "context": "272: \n273: // Error boundaries\n274: class ErrorBoundary extends React.Component {/*...*/}\n275: ```\n276: ",
      "severity": "info"
    },
    {
      "file": "docs/specialized/client-typescript-architecture.md",
      "line_number": 933,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "return <Slider value={ambientLight} onChange={/*...*/} />;",
      "context": "931:   const ambientLight = settingsStore.get('visualisation.rendering.ambientLightIntensity');\n932: \n933:   return <Slider value={ambientLight} onChange={/*...*/} />;\n934: }\n935: ```",
      "severity": "info"
    },
    {
      "file": "docs/guides/ontology-reasoning-integration.md",
      "line_number": 290,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "288:         label: Some(\"Person\".to-string()),\n289:         parent-classes: vec![],\n290:         // ...\n291:     },\n292:     OwlClass {",
      "severity": "info"
    },
    {
      "file": "docs/guides/ontology-reasoning-integration.md",
      "line_number": 296,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "294:         label: Some(\"Employee\".to-string()),\n295:         parent-classes: vec![\"http://example.org/Person\".to-string()],\n296:         // ...\n297:     },\n298: ];",
      "severity": "info"
    },
    {
      "file": "docs/guides/ontology-reasoning-integration.md",
      "line_number": 305,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "303:         subject: \"http://example.org/Employee\".to-string(),\n304:         object: \"http://example.org/Person\".to-string(),\n305:         // ...\n306:     },\n307: ];",
      "severity": "info"
    },
    {
      "file": "docs/guides/neo4j-migration.md",
      "line_number": 109,
      "marker_type": "PLACEHOLDER:# ... placeholder",
      "content": "# ...",
      "context": "107: # INFO  Neo4j URI: bolt://localhost:7687\n108: # INFO  Dry run: true\n109: # ...\n110: # [DRY RUN] Would migrate: visualisation.theme = \"dark\"\n111: # [DRY RUN] Would migrate: system.port = 8080",
      "severity": "info"
    },
    {
      "file": "docs/guides/neo4j-migration.md",
      "line_number": 112,
      "marker_type": "PLACEHOLDER:# ... placeholder",
      "content": "# ...",
      "context": "110: # [DRY RUN] Would migrate: visualisation.theme = \"dark\"\n111: # [DRY RUN] Would migrate: system.port = 8080\n112: # ...\n113: # DRY RUN COMPLETE - No changes were made\n114: ```",
      "severity": "info"
    },
    {
      "file": "docs/guides/neo4j-migration.md",
      "line_number": 134,
      "marker_type": "PLACEHOLDER:# ... placeholder",
      "content": "# ...",
      "context": "132: # DEBUG \u2705 Migrated: visualisation.theme\n133: # DEBUG \u2705 Migrated: system.port\n134: # ...\n135: # INFO  \u2705 MIGRATION COMPLETE\n136: ```",
      "severity": "info"
    },
    {
      "file": "docs/guides/neo4j-implementation-roadmap.md",
      "line_number": 534,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "532:    ```rust\n533:    pub struct AppState {\n534:        // ...\n535:        pub settings_repository: Arc<Neo4jSettingsRepository>,  // Updated type\n536:        // ...",
      "severity": "info"
    },
    {
      "file": "docs/guides/neo4j-implementation-roadmap.md",
      "line_number": 536,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "534:        // ...\n535:        pub settings_repository: Arc<Neo4jSettingsRepository>,  // Updated type\n536:        // ...\n537:    }\n538:    ```",
      "severity": "info"
    },
    {
      "file": "docs/guides/development-workflow.md",
      "line_number": 566,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "564: // \u274c Bad: Implicit any, no proper typing\n565: export const AgentNode = ({ agent, onSelect }) => {\n566:   // ...\n567: };\n568: ```",
      "severity": "info"
    },
    {
      "file": "docs/concepts/ontology-typed-system.md",
      "line_number": 202,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "200: \n201:         // Process results into NodeTypeSchema\n202:         // ...\n203:     }\n204: ",
      "severity": "info"
    },
    {
      "file": "docs/concepts/ontology-typed-system.md",
      "line_number": 215,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "213: \n214:         // Process results\n215:         // ...\n216:     }\n217: }",
      "severity": "info"
    },
    {
      "file": "docs/concepts/hierarchical-visualization.md",
      "line_number": 560,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "558:   depth0: 0xff00ff, // Purple roots\n559:   depth1: 0xff33ff,\n560:   // ...\n561: };\n562: ",
      "severity": "info"
    },
    {
      "file": "docs/concepts/architecture/cqrs-directive-template.md",
      "line_number": 697,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "695: pub async fn upload-and-process(\n696:     state: web::Data<AppState>,\n697:     // ...\n698: ) -> impl Responder {\n699:     // \u274c Old way: Send actor message",
      "severity": "info"
    },
    {
      "file": "docs/concepts/architecture/cqrs-directive-template.md",
      "line_number": 711,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "709: pub async fn upload-and-process(\n710:     state: web::Data<AppState>,\n711:     // ...\n712: ) -> impl Responder {\n713:     // \u2705 New way: Use directive handler",
      "severity": "info"
    },
    {
      "file": "docs/concepts/architecture/cqrs-directive-template.md",
      "line_number": 759,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "757:     pub graph-query-handlers: GraphQueryHandlers,  // \u2705 Already exists\n758:     pub graph-directive-handlers: GraphDirectiveHandlers,  // \ud83d\udd27 Add this\n759:     // ...\n760: }\n761: ",
      "severity": "info"
    },
    {
      "file": "docs/concepts/architecture/ports/04-ontology-repository.md",
      "line_number": 265,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "inferred-axioms: vec![/* ... */],",
      "context": "263: let results = InferenceResults {\n264:     timestamp: Utc::now(),\n265:     inferred-axioms: vec![/* ... */],\n266:     inference-time-ms: 1500,\n267:     reasoner-version: \"whelk-0.1.0\".to-string(),",
      "severity": "info"
    },
    {
      "file": "docs/guides/developer/json-serialization-patterns.md",
      "line_number": 361,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "rendering: { /* ... */ },",
      "context": "359:     }\n360:   },\n361:   rendering: { /* ... */ },\n362:   interaction: { /* ... */ }\n363: };",
      "severity": "info"
    },
    {
      "file": "docs/guides/developer/json-serialization-patterns.md",
      "line_number": 362,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "interaction: { /* ... */ }",
      "context": "360:   },\n361:   rendering: { /* ... */ },\n362:   interaction: { /* ... */ }\n363: };\n364: ",
      "severity": "info"
    },
    {
      "file": "tests/fixtures/README.md",
      "line_number": 273,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "fn test_person_class_validation() { /* ... */ }",
      "context": "271: \n272:     #[test]\n273:     fn test_person_class_validation() { /* ... */ }\n274: \n275:     #[test]",
      "severity": "info"
    },
    {
      "file": "tests/fixtures/README.md",
      "line_number": 276,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "fn test_employment_relationship() { /* ... */ }",
      "context": "274: \n275:     #[test]\n276:     fn test_employment_relationship() { /* ... */ }\n277: }\n278: ```",
      "severity": "info"
    },
    {
      "file": "archive/phase-5-reports-2025-11-06/COMPILATION_ERROR_RESOLUTION_COMPLETE.md",
      "line_number": 345,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "343: let axiom = match owl_axiom_type {\n344:     \"EquivalentClasses\" => AxiomType::EquivalentClass,  // Singular form\n345:     // ...\n346: }\n347: ```",
      "severity": "info"
    },
    {
      "file": "archive/phase-5-reports-2025-11-06/COMPILATION_ERROR_RESOLUTION_COMPLETE.md",
      "line_number": 355,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "Box::pin(async { /* ... */ })",
      "context": "353:     type Result = ResponseFuture<Result<MyResponse, String>>;  // Must match!\n354:     fn handle(&mut self, msg: MyMessage, _ctx: &mut Context<Self>) -> Self::Result {\n355:         Box::pin(async { /* ... */ })\n356:     }\n357: }",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_CONTINUED_UPGRADES.md",
      "line_number": 42,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "pub fn per_minute(max_requests: usize) -> Self { /* ... */ }",
      "context": "40: \n41: impl RateLimit {\n42:     pub fn per_minute(max_requests: usize) -> Self { /* ... */ }\n43:     pub fn per_hour(max_requests: usize) -> Self { /* ... */ }\n44:     pub fn per_second(max_requests: usize) -> Self { /* ... */ }",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_CONTINUED_UPGRADES.md",
      "line_number": 43,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "pub fn per_hour(max_requests: usize) -> Self { /* ... */ }",
      "context": "41: impl RateLimit {\n42:     pub fn per_minute(max_requests: usize) -> Self { /* ... */ }\n43:     pub fn per_hour(max_requests: usize) -> Self { /* ... */ }\n44:     pub fn per_second(max_requests: usize) -> Self { /* ... */ }\n45: }",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_CONTINUED_UPGRADES.md",
      "line_number": 44,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "pub fn per_second(max_requests: usize) -> Self { /* ... */ }",
      "context": "42:     pub fn per_minute(max_requests: usize) -> Self { /* ... */ }\n43:     pub fn per_hour(max_requests: usize) -> Self { /* ... */ }\n44:     pub fn per_second(max_requests: usize) -> Self { /* ... */ }\n45: }\n46: ```",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_CONTINUED_UPGRADES.md",
      "line_number": 196,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "impl From<serde_json::Error> for VisionFlowError { /* ... */ }",
      "context": "194: ```rust\n195: // serde_json::Error automatically converts to ParseError::JSON\n196: impl From<serde_json::Error> for VisionFlowError { /* ... */ }\n197: \n198: // All new error types convert to VisionFlowError",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_CONTINUED_UPGRADES.md",
      "line_number": 199,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "impl From<DatabaseError> for VisionFlowError { /* ... */ }",
      "context": "197: \n198: // All new error types convert to VisionFlowError\n199: impl From<DatabaseError> for VisionFlowError { /* ... */ }\n200: impl From<ValidationError> for VisionFlowError { /* ... */ }\n201: impl From<ParseError> for VisionFlowError { /* ... */ }",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_CONTINUED_UPGRADES.md",
      "line_number": 200,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "impl From<ValidationError> for VisionFlowError { /* ... */ }",
      "context": "198: // All new error types convert to VisionFlowError\n199: impl From<DatabaseError> for VisionFlowError { /* ... */ }\n200: impl From<ValidationError> for VisionFlowError { /* ... */ }\n201: impl From<ParseError> for VisionFlowError { /* ... */ }\n202: ```",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_CONTINUED_UPGRADES.md",
      "line_number": 201,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "impl From<ParseError> for VisionFlowError { /* ... */ }",
      "context": "199: impl From<DatabaseError> for VisionFlowError { /* ... */ }\n200: impl From<ValidationError> for VisionFlowError { /* ... */ }\n201: impl From<ParseError> for VisionFlowError { /* ... */ }\n202: ```\n203: ",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/H4_MESSAGE_ACKNOWLEDGMENT_DESIGN.md",
      "line_number": 38,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "36: gpu_addr.do_send(InitializeGPU {\n37:     graph: Arc::clone(graph_data),\n38:     // ...\n39: });\n40: ```",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/H4_MESSAGE_ACKNOWLEDGMENT_DESIGN.md",
      "line_number": 336,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "334:         self.gpu_initialized = true;\n335:         self.gpu_init_in_progress = false;\n336:         // ...\n337:     }\n338: }",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/H2_COMPLETE_SUMMARY.md",
      "line_number": 187,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "185:         count: self.count.load(Ordering::Relaxed),\n186:         peak: self.peak.load(Ordering::Relaxed),\n187:         // ...\n188:     }\n189: }",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/H2_COMPLETE_SUMMARY.md",
      "line_number": 216,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "214: \n215:     // Proceed safely\n216:     // ...\n217: }\n218: ```",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_SECURITY_AUDIT.md",
      "line_number": 253,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "251:         self.spawn_child_actors(ctx)?;  // Safe - &mut self prevents concurrent access\n252:     }\n253:     // ...\n254: }\n255: ```",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_FINAL_UPGRADES.md",
      "line_number": 192,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "let state = AppState { /* ... */ };",
      "context": "190: ```rust\n191: // In AppState::new()\n192: let state = AppState { /* ... */ };\n193: \n194: // Validate before returning",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_SUMMARY_FINAL_UPGRADES.md",
      "line_number": 433,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "let state = Self { /* ... */ };",
      "context": "431: ```rust\n432: // In AppState::new()\n433: let state = Self { /* ... */ };\n434: \n435: let validation_report = state.validate();",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/ERROR_HANDLING_H2_PHASE3.md",
      "line_number": 36,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "} else { /* ... */ };",
      "context": "34:         panic!(\"Expected value to be present\")\n35:     })\n36: } else { /* ... */ };\n37: \n38: // After (safe):",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/ERROR_HANDLING_H2_PHASE3.md",
      "line_number": 47,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "} else { /* ... */ };",
      "context": "45:         }\n46:     }\n47: } else { /* ... */ };\n48: ```\n49: ",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/ERROR_HANDLING_H2_PHASE3.md",
      "line_number": 144,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "142: \n143:     // Proceed with transfer\n144:     // ...\n145: }\n146: ```",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/H4_PHASE1_IMPLEMENTATION.md",
      "line_number": 463,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "461: \n462:         Self {\n463:             // ...\n464:             message_tracker: tracker,\n465:         }",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/H5_H6_STATUS.md",
      "line_number": 156,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "let state = AppState { /* ... */ };",
      "context": "154: // From src/app_state.rs (H3)\n155: \n156: let state = AppState { /* ... */ };\n157: \n158: // Validate before returning",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_H4_PHASE1_SUMMARY.md",
      "line_number": 271,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "269:     pub total_sent: AtomicU64,  // \u2705 Zero lock contention\n270:     pub total_acked: AtomicU64,\n271:     // ...\n272: }\n273: ```",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_H4_PHASE1_SUMMARY.md",
      "line_number": 440,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "438:         Self {\n439:             message_tracker: tracker,\n440:             // ...\n441:         }\n442:     }",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/SESSION_FINAL_SUMMARY.md",
      "line_number": 81,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "let state = AppState { /* ... */ };",
      "context": "79: **Integration:**\n80: ```rust\n81: let state = AppState { /* ... */ };\n82: let report = state.validate();\n83: report.log();",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/ERROR_HANDLING_H2_PHASE2.md",
      "line_number": 65,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "for cap in class_pattern.captures_iter(section) { /* ... */ }",
      "context": "63:     let class_pattern = regex::Regex::new(r\"owl:?_?class::...\")\n64:         .expect(\"Invalid regex pattern\");\n65:     for cap in class_pattern.captures_iter(section) { /* ... */ }\n66: }\n67: ",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/ERROR_HANDLING_H2_PHASE2.md",
      "line_number": 76,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "for cap in CLASS_PATTERN.captures_iter(section) { /* ... */ }",
      "context": "74: \n75: fn extract_classes(&self, section: &str, filename: &str) -> Vec<OwlClass> {\n76:     for cap in CLASS_PATTERN.captures_iter(section) { /* ... */ }\n77: }\n78: ```",
      "severity": "info"
    },
    {
      "file": "archive/working-notes-2025-11-06/ERROR_HANDLING_H2_PHASE2.md",
      "line_number": 177,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "for cap in PATTERN.captures_iter(text) { /* ... */ }",
      "context": "175: \n176: // Use directly:\n177: for cap in PATTERN.captures_iter(text) { /* ... */ }\n178: ```\n179: ",
      "severity": "info"
    },
    {
      "file": "archive/multi-agent-docker-isolated-docs-2025-11-05/docs/releases/HOTFIX-v1.2.1.md",
      "line_number": 204,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "202: ```typescript\n203: 'deepseek-r1': {\n204:   // ...\n205:   supports_tools: false, // DeepSeek R1 does NOT support tool/function calling\n206:   weaknesses: ['newer-model', 'no-tool-use'],",
      "severity": "info"
    },
    {
      "file": "archive/multi-agent-docker-isolated-docs-2025-11-05/docs/releases/HOTFIX-v1.2.1.md",
      "line_number": 207,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "205:   supports_tools: false, // DeepSeek R1 does NOT support tool/function calling\n206:   weaknesses: ['newer-model', 'no-tool-use'],\n207:   // ...\n208: },\n209: 'deepseek-chat-v3': {",
      "severity": "info"
    },
    {
      "file": "archive/multi-agent-docker-isolated-docs-2025-11-05/docs/releases/HOTFIX-v1.2.1.md",
      "line_number": 210,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "208: },\n209: 'deepseek-chat-v3': {\n210:   // ...\n211:   supports_tools: true,\n212:   // ...",
      "severity": "info"
    },
    {
      "file": "archive/multi-agent-docker-isolated-docs-2025-11-05/docs/releases/HOTFIX-v1.2.1.md",
      "line_number": 212,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "210:   // ...\n211:   supports_tools: true,\n212:   // ...\n213: },\n214: // All other models: supports_tools: true (except local ONNX)",
      "severity": "info"
    },
    {
      "file": "archive/working-docs-2025-11-06/audits/COMPREHENSIVE_AUDIT_REPORT.md",
      "line_number": 91,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "89:     // NO AUTH CHECK - anyone can load ontologies\n90:     let ontology_actor = state.ontology_actor_addr.as_ref();\n91:     // ...\n92: }\n93: ```",
      "severity": "info"
    },
    {
      "file": "archive/working-docs-2025-11-06/audits/COMPREHENSIVE_AUDIT_REPORT.md",
      "line_number": 208,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "206:     stress_solver: StressMajorizationSolver,\n207:     stress_step_counter: u32,\n208:     // ...\n209: \n210:     // GPU integration (3 fields)",
      "severity": "info"
    },
    {
      "file": "archive/working-docs-2025-11-06/audits/COMPREHENSIVE_AUDIT_REPORT.md",
      "line_number": 228,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "226:     auto_balance_history: Vec<f32>,\n227:     kinetic_energy_history: Vec<f32>,\n228:     // ...\n229: \n230:     // Message queue (4 fields)",
      "severity": "info"
    },
    {
      "file": "archive/working-docs-2025-11-06/audits/COMPREHENSIVE_AUDIT_REPORT.md",
      "line_number": 234,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "232:     queue_config: UpdateQueueConfig,\n233:     pending_broadcasts: u32,\n234:     // ...\n235: \n236:     // 15+ more fields...",
      "severity": "info"
    },
    {
      "file": "archive/working-docs-2025-11-06/audits/COMPREHENSIVE_AUDIT_REPORT.md",
      "line_number": 838,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "836:    struct GPUManagerActor {\n837:        children: OnceCell<ChildActors>,\n838:        // ...\n839:    }\n840:    ```",
      "severity": "info"
    },
    {
      "file": "archive/working-docs-2025-11-06/completed-work/SSSP_VALIDATION_REPORT.md",
      "line_number": 430,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "428: \n429:     // A* with SSSP heuristic + semantic weighting\n430:     // ...\n431: }\n432: ```",
      "severity": "info"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/JSON_WEBSOCKET_AUDIT.md",
      "line_number": 83,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "81:   damping?: number;\n82:   spring_constant?: number;\n83:   // ...\n84: }\n85: ",
      "severity": "info"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/JSON_WEBSOCKET_AUDIT.md",
      "line_number": 120,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "118: interface SettingsState {\n119:   settings: DeepPartial<Settings>;\n120:   // ...\n121: }\n122: ",
      "severity": "info"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/JSON_WEBSOCKET_AUDIT.md",
      "line_number": 130,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "128:   \"spring_constant\": 1.0,\n129:   \"damping\": 0.8,\n130:   // ...\n131: }\n132: ```",
      "severity": "info"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/JSON_WEBSOCKET_AUDIT.md",
      "line_number": 468,
      "marker_type": "PLACEHOLDER:// ... placeholder",
      "content": "// ...",
      "context": "466: // Attempt 2: 6s\n467: // Attempt 3: 9s\n468: // ...\n469: // Max: 30s\n470: ```",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/docs-alignment/SKILL.md",
      "line_number": 143,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "5. Find partial code blocks (// ... or /* ... */)",
      "context": "141:     3. Find FIXME, HACK, XXX markers\n142:     4. Find stub functions (unimplemented!(), todo!())\n143:     5. Find partial code blocks (// ... or /* ... */)\n144:     6. Store results in memory: swarm/stub-scanner/results\n145: ",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/docs-alignment/SKILL.md",
      "line_number": 346,
      "marker_type": "PLACEHOLDER:/* ... */ placeholder",
      "content": "- `// ...` or `/* ... */` placeholder patterns",
      "context": "344: - `todo!()` macros (Rust)\n345: - `raise NotImplementedError` (Python)\n346: - `// ...` or `/* ... */` placeholder patterns\n347: \n348: **Arguments:**",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/wardley-maps/IMPLEMENTATION_GUIDE.md",
      "line_number": 188,
      "marker_type": "PLACEHOLDER:# ... placeholder",
      "content": "#   ...",
      "context": "186: #   'INNOVATION LEADERSHIP: Accelerate development...',\n187: #   'COMPETITIVE MOAT: Protect custom differentiators...',\n188: #   ...\n189: # ]\n190: ",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/import-to-ontology/INTEGRATION-SUMMARY.md",
      "line_number": 249,
      "marker_type": "PLACEHOLDER:# ... placeholder",
      "content": "# ...",
      "context": "247: #\n248: # \ud83d\udccb Test 1: Valid OWL2 File\n249: # ...\n250: # \u2705 Test PASSED: File is valid as expected\n251: #",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/import-to-ontology/INTEGRATION-SUMMARY.md",
      "line_number": 253,
      "marker_type": "PLACEHOLDER:# ... placeholder",
      "content": "# ...",
      "context": "251: #\n252: # \ud83d\udccb Test 2: Invalid OWL2 File (Should Detect Errors)\n253: # ...\n254: # \u2705 Test PASSED: Errors detected as expected\n255: #",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/ontology-enrich/skill.md",
      "line_number": 149,
      "marker_type": "PLACEHOLDER:# ... placeholder",
      "content": "# ...",
      "context": "147:     'citations': [\n148:         {'source': 'Source Name', 'url': 'https://...', 'relevance': 0.95},\n149:         # ...\n150:     ],\n151:     'related_concepts': ['Concept1', 'Concept2']",
      "severity": "info"
    }
  ],
  "other": [
    {
      "file": "tests/semantic_physics_integration_test.rs",
      "line_number": 6,
      "marker_type": "NOTE",
      "content": "These tests are designed to compile independently",
      "context": "4: #[cfg(test)]\n5: mod semantic_physics_tests {\n6:     // Note: These tests are designed to compile independently\n7:     // They verify the architecture design even if the main codebase has issues\n8: ",
      "severity": "info"
    },
    {
      "file": "tests/cqrs_api_integration_tests.rs",
      "line_number": 10,
      "marker_type": "NOTE",
      "content": "These tests require a running actor system which is complex to set up",
      "context": "8: use webxr::app_state::AppState;\n9: \n10: // Note: These tests require a running actor system which is complex to set up\n11: // They are marked as #[ignore] and serve as documentation for manual testing\n12: ",
      "severity": "info"
    },
    {
      "file": "tests/ontology_api_test.rs",
      "line_number": 72,
      "marker_type": "NOTE",
      "content": "Actual implementation may vary based on API design",
      "context": "70: \n71:         // The endpoint should accept the validation request\n72:         // Note: Actual implementation may vary based on API design\n73:         assert!(resp.status().is_success() || resp.status().as_u16() == 202);\n74:     }",
      "severity": "info"
    },
    {
      "file": "tests/ontology_smoke_test.rs",
      "line_number": 720,
      "marker_type": "NOTE",
      "content": "Actual violations depend on the ontology and validation rules",
      "context": "718: \n719:         // Should detect some violations\n720:         // Note: Actual violations depend on the ontology and validation rules\n721:         println!(\"Detected {} violations\", report.violations.len());\n722: ",
      "severity": "info"
    },
    {
      "file": "tests/voice_agent_integration_test.rs",
      "line_number": 45,
      "marker_type": "NOTE",
      "content": "Tests might fail if MCP server is not running, which is expected",
      "context": "43:                 Err(e) => {\n44:                     println!(\"Error processing command '{}': {}\", command, e);\n45:                     // Note: Tests might fail if MCP server is not running, which is expected\n46:                 }\n47:             }",
      "severity": "info"
    },
    {
      "file": "tests/voice_agent_integration_test.rs",
      "line_number": 152,
      "marker_type": "NOTE",
      "content": "This would require making is_voice_command public or adding a test helper",
      "context": "150: \n151:         for (input, expected) in test_cases {\n152:             // Note: This would require making is_voice_command public or adding a test helper\n153:             // For now, we'll test through the full pipeline\n154:             let settings = Arc::new(RwLock::new(AppFullSettings::default()));",
      "severity": "info"
    },
    {
      "file": "tests/ontology_reasoning_integration_test.rs",
      "line_number": 98,
      "marker_type": "NOTE",
      "content": "whelk may infer transitive relationships like Employee -> Thing",
      "context": "96: \n97:     // Test inference (this will use whelk-rs)\n98:     // Note: whelk may infer transitive relationships like Employee -> Thing\n99:     let inferred = service\n100:         .infer_axioms(\"default\")",
      "severity": "info"
    },
    {
      "file": "tests/neo4j_settings_integration_tests.rs",
      "line_number": 308,
      "marker_type": "NOTE",
      "content": "To run these tests once compilation is fixed:",
      "context": "306: }\n307: \n308: // NOTE: To run these tests once compilation is fixed:\n309: // 1. Start Neo4j instance: docker run -d -p 7687:7687 -p 7474:7474 --env NEO4J_AUTH=neo4j/test neo4j:latest\n310: // 2. Run tests: cargo test --test neo4j_settings_integration_tests -- --ignored --test-threads=1",
      "severity": "info"
    },
    {
      "file": "tests/integration_settings_sync.rs",
      "line_number": 409,
      "marker_type": "NOTE",
      "content": "This will likely fail without proper Nostr signature validation",
      "context": "407:         let resp = test::call_service(&app, req).await;\n408: \n409:         // Note: This will likely fail without proper Nostr signature validation\n410:         // but we can test the endpoint structure\n411:         assert!(resp.status() == StatusCode::OK || resp.status() == StatusCode::UNAUTHORIZED);",
      "severity": "info"
    },
    {
      "file": "scripts/sync_neo4j.rs",
      "line_number": 21,
      "marker_type": "NOTE",
      "content": "This script is obsolete - Neo4j is now the primary database.",
      "context": "19: use webxr::adapters::neo4j_adapter::{Neo4jAdapter, Neo4jConfig};\n20: use webxr::ports::knowledge_graph_repository::KnowledgeGraphRepository;\n21: // Note: This script is obsolete - Neo4j is now the primary database.\n22: // unified_graph_repository was removed during SQLite \u2192 Neo4j migration.\n23: ",
      "severity": "info"
    },
    {
      "file": "examples/ontology_sync_example.rs",
      "line_number": 39,
      "marker_type": "NOTE",
      "content": "You'll need to provide your actual repository implementations",
      "context": "37:     let content_api = Arc::new(EnhancedContentAPI::new(github_client));\n38: \n39:     // Note: You'll need to provide your actual repository implementations\n40:     // This is just a placeholder\n41:     let kg_repo: Arc<dyn visionflow::ports::knowledge_graph_repository::KnowledgeGraphRepository> =",
      "severity": "info"
    },
    {
      "file": "src/app_state.rs",
      "line_number": 395,
      "marker_type": "NOTE",
      "content": "The actor is spawned by GPUManagerActor, so we'll retrieve it after initialization",
      "context": "393: \n394:             // Extract StressMajorizationActor from GPUManagerActor's child actors\n395:             // Note: The actor is spawned by GPUManagerActor, so we'll retrieve it after initialization\n396:             info!(\"[AppState::new] StressMajorizationActor will be available after GPU initialization\");\n397: ",
      "severity": "info"
    },
    {
      "file": "src/actors/messages.rs",
      "line_number": 1559,
      "marker_type": "NOTE",
      "content": "ComputeShortestPaths already defined above at line ~1107",
      "context": "1557: // GPU Pathfinding Messages (SemanticProcessorActor)\n1558: // ============================================================================\n1559: // Note: ComputeShortestPaths already defined above at line ~1107\n1560: // Note: PathfindingResult is defined in ports::gpu_semantic_analyzer\n1561: ",
      "severity": "info"
    },
    {
      "file": "src/actors/messages.rs",
      "line_number": 1560,
      "marker_type": "NOTE",
      "content": "PathfindingResult is defined in ports::gpu_semantic_analyzer",
      "context": "1558: // ============================================================================\n1559: // Note: ComputeShortestPaths already defined above at line ~1107\n1560: // Note: PathfindingResult is defined in ports::gpu_semantic_analyzer\n1561: \n1562: ///",
      "severity": "info"
    },
    {
      "file": "src/actors/physics_orchestrator_actor.rs",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "Do NOT set gpu_initialized here!",
      "context": "305:                 });\n306: \n307:                 // NOTE: Do NOT set gpu_initialized here!\n308:                 // Wait for GPUInitialized message from GPU actor (see handler at end of file)\n309:                 // self.gpu_initialized = true;  // REMOVED - wait for confirmation",
      "severity": "info"
    },
    {
      "file": "src/utils/neo4j_helpers.rs",
      "line_number": 16,
      "marker_type": "NOTE",
      "content": "neo4rs provides automatic From implementations for Vec<T> and HashMap<String, T>",
      "context": "14: /// This handles all JSON types including nested objects and arrays.\n15: ///\n16: /// Note: neo4rs provides automatic From implementations for Vec<T> and HashMap<String, T>\n17: /// where T: Into<BoltType>, so we use those instead of manually constructing BoltList/BoltMap.\n18: pub fn json_to_bolt(value: JsonValue) -> BoltType {",
      "severity": "info"
    },
    {
      "file": "src/adapters/neo4j_adapter.rs",
      "line_number": 352,
      "marker_type": "NOTE",
      "content": "neo4rs doesn't currently support query timeouts directly",
      "context": "350: \n351:         // TODO: Apply query timeout from config\n352:         // Note: neo4rs doesn't currently support query timeouts directly\n353:         // Consider implementing timeout at the application level\n354: ",
      "severity": "info"
    },
    {
      "file": "src/adapters/neo4j_adapter.rs",
      "line_number": 362,
      "marker_type": "NOTE",
      "content": "Neo4rs Row API doesn't provide direct access to all keys",
      "context": "360:         let mut results = Vec::new();\n361:         while let Ok(Some(_row)) = result.next().await {\n362:             // Note: Neo4rs Row API doesn't provide direct access to all keys\n363:             // For now, returning empty map - users should use specific field access\n364:             let row_map = HashMap::new();",
      "severity": "info"
    },
    {
      "file": "src/handlers/pipeline_admin_handler.rs",
      "line_number": 141,
      "marker_type": "NOTE",
      "content": "Actual trigger logic would call into OntologyPipelineService",
      "context": "139: \n140:     // Trigger pipeline\n141:     // Note: Actual trigger logic would call into OntologyPipelineService\n142:     // For now, return a success response\n143:     let response = TriggerPipelineResponse {",
      "severity": "info"
    },
    {
      "file": "src/handlers/constraints_handler.rs",
      "line_number": 8,
      "marker_type": "NOTE",
      "content": "Constraint imports available but currently unused - keeping for future enhancements",
      "context": "6: use log::{debug, error, info, warn};\n7: use serde_json::{json, Value};\n8: // Note: Constraint imports available but currently unused - keeping for future enhancements\n9: use crate::handlers::settings_validation_fix::validate_constraint;\n10: ",
      "severity": "info"
    },
    {
      "file": "src/handlers/socket_flow_handler.rs",
      "line_number": 35,
      "marker_type": "NOTE",
      "content": "Now using u32 node IDs throughout the system",
      "context": "33: }\n34: \n35: // Note: Now using u32 node IDs throughout the system\n36: \n37: ///",
      "severity": "info"
    },
    {
      "file": "src/handlers/socket_flow_handler.rs",
      "line_number": 591,
      "marker_type": "NOTE",
      "content": "client_id is assigned asynchronously via SetClientId message after RegisterClient completes",
      "context": "589:         self.last_activity = std::time::Instant::now();\n590: \n591:         // Note: client_id is assigned asynchronously via SetClientId message after RegisterClient completes\n592:         // We don't reset it here to avoid race conditions - it will be set by the async handler\n593: ",
      "severity": "info"
    },
    {
      "file": "src/handlers/graph_export_handler.rs",
      "line_number": 426,
      "marker_type": "NOTE",
      "content": "Using /graph-export to avoid conflict with /graph scope from api_handler/graph/mod.rs",
      "context": "424: ///\n425: pub fn configure_routes(cfg: &mut web::ServiceConfig) {\n426:     // Note: Using /graph-export to avoid conflict with /graph scope from api_handler/graph/mod.rs\n427:     // Routes become /api/graph-export/*\n428:     cfg.service(",
      "severity": "info"
    },
    {
      "file": "src/handlers/websocket_settings_handler.rs",
      "line_number": 14,
      "marker_type": "NOTE",
      "content": "SetSettingsByPaths available for future batch update features",
      "context": "12: use std::collections::HashMap;\n13: use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};\n14: // Note: SetSettingsByPaths available for future batch update features\n15: use crate::app_state::AppState;\n16: ",
      "severity": "info"
    },
    {
      "file": "src/services/file_service.rs",
      "line_number": 40,
      "marker_type": "TEMP",
      "content": "orary struct for extracting ontology data from markdown",
      "context": "38: }\n39: \n40: /// Temporary struct for extracting ontology data from markdown\n41: #[derive(Default)]\n42: struct OntologyData {",
      "severity": "warning"
    },
    {
      "file": "src/services/ontology_reasoning_service.rs",
      "line_number": 393,
      "marker_type": "NOTE",
      "content": "The table doesn't have user_defined column yet,",
      "context": "391: \n392:             // Store in owl_axioms table with user_defined=false\n393:             // Note: The table doesn't have user_defined column yet,\n394:             // we'll use annotations to track this\n395:             self.ontology_repo.add_axiom(&owl_axiom).await?;",
      "severity": "info"
    },
    {
      "file": "src/config/mod.rs",
      "line_number": 1608,
      "marker_type": "NOTE",
      "content": "ConstraintData has been moved to models/constraints.rs for GPU compatibility",
      "context": "1606: \n1607: // Constraint system structures\n1608: // Note: ConstraintData has been moved to models/constraints.rs for GPU compatibility\n1609: // The old simple structure has been replaced with a GPU-optimized version\n1610: ",
      "severity": "info"
    },
    {
      "file": "src/gpu/visual_analytics.rs",
      "line_number": 1496,
      "marker_type": "NOTE",
      "content": "frame field changed from i32 to u32 in canonical definition",
      "context": "1494: \n1495: // Import canonical RenderData from gpu::types\n1496: // Note: frame field changed from i32 to u32 in canonical definition\n1497: pub use crate::gpu::types::RenderData;\n1498: ",
      "severity": "info"
    },
    {
      "file": "src/ontology/actors/mod.rs",
      "line_number": 3,
      "marker_type": "NOTE",
      "content": "The OntologyActor is defined in src/actors/ontology_actor.rs (production version)",
      "context": "1: // src/ontology/actors/mod.rs\n2: \n3: // NOTE: The OntologyActor is defined in src/actors/ontology_actor.rs (production version)\n4: // The duplicate implementation that was here has been moved to ontology_actor.rs.backup\n5: ",
      "severity": "info"
    },
    {
      "file": "src/handlers/api_handler/semantic_forces.rs",
      "line_number": 96,
      "marker_type": "NOTE",
      "content": "This requires the GPU manager to route messages to SemanticForcesActor",
      "context": "94: \n95:     // Send configuration to semantic forces actor via GPU manager\n96:     // Note: This requires the GPU manager to route messages to SemanticForcesActor\n97:     // For now, we'll return success with the configuration\n98:     info!(\"DAG configuration applied: mode={:?}, enabled={}\",",
      "severity": "info"
    },
    {
      "file": "src/actors/gpu/shared.rs",
      "line_number": 99,
      "marker_type": "NOTE",
      "content": "SafeCudaStream provides thread safety guarantees",
      "context": "97: \n98: ///\n99: // Note: SafeCudaStream provides thread safety guarantees\n100: pub struct SharedGPUContext {\n101:     pub device: Arc<CudaDevice>,",
      "severity": "info"
    },
    {
      "file": "src/actors/gpu/force_compute_actor.rs",
      "line_number": 934,
      "marker_type": "NOTE",
      "content": "We don't have a direct physics orchestrator reference here,",
      "context": "932:         if let Some(correlation_id) = msg.correlation_id {\n933:             use crate::actors::messaging::MessageAck;\n934:             // Note: We don't have a direct physics orchestrator reference here,\n935:             // but acknowledgments can still be sent if the reference is added in the future\n936:             // For now, this demonstrates the pattern",
      "severity": "info"
    },
    {
      "file": "src/actors/gpu/force_compute_actor.rs",
      "line_number": 1164,
      "marker_type": "NOTE",
      "content": "Future enhancement - send ack to physics orchestrator if reference available",
      "context": "1162:             use crate::actors::messaging::MessageAck;\n1163:             debug!(\"SetSharedGPUContext completed with correlation_id: {}\", correlation_id);\n1164:             // Note: Future enhancement - send ack to physics orchestrator if reference available\n1165:         }\n1166: ",
      "severity": "info"
    },
    {
      "file": "tests/actors/integration_tests.rs",
      "line_number": 349,
      "marker_type": "NOTE",
      "content": "Full lifecycle test would require actual actor system",
      "context": "347:     assert!(!manager.is_healthy());\n348: \n349:     // Note: Full lifecycle test would require actual actor system\n350:     // This is a simplified version\n351: }",
      "severity": "info"
    },
    {
      "file": "tests/api/cqrs_integration_tests.rs",
      "line_number": 355,
      "marker_type": "NOTE",
      "content": "We can't directly publish DomainEvent without implementing",
      "context": "353:         // Publishing should succeed (even with no subscribers)\n354:         let bus = event_bus.read().await;\n355:         // Note: We can't directly publish DomainEvent without implementing\n356:         // the trait, so this is a placeholder for the test structure\n357:         drop(bus);",
      "severity": "info"
    },
    {
      "file": "tests/api/reasoning_api_tests.rs",
      "line_number": 14,
      "marker_type": "NOTE",
      "content": "Actual implementation depends on your API structure",
      "context": "12:     async fn test_health_check_endpoint() {\n13:         // Test basic health check\n14:         // Note: Actual implementation depends on your API structure\n15:         // This is a template that should be adapted to your specific endpoints\n16: ",
      "severity": "info"
    },
    {
      "file": "tests/api/reasoning_api_tests.rs",
      "line_number": 78,
      "marker_type": "NOTE",
      "content": "These tests are placeholders. The actual implementation",
      "context": "76: }\n77: \n78: // Note: These tests are placeholders. The actual implementation\n79: // depends on how your API exposes the reasoning service.\n80: // Common patterns:",
      "severity": "info"
    },
    {
      "file": "tests/unit/ontology_reasoning_test.rs",
      "line_number": 313,
      "marker_type": "NOTE",
      "content": "This depends on CustomReasoner implementation",
      "context": "311: \n312:         // Check if subproperty relationships are captured\n313:         // Note: This depends on CustomReasoner implementation\n314:         println!(\"Inferred axioms: {:?}\", inferred);\n315: ",
      "severity": "info"
    },
    {
      "file": "tests/performance/reasoning_benchmark.rs",
      "line_number": 394,
      "marker_type": "NOTE",
      "content": "Actual GPU version should be much faster",
      "context": "392:         println!(\"Per iteration: {}\u03bcs\", duration.as_micros() / 100);\n393: \n394:         // Note: Actual GPU version should be much faster\n395:         println!(\"Note: GPU implementation expected to be 10-100x faster\");\n396:     }",
      "severity": "info"
    },
    {
      "file": "client/src/api/settingsApi.ts",
      "line_number": 9,
      "marker_type": "NOTE",
      "content": "Empty string because the function paths already include '/api/'",
      "context": "7: // Use Vite proxy for API requests in development\n8: // In production, this will be served from the same origin\n9: // NOTE: Empty string because the function paths already include '/api/'\n10: const API_BASE = '';\n11: ",
      "severity": "info"
    },
    {
      "file": "client/src/types/nip07.d.ts",
      "line_number": 7,
      "marker_type": "NOTE",
      "content": "NIP-07 specifies the input event lacks id, pubkey, sig.",
      "context": "5: \n6: // Define the structure of the event object passed to signEvent\n7: // Note: NIP-07 specifies the input event lacks id, pubkey, sig.\n8: // nostr-tools' UnsignedEvent fits this description.\n9: type Nip07Event = Omit<UnsignedEvent, 'pubkey'>; ",
      "severity": "info"
    },
    {
      "file": "client/src/features/bots/components/BotsVisualizationFixed.tsx",
      "line_number": 1113,
      "marker_type": "NOTE",
      "content": "This is a pure rendering component that receives positions from server physics simulation",
      "context": "1111: \n1112: // Main Visualization Component\n1113: // Note: This is a pure rendering component that receives positions from server physics simulation\n1114: // via binary protocol. No client-side physics computation is performed.\n1115: export const BotsVisualization: React.FC = () => {",
      "severity": "info"
    },
    {
      "file": "client/src/features/visualisation/components/HolographicDataSphere.tsx",
      "line_number": 28,
      "marker_type": "NOTE",
      "content": "Settings system is brittle - don't update UX names yet",
      "context": "26: \n27: // TODO: Map these hardcoded values to settings system\n28: // Note: Settings system is brittle - don't update UX names yet\n29: // These values should eventually come from visualisation.hologram settings\n30: export const SCENE_CONFIG = {",
      "severity": "info"
    },
    {
      "file": "ONTOLOGY_ARCHITECTURE_ANALYSIS.md",
      "line_number": 384,
      "marker_type": "NOTE",
      "content": "s",
      "context": "382: - Foundation for semantic reasoning\n383: \n384: ## Notes\n385: \n386: - **Backward compatibility**: Default mode remains \"all\" (no breaking changes)",
      "severity": "info"
    },
    {
      "file": "docs/nostr-auth-implementation.md",
      "line_number": 162,
      "marker_type": "NOTE",
      "content": "s",
      "context": "160: 5. Session management and token expiration\n161: \n162: ## Notes\n163: \n164: - Authentication uses existing `nostrAuthService.ts` singleton",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0115-minting.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0119-economic-security.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/Digital Citizens\u2019 Assembly.md",
      "line_number": 68,
      "marker_type": "NOTE",
      "content": "s",
      "context": "66: \t\t  additional-sources:: UN Habitat Digital Civics \u00b7 OECD Civic Tech Framework\n67: \t\t  \n68: \t\t  ## Notes\n69: \t\t  Facilitates policy co-creation through XR-based public deliberation spaces.\n70: \t\t  ",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0120-incentive-alignment.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0113-emission-schedule.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/WebXR API.md",
      "line_number": 68,
      "marker_type": "NOTE",
      "content": "s",
      "context": "66: \t\t  additional-sources:: W3C WebXR \u00b7 EWG/MSF Taxonomy \u00b7 ETSI GR ARF 010\n67: \t\t  \n68: \t\t  ## Notes\n69: \t\t  Defines communication rules and exchange methods between digital systems to ensure interoperability.\n70: \t\t  ",
      "severity": "info"
    },
    {
      "file": "data/markdown/AI-0438-iot-ai-integration.md",
      "line_number": 226,
      "marker_type": "NOTE",
      "content": "s for later",
      "context": "224: \t\t- Integration challenges, limitations of AI, compatibility issues.\n225: \n226: - ### notes for later\n227: \t\t- Develop or utilize tools for issuing and managing RGB assets.\n228: \t- **Agent Integration:**",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0118-market-capitalization.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0107-gas-limit.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0103-halving.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/Placing on the Market.md",
      "line_number": 99,
      "marker_type": "TEMP",
      "content": "oral Scope",
      "context": "97: 14. **Corrective action** capability ready (Article 21)\n98: \n99: ## Temporal Scope\n100: \n101: \"Placing\" occurs at the moment of:",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Placing on the Market.md",
      "line_number": 288,
      "marker_type": "TEMP",
      "content": "oral Scope",
      "context": "286: 14. **Corrective action** capability ready (Article 21)\n287: \n288: ## Temporal Scope\n289: \n290: \"Placing\" occurs at the moment of:",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Placing on the Market.md",
      "line_number": 483,
      "marker_type": "TEMP",
      "content": "oral Scope",
      "context": "481: \t\t  14. **Corrective action** capability ready (Article 21)\n482: \t\t  \n483: \t\t  ## Temporal Scope\n484: \t\t  \n485: \t\t  \"Placing\" occurs at the moment of:",
      "severity": "warning"
    },
    {
      "file": "data/markdown/AI User.md",
      "line_number": 1349,
      "marker_type": "NOTE",
      "content": "s and Other Stuff by Relays",
      "context": "1347: \t\t\t- Conflict resolution: Nucleus Server automatically handles conflicts that may arise from simultaneous edits by multiple users\n1348: \n1349: - # Notes and Other Stuff by Relays\n1350: \t- Nostr [pronounced no-star] is a decentralized open protocol that aims to improve the social media experience by addressing issues of censorship and data collection. The protocol operates by allowing users to post and view notes on servers called relays, and view and post these notes through apps called clients. The open nature of the protocol allows for competition and a free flow of information, as users can choose to use different relays or clients if they are censored. This is because the protocol is decentralized and controlled by no one.\n1351: \t  id:: 67a49f1d-4088-4392-ad62-8e93673a2dff",
      "severity": "info"
    },
    {
      "file": "data/markdown/AI User.md",
      "line_number": 2676,
      "marker_type": "NOTE",
      "content": "s and Other Stuff by Relays",
      "context": "2674: \t\t\t- Conflict resolution: Nucleus Server automatically handles conflicts that may arise from simultaneous edits by multiple users\n2675: \n2676: - # Notes and Other Stuff by Relays\n2677: \t- Nostr [pronounced no-star] is a decentralized open protocol that aims to improve the social media experience by addressing issues of censorship and data collection. The protocol operates by allowing users to post and view notes on servers called relays, and view and post these notes through apps called clients. The open nature of the protocol allows for competition and a free flow of information, as users can choose to use different relays or clients if they are censored. This is because the protocol is decentralized and controlled by no one.\n2678: \t  id:: 67a49f1d-4088-4392-ad62-8e93673a2dff",
      "severity": "info"
    },
    {
      "file": "data/markdown/AI User.md",
      "line_number": 3360,
      "marker_type": "NOTE",
      "content": "s and Other Stuff by Relays",
      "context": "3358: \t\t\t- File locking: Nucleus Server supports file locking to prevent conflicts and ensure exclusive access to assets when needed\n3359: \n3360: - # Notes and Other Stuff by Relays\n3361: \t- Nostr [pronounced no-star] is a decentralized open protocol that aims to improve the social media experience by addressing issues of censorship and data collection. The protocol operates by allowing users to post and view notes on servers called relays, and view and post these notes through apps called clients. The open nature of the protocol allows for competition and a free flow of information, as users can choose to use different relays or clients if they are censored. This is because the protocol is decentralized and controlled by no one.\n3362: \t  id:: 67a49f1d-4088-4392-ad62-8e93673a2dff",
      "severity": "info"
    },
    {
      "file": "data/markdown/AI User.md",
      "line_number": 3779,
      "marker_type": "NOTE",
      "content": "s for later",
      "context": "3777:       subgraph Ethical Considerations\n3778: \n3779: - ### notes for later\n3780: \t- Notes on build-out The world database in the shared rooms in the metaverse is the global object master, educational materials, videos, audio content and branded objects are fungible tokens authentically proved by rgb client side validation between parties, only validated ones will be persisted in shared rooms like conferences and classes according to the room schema. That allows educators to monetise their content. That can work on lightning. NFT objects between parties like content crafted by participants (coursework, homework) are not on lightning and will attract main chain fees but are rarer. User authentication and communication will be through nostr.\n3781: \t\t- Integrate Nostr protocol for decentralized identity and messaging.",
      "severity": "info"
    },
    {
      "file": "data/markdown/AI User.md",
      "line_number": 4118,
      "marker_type": "HACK",
      "content": "md.io/@xr/monetization)",
      "context": "4116: - [The paper presents a monocular depth estimation method using denoising diffusion models. The goal is to generate accurate depth maps from single RGB images. The authors address the problem of noisy and incomplete depth maps in the training data by using step-unrolled denoising diffusion, an L1 loss, and depth infilling during training.  To overcome the limited availability of supervised training data, the authors leverage pre-training on self-supervised image-to-image translation tasks. Despite the simplicity of the approach, their model achieves state-of-the-art (SOTA) performance on the indoor NYU dataset and near SOTA results on the outdoor KITTI dataset.  The approach involves infilling missing depth in ground truth depth maps using nearest neighbor interpolation. Then, noise is added to the depth map and a neural network is trained to predict the noise given the RGB image and noisy depth map. During fine-tuning, one step of the forward pass is unrolled and the ground truth depth map is replaced with the model's prediction.  The DepthGen model achieves an absolute relative error of 0.074 on the indoor NYU dataset and a competitive relative error of 0.064 on the outdoor KITTI dataset, demonstrating its accuracy in depth estimation.  The paper also introduces a text-to-3D pipeline that combines DepthGen with off-the-shelf text-to-image and text-conditioned image completion models. This pipeline allows for generating 3D point clouds from text prompts.  In conclusion, the proposed method of monocular depth estimation using diffusion models achieves state-of-the-art performance, even with limited supervised training data. The approach is simple yet effective and can be integrated into a text-to-3D pipeline for generating 3D scenes from text prompts.](https://depth-gen.github.io/)\n4117: - [The text provided is a collection of video titles and descriptions related to Blender, AI, and 3D design. The videos cover topics such as creating isometric rooms, using AI in 3D design, Unreal Engine, toon shading in Blender, QR code art, GPT (Generative Pre-trained Transformer) engineering, creating Ghibli-style characters, new features in Blender 3.6, animation in Blender, and adding vegetation in Twinmotion. The videos are created by various individuals and brands, including vertex vendor, Unreal Sensei, Quick QR Art, ENFANT TERRIBLE, Matt Wolfe, Ian Wootten, Brandon's Drawings, Polyfjord, Charlie Barber, and vishal panjeta. The text also mentions a Google company and provides information about cookie usage and privacy settings when using Google services.](https://www.youtube.com/watch?v=GZO7TAlVE_8)\n4118: - [WebXR is a device API that allows for VR/AR experiences through web browsers. However, monetization has been a major issue for the platform, with indie creators struggling to capture value. Most WebXR apps appear as prototypes because developers find it difficult to justify investing more resources into the ecosystem. The current ways people pay for WebXR content include purchasing tickets, using cryptocurrency for virtual land, and accessing certain features by login or ownership of bot handles. The process of paying for WebXR content can be made easier and more frictionless by integrating payment methods like Apple Pay or Google Pay while in VR. Artists can get paid through various means such as commissions, Patreon, grants, VC investment, and event tickets. Non-payment based monetization strategies like advertising are also being explored. A list of 101 ideas for WebXR monetization includes platforms like Patreon and Github Sponsors, virtual market stalls, virtual land parcels, and in-world advertising. Other strategies include payment processing integration with platforms like PayPal or Discord, creating virtual actors and performers, storytelling, and podcast sponsorships. Advertisements targeted at 18-44 year old males interested in software, gaming, and VR have shown promising results. A Github repository for WebXR monetization examples is in progress. Despite these efforts, monetization in the WebXR ecosystem is still a work in progress, and more exploration and innovation is needed.](https://hackmd.io/@xr/monetization)\n4119: - The paper proposes a system called CLIP-Actor, which animates a 3D human mesh to conform to a text prompt by recommending a motion sequence and optimizing mesh style attributes. The system's novelty lies in its ability to recommend motion that conforms to the prompt in a pose-agnostic and temporally-consistent manner while leveraging multi-frame human motion and rejecting poorly rendered views. The authors demonstrate that CLIP-Actor produces plausible and human-recognizable style 3D human mesh in motion with detailed geometry and texture solely from a natural language prompt. The paper's methodology shows that CLIP-Actor is an effective and efficient way to generate plausible results when the pose of an artist-designed mesh does not conform to the text prompt from the beginning. The research has been sponsored by the Korean government's grant funded by the Institute of Information & communications Technology Planning & Evaluation (IITP). https://clip-actor.github.io\n4120: - [The paper CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes proposes a system for animating human meshes based on text prompts. The system, called CLIP-Actor, generates motion sequences and optimizes mesh style attributes to conform to a given text prompt.  The authors highlight a limitation of previous work, which struggled to produce realistic results when the starting pose of a pre-designed mesh did not align with the text prompt. To address this issue, CLIP-Actor leverages a large-scale human motion dataset with language labels to build a text-driven human motion recommendation system. It suggests a motion sequence that aligns with the given prompt in a coarse-to-fine manner.  In addition, the authors introduce a novel neural style optimization technique that adds detail and texture to the recommended mesh sequence in a temporally-consistent and pose-agnostic manner. They also propose spatio-temporal view augmentation and mask-weighted embedding attention techniques to stabilize the optimization process by incorporating multi-frame human motion and rejecting poorly rendered views.  The results of CLIP-Actor demonstrate its ability to generate plausible and human-recognizable 3D human meshes in motion with detailed geometry and texture solely from natural language prompts.  The paper includes the BibTeX citation for academic referencing and acknowledges the support received from the Institute of Information and Communications Technology Planning and Evaluation (IITP) in Korea for funding the research.  The website containing the paper and code is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. The source code for the system is mainly borrowed from Keunhong Park's Nerfies website, and feedback and questions can be directed to Kim Ji-Yeon.](https://clip-actor.github.io)",
      "severity": "warning"
    },
    {
      "file": "data/markdown/AI User.md",
      "line_number": 4483,
      "marker_type": "HACK",
      "content": "md.io/@xr/monetization)",
      "context": "4481: - [The paper presents a monocular depth estimation method using denoising diffusion models. The goal is to generate accurate depth maps from single RGB images. The authors address the problem of noisy and incomplete depth maps in the training data by using step-unrolled denoising diffusion, an L1 loss, and depth infilling during training.  To overcome the limited availability of supervised training data, the authors leverage pre-training on self-supervised image-to-image translation tasks. Despite the simplicity of the approach, their model achieves state-of-the-art (SOTA) performance on the indoor NYU dataset and near SOTA results on the outdoor KITTI dataset.  The approach involves infilling missing depth in ground truth depth maps using nearest neighbor interpolation. Then, noise is added to the depth map and a neural network is trained to predict the noise given the RGB image and noisy depth map. During fine-tuning, one step of the forward pass is unrolled and the ground truth depth map is replaced with the model's prediction.  The DepthGen model achieves an absolute relative error of 0.074 on the indoor NYU dataset and a competitive relative error of 0.064 on the outdoor KITTI dataset, demonstrating its accuracy in depth estimation.  The paper also introduces a text-to-3D pipeline that combines DepthGen with off-the-shelf text-to-image and text-conditioned image completion models. This pipeline allows for generating 3D point clouds from text prompts.  In conclusion, the proposed method of monocular depth estimation using diffusion models achieves state-of-the-art performance, even with limited supervised training data. The approach is simple yet effective and can be integrated into a text-to-3D pipeline for generating 3D scenes from text prompts.](https://depth-gen.github.io/)\n4482: - [The text provided is a collection of video titles and descriptions related to Blender, AI, and 3D design. The videos cover topics such as creating isometric rooms, using AI in 3D design, Unreal Engine, toon shading in Blender, QR code art, GPT (Generative Pre-trained Transformer) engineering, creating Ghibli-style characters, new features in Blender 3.6, animation in Blender, and adding vegetation in Twinmotion. The videos are created by various individuals and brands, including vertex vendor, Unreal Sensei, Quick QR Art, ENFANT TERRIBLE, Matt Wolfe, Ian Wootten, Brandon's Drawings, Polyfjord, Charlie Barber, and vishal panjeta. The text also mentions a Google company and provides information about cookie usage and privacy settings when using Google services.](https://www.youtube.com/watch?v=GZO7TAlVE_8)\n4483: - [WebXR is a device API that allows for VR/AR experiences through web browsers. However, monetization has been a major issue for the platform, with indie creators struggling to capture value. Most WebXR apps appear as prototypes because developers find it difficult to justify investing more resources into the ecosystem. The current ways people pay for WebXR content include purchasing tickets, using cryptocurrency for virtual land, and accessing certain features by login or ownership of bot handles. The process of paying for WebXR content can be made easier and more frictionless by integrating payment methods like Apple Pay or Google Pay while in VR. Artists can get paid through various means such as commissions, Patreon, grants, VC investment, and event tickets. Non-payment based monetization strategies like advertising are also being explored. A list of 101 ideas for WebXR monetization includes platforms like Patreon and Github Sponsors, virtual market stalls, virtual land parcels, and in-world advertising. Other strategies include payment processing integration with platforms like PayPal or Discord, creating virtual actors and performers, storytelling, and podcast sponsorships. Advertisements targeted at 18-44 year old males interested in software, gaming, and VR have shown promising results. A Github repository for WebXR monetization examples is in progress. Despite these efforts, monetization in the WebXR ecosystem is still a work in progress, and more exploration and innovation is needed.](https://hackmd.io/@xr/monetization)\n4484: - The paper proposes a system called CLIP-Actor, which animates a 3D human mesh to conform to a text prompt by recommending a motion sequence and optimizing mesh style attributes. The system's novelty lies in its ability to recommend motion that conforms to the prompt in a pose-agnostic and temporally-consistent manner while leveraging multi-frame human motion and rejecting poorly rendered views. The authors demonstrate that CLIP-Actor produces plausible and human-recognizable style 3D human mesh in motion with detailed geometry and texture solely from a natural language prompt. The paper's methodology shows that CLIP-Actor is an effective and efficient way to generate plausible results when the pose of an artist-designed mesh does not conform to the text prompt from the beginning. The research has been sponsored by the Korean government's grant funded by the Institute of Information & communications Technology Planning & Evaluation (IITP). https://clip-actor.github.io\n4485: - [The paper CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes proposes a system for animating human meshes based on text prompts. The system, called CLIP-Actor, generates motion sequences and optimizes mesh style attributes to conform to a given text prompt.  The authors highlight a limitation of previous work, which struggled to produce realistic results when the starting pose of a pre-designed mesh did not align with the text prompt. To address this issue, CLIP-Actor leverages a large-scale human motion dataset with language labels to build a text-driven human motion recommendation system. It suggests a motion sequence that aligns with the given prompt in a coarse-to-fine manner.  In addition, the authors introduce a novel neural style optimization technique that adds detail and texture to the recommended mesh sequence in a temporally-consistent and pose-agnostic manner. They also propose spatio-temporal view augmentation and mask-weighted embedding attention techniques to stabilize the optimization process by incorporating multi-frame human motion and rejecting poorly rendered views.  The results of CLIP-Actor demonstrate its ability to generate plausible and human-recognizable 3D human meshes in motion with detailed geometry and texture solely from natural language prompts.  The paper includes the BibTeX citation for academic referencing and acknowledges the support received from the Institute of Information and Communications Technology Planning and Evaluation (IITP) in Korea for funding the research.  The website containing the paper and code is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. The source code for the system is mainly borrowed from Keunhong Park's Nerfies website, and feedback and questions can be directed to Kim Ji-Yeon.](https://clip-actor.github.io)",
      "severity": "warning"
    },
    {
      "file": "data/markdown/3D Scene Exchange Protocol (SXP).md",
      "line_number": 68,
      "marker_type": "NOTE",
      "content": "s",
      "context": "66: \t\t  additional-sources:: MSF Interchange WG \u00b7 glTF \u00b7 USD / OMA3\n67: \t\t  \n68: \t\t  ## Notes\n69: \t\t  Enables interoperable transfer of 3D scenes, materials and animations between systems.\n70: \t\t  ",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0106-gas-price.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0110-fee-market.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/Algorithmic Transparency Index.md",
      "line_number": 71,
      "marker_type": "TEMP",
      "content": "oral tracking",
      "context": "69: \t\t  )\n70: \n71: \t\t  # Temporal tracking\n72: \t\t  SubClassOf(mv:AlgorithmicTransparencyIndex\n73: \t\t    DataSomeValuesFrom(mv:hasAssessmentDate rdfs:Literal)",
      "severity": "warning"
    },
    {
      "file": "data/markdown/BC-0111-deflationary-token.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/Object Detection.md",
      "line_number": 227,
      "marker_type": "NOTE",
      "content": "s for later",
      "context": "225: \t\t\t\t\t\t\t- Interacting with legal authorities as needed.\n226: \n227: - ### notes for later\n228: \t- Notes on build-out The world database in the shared rooms in the metaverse is the global object master, educational materials, videos, audio content and branded objects are fungible tokens authentically proved by rgb client side validation between parties, only validated ones will be persisted in shared rooms like conferences and classes according to the room schema. That allows educators to monetise their content. That can work on lightning. NFT objects between parties like content crafted by participants (coursework, homework) are not on lightning and will attract main chain fees but are rarer. User authentication and communication will be through nostr.\n229: \t  ",
      "severity": "info"
    },
    {
      "file": "data/markdown/Object Detection.md",
      "line_number": 235,
      "marker_type": "NOTE",
      "content": "s for later",
      "context": "233: -\n234: \n235: - ### notes for later\n236: \t- Notes on build-out The world database in the shared rooms in the metaverse is the global object master, educational materials, videos, audio content and branded objects are fungible tokens authentically proved by rgb client side validation between parties, only validated ones will be persisted in shared rooms like conferences and classes according to the room schema. That allows educators to monetise their content. That can work on lightning. NFT objects between parties like content crafted by participants (coursework, homework) are not on lightning and will attract main chain fees but are rarer. User authentication and communication will be through nostr.\n237: \t  ",
      "severity": "info"
    },
    {
      "file": "data/markdown/Object Detection.md",
      "line_number": 250,
      "marker_type": "NOTE",
      "content": "s for later",
      "context": "248: \t\t- Variant authoring: Creating and modifying variants using USD editing tools or supported 3D software applications\n249: \n250: - ### notes for later\n251: \t- Notes on build-out The world database in the shared rooms in the metaverse is the global object master, educational materials, videos, audio content and branded objects are fungible tokens authentically proved by rgb client side validation between parties, only validated ones will be persisted in shared rooms like conferences and classes according to the room schema. That allows educators to monetise their content. That can work on lightning. NFT objects between parties like content crafted by participants (coursework, homework) are not on lightning and will attract main chain fees but are rarer. User authentication and communication will be through nostr.\n252: \t  ",
      "severity": "info"
    },
    {
      "file": "data/markdown/Social Impact.md",
      "line_number": 191,
      "marker_type": "TEMP",
      "content": "oral Dimensions",
      "context": "189: - Institutional and norm evolution\n190: \n191: ## Temporal Dimensions\n192: \n193: Social impacts vary across timeframes:",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Social Impact.md",
      "line_number": 1118,
      "marker_type": "TEMP",
      "content": "oral Dimensions",
      "context": "1116: - Institutional and norm evolution\n1117: \n1118: ## Temporal Dimensions\n1119: \n1120: Social impacts vary across timeframes:",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Social Impact.md",
      "line_number": 1365,
      "marker_type": "TEMP",
      "content": "oral Dimensions",
      "context": "1363: \t\t  - Institutional and norm evolution\n1364: \t\t  \n1365: \t\t  ## Temporal Dimensions\n1366: \t\t  \n1367: \t\t  Social impacts vary across timeframes:",
      "severity": "warning"
    },
    {
      "file": "data/markdown/BC-0116-total-supply.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0102-inflation.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0010-decentralization.md",
      "line_number": 585,
      "marker_type": "HACK",
      "content": "age.haskell.org/package/erd)",
      "context": "583: - [Ditaa](http://ditaa.sourceforge.net/)\n584: - [Nomnoml](https://www.nomnoml.com/)\n585: - [Erd](https://hackage.haskell.org/package/erd)\n586: - [Pikchr](https://pikchr.org/)\n587: - [Structurizr](https://structurizr.com/)",
      "severity": "warning"
    },
    {
      "file": "data/markdown/BC-0010-decentralization.md",
      "line_number": 1005,
      "marker_type": "HACK",
      "content": "age.haskell.org/package/erd)",
      "context": "1003: - [Ditaa](http://ditaa.sourceforge.net/)\n1004: - [Nomnoml](https://www.nomnoml.com/)\n1005: - [Erd](https://hackage.haskell.org/package/erd)\n1006: - [Pikchr](https://pikchr.org/)\n1007: - [Structurizr](https://structurizr.com/)",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Accountability.md",
      "line_number": 249,
      "marker_type": "TEMP",
      "content": "oral Distance",
      "context": "247: - Override capabilities\n248: \n249: ### Temporal Distance\n250: \n251: **Challenge**: Gap between development and harm",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Accountability.md",
      "line_number": 756,
      "marker_type": "TEMP",
      "content": "oral Distance",
      "context": "754: - Override capabilities\n755: \n756: ### Temporal Distance\n757: \n758: **Challenge**: Gap between development and harm",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Accountability.md",
      "line_number": 1260,
      "marker_type": "TEMP",
      "content": "oral Distance",
      "context": "1258: \t\t  - Override capabilities\n1259: \t\t  \n1260: \t\t  ### Temporal Distance\n1261: \t\t  \n1262: \t\t  **Challenge**: Gap between development and harm",
      "severity": "warning"
    },
    {
      "file": "data/markdown/BC-0109-priority-fee.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0112-inflationary-token.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0117-circulating-supply.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/Knowledge Distillation.md",
      "line_number": 76,
      "marker_type": "NOTE",
      "content": "about links",
      "context": "74: \t\t- ![Screenshot 2024-01-30 093017.png](assets/Screenshot_2024-01-30_093017_1706607172633_0.png)\n75: \n76: \t- ## Note about links\n77: \t\t- In *my* version of the knowledge graph all the twitter links render interactively, inline, you will just see \"loading\" on the web. There are a lot of them, like this one. Sometimes you might just seen a loading with no link, that means I forgot that one, it's not going to load.\n78: \t\t- [twitter link to the render loading below](https://twitter.com/bentossell/status/1758235873433243950)",
      "severity": "info"
    },
    {
      "file": "data/markdown/Knowledge Distillation.md",
      "line_number": 112,
      "marker_type": "NOTE",
      "content": "about links",
      "context": "110: \t\t- ![Screenshot 2024-01-30 093017.png](assets/Screenshot_2024-01-30_093017_1706607172633_0.png)\n111: \n112: \t- ## Note about links\n113: \t\t- In *my* version of the knowledge graph all the twitter links render interactively, inline, you will just see \"loading\" on the web. There are a lot of them, like this one. Sometimes you might just seen a loading with no link, that means I forgot that one, it's not going to load.\n114: \t\t- [twitter link to the render loading below](https://twitter.com/bentossell/status/1758235873433243950)",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0104-supply-cap.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/Co-Training.md",
      "line_number": 294,
      "marker_type": "IDEA",
      "content": "s-ncbr.pl/en/research/neural-rendering/).",
      "context": "292: \t- Plenoxels (Plenoptic Voxels): This method replaces neural networks with a sparse 3D grid of spherical harmonics, enabling faster training and competitive quality compared to NeRFs[](https://github.com/weihaox/awesome-neural-rendering/blob/master/docs/INTRODUCTION-AND-SURVEY.md).\n293: \t- NGLOD (Neural Geometric Level of Detail): This approach combines neural implicit representations with explicit geometric representations, allowing for multi-resolution rendering and faster training[](https://arxiv.org/abs/2402.00028).\n294: \t- NeRF-MAE (Masked AutoEncoders for NeRFs): This technique applies the concept of masked autoencoders to NeRFs for self-supervised 3D representation learning, potentially improving generalization and efficiency[](https://ideas-ncbr.pl/en/research/neural-rendering/).\n295: \n296: \t- ### Key Techniques and Tools",
      "severity": "info"
    },
    {
      "file": "data/markdown/Co-Training.md",
      "line_number": 692,
      "marker_type": "IDEA",
      "content": "s-ncbr.pl/en/research/neural-rendering/).",
      "context": "690: \t- Plenoxels (Plenoptic Voxels): This method replaces neural networks with a sparse 3D grid of spherical harmonics, enabling faster training and competitive quality compared to NeRFs[](https://github.com/weihaox/awesome-neural-rendering/blob/master/docs/INTRODUCTION-AND-SURVEY.md).\n691: \t- NGLOD (Neural Geometric Level of Detail): This approach combines neural implicit representations with explicit geometric representations, allowing for multi-resolution rendering and faster training[](https://arxiv.org/abs/2402.00028).\n692: \t- NeRF-MAE (Masked AutoEncoders for NeRFs): This technique applies the concept of masked autoencoders to NeRFs for self-supervised 3D representation learning, potentially improving generalization and efficiency[](https://ideas-ncbr.pl/en/research/neural-rendering/).\n693: \n694: \t- ### Key Techniques and Tools",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0096-token.md",
      "line_number": 431,
      "marker_type": "NOTE",
      "content": "s.ethereum.org/@djrtwo/risks-of-lsd)or malicious actors who have sufficient control of the existing history of the chain, thought to be [in the region of $50M](https://twitter.com/MTorgin/status/1521433474820890624)[[mackinga2022twap]]. Like much of the rest of \u2018crypto\u2019, the proposed changes will concentrate decisions and economic rewards in the hands of major players, early investors, and incumbents.",
      "context": "429: \t- In addition, a recent upgrade (EIP-1559) results in tokens now being burnt at a higher rate than they are produced, deliberately leading to a diminishing supply. In theory, this increases the value of each ETH on the network at around 1% per year. It\u2019s very complex, with impacts on transaction fees, waiting time, and consensus security, as examined by Liu et al.[[liu2022empirical]]. Additionally, there is now talk (by [Buterin](https://time.com/6158182/vitalik-buterin-ethereum-profile/), the creator of Ethereum) of extending this burn mechanism [further into the network](https://ethresear.ch/t/multidimensional-eip-1559/11651).\n430: - Ethereum was designed from the beginning to move to a \u2018proof of stake\u2019 model where token holders underpin network consensus through complex automated voting systems based upon their token holding. This is now called [Ethereum Consensus Layer](https://blog.ethereum.org/2022/01/24/the-great-eth2-renaming/). This recent \u2018Merge\u2019 upgrade has reduced the carbon footprint of the network, a laudable thing, though it seems the GPUs and data centres have just gone on to be elsewhere. It has not lowered the cost to users nor improved performance. As part of the switching roadmap, users were asked to lock up 32 ETH tokens each (a substantial allocation of capital). In total, there are around 14 million of these tokens, and it is those users who now control the network. This money is likely stuck on the network until at least 2024, a significant delay when compared to the original promises.\n431: - This means that proof of stake has problems in that the majority owners \u2018decide\u2019 the truth of the chain to a degree and must by design have the ability to override prior consensus choices. Remember that these users are now trapped in their positions. Four major entities now control the rules of the chain and have already agreed to censor certain banned addresses. Proof of stake is probably inherently broken[[poelstra2015stake]]. This has [f](https://notes.ethereum.org/@djrtwo/risks-of-lsd)or malicious actors who have sufficient control of the existing history of the chain, thought to be [in the region of $50M](https://twitter.com/MTorgin/status/1521433474820890624)[[mackinga2022twap]]. Like much of the rest of \u2018crypto\u2019, the proposed changes will concentrate decisions and economic rewards in the hands of major players, early investors, and incumbents.\n432: - ![image.png](assets/image_1742487476628_0.png)\n433: - This is a far cry from the stated aims of the technology. The move to proof of stake has recently earned it the [MIT breakthrough technology award](https://www.technologyreview.com/2022/02/23/1044960/proof-of-stake-cryptocurrency/), despite not being complete (validators cannot yet sell their voting stakes). It\u2019s clearly a technology that is designed to innovate at the expense of predictability. This might work out very well for the platform, but right now the barrier to participation (in gas fees) is so high that we do not intend for Ethereum to be in scope as a method for value transfer within metaverses.",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0096-token.md",
      "line_number": 442,
      "marker_type": "NOTE",
      "content": "s.ethereum.org/@djrtwo/risks-of-lsd)or malicious actors who have sufficient control of the existing history of the chain, thought to be [in the region of $50M](https://twitter.com/MTorgin/status/1521433474820890624)[[mackinga2022twap]]. Like much of the rest of \u2018crypto\u2019, the proposed changes will concentrate decisions and economic rewards in the hands of major players, early investors, and incumbents.",
      "context": "440: \t- In addition, a recent upgrade (EIP-1559) results in tokens now being burnt at a higher rate than they are produced, deliberately leading to a diminishing supply. In theory, this increases the value of each ETH on the network at around 1% per year. It\u2019s very complex, with impacts on transaction fees, waiting time, and consensus security, as examined by Liu et al.[[liu2022empirical]]. Additionally, there is now talk (by [Buterin](https://time.com/6158182/vitalik-buterin-ethereum-profile/), the creator of Ethereum) of extending this burn mechanism [further into the network](https://ethresear.ch/t/multidimensional-eip-1559/11651).\n441: - Ethereum was designed from the beginning to move to a \u2018proof of stake\u2019 model where token holders underpin network consensus through complex automated voting systems based upon their token holding. This is now called [Ethereum Consensus Layer](https://blog.ethereum.org/2022/01/24/the-great-eth2-renaming/). This recent \u2018Merge\u2019 upgrade has reduced the carbon footprint of the network, a laudable thing, though it seems the GPUs and data centres have just gone on to be elsewhere. It has not lowered the cost to users nor improved performance. As part of the switching roadmap, users were asked to lock up 32 ETH tokens each (a substantial allocation of capital). In total, there are around 14 million of these tokens, and it is those users who now control the network. This money is likely stuck on the network until at least 2024, a significant delay when compared to the original promises.\n442: - This means that proof of stake has problems in that the majority owners \u2018decide\u2019 the truth of the chain to a degree and must by design have the ability to override prior consensus choices. Remember that these users are now trapped in their positions. Four major entities now control the rules of the chain and have already agreed to censor certain banned addresses. Proof of stake is probably inherently broken[[poelstra2015stake]]. This has [f](https://notes.ethereum.org/@djrtwo/risks-of-lsd)or malicious actors who have sufficient control of the existing history of the chain, thought to be [in the region of $50M](https://twitter.com/MTorgin/status/1521433474820890624)[[mackinga2022twap]]. Like much of the rest of \u2018crypto\u2019, the proposed changes will concentrate decisions and economic rewards in the hands of major players, early investors, and incumbents.\n443: - ![image.png](assets/image_1742487476628_0.png)\n444: - This is a far cry from the stated aims of the technology. The move to proof of stake has recently earned it the [MIT breakthrough technology award](https://www.technologyreview.com/2022/02/23/1044960/proof-of-stake-cryptocurrency/), despite not being complete (validators cannot yet sell their voting stakes). It\u2019s clearly a technology that is designed to innovate at the expense of predictability. This might work out very well for the platform, but right now the barrier to participation (in gas fees) is so high that we do not intend for Ethereum to be in scope as a method for value transfer within metaverses.",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0096-token.md",
      "line_number": 475,
      "marker_type": "NOTE",
      "content": "s.ethereum.org/@djrtwo/risks-of-lsd)or malicious actors who have sufficient control of the existing history of the chain, thought to be [in the region of $50M](https://twitter.com/MTorgin/status/1521433474820890624)[[mackinga2022twap]]. Like much of the rest of \u2018crypto\u2019, the proposed changes will concentrate decisions and economic rewards in the hands of major players, early investors, and incumbents.",
      "context": "473: \t- In addition, a recent upgrade (EIP-1559) results in tokens now being burnt at a higher rate than they are produced, deliberately leading to a diminishing supply. In theory, this increases the value of each ETH on the network at around 1% per year. It\u2019s very complex, with impacts on transaction fees, waiting time, and consensus security, as examined by Liu et al.[[liu2022empirical]]. Additionally, there is now talk (by [Buterin](https://time.com/6158182/vitalik-buterin-ethereum-profile/), the creator of Ethereum) of extending this burn mechanism [further into the network](https://ethresear.ch/t/multidimensional-eip-1559/11651).\n474: - Ethereum was designed from the beginning to move to a \u2018proof of stake\u2019 model where token holders underpin network consensus through complex automated voting systems based upon their token holding. This is now called [Ethereum Consensus Layer](https://blog.ethereum.org/2022/01/24/the-great-eth2-renaming/). This recent \u2018Merge\u2019 upgrade has reduced the carbon footprint of the network, a laudable thing, though it seems the GPUs and data centres have just gone on to be elsewhere. It has not lowered the cost to users nor improved performance. As part of the switching roadmap, users were asked to lock up 32 ETH tokens each (a substantial allocation of capital). In total, there are around 14 million of these tokens, and it is those users who now control the network. This money is likely stuck on the network until at least 2024, a significant delay when compared to the original promises.\n475: - This means that proof of stake has problems in that the majority owners \u2018decide\u2019 the truth of the chain to a degree and must by design have the ability to override prior consensus choices. Remember that these users are now trapped in their positions. Four major entities now control the rules of the chain and have already agreed to censor certain banned addresses. Proof of stake is probably inherently broken[[poelstra2015stake]]. This has [f](https://notes.ethereum.org/@djrtwo/risks-of-lsd)or malicious actors who have sufficient control of the existing history of the chain, thought to be [in the region of $50M](https://twitter.com/MTorgin/status/1521433474820890624)[[mackinga2022twap]]. Like much of the rest of \u2018crypto\u2019, the proposed changes will concentrate decisions and economic rewards in the hands of major players, early investors, and incumbents.\n476: - ![image.png](assets/image_1742487476628_0.png)\n477: - This is a far cry from the stated aims of the technology. The move to proof of stake has recently earned it the [MIT breakthrough technology award](https://www.technologyreview.com/2022/02/23/1044960/proof-of-stake-cryptocurrency/), despite not being complete (validators cannot yet sell their voting stakes). It\u2019s clearly a technology that is designed to innovate at the expense of predictability. This might work out very well for the platform, but right now the barrier to participation (in gas fees) is so high that we do not intend for Ethereum to be in scope as a method for value transfer within metaverses.",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0074-light-node.md",
      "line_number": 195,
      "marker_type": "HACK",
      "content": "aday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)",
      "context": "193: \t- **Early Foundations of NeRF:**\n194: \t\t- **Early Photography and Photosculpture (ca 1850):** Pioneers in photography began experimenting with aerial photogrammetry and photosculptures, creating 3D representations from multiple 2D photographs, laying groundwork for future 3D capture technologies.\n195: \t\t\t- [More on early photography](https://hackaday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)\n196: \t- **Plenoptic Function and Light Fields (1908 & 1936):**\n197: \t\t- Gabriel Lippmann introduces the concept of the **plenoptic function** which evolves into the **light field** concept, simplifying the capture and representation of light as it travels through space.",
      "severity": "warning"
    },
    {
      "file": "data/markdown/BC-0074-light-node.md",
      "line_number": 245,
      "marker_type": "HACK",
      "content": "aday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)",
      "context": "243: \t- **Early Foundations of NeRF:**\n244: \t\t- **Early Photography and Photosculpture (ca 1850):** Pioneers in photography began experimenting with aerial photogrammetry and photosculptures, creating 3D representations from multiple 2D photographs, laying groundwork for future 3D capture technologies.\n245: \t\t\t- [More on early photography](https://hackaday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)\n246: \t- **Plenoptic Function and Light Fields (1908 & 1936):**\n247: \t\t- Gabriel Lippmann introduces the concept of the **plenoptic function** which evolves into the **light field** concept, simplifying the capture and representation of light as it travels through space.",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Compliance Audit Trail.md",
      "line_number": 74,
      "marker_type": "TEMP",
      "content": "oral tracking",
      "context": "72: \t\t  )\n73: \n74: \t\t  # Temporal tracking\n75: \t\t  SubClassOf(mv:ComplianceAuditTrail\n76: \t\t    ObjectAllValuesFrom(mv:includes mv:Timestamp)",
      "severity": "warning"
    },
    {
      "file": "data/markdown/Policy Engine.md",
      "line_number": 68,
      "marker_type": "NOTE",
      "content": "s",
      "context": "66: \t\t  additional-sources:: ETSI GR ARF 010 \u00b7 OMA3 WG \u00b7 ISO 37301\n67: \t\t  \n68: \t\t  ## Notes\n69: \t\t  Relates to oversight and decision mechanisms for standards and user conduct within metaverse ecosystems.\n70: \t\t  ",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0037-public-key.md",
      "line_number": 640,
      "marker_type": "HACK",
      "content": "ernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.",
      "context": "638: \t- Public Page Automatically published\n639: - Ethereum[[buterin2013ethereum]] is the second most [secure](https://www.crypto51.app/) public blockchain (by about [50%](https://howmanyconfs.com/))[[sayeed2019assessing]], and second most valuable by [market capitalisation](https://coinmarketcap.com/) (though this comparison is somewhat stretched). It is the natural connection from Web3 to [[Blockchain]].\n640: - It is touted as \u2018programmable money\u2019. It, unlike Bitcoin, is [nearly](https://hackernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.\n641: - There are trade-offs and problems with Ethereum (Eth/Ether) that currently increase the \u2018participation floor\u2019 and make the network far less suitable for entry-level business-to-business use. The ledger itself, being a computational engine with write-only properties, is enormous. Specialist cloud hardware is required to run a full node (a copy of the ledger), and partial nodes are the norm. Many partial nodes are run by one specialist cloud provider ([Infura](https://consensys.net/blog/news/why-infura-is-the-secret-weapon-of-ethereum-infrastructure/)), which has been demonstrated capable of being forced to [exclude countries like Venezuela](https://finance.yahoo.com/news/metamask-infura-block-certain-areas-173749914.html) from the network. Network validators [refused to process](https://mevwatch.info) addresses on an [OFAC sanction list](https://home.treasury.gov/policy-issues/office-of-foreign-assets-control-sanctions-programs-and-information). A staggering 58% run on [Amazon AWS servers](https://ethernodes.org/networkType/Hosting). Critics of the project point to these vulnerabilities to outside influence as an existential threat to the aims of the technology. If it can be censured, then what advantage is there over the [founders](https://protos.com/consensys-lawsuit-jpmorgan-owns-critical-ethereum-infrastructure/) simply running a high-speed database for the same purpose?\n642: - This is a function of the so-called \u2018scalability trilemma\u2019[[hafid2020scaling]], in which it seems that only two features from the list of decentralisation, scalability, or security can be chosen for blockchains[[bonneau2015sok]].",
      "severity": "warning"
    },
    {
      "file": "data/markdown/BC-0037-public-key.md",
      "line_number": 834,
      "marker_type": "IDEA",
      "content": "s.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).",
      "context": "832: \t- A compelling correlation exists between environmental lead and violent crime trends. Both charts show a similar peak and decline \u2013 with lead exposure preceding the crime spike by roughly 23 years.\n833: \t- Economists like Rick Nevin and Jessica Reyes have studied this relationship extensively, finding convincing links across numerous countries.\n834: \t- A 2022 meta-analysis further strengthens the lead-crime hypothesis ([https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).\n835: - **How Lead Ravages the Body and Mind**\n836: \t- Lead mimics calcium, disrupting essential processes in bones, muscles, and crucially, the brain.",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0037-public-key.md",
      "line_number": 2109,
      "marker_type": "HACK",
      "content": "ernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.",
      "context": "2107: \t- Public Page Automatically published\n2108: - Ethereum[[buterin2013ethereum]] is the second most [secure](https://www.crypto51.app/) public blockchain (by about [50%](https://howmanyconfs.com/))[[sayeed2019assessing]], and second most valuable by [market capitalisation](https://coinmarketcap.com/) (though this comparison is somewhat stretched). It is the natural connection from Web3 to [[Blockchain]].\n2109: - It is touted as \u2018programmable money\u2019. It, unlike Bitcoin, is [nearly](https://hackernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.\n2110: - There are trade-offs and problems with Ethereum (Eth/Ether) that currently increase the \u2018participation floor\u2019 and make the network far less suitable for entry-level business-to-business use. The ledger itself, being a computational engine with write-only properties, is enormous. Specialist cloud hardware is required to run a full node (a copy of the ledger), and partial nodes are the norm. Many partial nodes are run by one specialist cloud provider ([Infura](https://consensys.net/blog/news/why-infura-is-the-secret-weapon-of-ethereum-infrastructure/)), which has been demonstrated capable of being forced to [exclude countries like Venezuela](https://finance.yahoo.com/news/metamask-infura-block-certain-areas-173749914.html) from the network. Network validators [refused to process](https://mevwatch.info) addresses on an [OFAC sanction list](https://home.treasury.gov/policy-issues/office-of-foreign-assets-control-sanctions-programs-and-information). A staggering 58% run on [Amazon AWS servers](https://ethernodes.org/networkType/Hosting). Critics of the project point to these vulnerabilities to outside influence as an existential threat to the aims of the technology. If it can be censured, then what advantage is there over the [founders](https://protos.com/consensys-lawsuit-jpmorgan-owns-critical-ethereum-infrastructure/) simply running a high-speed database for the same purpose?\n2111: - This is a function of the so-called \u2018scalability trilemma\u2019[[hafid2020scaling]], in which it seems that only two features from the list of decentralisation, scalability, or security can be chosen for blockchains[[bonneau2015sok]].",
      "severity": "warning"
    },
    {
      "file": "data/markdown/BC-0037-public-key.md",
      "line_number": 2294,
      "marker_type": "IDEA",
      "content": "s.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).",
      "context": "2292: \t- A compelling correlation exists between environmental lead and violent crime trends. Both charts show a similar peak and decline \u2013 with lead exposure preceding the crime spike by roughly 23 years.\n2293: \t- Economists like Rick Nevin and Jessica Reyes have studied this relationship extensively, finding convincing links across numerous countries.\n2294: \t- A 2022 meta-analysis further strengthens the lead-crime hypothesis ([https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).\n2295: - **How Lead Ravages the Body and Mind**\n2296: \t- Lead mimics calcium, disrupting essential processes in bones, muscles, and crucially, the brain.",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0101-transaction-fee.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0114-burning-mechanism.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/Metaverse Architecture Stack.md",
      "line_number": 76,
      "marker_type": "NOTE",
      "content": "Arithmetic comparison (lessThan) not expressible in OWL 2 DL",
      "context": "74: \n75: \t\t  # Layer dependency constraints - upper layers depend on lower layers\n76: \t\t  # Note: Arithmetic comparison (lessThan) not expressible in OWL 2 DL\n77: \t\t  # Layer ordering must be validated through application logic\n78: \t\t  SubClassOf(mv:MetaverseArchitectureStack",
      "severity": "info"
    },
    {
      "file": "data/markdown/OntologyDefinition.md",
      "line_number": 204,
      "marker_type": "NOTE",
      "content": "Hardware and Software are kept disjoint. Firmware is a type of Software with a specific relation to Hardware.",
      "context": "202: \t\t        )\n203: \t\t      )\n204: \t\t      # Note: Hardware and Software are kept disjoint. Firmware is a type of Software with a specific relation to Hardware.\n205: \t\t      DisjointClasses(mv:Hardware mv:Software)\n206: \t\t    )",
      "severity": "info"
    },
    {
      "file": "data/markdown/Pre-Training.md",
      "line_number": 751,
      "marker_type": "IDEA",
      "content": "s-ncbr.pl/en/research/neural-rendering/).",
      "context": "749: \t- Plenoxels (Plenoptic Voxels): This method replaces neural networks with a sparse 3D grid of spherical harmonics, enabling faster training and competitive quality compared to NeRFs[](https://github.com/weihaox/awesome-neural-rendering/blob/master/docs/INTRODUCTION-AND-SURVEY.md).\n750: \t- NGLOD (Neural Geometric Level of Detail): This approach combines neural implicit representations with explicit geometric representations, allowing for multi-resolution rendering and faster training[](https://arxiv.org/abs/2402.00028).\n751: \t- NeRF-MAE (Masked AutoEncoders for NeRFs): This technique applies the concept of masked autoencoders to NeRFs for self-supervised 3D representation learning, potentially improving generalization and efficiency[](https://ideas-ncbr.pl/en/research/neural-rendering/).\n752: \n753: \t\t- #### Protein Language Models",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0105-tokenomics.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/markdown/VERIFICATION-REPORT.md",
      "line_number": 150,
      "marker_type": "NOTE",
      "content": "BC-0456 attempted but file empty",
      "context": "148: \t\t  ### 3. Digital Identity (BC-0456 to BC-0460) - 5 Terms \u2705\n149: \t\t  \n150: \t\t  #### Note: BC-0456 attempted but file empty\n151: \t\t  - \u26a0\ufe0f BC-0456: Self-Sovereign Identity (Content prepared, 4,800 words ready)\n152: \t\t    - 10 principles of SSI",
      "severity": "info"
    },
    {
      "file": "data/markdown/BC-0108-base-fee.md",
      "line_number": 304,
      "marker_type": "NOTE",
      "content": "s",
      "context": "302: \t\t  - Dependent concepts from other categories\n303: \t\t  \n304: \t\t  ## Notes\n305: \t\t  - Implementation-specific considerations\n306: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0112 inflationary token.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/Public Key.md",
      "line_number": 647,
      "marker_type": "HACK",
      "content": "ernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.",
      "context": "645: \t- Public Page Automatically published\n646: - Ethereum[[buterin2013ethereum]] is the second most [secure](https://www.crypto51.app/) public blockchain (by about [50%](https://howmanyconfs.com/))[[sayeed2019assessing]], and second most valuable by [market capitalisation](https://coinmarketcap.com/) (though this comparison is somewhat stretched). It is the natural connection from Web3 to [[Blockchain]].\n647: - It is touted as \u2018programmable money\u2019. It, unlike Bitcoin, is [nearly](https://hackernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.\n648: - There are trade-offs and problems with Ethereum (Eth/Ether) that currently increase the \u2018participation floor\u2019 and make the network far less suitable for entry-level business-to-business use. The ledger itself, being a computational engine with write-only properties, is enormous. Specialist cloud hardware is required to run a full node (a copy of the ledger), and partial nodes are the norm. Many partial nodes are run by one specialist cloud provider ([Infura](https://consensys.net/blog/news/why-infura-is-the-secret-weapon-of-ethereum-infrastructure/)), which has been demonstrated capable of being forced to [exclude countries like Venezuela](https://finance.yahoo.com/news/metamask-infura-block-certain-areas-173749914.html) from the network. Network validators [refused to process](https://mevwatch.info) addresses on an [OFAC sanction list](https://home.treasury.gov/policy-issues/office-of-foreign-assets-control-sanctions-programs-and-information). A staggering 58% run on [Amazon AWS servers](https://ethernodes.org/networkType/Hosting). Critics of the project point to these vulnerabilities to outside influence as an existential threat to the aims of the technology. If it can be censured, then what advantage is there over the [founders](https://protos.com/consensys-lawsuit-jpmorgan-owns-critical-ethereum-infrastructure/) simply running a high-speed database for the same purpose?\n649: - This is a function of the so-called \u2018scalability trilemma\u2019[[hafid2020scaling]], in which it seems that only two features from the list of decentralisation, scalability, or security can be chosen for blockchains[[bonneau2015sok]].",
      "severity": "warning"
    },
    {
      "file": "data/pages/Public Key.md",
      "line_number": 841,
      "marker_type": "IDEA",
      "content": "s.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).",
      "context": "839: \t- A compelling correlation exists between environmental lead and violent crime trends. Both charts show a similar peak and decline \u2013 with lead exposure preceding the crime spike by roughly 23 years.\n840: \t- Economists like Rick Nevin and Jessica Reyes have studied this relationship extensively, finding convincing links across numerous countries.\n841: \t- A 2022 meta-analysis further strengthens the lead-crime hypothesis ([https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).\n842: - **How Lead Ravages the Body and Mind**\n843: \t- Lead mimics calcium, disrupting essential processes in bones, muscles, and crucially, the brain.",
      "severity": "info"
    },
    {
      "file": "data/pages/Public Key.md",
      "line_number": 2116,
      "marker_type": "HACK",
      "content": "ernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.",
      "context": "2114: \t- Public Page Automatically published\n2115: - Ethereum[[buterin2013ethereum]] is the second most [secure](https://www.crypto51.app/) public blockchain (by about [50%](https://howmanyconfs.com/))[[sayeed2019assessing]], and second most valuable by [market capitalisation](https://coinmarketcap.com/) (though this comparison is somewhat stretched). It is the natural connection from Web3 to [[Blockchain]].\n2116: - It is touted as \u2018programmable money\u2019. It, unlike Bitcoin, is [nearly](https://hackernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.\n2117: - There are trade-offs and problems with Ethereum (Eth/Ether) that currently increase the \u2018participation floor\u2019 and make the network far less suitable for entry-level business-to-business use. The ledger itself, being a computational engine with write-only properties, is enormous. Specialist cloud hardware is required to run a full node (a copy of the ledger), and partial nodes are the norm. Many partial nodes are run by one specialist cloud provider ([Infura](https://consensys.net/blog/news/why-infura-is-the-secret-weapon-of-ethereum-infrastructure/)), which has been demonstrated capable of being forced to [exclude countries like Venezuela](https://finance.yahoo.com/news/metamask-infura-block-certain-areas-173749914.html) from the network. Network validators [refused to process](https://mevwatch.info) addresses on an [OFAC sanction list](https://home.treasury.gov/policy-issues/office-of-foreign-assets-control-sanctions-programs-and-information). A staggering 58% run on [Amazon AWS servers](https://ethernodes.org/networkType/Hosting). Critics of the project point to these vulnerabilities to outside influence as an existential threat to the aims of the technology. If it can be censured, then what advantage is there over the [founders](https://protos.com/consensys-lawsuit-jpmorgan-owns-critical-ethereum-infrastructure/) simply running a high-speed database for the same purpose?\n2118: - This is a function of the so-called \u2018scalability trilemma\u2019[[hafid2020scaling]], in which it seems that only two features from the list of decentralisation, scalability, or security can be chosen for blockchains[[bonneau2015sok]].",
      "severity": "warning"
    },
    {
      "file": "data/pages/Public Key.md",
      "line_number": 2301,
      "marker_type": "IDEA",
      "content": "s.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).",
      "context": "2299: \t- A compelling correlation exists between environmental lead and violent crime trends. Both charts show a similar peak and decline \u2013 with lead exposure preceding the crime spike by roughly 23 years.\n2300: \t- Economists like Rick Nevin and Jessica Reyes have studied this relationship extensively, finding convincing links across numerous countries.\n2301: \t- A 2022 meta-analysis further strengthens the lead-crime hypothesis ([https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).\n2302: - **How Lead Ravages the Body and Mind**\n2303: \t- Lead mimics calcium, disrupting essential processes in bones, muscles, and crucially, the brain.",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0118 market capitalization.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/solar plans.md",
      "line_number": 145,
      "marker_type": "IDEA",
      "content": "l Applications",
      "context": "143:   4. **Shading Losses**: 8-35% efficiency loss on lower tiers\n144: - ## Recommendations\n145: - ### Ideal Applications\n146:   \n147:   This three-tier system is particularly suitable for:",
      "severity": "info"
    },
    {
      "file": "data/pages/Diagrams as Code.md",
      "line_number": 444,
      "marker_type": "HACK",
      "content": "age.haskell.org/package/erd)",
      "context": "442: - [Ditaa](http://ditaa.sourceforge.net/)\n443: - [Nomnoml](https://www.nomnoml.com/)\n444: - [Erd](https://hackage.haskell.org/package/erd)\n445: - [Pikchr](https://pikchr.org/)\n446: - [Structurizr](https://structurizr.com/)",
      "severity": "warning"
    },
    {
      "file": "data/pages/Various Links.md",
      "line_number": 178,
      "marker_type": "HACK",
      "content": "ernoon.com/grayscales-gbtc-pump-effect-means-2021-will-start-slow-0gu340l)",
      "context": "176: - [Gourieff/comfyui-reactor-node](https://github.com/Gourieff/comfyui-reactor-node)\n177: - [Government power to reverse crypto](https://www.marketwatch.com/story/government-must-have-power-to-reverse-crypto-transactions-says-co-chair-of-blockchain-caucus-11624995008)\n178: - [Grayscale's GBTC](https://hackernoon.com/grayscales-gbtc-pump-effect-means-2021-will-start-slow-0gu340l)\n179: - [Guideline for Augmented Reality](https://ieeexplore.ieee.org/document/9284693)\n180: - [Guohao Li Camel ChatBot](https://www.linkedin.com/posts/guohao-li-9a573b136_camel-chatbot-demo-activity-7051390760327225344-8D2A)",
      "severity": "warning"
    },
    {
      "file": "data/pages/Decentralization.md",
      "line_number": 644,
      "marker_type": "HACK",
      "content": "age.haskell.org/package/erd)",
      "context": "642: - [Ditaa](http://ditaa.sourceforge.net/)\n643: - [Nomnoml](https://www.nomnoml.com/)\n644: - [Erd](https://hackage.haskell.org/package/erd)\n645: - [Pikchr](https://pikchr.org/)\n646: - [Structurizr](https://structurizr.com/)",
      "severity": "warning"
    },
    {
      "file": "data/pages/Decentralization.md",
      "line_number": 1064,
      "marker_type": "HACK",
      "content": "age.haskell.org/package/erd)",
      "context": "1062: - [Ditaa](http://ditaa.sourceforge.net/)\n1063: - [Nomnoml](https://www.nomnoml.com/)\n1064: - [Erd](https://hackage.haskell.org/package/erd)\n1065: - [Pikchr](https://pikchr.org/)\n1066: - [Structurizr](https://structurizr.com/)",
      "severity": "warning"
    },
    {
      "file": "data/pages/Traceability Mechanism.md",
      "line_number": 83,
      "marker_type": "TEMP",
      "content": "oral Properties",
      "context": "81:     (ObjectSomeValuesFrom aigo:tracesModification aigo:SystemModification))\n82: \n83:   ## Temporal Properties\n84:   SubClassOf(aigo:TraceabilityMechanism\n85:     (DataSomeValuesFrom aigo:hasRetentionPeriod xsd:duration))",
      "severity": "warning"
    },
    {
      "file": "data/pages/BC 0105 tokenomics.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0101 transaction fee.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0113 emission schedule.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0107 gas limit.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/Ethereum.md",
      "line_number": 5,
      "marker_type": "HACK",
      "content": "ernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.",
      "context": "3: \t- Public Page Automatically published\n4: - Ethereum[[buterin2013ethereum]] is the second most [secure](https://www.crypto51.app/) public blockchain (by about [50%](https://howmanyconfs.com/))[[sayeed2019assessing]], and second most valuable by [market capitalisation](https://coinmarketcap.com/) (though this comparison is somewhat stretched). It is the natural connection from Web3 to [[Blockchain]].\n5: - It is touted as \u2018programmable money\u2019. It, unlike Bitcoin, is [nearly](https://hackernoon.com/turing-completeness-and-the-ethereum-blockchain-c5a93b865c1a) Turing complete[[petzold2008annotated]], able to run a [virtual machine](https://ethereum.org/en/developers/docs/evm/) within the distributed network (albeit slowly), and can therefore process complex transactional contracts in the settlement of value. This has given rise to the new field of \u2018distributed finance\u2019, or DeFi (described later), alongside many interesting trust-minimised immutable ledger public database ideas.\n6: - There are trade-offs and problems with Ethereum (Eth/Ether) that currently increase the \u2018participation floor\u2019 and make the network far less suitable for entry-level business-to-business use. The ledger itself, being a computational engine with write-only properties, is enormous. Specialist cloud hardware is required to run a full node (a copy of the ledger), and partial nodes are the norm. Many partial nodes are run by one specialist cloud provider ([Infura](https://consensys.net/blog/news/why-infura-is-the-secret-weapon-of-ethereum-infrastructure/)), which has been demonstrated capable of being forced to [exclude countries like Venezuela](https://finance.yahoo.com/news/metamask-infura-block-certain-areas-173749914.html) from the network. Network validators [refused to process](https://mevwatch.info) addresses on an [OFAC sanction list](https://home.treasury.gov/policy-issues/office-of-foreign-assets-control-sanctions-programs-and-information). A staggering 58% run on [Amazon AWS servers](https://ethernodes.org/networkType/Hosting). Critics of the project point to these vulnerabilities to outside influence as an existential threat to the aims of the technology. If it can be censured, then what advantage is there over the [founders](https://protos.com/consensys-lawsuit-jpmorgan-owns-critical-ethereum-infrastructure/) simply running a high-speed database for the same purpose?\n7: - This is a function of the so-called \u2018scalability trilemma\u2019[[hafid2020scaling]], in which it seems that only two features from the list of decentralisation, scalability, or security can be chosen for blockchains[[bonneau2015sok]].",
      "severity": "warning"
    },
    {
      "file": "data/pages/Ethereum.md",
      "line_number": 27,
      "marker_type": "NOTE",
      "content": "s.ethereum.org/@djrtwo/risks-of-lsd)or malicious actors who have sufficient control of the existing history of the chain, thought to be [in the region of $50M](https://twitter.com/MTorgin/status/1521433474820890624)[[mackinga2022twap]]. Like much of the rest of \u2018crypto\u2019, the proposed changes will concentrate decisions and economic rewards in the hands of major players, early investors, and incumbents.",
      "context": "25: \t- In addition, a recent upgrade (EIP-1559) results in tokens now being burnt at a higher rate than they are produced, deliberately leading to a diminishing supply. In theory, this increases the value of each ETH on the network at around 1% per year. It\u2019s very complex, with impacts on transaction fees, waiting time, and consensus security, as examined by Liu et al.[[liu2022empirical]]. Additionally, there is now talk (by [Buterin](https://time.com/6158182/vitalik-buterin-ethereum-profile/), the creator of Ethereum) of extending this burn mechanism [further into the network](https://ethresear.ch/t/multidimensional-eip-1559/11651).\n26: - Ethereum was designed from the beginning to move to a \u2018proof of stake\u2019 model where token holders underpin network consensus through complex automated voting systems based upon their token holding. This is now called [Ethereum Consensus Layer](https://blog.ethereum.org/2022/01/24/the-great-eth2-renaming/). This recent \u2018Merge\u2019 upgrade has reduced the carbon footprint of the network, a laudable thing, though it seems the GPUs and data centres have just gone on to be elsewhere. It has not lowered the cost to users nor improved performance. As part of the switching roadmap, users were asked to lock up 32 ETH tokens each (a substantial allocation of capital). In total, there are around 14 million of these tokens, and it is those users who now control the network. This money is likely stuck on the network until at least 2024, a significant delay when compared to the original promises.\n27: - This means that proof of stake has problems in that the majority owners \u2018decide\u2019 the truth of the chain to a degree and must by design have the ability to override prior consensus choices. Remember that these users are now trapped in their positions. Four major entities now control the rules of the chain and have already agreed to censor certain banned addresses. Proof of stake is probably inherently broken[[poelstra2015stake]]. This has [f](https://notes.ethereum.org/@djrtwo/risks-of-lsd)or malicious actors who have sufficient control of the existing history of the chain, thought to be [in the region of $50M](https://twitter.com/MTorgin/status/1521433474820890624)[[mackinga2022twap]]. Like much of the rest of \u2018crypto\u2019, the proposed changes will concentrate decisions and economic rewards in the hands of major players, early investors, and incumbents.\n28: - ![image.png](../assets/image_1742487476628_0.png)\n29: - This is a far cry from the stated aims of the technology. The move to proof of stake has recently earned it the [MIT breakthrough technology award](https://www.technologyreview.com/2022/02/23/1044960/proof-of-stake-cryptocurrency/), despite not being complete (validators cannot yet sell their voting stakes). It\u2019s clearly a technology that is designed to innovate at the expense of predictability. This might work out very well for the platform, but right now the barrier to participation (in gas fees) is so high that we do not intend for Ethereum to be in scope as a method for value transfer within metaverses.",
      "severity": "info"
    },
    {
      "file": "data/pages/Light Node.md",
      "line_number": 202,
      "marker_type": "HACK",
      "content": "aday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)",
      "context": "200: \t- **Early Foundations of NeRF:**\n201: \t\t- **Early Photography and Photosculpture (ca 1850):** Pioneers in photography began experimenting with aerial photogrammetry and photosculptures, creating 3D representations from multiple 2D photographs, laying groundwork for future 3D capture technologies.\n202: \t\t\t- [More on early photography](https://hackaday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)\n203: \t- **Plenoptic Function and Light Fields (1908 & 1936):**\n204: \t\t- Gabriel Lippmann introduces the concept of the **plenoptic function** which evolves into the **light field** concept, simplifying the capture and representation of light as it travels through space.",
      "severity": "warning"
    },
    {
      "file": "data/pages/Light Node.md",
      "line_number": 252,
      "marker_type": "HACK",
      "content": "aday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)",
      "context": "250: \t- **Early Foundations of NeRF:**\n251: \t\t- **Early Photography and Photosculpture (ca 1850):** Pioneers in photography began experimenting with aerial photogrammetry and photosculptures, creating 3D representations from multiple 2D photographs, laying groundwork for future 3D capture technologies.\n252: \t\t\t- [More on early photography](https://hackaday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)\n253: \t- **Plenoptic Function and Light Fields (1908 & 1936):**\n254: \t\t- Gabriel Lippmann introduces the concept of the **plenoptic function** which evolves into the **light field** concept, simplifying the capture and representation of light as it travels through space.",
      "severity": "warning"
    },
    {
      "file": "data/pages/BC 0108 base fee.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/Algorithmic Transparency Index.md",
      "line_number": 72,
      "marker_type": "TEMP",
      "content": "oral tracking",
      "context": "70: \t\t  )\n71: \n72: \t\t  # Temporal tracking\n73: \t\t  SubClassOf(mv:AlgorithmicTransparencyIndex\n74: \t\t    DataSomeValuesFrom(mv:hasAssessmentDate rdfs:Literal)",
      "severity": "warning"
    },
    {
      "file": "data/pages/BC 0102 inflation.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/Logseq Test API Calls from Python.md",
      "line_number": 312,
      "marker_type": "NOTE",
      "content": "page name must be lower case",
      "context": "310: \t\t\t  import js\n311: \t\t\t  pagerequested=\"PythonSample1\"\n312: \t\t\t  page=js.logseq.api.get_page(pagerequested.lower()) #note page name must be lower case \n313: \t\t\t  if page == None:\n314: \t\t\t    js.alert(f\"page '{pagerequested}' not found\")",
      "severity": "info"
    },
    {
      "file": "data/pages/Logseq Test API Calls from Python.md",
      "line_number": 325,
      "marker_type": "NOTE",
      "content": "page name must be lower case",
      "context": "323: \t\t\t  import js\n324: \t\t\t  pagerequested=\"PythonSample1\"\n325: \t\t\t  page=js.logseq.api.get_page_blocks_tree(pagerequested.lower()) #note page name must be lower case \n326: \t\t\t  js.alert(props(\"Page PythonSample1 Block Tree\",page))\n327: \t\t\t  \"Done\"",
      "severity": "info"
    },
    {
      "file": "data/pages/Logseq Test API Calls from Python.md",
      "line_number": 334,
      "marker_type": "NOTE",
      "content": "page name must be lower case",
      "context": "332: \t\t\t  import js\n333: \t\t\t  pagerequested=\"PythonSample1\"\n334: \t\t\t  pagelinkedrefs=js.logseq.api.get_page_linked_references(pagerequested.lower()) #note page name must be lower case \n335: \t\t\t  output=\"\"\n336: \t\t\t  for linkedrefobj in pagelinkedrefs:",
      "severity": "info"
    },
    {
      "file": "data/pages/Logseq Test API Calls from Python.md",
      "line_number": 594,
      "marker_type": "TEMP",
      "content": "lates API",
      "context": "592: \t\t- exper_request #apicall-ignore\n593: \t\t- http_request_abort #apicall-ignore\n594: \t- ### Templates API\n595: \t\t- get_template(templatename) #apicall-ignore\n596: \t\t- insert_template(targetUUID,templatename) #apicall-ignore",
      "severity": "warning"
    },
    {
      "file": "data/pages/BC 0103 halving.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0110 fee market.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/Mixed reality.md",
      "line_number": 191,
      "marker_type": "HACK",
      "content": "md.io/@xr/monetization)",
      "context": "189: - [The paper presents a monocular depth estimation method using denoising diffusion models. The goal is to generate accurate depth maps from single RGB images. The authors address the problem of noisy and incomplete depth maps in the training data by using step-unrolled denoising diffusion, an L1 loss, and depth infilling during training.  To overcome the limited availability of supervised training data, the authors leverage pre-training on self-supervised image-to-image translation tasks. Despite the simplicity of the approach, their model achieves state-of-the-art (SOTA) performance on the indoor NYU dataset and near SOTA results on the outdoor KITTI dataset.  The approach involves infilling missing depth in ground truth depth maps using nearest neighbor interpolation. Then, noise is added to the depth map and a neural network is trained to predict the noise given the RGB image and noisy depth map. During fine-tuning, one step of the forward pass is unrolled and the ground truth depth map is replaced with the model's prediction.  The DepthGen model achieves an absolute relative error of 0.074 on the indoor NYU dataset and a competitive relative error of 0.064 on the outdoor KITTI dataset, demonstrating its accuracy in depth estimation.  The paper also introduces a text-to-3D pipeline that combines DepthGen with off-the-shelf text-to-image and text-conditioned image completion models. This pipeline allows for generating 3D point clouds from text prompts.  In conclusion, the proposed method of monocular depth estimation using diffusion models achieves state-of-the-art performance, even with limited supervised training data. The approach is simple yet effective and can be integrated into a text-to-3D pipeline for generating 3D scenes from text prompts.](https://depth-gen.github.io/)\n190: - [The text provided is a collection of video titles and descriptions related to Blender, AI, and 3D design. The videos cover topics such as creating isometric rooms, using AI in 3D design, Unreal Engine, toon shading in Blender, QR code art, GPT (Generative Pre-trained Transformer) engineering, creating Ghibli-style characters, new features in Blender 3.6, animation in Blender, and adding vegetation in Twinmotion. The videos are created by various individuals and brands, including vertex vendor, Unreal Sensei, Quick QR Art, ENFANT TERRIBLE, Matt Wolfe, Ian Wootten, Brandon's Drawings, Polyfjord, Charlie Barber, and vishal panjeta. The text also mentions a Google company and provides information about cookie usage and privacy settings when using Google services.](https://www.youtube.com/watch?v=GZO7TAlVE_8)\n191: - [WebXR is a device API that allows for VR/AR experiences through web browsers. However, monetization has been a major issue for the platform, with indie creators struggling to capture value. Most WebXR apps appear as prototypes because developers find it difficult to justify investing more resources into the ecosystem. The current ways people pay for WebXR content include purchasing tickets, using cryptocurrency for virtual land, and accessing certain features by login or ownership of bot handles. The process of paying for WebXR content can be made easier and more frictionless by integrating payment methods like Apple Pay or Google Pay while in VR. Artists can get paid through various means such as commissions, Patreon, grants, VC investment, and event tickets. Non-payment based monetization strategies like advertising are also being explored. A list of 101 ideas for WebXR monetization includes platforms like Patreon and Github Sponsors, virtual market stalls, virtual land parcels, and in-world advertising. Other strategies include payment processing integration with platforms like PayPal or Discord, creating virtual actors and performers, storytelling, and podcast sponsorships. Advertisements targeted at 18-44 year old males interested in software, gaming, and VR have shown promising results. A Github repository for WebXR monetization examples is in progress. Despite these efforts, monetization in the WebXR ecosystem is still a work in progress, and more exploration and innovation is needed.](https://hackmd.io/@xr/monetization)\n192: - The paper proposes a system called CLIP-Actor, which animates a 3D human mesh to conform to a text prompt by recommending a motion sequence and optimizing mesh style attributes. The system's novelty lies in its ability to recommend motion that conforms to the prompt in a pose-agnostic and temporally-consistent manner while leveraging multi-frame human motion and rejecting poorly rendered views. The authors demonstrate that CLIP-Actor produces plausible and human-recognizable style 3D human mesh in motion with detailed geometry and texture solely from a natural language prompt. The paper's methodology shows that CLIP-Actor is an effective and efficient way to generate plausible results when the pose of an artist-designed mesh does not conform to the text prompt from the beginning. The research has been sponsored by the Korean government's grant funded by the Institute of Information & communications Technology Planning & Evaluation (IITP). https://clip-actor.github.io\n193: - [The paper CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes proposes a system for animating human meshes based on text prompts. The system, called CLIP-Actor, generates motion sequences and optimizes mesh style attributes to conform to a given text prompt.  The authors highlight a limitation of previous work, which struggled to produce realistic results when the starting pose of a pre-designed mesh did not align with the text prompt. To address this issue, CLIP-Actor leverages a large-scale human motion dataset with language labels to build a text-driven human motion recommendation system. It suggests a motion sequence that aligns with the given prompt in a coarse-to-fine manner.  In addition, the authors introduce a novel neural style optimization technique that adds detail and texture to the recommended mesh sequence in a temporally-consistent and pose-agnostic manner. They also propose spatio-temporal view augmentation and mask-weighted embedding attention techniques to stabilize the optimization process by incorporating multi-frame human motion and rejecting poorly rendered views.  The results of CLIP-Actor demonstrate its ability to generate plausible and human-recognizable 3D human meshes in motion with detailed geometry and texture solely from natural language prompts.  The paper includes the BibTeX citation for academic referencing and acknowledges the support received from the Institute of Information and Communications Technology Planning and Evaluation (IITP) in Korea for funding the research.  The website containing the paper and code is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. The source code for the system is mainly borrowed from Keunhong Park's Nerfies website, and feedback and questions can be directed to Kim Ji-Yeon.](https://clip-actor.github.io)",
      "severity": "warning"
    },
    {
      "file": "data/pages/BC 0116 total supply.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/Display Metrology.md",
      "line_number": 169,
      "marker_type": "NOTE",
      "content": "s on Revision",
      "context": "167:   - Continued evolution of human visual response modelling to account for advances in display technology and viewing conditions\n168: \n169: ## Notes on Revision\n170: \n171: The original definition conflated display metrology (the science of measurement) with measurement instrumentation. The revised entry clarifies that display metrology is a disciplinary approach encompassing standardised methods, frameworks, and scientific principles\u2014of which instruments are merely tools. The current definition was overly narrow and equipment-focused, whereas display metrology encompasses theoretical foundations, standardisation efforts, and evolving measurement methodologies. The 2025 release of IDMS v1.3 represents a significant development warranting inclusion, though time-sensitive announcements regarding release dates have been contextualised appropriately. UK-specific context remains limited in available literature; this reflects genuine gaps in the search results rather than oversight.",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0111 deflationary token.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0120 incentive alignment.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/Product Design.md",
      "line_number": 825,
      "marker_type": "NOTE",
      "content": "s for later",
      "context": "823: - ### Security evaluation\n824: \t- As part of developing our stack we will penetration test the deployment as detailed using [Hexway](https://hexway.io/)\n825: - ### notes for later\n826: \t- Notes on build-out The world database in the shared rooms in the metaverse is the global object master, educational materials, videos, audio content and branded objects are fungible tokens authentically proved by rgb client side validation between parties, only validated ones will be persisted in shared rooms like conferences and classes according to the room schema. That allows educators to monetise their content. That can work on lightning. NFT objects between parties like content crafted by participants (coursework, homework) are not on lightning and will attract main chain fees but are rarer. User authentication and communication will be through nostr.\n827: \t  ",
      "severity": "info"
    },
    {
      "file": "data/pages/Compliance Audit Trail.md",
      "line_number": 75,
      "marker_type": "TEMP",
      "content": "oral tracking",
      "context": "73: \t\t  )\n74: \n75: \t\t  # Temporal tracking\n76: \t\t  SubClassOf(mv:ComplianceAuditTrail\n77: \t\t    ObjectAllValuesFrom(mv:includes mv:Timestamp)",
      "severity": "warning"
    },
    {
      "file": "data/pages/Nostr protocol.md",
      "line_number": 5,
      "marker_type": "NOTE",
      "content": "s and Other Stuff by Relays",
      "context": "3: - #Public page automatically published\n4: - [Decentralized publishing for the web (nostr.how)](https://nostr.how/en/what-is-nostr)\n5: - # Notes and Other Stuff by Relays\n6: \t- Nostr [pronounced no-star] is a decentralized open protocol that aims to improve the social media experience by addressing issues of censorship and data collection. The protocol operates by allowing users to post and view notes on servers called relays, and view and post these notes through apps called clients. The open nature of the protocol allows for competition and a free flow of information, as users can choose to use different relays or clients if they are censored. This is because the protocol is decentralized and controlled by no one.\n7: \t  id:: 67a49f1d-4088-4392-ad62-8e93673a2dff",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0114 burning mechanism.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0109 priority fee.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0115 minting.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0106 gas price.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/Metaverse Architecture Stack.md",
      "line_number": 77,
      "marker_type": "NOTE",
      "content": "Arithmetic comparison (lessThan) not expressible in OWL 2 DL",
      "context": "75: \n76: \t\t  # Layer dependency constraints - upper layers depend on lower layers\n77: \t\t  # Note: Arithmetic comparison (lessThan) not expressible in OWL 2 DL\n78: \t\t  # Layer ordering must be validated through application logic\n79: \t\t  SubClassOf(mv:MetaverseArchitectureStack",
      "severity": "info"
    },
    {
      "file": "data/pages/AI Development.md",
      "line_number": 14,
      "marker_type": "REVIEW",
      "content": "ed and Improved Ontology Entry: AI Development",
      "context": "12: \n13: \n14: # Reviewed and Improved Ontology Entry: AI Development\n15: \n16: ## Academic Context",
      "severity": "info"
    },
    {
      "file": "data/pages/Lead Poisoning Hypothesis.md",
      "line_number": 14,
      "marker_type": "IDEA",
      "content": "s.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).",
      "context": "12: \t- A compelling correlation exists between environmental lead and violent crime trends. Both charts show a similar peak and decline \u2013 with lead exposure preceding the crime spike by roughly 23 years.\n13: \t- Economists like Rick Nevin and Jessica Reyes have studied this relationship extensively, finding convincing links across numerous countries.\n14: \t- A 2022 meta-analysis further strengthens the lead-crime hypothesis ([https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html](https://ideas.repec.org/a/eee/regeco/v97y2022ics0166046222000667.html)).\n15: - **How Lead Ravages the Body and Mind**\n16: \t- Lead mimics calcium, disrupting essential processes in bones, muscles, and crucially, the brain.",
      "severity": "info"
    },
    {
      "file": "data/pages/Scene Capture and Reconstruction.md",
      "line_number": 7,
      "marker_type": "HACK",
      "content": "aday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)",
      "context": "5: \t- **Early Foundations of NeRF:**\n6: \t\t- **Early Photography and Photosculpture (ca 1850):** Pioneers in photography began experimenting with aerial photogrammetry and photosculptures, creating 3D representations from multiple 2D photographs, laying groundwork for future 3D capture technologies.\n7: \t\t\t- [More on early photography](https://hackaday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)\n8: \t- **Plenoptic Function and Light Fields (1908 & 1936):**\n9: \t\t- Gabriel Lippmann introduces the concept of the **plenoptic function** which evolves into the **light field** concept, simplifying the capture and representation of light as it travels through space.",
      "severity": "warning"
    },
    {
      "file": "data/pages/IoT AI Integration (AI-0438).md",
      "line_number": 234,
      "marker_type": "NOTE",
      "content": "s for later",
      "context": "232: \t\t- Integration challenges, limitations of AI, compatibility issues.\n233: \n234: - ### notes for later\n235: \t\t- Develop or utilize tools for issuing and managing RGB assets.\n236: \t- **Agent Integration:**",
      "severity": "info"
    },
    {
      "file": "data/pages/Gaussian splatting and Similar.md",
      "line_number": 31,
      "marker_type": "IDEA",
      "content": "s-ncbr.pl/en/research/neural-rendering/).",
      "context": "29: \t- Plenoxels (Plenoptic Voxels): This method replaces neural networks with a sparse 3D grid of spherical harmonics, enabling faster training and competitive quality compared to NeRFs[](https://github.com/weihaox/awesome-neural-rendering/blob/master/docs/INTRODUCTION-AND-SURVEY.md).\n30: \t- NGLOD (Neural Geometric Level of Detail): This approach combines neural implicit representations with explicit geometric representations, allowing for multi-resolution rendering and faster training[](https://arxiv.org/abs/2402.00028).\n31: \t- NeRF-MAE (Masked AutoEncoders for NeRFs): This technique applies the concept of masked autoencoders to NeRFs for self-supervised 3D representation learning, potentially improving generalization and efficiency[](https://ideas-ncbr.pl/en/research/neural-rendering/).\n32: \t- ## NeRFs vs Hardware Acceleration\n33: \t\t- old page, needs [[Update Cycle]]",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0104 supply cap.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0117 circulating supply.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/BC 0119 economic security.md",
      "line_number": 307,
      "marker_type": "NOTE",
      "content": "s",
      "context": "305: \t\t  - Dependent concepts from other categories\n306: \t\t  \n307: \t\t  ## Notes\n308: \t\t  - Implementation-specific considerations\n309: \t\t  - Historical context and terminology evolution",
      "severity": "info"
    },
    {
      "file": "data/pages/Introduction to me.md",
      "line_number": 64,
      "marker_type": "NOTE",
      "content": "about links",
      "context": "62: - ### Housekeeping\n63:   collapsed:: true\n64: \t- ## Note about links\n65: \t\t- In *my* version of the knowledge graph all the twitter links render interactively, inline, you will just see \"loading\" on the web. There are a lot of them, like this one. Sometimes you might just seen a loading with no link, that means I forgot that one, it's not going to load.\n66: \t\t- [twitter link to the render loading below](https://twitter.com/bentossell/status/1758235873433243950)",
      "severity": "info"
    },
    {
      "file": "client/src/features/ontology/README_ONTOLOGY_RENDERING.md",
      "line_number": 160,
      "marker_type": "NOTE",
      "content": "s",
      "context": "158: 8. \ud83d\udd32 Integrate with hornedowl reasoning\n159: \n160: ## Notes\n161: \n162: - Client-side collapsing logic: NOT YET IMPLEMENTED (user requested checking)",
      "severity": "info"
    },
    {
      "file": "docs/audits/neo4j-migration-action-plan.md",
      "line_number": 435,
      "marker_type": "NOTE",
      "content": "This won't work because sqlite_settings_repository.rs is deleted",
      "context": "433: \n434: # Verify old tests pass (requires SQLite setup)\n435: # Note: This won't work because sqlite_settings_repository.rs is deleted\n436: ```\n437: ",
      "severity": "info"
    },
    {
      "file": "docs/multi-agent-docker/docker-environment.md",
      "line_number": 121,
      "marker_type": "TEMP",
      "content": "late for .env",
      "context": "119: \u251c\u2500\u2500 Dockerfile                  # Main container build definition\n120: \u251c\u2500\u2500 .env                        # Environment configuration (user-created)\n121: \u251c\u2500\u2500 .env.example                # Template for .env\n122: \u251c\u2500\u2500 entrypoint.sh               # Container init (simplified)\n123: \u251c\u2500\u2500 entrypoint-wrapper.sh       # Full init with security checks",
      "severity": "warning"
    },
    {
      "file": "docs/guides/vircadia-xr-complete-guide.md",
      "line_number": 310,
      "marker_type": "IDEA",
      "content": "l edge length (meters)",
      "context": "308:   private readonly REPULSION-STRENGTH = 1000;\n309:   private readonly SPRING-STRENGTH = 0.1;\n310:   private readonly SPRING-LENGTH = 5.0;      // Ideal edge length (meters)\n311:   private readonly DAMPING = 0.9;\n312:   private readonly CENTERING-STRENGTH = 0.01;",
      "severity": "info"
    },
    {
      "file": "docs/guides/orchestrating-agents.md",
      "line_number": 339,
      "marker_type": "REVIEW",
      "content": "best practices",
      "context": "337:         security-issues = await self.check-security(code-submission)\n338: \n339:         # Review best practices\n340:         practice-issues = await self.check-best-practices(code-submission)\n341: ",
      "severity": "info"
    },
    {
      "file": "docs/guides/troubleshooting.md",
      "line_number": 98,
      "marker_type": "TEMP",
      "content": "orary fix (session only)",
      "context": "96: **Solution**:\n97: ```bash\n98: # Temporary fix (session only)\n99: sudo chmod 666 /var/run/docker.sock\n100: ",
      "severity": "warning"
    },
    {
      "file": "docs/guides/development-workflow.md",
      "line_number": 1075,
      "marker_type": "REVIEW",
      "content": "Checklist",
      "context": "1073: ## Code Review Process\n1074: \n1075: ### Review Checklist\n1076: \n1077: **For Reviewers**:",
      "severity": "info"
    },
    {
      "file": "docs/guides/development-workflow.md",
      "line_number": 1120,
      "marker_type": "REVIEW",
      "content": "Process",
      "context": "1118: - \u2705 Dependencies secure\n1119: \n1120: ### Review Process\n1121: \n1122: 1. **Automated Checks**",
      "severity": "info"
    },
    {
      "file": "docs/guides/development-workflow.md",
      "line_number": 1135,
      "marker_type": "REVIEW",
      "content": "Comments",
      "context": "1133: 3. **Provide Feedback**\n1134:    ```markdown\n1135:    ## Review Comments\n1136: \n1137:    ### Must Fix (Blocking)",
      "severity": "info"
    },
    {
      "file": "docs/implementation/p1-1-summary.md",
      "line_number": 171,
      "marker_type": "NOTE",
      "content": "s",
      "context": "169: 6. Monitor logs for configuration update messages\n170: \n171: ## Notes\n172: \n173: - Implementation follows existing patterns in the codebase",
      "severity": "info"
    },
    {
      "file": "docs/concepts/neo4j-integration.md",
      "line_number": 410,
      "marker_type": "XXX",
      "content": "xx.databases.neo4j.io",
      "context": "408: ```bash\n409: # Use managed Neo4j (AuraDB, EC2, etc.)\n410: NEO4J-URI=neo4j+s://xxxxx.databases.neo4j.io\n411: \n412: # Enable TLS",
      "severity": "error"
    },
    {
      "file": "docs/concepts/ontology-reasoning.md",
      "line_number": 334,
      "marker_type": "NOTE",
      "content": "Other codebase errors exist but are unrelated to this deliverable",
      "context": "332: cargo check --lib --features ontology\n333: # Result: \u2705 Reasoning module compiles successfully\n334: # Note: Other codebase errors exist but are unrelated to this deliverable\n335: ```\n336: ",
      "severity": "info"
    },
    {
      "file": "docs/concepts/architecture/cqrs-directive-template.md",
      "line_number": 6,
      "marker_type": "TEMP",
      "content": "late Structure",
      "context": "4: ---\n5: \n6: ## Template Structure\n7: \n8: ### File: `src/application/graph/directives.rs`",
      "severity": "warning"
    },
    {
      "file": "docs/concepts/architecture/cqrs-directive-template.md",
      "line_number": 479,
      "marker_type": "NOTE",
      "content": "Repository needs delete-node method",
      "context": "477: \n478:         tokio::runtime::Handle::current().block-on(async move {\n479:             // Note: Repository needs delete-node method\n480:             // This is a placeholder - actual implementation depends on repository trait\n481:             // repository.delete-node(node-id).await.map-err(|e| {",
      "severity": "info"
    },
    {
      "file": "docs/concepts/architecture/cqrs-directive-template.md",
      "line_number": 559,
      "marker_type": "NOTE",
      "content": "Repository needs delete-edge method",
      "context": "557: \n558:         tokio::runtime::Handle::current().block-on(async move {\n559:             // Note: Repository needs delete-edge method\n560:             // repository.delete-edge(&edge-id).await.map-err(|e| {\n561:             //     Hexserror::adapter(",
      "severity": "info"
    },
    {
      "file": "docs/concepts/architecture/semantic-physics-system.md",
      "line_number": 92,
      "marker_type": "IDEA",
      "content": "l distance",
      "context": "90:     pub node-a: String,\n91:     pub node-b: String,\n92:     pub distance: f32,         // Ideal distance\n93:     pub strength: f32,         // Symmetrical force\n94:     pub priority: u8,",
      "severity": "info"
    },
    {
      "file": "docs/concepts/architecture/semantic-forces-system.md",
      "line_number": 108,
      "marker_type": "TEMP",
      "content": "oral or ordered",
      "context": "106:     Hierarchy,   // Parent-child\n107:     Association,\n108:     Sequence,    // Temporal or ordered\n109:     SubClassOf,  // Ontology\n110:     InstanceOf,  // Ontology",
      "severity": "warning"
    },
    {
      "file": "docs/guides/developer/01-development-setup.md",
      "line_number": 581,
      "marker_type": "TEMP",
      "content": "late",
      "context": "579: \n580: ```\n581: .env.example         # Template\n582: .env                 # Local development (gitignored)\n583: .env.test           # Test environment",
      "severity": "warning"
    },
    {
      "file": "tests/endpoint-analysis/COMPREHENSIVE_FINDINGS.md",
      "line_number": 180,
      "marker_type": "REVIEW",
      "content": "package.json",
      "context": "178: docker exec visionflow_container ls -la /app/backend/data/\n179: \n180: # Review package.json\n181: docker exec visionflow_container cat /app/backend/package.json | grep sqlite\n182: ",
      "severity": "info"
    },
    {
      "file": "archive/phase-5-reports-2025-11-06/PHASE-5-QUALITY-SUMMARY.md",
      "line_number": 230,
      "marker_type": "TEMP",
      "content": "late:",
      "context": "228: python3 scripts/add_frontmatter.py\n229: \n230: # Template:\n231: ---\n232: title: \"Document Title\"",
      "severity": "warning"
    },
    {
      "file": "archive/multi-agent-docker-isolated-docs-2025-11-05/docs/DOCKER_MANAGER_BUILD_FIXES.md",
      "line_number": 247,
      "marker_type": "NOTE",
      "content": "s",
      "context": "245: - Container Architecture: `/home/devuser/workspace/project/multi-agent-docker/CLAUDE.md`\n246: \n247: ## Notes\n248: \n249: ### Security Considerations",
      "severity": "info"
    },
    {
      "file": "archive/multi-agent-docker-isolated-docs-2025-11-05/docs/NODE_VERSION_UPDATE.md",
      "line_number": 129,
      "marker_type": "NOTE",
      "content": "s",
      "context": "127: - [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)\n128: \n129: ## Notes\n130: - Node.js v23.x is the current stable release as of October 2025\n131: - v22.12.0 LTS would also be compatible, but v23.11.1 is more current",
      "severity": "info"
    },
    {
      "file": "archive/multi-agent-docker-isolated-docs-2025-11-05/docs/releases/NPM-PUBLISH-GUIDE-v1.2.0.md",
      "line_number": 268,
      "marker_type": "NOTE",
      "content": "After 72 hours, can only deprecate",
      "context": "266: npm unpublish agentic-flow@1.2.0\n267: \n268: # Note: After 72 hours, can only deprecate\n269: ```\n270: ",
      "severity": "info"
    },
    {
      "file": "archive/multi-agent-docker-isolated-docs-2025-11-05/docs/developer/command-reference.md",
      "line_number": 417,
      "marker_type": "REVIEW",
      "content": "results",
      "context": "415: cf-sparc-plan        # Create execution plan\n416: cf-sparc-execute     # Execute plan\n417: cf-sparc-review      # Review results\n418: ```\n419: ",
      "severity": "info"
    },
    {
      "file": "archive/multi-agent-docker-isolated-docs-2025-11-05/docs/developer/command-reference.md",
      "line_number": 1201,
      "marker_type": "REVIEW",
      "content": "with optimized model",
      "context": "1199: af-gemini-task tester \"Generate basic tests\"\n1200: \n1201: # Review with optimized model\n1202: af-optimize-speed --agent reviewer --task \"Quick review\"\n1203: ```",
      "severity": "info"
    },
    {
      "file": "archive/archive/working-documents-2025-11-05/SEMANTIC_FEATURES_INTEGRATION_PLAN.md",
      "line_number": 60,
      "marker_type": "TEMP",
      "content": "oral or ordered",
      "context": "58:     Hierarchy,   // Parent-child\n59:     Association,\n60:     Sequence,    // Temporal or ordered\n61:     SubClassOf,  // Ontology\n62:     InstanceOf,  // Ontology",
      "severity": "warning"
    },
    {
      "file": "multi-agent-docker/docs/TERMINAL_GRID.md",
      "line_number": 138,
      "marker_type": "NOTE",
      "content": "on tmux",
      "context": "136: 3. Or manually close and relaunch terminals\n137: \n138: ## Note on tmux\n139: \n140: VNC terminals do NOT use tmux - they are independent shell sessions. This prevents the confusing behavior of multiple terminals showing the same tmux session.",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/docs/fixes/GPU_BUILD_STATUS.md",
      "line_number": 192,
      "marker_type": "NOTE",
      "content": "s",
      "context": "190: - **Testing & Validation**: 1-2 hours\n191: \n192: ## Notes\n193: \n194: - Many errors were auto-fixed by the Rust linter during development",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/ffmpeg-processing/SKILL.md",
      "line_number": 271,
      "marker_type": "NOTE",
      "content": "s",
      "context": "269: - docker-manager - Batch processing in containers\n270: \n271: ## Notes\n272: \n273: - FFmpeg 8.0 with full codec support",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/rust-development/SKILL.md",
      "line_number": 144,
      "marker_type": "NOTE",
      "content": "s",
      "context": "142: - git - Version control for projects\n143: \n144: ## Notes\n145: \n146: - Compile times can be significant for large projects",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/jupyter-notebooks/SKILL.md",
      "line_number": 41,
      "marker_type": "NOTE",
      "content": "book Management",
      "context": "39: ## Available Operations\n40: \n41: ### Notebook Management\n42: - `create_notebook` - Create new notebook with optional cells\n43: - `list_notebooks` - List all notebooks in directory",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/jupyter-notebooks/SKILL.md",
      "line_number": 227,
      "marker_type": "NOTE",
      "content": "s",
      "context": "225: ```\n226: \n227: ## Notes\n228: \n229: - Compatible with Claude Code and other MCP clients",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/latex-documents/SKILL.md",
      "line_number": 618,
      "marker_type": "NOTE",
      "content": "s Pages",
      "context": "616: ```\n617: \n618: #### Notes Pages\n619: \n620: Add speaker notes:",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/latex-documents/SKILL.md",
      "line_number": 910,
      "marker_type": "NOTE",
      "content": "s",
      "context": "908: - **Additional packages**: Install via `tlmgr`\n909: \n910: ## Notes\n911: \n912: - Basic TeX Live installation (~500MB)",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/latex-documents/BEAMER-QUICK-REFERENCE.md",
      "line_number": 244,
      "marker_type": "TEMP",
      "content": "lates",
      "context": "242: ```\n243: \n244: ## Templates\n245: \n246: | Template | Use Case |",
      "severity": "warning"
    },
    {
      "file": "multi-agent-docker/skills/latex-documents/BEAMER-QUICK-REFERENCE.md",
      "line_number": 256,
      "marker_type": "TEMP",
      "content": "lates",
      "context": "254: ```\n255: latex-documents/\n256: \u251c\u2500\u2500 templates/beamer/          # Templates\n257: \u2502   \u251c\u2500\u2500 amurmaple-basic.tex\n258: \u2502   \u251c\u2500\u2500 amurmaple-academic.tex",
      "severity": "warning"
    },
    {
      "file": "multi-agent-docker/skills/pytorch-ml/SKILL.md",
      "line_number": 249,
      "marker_type": "NOTE",
      "content": "s",
      "context": "247: 6. Profile with `torch.profiler` for optimization\n248: \n249: ## Notes\n250: \n251: - CUDA 12+ installed with cuDNN",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/import-to-ontology/SKILL.md",
      "line_number": 935,
      "marker_type": "REVIEW",
      "content": "report, then import",
      "context": "933: claude-code \"Use import-to-ontology skill to dry-run source-notes.md\"\n934: \n935: # Review report, then import\n936: claude-code \"Use import-to-ontology skill to import source-notes.md\"\n937: ```",
      "severity": "info"
    },
    {
      "file": "multi-agent-docker/skills/latex-documents/examples/beamer/README.md",
      "line_number": 20,
      "marker_type": "TEMP",
      "content": "lates (in `../../templates/beamer/`)",
      "context": "18: ## Available Files\n19: \n20: ### Templates (in `../../templates/beamer/`)\n21: \n22: 1. **amurmaple-basic.tex**",
      "severity": "warning"
    }
  ],
  "summary": {
    "error_count": 10,
    "warning_count": 193,
    "info_count": 255
  }
}