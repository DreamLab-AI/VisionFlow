//! Vector embeddings and semantic indexing
//!
//! High-performance embedding generation and vector similarity search

use crate::error::{Result, RetrieverError};
use crate::config::VectorConfig;
use async_trait::async_trait;\nuse candle_core::{Device, Tensor};\nuse candle_nn::VarBuilder;\nuse candle_transformers::models::bert::BertModel;\nuse futures::stream::{self, StreamExt};\nuse hnsw::{Hnsw, Params};\nuse indexmap::IndexMap;\nuse rayon::prelude::*;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\n\n/// Vector embedding representation\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Embedding {\n    /// Document or text ID\n    pub id: String,\n    \n    /// Dense vector representation\n    pub vector: Vec<f32>,\n    \n    /// Original text\n    pub text: String,\n    \n    /// Metadata\n    pub metadata: HashMap<String, String>,\n    \n    /// Timestamp\n    pub timestamp: i64,\n}\n\n/// Similarity search result\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SimilarityResult {\n    /// Document ID\n    pub id: String,\n    \n    /// Similarity score (0.0 to 1.0)\n    pub score: f32,\n    \n    /// Distance metric\n    pub distance: f32,\n    \n    /// Associated embedding\n    pub embedding: Embedding,\n}\n\n/// Batch embedding request\n#[derive(Debug, Clone)]\npub struct EmbeddingBatch {\n    /// Text chunks to embed\n    pub texts: Vec<String>,\n    \n    /// Associated IDs\n    pub ids: Vec<String>,\n    \n    /// Metadata for each text\n    pub metadata: Vec<HashMap<String, String>>,\n}\n\n/// Embedding model trait for different implementations\n#[async_trait]\npub trait EmbeddingModel: Send + Sync {\n    /// Generate embedding for a single text\n    async fn embed_text(&self, text: &str) -> Result<Vec<f32>>;\n    \n    /// Generate embeddings for multiple texts in batch\n    async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>>;\n    \n    /// Get embedding dimension\n    fn dimension(&self) -> usize;\n    \n    /// Get model information\n    fn model_info(&self) -> ModelInfo;\n}\n\n/// Model information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ModelInfo {\n    pub name: String,\n    pub version: String,\n    pub dimension: usize,\n    pub max_sequence_length: usize,\n    pub tokenizer_vocab_size: usize,\n}\n\n/// Transformer-based embedding model\npub struct TransformerEmbeddingModel {\n    model: BertModel,\n    device: Device,\n    config: VectorConfig,\n    tokenizer: Arc<tokenizers::Tokenizer>,\n}\n\nimpl TransformerEmbeddingModel {\n    /// Create new transformer embedding model\n    pub async fn new(config: VectorConfig) -> Result<Self> {\n        let device = if config.use_gpu && Device::cuda_if_available(0).is_cuda() {\n            Device::cuda_if_available(0)\n        } else {\n            Device::Cpu\n        };\n        \n        // Load tokenizer\n        let tokenizer = tokenizers::Tokenizer::from_file(&format!(\"{}/tokenizer.json\", config.model_path))\n            .map_err(|e| RetrieverError::model_load(format!(\"Failed to load tokenizer: {}\", e)))?;\n        \n        // Load model weights\n        let model = Self::load_model(&config.model_path, &device).await?;\n        \n        Ok(Self {\n            model,\n            device,\n            config,\n            tokenizer: Arc::new(tokenizer),\n        })\n    }\n    \n    async fn load_model(model_path: &str, device: &Device) -> Result<BertModel> {\n        // Simplified model loading - in practice, you'd load from HuggingFace or local files\n        let var_builder = VarBuilder::from_tensors(HashMap::new(), candle_core::DType::F32, device);\n        \n        // This is a placeholder - actual implementation would load pre-trained weights\n        BertModel::load(&var_builder, &Default::default())\n            .map_err(|e| RetrieverError::model_load(format!(\"Failed to load BERT model: {}\", e)))\n    }\n    \n    fn preprocess_text(&self, text: &str) -> String {\n        // Text preprocessing\n        let mut processed = text.to_lowercase();\n        \n        // Truncate if too long\n        if processed.len() > self.config.max_text_length {\n            processed.truncate(self.config.max_text_length);\n        }\n        \n        processed\n    }\n    \n    fn tokenize_text(&self, text: &str) -> Result<Vec<u32>> {\n        let encoding = self.tokenizer\n            .encode(text, true)\n            .map_err(|e| RetrieverError::embedding(format!(\"Tokenization failed: {}\", e)))?;\n        \n        Ok(encoding.get_ids().to_vec())\n    }\n    \n    async fn forward_pass(&self, input_ids: &[u32]) -> Result<Vec<f32>> {\n        // Convert to tensor\n        let input_tensor = Tensor::new(input_ids, &self.device)\n            .map_err(|e| RetrieverError::embedding(format!(\"Tensor creation failed: {}\", e)))?;\n        \n        // Forward pass through model\n        let output = self.model.forward(&input_tensor)\n            .map_err(|e| RetrieverError::embedding(format!(\"Model forward pass failed: {}\", e)))?;\n        \n        // Extract embeddings (mean pooling)\n        let embeddings = self.mean_pooling(&output).await?;\n        \n        // Normalize embeddings\n        self.normalize_embedding(embeddings)\n    }\n    \n    async fn mean_pooling(&self, hidden_states: &Tensor) -> Result<Vec<f32>> {\n        // Mean pooling implementation\n        let mean_pooled = hidden_states.mean(1)\n            .map_err(|e| RetrieverError::embedding(format!(\"Mean pooling failed: {}\", e)))?;\n        \n        let values = mean_pooled.to_vec1::<f32>()\n            .map_err(|e| RetrieverError::embedding(format!(\"Tensor to vector conversion failed: {}\", e)))?;\n        \n        Ok(values)\n    }\n    \n    fn normalize_embedding(&self, mut embedding: Vec<f32>) -> Result<Vec<f32>> {\n        // L2 normalization\n        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();\n        \n        if norm > 0.0 {\n            for val in &mut embedding {\n                *val /= norm;\n            }\n        }\n        \n        Ok(embedding)\n    }\n}\n\n#[async_trait]\nimpl EmbeddingModel for TransformerEmbeddingModel {\n    async fn embed_text(&self, text: &str) -> Result<Vec<f32>> {\n        let processed_text = self.preprocess_text(text);\n        let tokens = self.tokenize_text(&processed_text)?;\n        self.forward_pass(&tokens).await\n    }\n    \n    async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        // Process texts in parallel batches\n        let batch_size = self.config.batch_size;\n        let mut results = Vec::with_capacity(texts.len());\n        \n        for chunk in texts.chunks(batch_size) {\n            let batch_results: Result<Vec<_>> = stream::iter(chunk)\n                .map(|text| async move {\n                    self.embed_text(text).await\n                })\n                .buffer_unordered(self.config.batch_size)\n                .collect::<Vec<_>>()\n                .await\n                .into_iter()\n                .collect();\n            \n            results.extend(batch_results?);\n        }\n        \n        Ok(results)\n    }\n    \n    fn dimension(&self) -> usize {\n        self.config.dimension\n    }\n    \n    fn model_info(&self) -> ModelInfo {\n        ModelInfo {\n            name: self.config.model_path.clone(),\n            version: \"1.0\".to_string(),\n            dimension: self.config.dimension,\n            max_sequence_length: self.config.max_text_length,\n            tokenizer_vocab_size: 30522, // BERT vocab size\n        }\n    }\n}\n\n/// High-performance vector index using HNSW\npub struct VectorIndex {\n    /// HNSW index\n    index: Arc<RwLock<Hnsw<f32, usize>>>,\n    \n    /// Embedding storage\n    embeddings: Arc<RwLock<IndexMap<String, Embedding>>>,\n    \n    /// ID to index mapping\n    id_to_index: Arc<RwLock<HashMap<String, usize>>>,\n    \n    /// Index to ID mapping\n    index_to_id: Arc<RwLock<HashMap<usize, String>>>,\n    \n    /// Configuration\n    config: VectorConfig,\n    \n    /// Next available index\n    next_index: Arc<RwLock<usize>>,\n}\n\nimpl VectorIndex {\n    /// Create new vector index\n    pub fn new(config: VectorConfig) -> Result<Self> {\n        let params = Params::new()\n            .ef_construction(config.hnsw.ef_construction)\n            .m(config.hnsw.m)\n            .ml(config.hnsw.ml)\n            .ef(config.hnsw.ef_search);\n        \n        let index = Hnsw::new(params, config.dimension);\n        \n        Ok(Self {\n            index: Arc::new(RwLock::new(index)),\n            embeddings: Arc::new(RwLock::new(IndexMap::new())),\n            id_to_index: Arc::new(RwLock::new(HashMap::new())),\n            index_to_id: Arc::new(RwLock::new(HashMap::new())),\n            config,\n            next_index: Arc::new(RwLock::new(0)),\n        })\n    }\n    \n    /// Add embedding to index\n    pub async fn add_embedding(&self, embedding: Embedding) -> Result<()> {\n        let mut index = self.index.write().await;\n        let mut embeddings = self.embeddings.write().await;\n        let mut id_to_index = self.id_to_index.write().await;\n        let mut index_to_id = self.index_to_id.write().await;\n        let mut next_index = self.next_index.write().await;\n        \n        let current_index = *next_index;\n        \n        // Add to HNSW index\n        index.add_point(current_index, &embedding.vector)\n            .map_err(|e| RetrieverError::vector_index(format!(\"Failed to add to HNSW index: {:?}\", e)))?;\n        \n        // Update mappings\n        id_to_index.insert(embedding.id.clone(), current_index);\n        index_to_id.insert(current_index, embedding.id.clone());\n        embeddings.insert(embedding.id.clone(), embedding);\n        \n        *next_index += 1;\n        \n        Ok(())\n    }\n    \n    /// Add multiple embeddings in batch\n    pub async fn add_embeddings_batch(&self, embeddings: Vec<Embedding>) -> Result<()> {\n        // Process embeddings in parallel\n        let results: Vec<Result<()>> = stream::iter(embeddings)\n            .map(|embedding| async move {\n                self.add_embedding(embedding).await\n            })\n            .buffer_unordered(self.config.batch_size)\n            .collect()\n            .await;\n        \n        // Check for errors\n        for result in results {\n            result?;\n        }\n        \n        Ok(())\n    }\n    \n    /// Search for similar vectors\n    pub async fn search(&self, query_vector: &[f32], k: usize) -> Result<Vec<SimilarityResult>> {\n        let index = self.index.read().await;\n        let embeddings = self.embeddings.read().await;\n        let index_to_id = self.index_to_id.read().await;\n        \n        // Search HNSW index\n        let search_results = index.search(query_vector, k)\n            .map_err(|e| RetrieverError::vector_index(format!(\"HNSW search failed: {:?}\", e)))?;\n        \n        let mut results = Vec::with_capacity(search_results.len());\n        \n        for (distance, index_id) in search_results {\n            if let Some(doc_id) = index_to_id.get(&index_id) {\n                if let Some(embedding) = embeddings.get(doc_id) {\n                    // Convert distance to similarity score\n                    let similarity = 1.0 / (1.0 + distance);\n                    \n                    // Filter by threshold\n                    if similarity >= self.config.similarity_threshold {\n                        results.push(SimilarityResult {\n                            id: doc_id.clone(),\n                            score: similarity,\n                            distance,\n                            embedding: embedding.clone(),\n                        });\n                    }\n                }\n            }\n        }\n        \n        // Sort by similarity score (descending)\n        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal));\n        \n        Ok(results)\n    }\n    \n    /// Get embedding by ID\n    pub async fn get_embedding(&self, id: &str) -> Result<Option<Embedding>> {\n        let embeddings = self.embeddings.read().await;\n        Ok(embeddings.get(id).cloned())\n    }\n    \n    /// Remove embedding by ID\n    pub async fn remove_embedding(&self, id: &str) -> Result<bool> {\n        let mut embeddings = self.embeddings.write().await;\n        let mut id_to_index = self.id_to_index.write().await;\n        let mut index_to_id = self.index_to_id.write().await;\n        \n        if let Some(index_id) = id_to_index.remove(id) {\n            embeddings.remove(id);\n            index_to_id.remove(&index_id);\n            \n            // Note: HNSW doesn't support removal, so we just mark as deleted in our mappings\n            Ok(true)\n        } else {\n            Ok(false)\n        }\n    }\n    \n    /// Get index statistics\n    pub async fn stats(&self) -> IndexStats {\n        let embeddings = self.embeddings.read().await;\n        let next_index = self.next_index.read().await;\n        \n        IndexStats {\n            total_embeddings: embeddings.len(),\n            dimension: self.config.dimension,\n            next_index: *next_index,\n            memory_usage_mb: self.estimate_memory_usage().await,\n        }\n    }\n    \n    async fn estimate_memory_usage(&self) -> f64 {\n        let embeddings = self.embeddings.read().await;\n        let embedding_size = self.config.dimension * 4; // f32 = 4 bytes\n        let total_embedding_memory = embeddings.len() * embedding_size;\n        \n        // Add overhead for mappings and index structures\n        let overhead = embeddings.len() * 100; // Rough estimate\n        \n        (total_embedding_memory + overhead) as f64 / 1024.0 / 1024.0\n    }\n    \n    /// Bulk similarity search for multiple queries\n    pub async fn bulk_search(&self, queries: &[Vec<f32>], k: usize) -> Result<Vec<Vec<SimilarityResult>>> {\n        // Process queries in parallel\n        let results: Result<Vec<_>> = stream::iter(queries)\n            .map(|query| async move {\n                self.search(query, k).await\n            })\n            .buffer_unordered(self.config.batch_size)\n            .collect::<Vec<_>>()\n            .await\n            .into_iter()\n            .collect();\n        \n        results\n    }\n    \n    /// Update embedding\n    pub async fn update_embedding(&self, embedding: Embedding) -> Result<()> {\n        // Remove old embedding if exists\n        self.remove_embedding(&embedding.id).await?;\n        \n        // Add new embedding\n        self.add_embedding(embedding).await\n    }\n    \n    /// Get similar embeddings by ID\n    pub async fn get_similar_by_id(&self, id: &str, k: usize) -> Result<Vec<SimilarityResult>> {\n        if let Some(embedding) = self.get_embedding(id).await? {\n            self.search(&embedding.vector, k + 1).await.map(|mut results| {\n                // Remove the query embedding itself\n                results.retain(|r| r.id != id);\n                results.truncate(k);\n                results\n            })\n        } else {\n            Err(RetrieverError::not_found(format!(\"Embedding with ID: {}\", id)))\n        }\n    }\n}\n\n/// Index statistics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct IndexStats {\n    pub total_embeddings: usize,\n    pub dimension: usize,\n    pub next_index: usize,\n    pub memory_usage_mb: f64,\n}\n\n/// Parallel embedding processor\npub struct EmbeddingProcessor {\n    model: Arc<dyn EmbeddingModel>,\n    config: VectorConfig,\n}\n\nimpl EmbeddingProcessor {\n    /// Create new embedding processor\n    pub fn new(model: Arc<dyn EmbeddingModel>, config: VectorConfig) -> Self {\n        Self { model, config }\n    }\n    \n    /// Process text batch and generate embeddings\n    pub async fn process_batch(&self, batch: EmbeddingBatch) -> Result<Vec<Embedding>> {\n        if batch.texts.len() != batch.ids.len() || batch.texts.len() != batch.metadata.len() {\n            return Err(RetrieverError::invalid_input(\"Batch texts, IDs, and metadata must have same length\"));\n        }\n        \n        // Generate embeddings\n        let vectors = self.model.embed_batch(&batch.texts).await?;\n        \n        // Create embedding objects\n        let timestamp = chrono::Utc::now().timestamp();\n        let embeddings: Vec<Embedding> = batch.texts\n            .into_iter()\n            .zip(batch.ids)\n            .zip(batch.metadata)\n            .zip(vectors)\n            .map(|(((text, id), metadata), vector)| Embedding {\n                id,\n                vector,\n                text,\n                metadata,\n                timestamp,\n            })\n            .collect();\n        \n        Ok(embeddings)\n    }\n    \n    /// Process single text\n    pub async fn process_single(&self, id: String, text: String, metadata: HashMap<String, String>) -> Result<Embedding> {\n        let vector = self.model.embed_text(&text).await?;\n        let timestamp = chrono::Utc::now().timestamp();\n        \n        Ok(Embedding {\n            id,\n            vector,\n            text,\n            metadata,\n            timestamp,\n        })\n    }\n    \n    /// Process texts in parallel with optimal batching\n    pub async fn process_parallel(&self, texts: Vec<(String, String, HashMap<String, String>)>) -> Result<Vec<Embedding>> {\n        let batch_size = self.config.batch_size;\n        let mut all_embeddings = Vec::with_capacity(texts.len());\n        \n        // Process in chunks\n        for chunk in texts.chunks(batch_size) {\n            let (ids, texts, metadata): (Vec<_>, Vec<_>, Vec<_>) = chunk.iter().cloned().unzip3();\n            \n            let batch = EmbeddingBatch {\n                texts,\n                ids,\n                metadata,\n            };\n            \n            let embeddings = self.process_batch(batch).await?;\n            all_embeddings.extend(embeddings);\n        }\n        \n        Ok(all_embeddings)\n    }\n}\n\n/// Helper trait for unzipping 3-tuples\ntrait Unzip3<A, B, C> {\n    fn unzip3(self) -> (Vec<A>, Vec<B>, Vec<C>);\n}\n\nimpl<A, B, C, I> Unzip3<A, B, C> for I\nwhere\n    I: Iterator<Item = (A, B, C)>,\n{\n    fn unzip3(self) -> (Vec<A>, Vec<B>, Vec<C>) {\n        let mut vec_a = Vec::new();\n        let mut vec_b = Vec::new();\n        let mut vec_c = Vec::new();\n        \n        for (a, b, c) in self {\n            vec_a.push(a);\n            vec_b.push(b);\n            vec_c.push(c);\n        }\n        \n        (vec_a, vec_b, vec_c)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::collections::HashMap;\n    \n    #[tokio::test]\n    async fn test_vector_index_operations() {\n        let config = VectorConfig::default();\n        let index = VectorIndex::new(config).unwrap();\n        \n        // Create test embedding\n        let embedding = Embedding {\n            id: \"test1\".to_string(),\n            vector: vec![0.1, 0.2, 0.3, 0.4],\n            text: \"test text\".to_string(),\n            metadata: HashMap::new(),\n            timestamp: 123456789,\n        };\n        \n        // Add embedding\n        index.add_embedding(embedding.clone()).await.unwrap();\n        \n        // Retrieve embedding\n        let retrieved = index.get_embedding(\"test1\").await.unwrap();\n        assert!(retrieved.is_some());\n        assert_eq!(retrieved.unwrap().id, \"test1\");\n        \n        // Search for similar\n        let query_vector = vec![0.1, 0.2, 0.3, 0.4];\n        let results = index.search(&query_vector, 5).await.unwrap();\n        assert!(!results.is_empty());\n        assert_eq!(results[0].id, \"test1\");\n    }\n    \n    #[tokio::test]\n    async fn test_embedding_batch_processing() {\n        let config = VectorConfig::default();\n        \n        let batch = EmbeddingBatch {\n            texts: vec![\"text1\".to_string(), \"text2\".to_string()],\n            ids: vec![\"id1\".to_string(), \"id2\".to_string()],\n            metadata: vec![HashMap::new(), HashMap::new()],\n        };\n        \n        assert_eq!(batch.texts.len(), batch.ids.len());\n        assert_eq!(batch.texts.len(), batch.metadata.len());\n    }\n    \n    #[test]\n    fn test_similarity_score_calculation() {\n        let distance = 0.5;\n        let similarity = 1.0 / (1.0 + distance);\n        assert!((similarity - 0.6667).abs() < 0.001);\n    }\n}\n"