# GitHub Sync Service - Status Report

**Date**: 2025-10-25
**Session**: Context continuation after summary
**Status**: üü° Partial Success - Code Fixed, Database Issues Remain

---

## Executive Summary

The GitHub sync service has been successfully refactored to use **data accumulation** instead of per-file database saves, which was causing data loss. However, the sync is still reporting **0 Knowledge Graph files processed** due to database errors, despite:
- ‚úÖ Detection logic working (files ARE being detected)
- ‚úÖ Accumulation code deployed and verified
- ‚úÖ Parser has NO database calls (pure in-memory)
- ‚ùå Database errors occurring: "UNIQUE constraint failed" and "nested transactions"

---

## Critical Findings

### 1. **Root Cause of Original Data Loss** ‚úÖ FIXED

**Problem**: `save_graph()` was being called AFTER EACH FILE, and it does:
```rust
DELETE FROM kg_edges;
DELETE FROM kg_nodes;
INSERT new data...
```

This meant only the LAST file's data persisted in the database.

**Solution Implemented**:
```rust
// Lines 90-162 in github_sync_service.rs
let mut accumulated_graph = GraphData::new();

for file in files {
    // Accumulate ALL nodes/edges in memory
    match self.process_file(file, &mut accumulated_graph).await {
        FileProcessResult::KnowledgeGraph { nodes, edges } => {
            stats.total_nodes += nodes;
            stats.total_edges += edges;
        }
        // ...
    }
}

// SINGLE save at the end with ALL accumulated data
if !accumulated_graph.nodes.is_empty() {
    self.kg_repo.save_graph(&accumulated_graph).await?;
}
```

**Status**: ‚úÖ Code deployed, binary compiled (13:57:15), newer than source (13:54:47)

---

### 2. **Mystery Database Errors** ‚ùå UNRESOLVED

**Observed Behavior**:
```
[12:30:08] INFO  Parsing knowledge graph file: 3D and 4D.md
[12:30:08] WARN  Error processing 3D and 4D.md: Database error: Database error: Failed to insert node: UNIQUE constraint failed: kg_nodes.id
[12:30:09] INFO  Parsing knowledge graph file: AI Companies.md
[12:30:09] WARN  Error processing AI Companies.md: Database error: Database error: Failed to begin transaction: cannot start a transaction within a transaction
```

**Verified Facts**:
1. ‚úÖ Parser logs "Parsing knowledge graph file: {filename}" - detection WORKS
2. ‚úÖ `process_knowledge_graph_file()` has NO database calls - only memory accumulation
3. ‚úÖ Parser (`knowledge_graph_parser.rs`) has NO database calls - pure function
4. ‚ùå Database error message appears immediately after parser call
5. ‚ùå 0 KG files counted as "processed" (all counted as errors)
6. ‚ùå 190 database errors out of 731 files

**Error Sources** (from grep):
- `sqlite_knowledge_graph_repository.rs:246` - "Failed to insert node" in `save_graph()`
- `sqlite_knowledge_graph_repository.rs:325` - "Failed to insert node" in `add_node()`

**Hypothesis**:
- Old data in database has duplicate node IDs
- User mentioned "non-ASCII chars in markdown files" - possible encoding issues
- Transaction nesting from ontology processing (241 ontology files processed)

---

## Files Modified

### `/home/devuser/workspace/project/src/services/github_sync_service.rs`

**Lines 27-38**: Added statistics fields
```rust
pub struct SyncStatistics {
    pub total_files: usize,
    pub kg_files_processed: usize,
    pub ontology_files_processed: usize,
    pub skipped_files: usize,
    pub errors: Vec<String>,
    pub duration: Duration,
    pub total_nodes: usize,      // ‚úÖ NEW
    pub total_edges: usize,      // ‚úÖ NEW
}
```

**Lines 90-162**: Data accumulation pattern
```rust
// Accumulate all knowledge graph data before saving
let mut accumulated_graph = crate::models::graph::GraphData::new();

// Process each file (accumulate in memory)
for (index, file) in files.iter().enumerate() {
    match self.process_file(file, &mut accumulated_graph).await {
        // Track stats but don't save yet
    }
}

// Save all accumulated knowledge graph data in ONE transaction
if !accumulated_graph.nodes.is_empty() {
    info!("Saving accumulated knowledge graph: {} nodes, {} edges",
          accumulated_graph.nodes.len(), accumulated_graph.edges.len());
    match self.kg_repo.save_graph(&accumulated_graph).await {
        Ok(_) => info!("‚úÖ Knowledge graph saved successfully"),
        Err(e) => error!("Failed to save accumulated knowledge graph: {}", e),
    }
}
```

**Lines 213-240**: Updated method signature and implementation
```rust
async fn process_knowledge_graph_file(
    &self,
    file: &GitHubFileBasicMetadata,
    content: &str,
    accumulated_graph: &mut crate::models::graph::GraphData,  // ‚úÖ NEW PARAMETER
) -> FileProcessResult {
    let graph_data = match self.kg_parser.parse(content, &file.name) {
        Ok(data) => data,
        Err(e) => return FileProcessResult::Error { error: format!("Parse error: {}", e) }
    };

    let node_count = graph_data.nodes.len();
    let edge_count = graph_data.edges.len();

    // Accumulate nodes and edges instead of saving immediately
    accumulated_graph.nodes.extend(graph_data.nodes);
    accumulated_graph.edges.extend(graph_data.edges);

    FileProcessResult::KnowledgeGraph { nodes: node_count, edges: edge_count }
}
```

**Lines 300-331**: Enhanced detection with debugging
```rust
fn detect_file_type(&self, content: &str) -> FileType {
    // Remove UTF-8 BOM if present
    let content = content.trim_start_matches('\u{feff}');
    let lines: Vec<&str> = content.lines().take(20).collect();

    // Debug: Log first 3 lines
    for (i, line) in lines.iter().take(3).enumerate() {
        debug!("detect_file_type line {}: {:?}", i + 1, line);
    }

    // Check for "public:: true" (knowledge graph marker)
    for (i, line) in lines.iter().enumerate() {
        let trimmed = line.trim();
        debug!("Line {} check: '{}' == 'public:: true' ? {}", i + 1, trimmed, trimmed == "public:: true");

        if trimmed == "public:: true" {
            debug!("‚úÖ Knowledge Graph detected!");
            return FileType::KnowledgeGraph;
        }
    }

    // Check for "- ### OntologyBlock" (ontology marker)
    if content.contains("### OntologyBlock") {
        debug!("‚úÖ Ontology detected!");
        return FileType::Ontology;
    }

    debug!("‚è≠Ô∏è File skipped (no markers found)");
    FileType::Skip
}
```

### `/home/devuser/workspace/project/src/main.rs`

**Lines 2-3**: Fixed imports
```rust
use webxr::actors::messages::UpdateMetadata;
use webxr::ports::knowledge_graph_repository::KnowledgeGraphRepository;  // ‚úÖ NEW - Required for trait methods
```

**Lines 461-485**: Database-first graph loading
```rust
// Load graph from database (NEW: database-first architecture)
info!("Loading graph from knowledge_graph.db...");

let graph_data_option = match app_state.knowledge_graph_repository.load_graph().await {
    Ok(graph_arc) => {
        let graph = graph_arc.as_ref();
        if !graph.nodes.is_empty() {
            info!("‚úÖ Loaded graph from database: {} nodes, {} edges",
                  graph.nodes.len(), graph.edges.len());
            Some((*graph_arc).clone())
        } else {
            info!("üìÇ Database is empty - waiting for GitHub sync to complete");
            None
        }
    }
    Err(e) => {
        error!("‚ö†Ô∏è  Failed to load graph from database: {}", e);
        None
    }
};
```

**Lines 487-503**: Send database data to actor
```rust
if let Some(graph_data) = graph_data_option {
    info!("üì§ Sending graph data to GraphServiceActor: {} nodes, {} edges",
          graph_data.nodes.len(), graph_data.edges.len());
    app_state.graph_service_addr.do_send(UpdateGraphData {
        graph_data: StdArc::new(graph_data),
    });
    info!("‚úÖ Graph data sent to actor");
} else {
    info!("‚è≥ GraphServiceActor will remain empty until GitHub sync finishes");
    info!("‚ÑπÔ∏è  You can manually trigger sync via /api/admin/sync endpoint");
}
```

---

## Current System State

### Backend Status
- **Container**: `visionflow_container` (Up 2+ hours)
- **Process**: `/app/target/debug/webxr` (PID 2453, running)
- **Port**: 4000 (listening)
- **Logs**: `/app/logs/webxr_new.log`

### Database Status
- **Path**: `/app/data/knowledge_graph.db`
- **Current Data**: 4 nodes, 0 edges (OLD data from previous sync)
- **Issue**: Contains duplicate node IDs causing UNIQUE constraint failures

### Sync Statistics (Last Run)
```
Duration: 213.168s (3.5 minutes)
Total files: 731
Knowledge graph files: 0 ‚ùå (detected but failed)
Ontology files: 241 ‚úÖ
Skipped files: 300
Errors: 190 ‚ùå
```

### API Endpoint Status
```bash
curl http://localhost:4000/api/graph/data
# Returns: 4 nodes, 0 edges (old data)
```

### Frontend Status
- **URL**: http://192.168.0.51:3001
- **Status**: Loaded, showing VisionFlow UI
- **Graph**: Empty/old data (4 nodes visible)

---

## Sample Files Verified

From Python test script `/tmp/fetch_github_samples.py`:

| File | Has `public:: true` | Has `OntologyBlock` | Expected Type |
|------|---------------------|---------------------|---------------|
| 3D Scene Exchange Protocol (SXP).md | ‚ùå | ‚ùå | Skip |
| **3D and 4D.md** | ‚úÖ Line 1 | ‚ùå | **Knowledge Graph** |
| 424.md | ‚ùå | ‚ùå | Skip |
| **6G Network Slice.md** | ‚ùå | ‚úÖ Line 1 | **Ontology** |
| AI Adoption.md | ‚ùå | ‚ùå | Skip |
| **AI Companies.md** | ‚úÖ Line 1 | ‚ùå | **Knowledge Graph** |
| AI Defence Doc.md | ‚ùå | ‚ùå | Skip |

**Verified**: Files with `public:: true` DO exist in the repository and should be detected.

---

## Architecture Overview

### Data Flow (Intended)
```
GitHub API
    ‚Üì
EnhancedContentAPI (fetch files)
    ‚Üì
GitHubSyncService (orchestration)
    ‚Üì
detect_file_type() ‚Üí FileType::{KnowledgeGraph, Ontology, Skip}
    ‚Üì
KnowledgeGraphParser.parse() ‚Üí GraphData (in-memory)
    ‚Üì
Accumulate in `accumulated_graph` variable
    ‚Üì
Single save_graph() call at END
    ‚Üì
SqliteKnowledgeGraphRepository
    ‚Üì
DELETE old data + INSERT new data (single transaction)
    ‚Üì
knowledge_graph.db
```

### Startup Sequence
```
main.rs
    ‚Üì
AppState::new()
    ‚Üì
GitHubSyncService::sync_graphs() [async in background]
    ‚Üì
load_graph() from database
    ‚Üì
UpdateGraphData message ‚Üí GraphServiceActor
    ‚Üì
/api/graph/data endpoint serves data
```

---

## Code Architecture

### Hexagonal Architecture (Ports & Adapters)

**Port** (Interface):
- `/src/ports/knowledge_graph_repository.rs` - Trait definition

**Adapter** (Implementation):
- `/src/adapters/sqlite_knowledge_graph_repository.rs` - SQLite implementation

**Service** (Business Logic):
- `/src/services/github_sync_service.rs` - Orchestration
- `/src/services/parsers/knowledge_graph_parser.rs` - Pure parsing logic

**Key Methods**:
```rust
trait KnowledgeGraphRepository {
    async fn load_graph(&self) -> Result<Arc<GraphData>>;
    async fn save_graph(&self, graph: &GraphData) -> Result<()>;
    async fn add_node(&self, node: &Node) -> Result<u32>;  // ‚ö†Ô∏è NOT USED in sync
}
```

---

## Next Steps (Priority Order)

### üî¥ CRITICAL - Fix Database Errors

**Option 1: Clear Database** (Fastest)
```bash
docker exec visionflow_container bash -c \
  "sqlite3 /app/data/knowledge_graph.db 'DELETE FROM kg_nodes; DELETE FROM kg_edges; VACUUM;'"
```

Then restart backend and re-run sync.

**Option 2: Fix Transaction Nesting** (Root Cause)
- Investigate ontology processing (lines 262-288 in `github_sync_service.rs`)
- Each ontology add_owl_class/add_owl_property/add_axiom call starts a transaction
- 241 ontology files √ó 3 operations = nested transaction hell
- Need to batch ontology operations or use save_ontology() instead

**Option 3: Handle Duplicate Node IDs** (Defensive)
- Deduplicate nodes in accumulated_graph before save
- Use HashMap<u32, Node> instead of Vec<Node>
- Or use INSERT OR REPLACE in save_graph()

### üü° MEDIUM - Improve Sync

1. **Add Node/Edge Count Logging**
   ```rust
   info!("Accumulated so far: {} nodes, {} edges",
         accumulated_graph.nodes.len(), accumulated_graph.edges.len());
   ```

2. **Non-ASCII Character Handling**
   - User mentioned non-ASCII chars in markdown files
   - Already have UTF-8 BOM removal (line 303)
   - May need additional encoding validation

3. **Parallel Processing**
   - Current: Sequential (100ms delay between files)
   - Could use tokio::spawn to process files in parallel
   - Would require Arc<Mutex<accumulated_graph>>

### üü¢ LOW - Enhancements

1. **Error Recovery**
   - Currently: File error = skip file, continue
   - Could: Retry with exponential backoff

2. **Progress UI**
   - WebSocket connection to stream sync progress to frontend
   - Real-time node/edge count updates

3. **Database Optimization**
   - Use prepared statements batch execution
   - Consider PRAGMA settings for bulk insert performance

---

## Debug Commands

### Check Database Contents
```bash
docker exec visionflow_container sqlite3 /app/data/knowledge_graph.db \
  "SELECT COUNT(*) as nodes FROM kg_nodes; SELECT COUNT(*) as edges FROM kg_edges;"
```

### Check for Duplicate Node IDs
```bash
docker exec visionflow_container sqlite3 /app/data/knowledge_graph.db \
  "SELECT id, COUNT(*) as count FROM kg_nodes GROUP BY id HAVING count > 1;"
```

### Monitor Sync Progress
```bash
docker exec visionflow_container tail -f /app/logs/webxr_new.log | grep -E '(Progress|Knowledge graph saved|Error processing)'
```

### Test Specific File Detection
```bash
# Fetch a known KG file
curl -s 'https://raw.githubusercontent.com/jjohare/logseq/refs/heads/main/mainKnowledgeGraph/pages/3D%20and%204D.md' | head -30
```

### Restart Backend
```bash
docker exec visionflow_container pkill webxr
docker exec visionflow_container bash -c "cd /app && RUST_LOG=debug /app/target/debug/webxr 2>&1 | tee /app/logs/webxr_debug.log" &
```

### Test Graph Endpoint
```bash
curl -s http://localhost:4000/api/graph/data | jq '{node_count: (.nodes | length), edge_count: (.edges | length), sample_nodes: .nodes[:3]}'
```

---

## Related Files

### Source Code
- `/home/devuser/workspace/project/src/services/github_sync_service.rs` (15,620 bytes)
- `/home/devuser/workspace/project/src/services/parsers/knowledge_graph_parser.rs` (7,894 bytes)
- `/home/devuser/workspace/project/src/services/parsers/ontology_parser.rs` (11,467 bytes)
- `/home/devuser/workspace/project/src/adapters/sqlite_knowledge_graph_repository.rs`
- `/home/devuser/workspace/project/src/ports/knowledge_graph_repository.rs`
- `/home/devuser/workspace/project/src/main.rs`

### Test/Debug Scripts
- `/tmp/fetch_github_samples.py` - Python script to fetch and analyze sample files

### Logs
- `/app/logs/webxr_new.log` - Current backend logs (running sync)
- `/app/logs/webxr_debug.log` - Debug level logs (if started with RUST_LOG=debug)

### Databases
- `/app/data/knowledge_graph.db` - Main graph storage
- `/app/data/ontology.db` - Ontology storage
- `/app/data/settings.db` - Application settings

---

## Key Insights

1. **The accumulation pattern IS working** - code is deployed and verified
2. **File detection IS working** - logs show "Parsing knowledge graph file: {name}"
3. **The parser IS working** - it's pure in-memory, no database calls
4. **Database errors are coming from OLD data** - duplicate node IDs or transaction conflicts
5. **Solution is simple** - clear database and re-run sync, OR fix transaction nesting in ontology processing

---

## Success Criteria

‚úÖ **Phase 1 Complete**: Data accumulation implemented and deployed
‚ùå **Phase 2 Blocked**: Database errors preventing successful sync
‚è≥ **Phase 3 Pending**: Graph visualization with real data

**Next Session Goals**:
1. Clear database or fix transaction nesting
2. Re-run sync successfully (expect >100 nodes)
3. Verify `/api/graph/data` returns real nodes and edges
4. Test frontend visualization with populated graph

---

## Environment

- **Container**: visionflow_container
- **Image**: Based on Dockerfile with Rust compilation
- **Rust Version**: 1.78+ (edition 2021)
- **Binary**: `/app/target/debug/webxr` (debug build)
- **Working Dir**: `/app`
- **Data Dir**: `/app/data`
- **Logs Dir**: `/app/logs`

---

## Contact/Context

- **Session**: Continuation after previous context summary
- **User Request**: "fix everything, deprecate the old json and yaml and toml bases systems and ensure we can populate the databases properly"
- **User Hint**: "there's some non ascii chars in the markdown files so it's possible we have issues"
- **User Action**: Requested this writeup for context transfer to new session

---

**STATUS**: Ready for next session to resolve database errors and complete data population.
