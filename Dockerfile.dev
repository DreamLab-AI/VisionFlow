FROM nvidia/cuda:12.8.1-devel-ubuntu22.04

# Build argument for CUDA architecture
ARG CUDA_ARCH=86

ENV DEBIAN_FRONTEND=noninteractive \
    RUST_LOG=${RUST_LOG:-warn} \
    PATH="/root/.cargo/bin:${PATH}" \
    NVIDIA_DRIVER_CAPABILITIES=all \
    CUDA_HOME=/usr/local/cuda \
    LD_LIBRARY_PATH="/usr/local/cuda/lib64:${LD_LIBRARY_PATH}" \
    CUDA_PATH=/usr/local/cuda

# Install dependencies including compilation tools
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    git \
    build-essential \
    gcc-11 \
    g++-11 \
    pkg-config \
    libssl-dev \
    netcat-openbsd \
    lsof \
    gzip \
    expect \
    docker.io \
    supervisor \
    nginx \
    && rm -rf /var/lib/apt/lists/*

# Create Nginx log and run directories and verify nginx installation
RUN mkdir -p /var/log/nginx /var/run/nginx && \
    chown -R www-data:www-data /var/run/nginx && \
    which nginx && \
    nginx -v

# Set gcc-11 as default compiler (needed for CUDA compilation)
RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100 \
    && update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 100

# Install Node.js
RUN curl -fsSL https://deb.nodesource.com/setup_20.x | bash - && \
    apt-get install -y nodejs

# Install Rust (latest stable) and cargo-watch for hot reloading
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --default-toolchain stable && \
    . $HOME/.cargo/env && \
    cargo install cargo-watch

WORKDIR /app

# Create necessary directories
RUN mkdir -p /app/data/markdown \
    /app/data/metadata \
    /app/data/runtime \
    /app/user_settings \
    /app/client

# Copy Rust files
COPY Cargo.toml ./
COPY src ./src

# Copy client directory with all frontend files
COPY client ./client

# Install Node.js dependencies
WORKDIR /app/client
RUN npm install

WORKDIR /app

# Copy Nginx config for development
COPY nginx.dev.conf /etc/nginx/nginx.conf

# Build dependencies first (caching layer)
RUN cargo fetch

# Verify CUDA installation and architecture support
RUN nvcc --version && \
    ls -la /usr/local/cuda/lib64/libcudart.so* || true && \
    echo "Testing CUDA compilation..." && \
    echo "__global__ void test() {}" > test.cu && \
    nvcc -arch=sm_${CUDA_ARCH} -c test.cu -o test.o && \
    echo "CUDA compilation test passed for sm_${CUDA_ARCH}" && \
    rm -f test.cu test.o

# Compile the unified PTX kernel inside the container
RUN mkdir -p /app/src/utils/ptx && \
    cd /app && \
    nvcc -ptx \
        -arch=sm_${CUDA_ARCH} \
        -O3 \
        --use_fast_math \
        --restrict \
        --ftz=true \
        --prec-div=false \
        --prec-sqrt=false \
        src/utils/visionflow_unified.cu \
        -o src/utils/ptx/visionflow_unified.ptx && \
    echo "PTX kernel compiled successfully for sm_${CUDA_ARCH}" && \
    ls -la src/utils/ptx/

# Build argument for features (gpu or empty for CPU-only)
ARG FEATURES=gpu

# Build Rust with GPU features (always)
# Disable SIMD features that require nightly Rust
ENV CARGO_CFG_PORTABLE_SIMD=0 \
    RUSTFLAGS="-C target-cpu=x86-64"
RUN echo "Building with GPU features enabled" && \
    cargo build --release --features gpu && \
    cp target/release/webxr /app/webxr && \
    echo "Binary built with GPU support"

# Copy settings file
COPY data/settings.yaml /app/settings.yaml

# Copy pre-compiled PTX files (compiled outside container for reliability)
# These were compiled with CUDA 12.9 for SM_86 (RTX 30 series/A6000)
RUN mkdir -p /app/src/utils/ptx
COPY src/utils/ptx/*.ptx /app/src/utils/ptx/

# Development entrypoint script
COPY scripts/dev-entrypoint.sh ./
RUN chmod +x ./dev-entrypoint.sh

EXPOSE 3001 4000 5173 24678

ENTRYPOINT ["./dev-entrypoint.sh"]
