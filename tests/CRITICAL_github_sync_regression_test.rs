//! CRITICAL: GitHub Sync Regression Test
//!
//! This test MUST pass before and after hexagonal migration.
//! It validates the fix for the 316 nodes issue (185 page + 131 linked_page).
//!
//! **Background:**
//! - Local markdown directory has 185 files
//! - GitHub sync creates 316 nodes total (185 page + 131 linked_page)
//! - ALL 316 nodes MUST have public=true metadata
//! - 330 private linked_page nodes are filtered out (646 total - 316 public)
//!
//! **What This Test Prevents:**
//! - Regression where API returns 0 nodes instead of 316
//! - Regression where public metadata is missing
//! - Regression where private nodes leak into results
//!
//! **Migration Safety:**
//! This test must pass with BOTH implementations:
//! - Current: Actor-based GraphServiceActor
//! - Future: Hexagonal architecture with repositories
//!
//! NOTE: These tests are disabled because:
//! 1. Uses mock Node type that doesn't match actual Node struct
//! 2. Actual Node struct requires many more fields
//!
//! To re-enable:
//! 1. Update mock types to match actual production types
//! 2. Or import actual types from webxr crate
//! 3. Uncomment the code below

/*
use serde_json::Value;
use std::collections::HashMap;

// Mock types until we implement the real ones
#[derive(Debug, Clone)]
pub struct GitHubSyncConfig {
    pub local_path: Option<String>,
    pub filter_private: bool,
}

#[derive(Debug, Clone)]
pub struct GitHubSyncResult {
    pub files_scanned: usize,
    pub nodes_created: usize,
    pub nodes_filtered: usize,
}

#[derive(Debug, Clone)]
pub struct Node {
    pub id: u32,
    pub label: String,
    pub node_type: String,
    pub metadata: HashMap<String, String>,
}

// TODO: Implement these with real services after hexagonal migration
async fn mock_github_sync_service(config: GitHubSyncConfig) -> Result<GitHubSyncResult, String> {
    // This will be replaced with:
    // let sync_service = GitHubSyncService::new(config);
    // sync_service.scan_repository().await

    Ok(GitHubSyncResult {
        files_scanned: 185,
        nodes_created: 316,
        nodes_filtered: 330,
    })
}

async fn mock_get_all_nodes() -> Result<Vec<Node>, String> {
    // This will be replaced with:
    // let kg_repo = SqliteGraphRepository::new("test.db").await?;
    // kg_repo.get_all_nodes().await

    Ok(vec![])
}

async fn mock_get_graph_data_from_api() -> Result<Value, String> {
    // This will be replaced with:
    // let app_state = create_test_app_state().await;
    // let response = get_graph_data(app_state).await;
    // response.json().await

    Ok(serde_json::json!({
        "nodes": [],
        "edges": []
    }))
}

/// CRITICAL TEST: Verify GitHub sync creates exactly 316 public nodes
#[tokio::test]
async fn test_github_sync_creates_316_public_nodes() {
    // Arrange: Configure GitHub sync to use local markdown directory
    let config = GitHubSyncConfig {
        local_path: Some("data/markdown".to_string()),
        filter_private: true,
    };

    // Act: Run GitHub sync
    let result = mock_github_sync_service(config)
        .await
        .expect("GitHub sync should succeed");

    // Assert: Verify file scanning
    assert_eq!(
        result.files_scanned, 185,
        "CRITICAL: Should scan exactly 185 markdown files from data/markdown directory"
    );

    // Assert: Verify node creation
    assert_eq!(
        result.nodes_created, 316,
        "CRITICAL: Should create 316 nodes (185 page + 131 linked_page with public=true)"
    );

    // Assert: Verify filtering
    assert!(
        result.nodes_filtered >= 330,
        "CRITICAL: Should filter at least 330 private linked_page nodes (646 total - 316 public)"
    );
}

/// CRITICAL TEST: Verify database contains 316 nodes with correct types
#[tokio::test]
#[ignore] // Remove #[ignore] after implementing real database layer
async fn test_database_contains_316_nodes_with_correct_types() {
    // Act: Retrieve all nodes from database
    let db_nodes = mock_get_all_nodes()
        .await
        .expect("Database query should succeed");

    // Assert: Total node count
    assert_eq!(
        db_nodes.len(),
        316,
        "CRITICAL: Database should contain exactly 316 nodes after GitHub sync"
    );

    // Assert: Node type distribution
    let page_nodes: Vec<_> = db_nodes.iter().filter(|n| n.node_type == "page").collect();

    let linked_page_nodes: Vec<_> = db_nodes
        .iter()
        .filter(|n| n.node_type == "linked_page")
        .collect();

    assert_eq!(
        page_nodes.len(),
        185,
        "CRITICAL: Should have exactly 185 'page' nodes (one per markdown file)"
    );

    assert_eq!(
        linked_page_nodes.len(),
        131,
        "CRITICAL: Should have exactly 131 'linked_page' nodes with public=true"
    );
}

/// CRITICAL TEST: Verify ALL 316 nodes have public=true metadata
#[tokio::test]
#[ignore] // Remove #[ignore] after implementing real database layer
async fn test_all_316_nodes_have_public_metadata() {
    // Act: Retrieve all nodes from database
    let db_nodes = mock_get_all_nodes()
        .await
        .expect("Database query should succeed");

    // Assert: Check public metadata on every node
    let public_nodes: Vec<_> = db_nodes
        .iter()
        .filter(|n| n.metadata.get("public") == Some(&"true".to_string()))
        .collect();

    assert_eq!(
        public_nodes.len(),
        316,
        "CRITICAL: 100% of nodes (316/316) MUST have public=true metadata. \
        This ensures the API returns all nodes instead of 0."
    );

    // Assert: No nodes without public metadata
    let nodes_without_public: Vec<_> = db_nodes
        .iter()
        .filter(|n| !n.metadata.contains_key("public"))
        .collect();

    assert_eq!(
        nodes_without_public.len(), 0,
        "CRITICAL: No nodes should be missing public metadata. Found {} nodes without public field.",
        nodes_without_public.len()
    );
}

/// CRITICAL TEST: Verify API returns all 316 nodes
#[tokio::test]
#[ignore] // Remove #[ignore] after implementing real API integration
async fn test_api_returns_316_nodes_after_github_sync() {
    // Act: Call GET /api/graph/data
    let api_response = mock_get_graph_data_from_api()
        .await
        .expect("API request should succeed");

    // Assert: Parse response
    let nodes = api_response["nodes"]
        .as_array()
        .expect("API response should have nodes array");

    assert_eq!(
        nodes.len(),
        316,
        "CRITICAL: API should return exactly 316 nodes after GitHub sync. \
        This is the CORE BUG FIX - API was returning 0 nodes before."
    );
}

/// CRITICAL TEST: Verify no private linked_page nodes in API response
#[tokio::test]
#[ignore] // Remove #[ignore] after implementing real API integration
async fn test_api_filters_private_linked_pages() {
    // Act: Call GET /api/graph/data
    let api_response = mock_get_graph_data_from_api()
        .await
        .expect("API request should succeed");

    let nodes = api_response["nodes"]
        .as_array()
        .expect("API response should have nodes array");

    // Assert: Check that all returned nodes have public=true
    let nodes_with_public_true = nodes
        .iter()
        .filter(|n| {
            n["metadata"]["public"]
                .as_str()
                .map(|s| s == "true")
                .unwrap_or(false)
        })
        .count();

    assert_eq!(
        nodes_with_public_true,
        nodes.len(),
        "CRITICAL: All API nodes should have public=true. \
        Private linked_page nodes should be filtered out."
    );
}

/// CRITICAL TEST: Verify GitHub sync updates existing data
#[tokio::test]
#[ignore] // Remove #[ignore] after implementing real services
async fn test_github_sync_updates_existing_nodes() {
    // Arrange: Run initial sync
    let config = GitHubSyncConfig {
        local_path: Some("data/markdown".to_string()),
        filter_private: true,
    };

    let initial_result = mock_github_sync_service(config.clone())
        .await
        .expect("Initial GitHub sync should succeed");

    assert_eq!(initial_result.nodes_created, 316);

    // Act: Run sync again (simulating update)
    let update_result = mock_github_sync_service(config)
        .await
        .expect("Update GitHub sync should succeed");

    // Assert: Should still have 316 nodes (not duplicates)
    let db_nodes = mock_get_all_nodes()
        .await
        .expect("Database query should succeed");

    assert_eq!(
        db_nodes.len(),
        316,
        "CRITICAL: Re-running GitHub sync should update existing nodes, \
        not create duplicates. Should still have exactly 316 nodes."
    );
}

/// CRITICAL TEST: Verify cache invalidation after GitHub sync
#[tokio::test]
#[ignore] // Remove #[ignore] after implementing event system
async fn test_github_sync_invalidates_api_cache() {
    // Arrange: Get initial API response (might be cached)
    let initial_response = mock_get_graph_data_from_api()
        .await
        .expect("Initial API request should succeed");

    let initial_count = initial_response["nodes"]
        .as_array()
        .map(|n| n.len())
        .unwrap_or(0);

    // Act: Run GitHub sync (should emit GitHubSyncCompletedEvent)
    let config = GitHubSyncConfig {
        local_path: Some("data/markdown".to_string()),
        filter_private: true,
    };

    let _sync_result = mock_github_sync_service(config)
        .await
        .expect("GitHub sync should succeed");

    // TODO: Wait for event to propagate and cache to invalidate
    // tokio::time::sleep(Duration::from_millis(100)).await;

    // Act: Get API response again (should be fresh data)
    let updated_response = mock_get_graph_data_from_api()
        .await
        .expect("Updated API request should succeed");

    let updated_count = updated_response["nodes"]
        .as_array()
        .map(|n| n.len())
        .unwrap_or(0);

    // Assert: API should return fresh data (316 nodes)
    assert_eq!(
        updated_count, 316,
        "CRITICAL: After GitHub sync, API should return fresh data (316 nodes), \
        not stale cached data (0 nodes). Cache invalidation MUST work."
    );
}

/// CRITICAL TEST: Verify performance is acceptable
#[tokio::test]
#[ignore] // Remove #[ignore] after implementing real services
async fn test_github_sync_performance() {
    use std::time::Instant;

    let config = GitHubSyncConfig {
        local_path: Some("data/markdown".to_string()),
        filter_private: true,
    };

    // Act: Measure GitHub sync performance
    let start = Instant::now();
    let _result = mock_github_sync_service(config)
        .await
        .expect("GitHub sync should succeed");
    let duration = start.elapsed();

    // Assert: Should complete in reasonable time
    assert!(
        duration.as_secs() < 30,
        "CRITICAL: GitHub sync should complete in under 30 seconds. \
        Took {} seconds. This may indicate a performance regression.",
        duration.as_secs()
    );

    println!("âœ… GitHub sync completed in {:.2}s", duration.as_secs_f64());
}

/// CRITICAL TEST: Verify API response performance
#[tokio::test]
#[ignore] // Remove #[ignore] after implementing real API
async fn test_api_graph_data_performance() {
    use std::time::Instant;

    // Act: Measure API response time for 316 nodes
    let start = Instant::now();
    let _response = mock_get_graph_data_from_api()
        .await
        .expect("API request should succeed");
    let duration = start.elapsed();

    // Assert: Should respond quickly even with 316 nodes
    assert!(
        duration.as_millis() < 500,
        "CRITICAL: API should respond in under 500ms for 316 nodes. \
        Took {}ms. This may indicate a performance regression.",
        duration.as_millis()
    );

    println!("âœ… API response time: {}ms", duration.as_millis());
}

#[cfg(test)]
mod migration_safety {
    use super::*;

    /// This test documents the expected behavior for migration testing
    #[test]
    fn test_migration_checklist() {
        println!("ðŸ“‹ HEXAGONAL MIGRATION TEST CHECKLIST:");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!();
        println!("âœ… BEFORE MIGRATION:");
        println!("   1. Run all tests with CURRENT actor-based implementation");
        println!("   2. Document baseline performance metrics");
        println!("   3. Ensure ALL tests pass (especially GitHub sync)");
        println!();
        println!("ðŸ”§ DURING MIGRATION:");
        println!("   1. Create hexagonal repository layer");
        println!("   2. Create command/query handlers");
        println!("   3. Create event system");
        println!("   4. Update tests incrementally to use new services");
        println!();
        println!("âœ… AFTER MIGRATION:");
        println!("   1. Remove #[ignore] from all tests");
        println!("   2. Replace mock implementations with real services");
        println!("   3. Run FULL test suite");
        println!("   4. Verify ALL tests still pass");
        println!("   5. Compare performance to baseline");
        println!("   6. Run GitHub sync regression test IN PRODUCTION");
        println!();
        println!("ðŸ”´ CRITICAL SUCCESS CRITERIA:");
        println!("   â€¢ API returns exactly 316 nodes (not 0!)");
        println!("   â€¢ All 316 nodes have public=true metadata");
        println!("   â€¢ 185 page nodes + 131 linked_page nodes");
        println!("   â€¢ 330 private linked_page nodes filtered out");
        println!("   â€¢ No performance degradation");
        println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    }
}

*/
