# 2023_03_31
- The Current List of CBDCs In Development Around the World
  (https://www.atlanticcouncil.org/cbdctracker/)
- Coding support
- FastChat based on llama 13b
  (https://github.com/lm-sys/FastChat)
- GPT-NeoXT-Chat-Base-20B human optimised free model
  (https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B)
- ????????? is an easy-to-use deep learning optimization software suite that enables unprecedented scale and speed for DL Training and Inference. Visit us at deepspeed.ai or our Github repo.
  
  ?Megatron-LM GPT2 tutorial: https://lnkd.in/gXvPhXqb
  (https://github.com/microsoft/DeepSpeed)
- ???? ?????????? (mtf) is a language for distributed deep learning, capable of specifying a broad class of distributed tensor computations. The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: "Split the batch over rows of processors and split the units in the hidden layer across columns of processors." Mesh TensorFlow is implemented as a layer over TensorFlow.
  (https://github.com/tensorflow/mesh)
- ??????? is an efficient large [[model training]] toolkit that can be used to train large models with tens of billions of parameters. It can train models in a distributed manner while keeping the code as simple as stand-alone training.
  (https://github.com/OpenBMB/BMTrain)
- ????????-?? provides a collection of parallel components for you. It aim to support us to write our distributed deep learning models just like how we write our model on our laptop. It provide user-friendly tools to kickstart distributed training and inference in a few lines.
  ?Open source solution replicates ChatGPT training process.Ready to go with only 1.6GB GPU memory and gives you 7.73 times faster training: https://lnkd.in/gp4XTCnz
  (https://colossalai.org/)
- ???? is a system for training and serving large-scale neural networks. Scaling neural networks to hundreds of billions of parameters has enabled dramatic breakthroughs such as GPT-3, but training and serving these large-scale neural networks require complicated distributed system techniques. Alpa aims to automate large-scale distributed training and serving with just a few lines of code.
  
  
  
  
  
  
  
  
  
      ?Alpa:
    
  
  
  
      ?Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa: https://lnkd.in/g_ANHH6f
    
  
  
  
  
  
  (https://github.com/alpa-projects/alpa)
- ????????-?? / Megatron is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. Below repository is for ongoing research on training large transformer language models at scale. Developing efficient, model-parallel (tensor, sequence, and pipeline), and multi-node pre-training of transformer based models such as GPT, BERT, and T5 using mixed precision.
  
  ?pretrain_gpt3_175B.sh: https://lnkd.in/gFA9h8ns
  (https://github.com/NVIDIA/Megatron-LM)
- CLIP-Actor
  Text-Driven Recommendation and Stylization for Animating Human Meshes
  (https://clip-actor.github.io/)
- Language embedded [[Neural 3D Generation]] [LERFS](https://www.lerf.io/)