# 2023_04_14
- Locally run 13B coding optimised model
  (https://huggingface.co/ehartford/alpaca1337-13b-4bit/tree/main)
- Anonymous alpaca gpt train
  (https://github.com/oobabooga/text-generation-webui/discussions/727)
- Vircuna 7B
  (https://github.com/lm-sys/FastChat#fine-tuning-vicuna-7b-with-local-gpus)
- Llamaindex data connections
  (https://github.com/jerryjliu/llama_index)
- CPU offload lora training
  (https://github.com/oobabooga/text-generation-webui/commit/09d8119e3cf36257496acfb44e6445a9f40c3d02)
- Deepspeed chat retraining in hours
- microsoft just released a new finetuning pipeline
  they finetuned a 65B model in 10 hours using RLHF
- Hardware requirements for retraining (links to state of the art)