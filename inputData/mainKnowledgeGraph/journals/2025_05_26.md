- https://www.theunwindai.com/p/build-an-ai-domain-deep-research-agent
- https://www.lennysnewsletter.com/p/a-3-step-ai-coding-workflow-for-solo?utm_source=post-email-title&publication_id=10845&post_id=161494696&utm_campaign=email-post-title&isFreemail=true&r=1qpry6&triedRedirect=true
- https://github.com/sakalond/StableGen
- [[Visionflow]] [[Training and fine tuning]] [[LoRA DoRA etc]] [[junkiejarvis]]
	- ##   Hugging Face PEFT
	- **Why choose it?**
	  Hugging Face PEFT (Parameter-Efficient Fine-Tuning) is the most beginner-friendly and widely used LoRA library. It integrates seamlessly with the Hugging Face Transformers ecosystem, which supports models like Llama, Mistral, and Gemma. There are many step-by-step guides and community resources[5](https://neptune.ai/blog/fine-tuning-llama-3-with-lora)[3](https://developer.nvidia.com/blog/tune-and-deploy-lora-llms-with-nvidia-tensorrt-llm/).
	- **How to use it?**
		- Prepare your Q&A dataset in a JSON or CSV format.
		- Use prompt templates to structure your data (e.g., "Question: ... Answer: ...").
		- Use the `peft` library to wrap your model and train with your dataset.
		- Example:
		  
		  ```
		  python
		  
		  from peft import LoraConfig, get_peft_model
		  *# ...load your base model...*
		  lora_config = LoraConfig(
		   r=8,
		   lora_alpha=16,
		   target_modules=['q_proj', 'v_proj'],
		   lora_dropout=0.05,
		   bias="none",
		   task_type="CAUSAL_LM"
		  )
		  model = get_peft_model(base_model, lora_config)
		  *# ...train with your Q&A data...*
		  ```
	- **Best for:**
	  Beginners with standard hardware and datasets up to ~100k examples.