- ### OntologyBlock
  id:: electra-ontology
  collapsed:: true
	- ontology:: true
	- term-id:: AI-0219
	- preferred-term:: ELECTRA
	- source-domain:: metaverse
	- status:: draft
	- definition:: Efficiently Learning an Encoder that Classifies Token Replacements Accurately: a pre-training approach that trains a discriminator to detect replaced tokens rather than reconstructing masked inputs, improving sample efficiency.

## Characteristics

- **Discriminative Pre-training**: Detects replaced tokens rather than predicting them
- **Generator-Discriminator**: Uses small generator to create replacements
- **Sample Efficiency**: Learns from all tokens, not just masked ones
- **Efficient Training**: Achieves strong performance with less compute

## Academic Foundations

**Primary Source**: Clark et al., "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", arXiv:2003.10555 (2020)

**Efficiency**: Achieves comparable performance to RoBERTa and XLNet using less than 1/4 of their compute.

## Technical Context

ELECTRA trains the model to distinguish real input tokens from plausible replacements generated by a small generator network. This replaced token detection task is more sample-efficient than masked language modelling because the model learns from all positions, not just masked ones.

## Ontological Relationships

- **Broader Term**: Pre-trained Language Model
- **Related Terms**: BERT, Masked Language Model, Generator-Discriminator
- **Innovation**: Replaced Token Detection

## Usage Context

"ELECTRA's replaced token detection pre-training achieves strong performance with significantly less computational cost than masked language modelling."

## OWL Functional Syntax

```clojure
(Declaration (Class :ELECTRA))
(AnnotationAssertion rdfs:label :ELECTRA "ELECTRA"@en)
(AnnotationAssertion rdfs:comment :ELECTRA
  "Efficiently Learning an Encoder that Classifies Token Replacements Accurately through discriminative pre-training."@en)
(AnnotationAssertion :hasSource :ELECTRA
  "Clark et al., 'ELECTRA: Pre-training Text Encoders as Discriminators', arXiv:2003.10555 (2020)"@en)

;; Taxonomic relationships
(SubClassOf :ELECTRA :PreTrainedLanguageModel)
(SubClassOf :ELECTRA :TransformerArchitecture)

;; Architecture pattern
(SubClassOf :ELECTRA
  (ObjectSomeValuesFrom :uses :GeneratorDiscriminatorPattern))
(SubClassOf :ELECTRA
  (ObjectSomeValuesFrom :hasComponent :Generator))
(SubClassOf :ELECTRA
  (ObjectSomeValuesFrom :hasComponent :Discriminator))

;; Pre-training approach
(SubClassOf :ELECTRA
  (ObjectSomeValuesFrom :usesPre-training :ReplacedTokenDetection))

;; Efficiency advantage
(SubClassOf :ELECTRA
  (ObjectSomeValuesFrom :moreEfficientThan :MaskedLanguageModelling))

;; Properties
(DataPropertyAssertion :isDiscriminative :ELECTRA "true"^^xsd:boolean)
(DataPropertyAssertion :isSampleEfficient :ELECTRA "true"^^xsd:boolean)
(DataPropertyAssertion :learnsFromAllTokens :ELECTRA "true"^^xsd:boolean)
(DataPropertyAssertion :computeReduction :ELECTRA "4x less than RoBERTa/XLNet"^^xsd:string)
(DataPropertyAssertion :performanceLevel :ELECTRA "comparable to RoBERTa"^^xsd:string)

;; Related models
(DisjointClasses :ELECTRA :BERT)
(DisjointClasses :ELECTRA :RoBERTa)
```

## References

- Clark, K., et al. (2020). "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators". arXiv:2003.10555

---

*Ontology Term managed by AI-Grounded Ontology Working Group*
*UK English Spelling Standards Applied*
	- maturity:: draft
	- owl:class:: mv:ELECTRA
	- owl:physicality:: ConceptualEntity
	- owl:role:: Concept
	- is-subclass-of:: [[ArtificialIntelligence]]
	- belongsToDomain:: [[MetaverseDomain]]
