- ### OntologyBlock
  id:: high-risk-ai-system-ontology
  collapsed:: true
	- ontology:: true
	- term-id:: AI-0081
	- preferred-term:: High Risk AI System
	- source-domain:: ai
	- status:: draft
	- public-access:: true


### Relationships
- is-subclass-of:: [[AIGovernance]]

## Academic Context

- Brief contextual overview
  - The classification of AI systems as "high risk" is a cornerstone of the EU Artificial Intelligence Act, reflecting a risk-based regulatory approach that prioritises the protection of health, safety, and fundamental rights
  - The concept is rooted in earlier EU regulatory frameworks, such as the General Data Protection Regulation (GDPR), and builds on academic work in risk assessment, ethics, and governance of emerging technologies
  - The EU AI Act’s approach has influenced similar frameworks globally, including the UK’s own AI regulatory initiatives

- Key developments and current state
  - The EU AI Act, formally adopted in 2024, establishes a four-tier risk classification: unacceptable, high, limited, and minimal risk
  - High-risk AI systems are those that, by their intended purpose or deployment context, could cause significant harm to individuals or society if they fail or are misused
  - The Act’s definitions and requirements are now widely referenced in academic and policy discussions, with ongoing research into the practical implications of risk classification

- Academic foundations
  - The risk-based approach draws on foundational work in risk management, such as the ISO 31000 standard, and ethical frameworks for AI, including the EU’s Ethics Guidelines for Trustworthy AI
  - Scholars have debated the granularity of risk classification, the role of context, and the balance between innovation and regulation

## Current Landscape (2025)

- Industry adoption and implementations
  - Organisations across sectors—including healthcare, finance, transport, and public services—are adapting their AI systems to comply with high-risk requirements
  - Notable platforms and tools, such as Dataiku Govern, are helping organisations operationalise compliance, from risk management to post-market monitoring
  - In the UK, companies like Babylon Health (London) and Faculty (Cambridge) are actively engaging with high-risk AI compliance, with growing interest in North England

- UK and North England examples where relevant
  - In Manchester, the Greater Manchester AI Alliance is supporting local businesses in understanding and implementing high-risk AI requirements
  - Leeds-based organisations, such as the Leeds Institute for Data Analytics, are collaborating with industry on AI governance and risk assessment
  - Newcastle’s Digital Catapult North East and Cumbria is fostering innovation in AI while ensuring compliance with regulatory standards
  - Sheffield’s Advanced Manufacturing Research Centre (AMRC) is exploring the use of high-risk AI in industrial automation and safety-critical systems

- Technical capabilities and limitations
  - High-risk AI systems often involve complex machine learning models, real-time decision-making, and integration with safety-critical infrastructure
  - Limitations include the challenge of ensuring robustness, explainability, and human oversight, particularly in dynamic or unpredictable environments
  - Ongoing research is focused on improving the reliability and transparency of high-risk AI systems

- Standards and frameworks
  - The EU AI Act sets out specific requirements for high-risk AI systems, including risk management, data governance, transparency, and human oversight
  - Complementary standards, such as ISO/IEC 42001 (AI management systems), are being adopted to support compliance
  - The UK’s AI Standards Hub is developing guidance and best practices for high-risk AI, with a focus on practical implementation

## Research & Literature

- Key academic papers and sources
  - Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature Machine Intelligence, 1(9), 389–399. https://doi.org/10.1038/s42256-019-0088-2
  - Floridi, L., Cowls, J., Beltrametti, M., et al. (2018). AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. Minds and Machines, 28(4), 689–707. https://doi.org/10.1007/s11023-018-9482-5
  - Wachter, S., Mittelstadt, B., & Floridi, L. (2017). Transparent, explainable, and accountable AI for robotics. Science Robotics, 2(6), eaan6080. https://doi.org/10.1126/scirobotics.aan6080
  - European Commission. (2024). Artificial Intelligence Act. https://artificialintelligenceact.eu/
  - UK Government. (2024). AI Regulation: Pro-innovation approach. https://www.gov.uk/government/publications/ai-regulation-pro-innovation-approach

- Ongoing research directions
  - Improving the accuracy and fairness of risk classification
  - Developing methods for real-time monitoring and incident reporting
  - Exploring the role of human oversight in high-risk AI systems
  - Investigating the impact of regulatory requirements on innovation and competitiveness

## UK Context

- British contributions and implementations
  - The UK has established the AI Standards Hub to support the development and adoption of AI standards, including those for high-risk systems
  - The Centre for Data Ethics and Innovation (CDEI) is providing guidance on AI governance and risk assessment
  - The UK’s approach to AI regulation is informed by both EU frameworks and domestic priorities, with a focus on balancing innovation and public trust

- North England innovation hubs (if relevant)
  - Manchester’s AI and data science community is actively engaged in high-risk AI research and implementation
  - Leeds is home to several research centres and industry partnerships focused on AI governance and risk management
  - Newcastle’s Digital Catapult North East and Cumbria is supporting the development of high-risk AI applications in sectors such as healthcare and manufacturing
  - Sheffield’s AMRC is exploring the use of high-risk AI in advanced manufacturing and safety-critical systems

- Regional case studies
  - Manchester: The Greater Manchester AI Alliance is working with local businesses to implement high-risk AI compliance, with a focus on healthcare and public services
  - Leeds: The Leeds Institute for Data Analytics is collaborating with industry on AI governance and risk assessment, particularly in the financial sector
  - Newcastle: Digital Catapult North East and Cumbria is supporting the development of high-risk AI applications in healthcare and manufacturing
  - Sheffield: The AMRC is exploring the use of high-risk AI in industrial automation and safety-critical systems, with a focus on real-time monitoring and incident reporting

## Future Directions

- Emerging trends and developments
  - Increasing use of high-risk AI in safety-critical sectors, such as healthcare, transport, and energy
  - Growing interest in explainable AI and human oversight to ensure transparency and accountability
  - Development of new standards and frameworks to support compliance and innovation

- Anticipated challenges
  - Ensuring robustness and reliability of high-risk AI systems in dynamic or unpredictable environments
  - Balancing regulatory requirements with the need for innovation and competitiveness
  - Addressing the ethical and societal implications of high-risk AI

- Research priorities
  - Improving the accuracy and fairness of risk classification
  - Developing methods for real-time monitoring and incident reporting
  - Exploring the role of human oversight in high-risk AI systems
  - Investigating the impact of regulatory requirements on innovation and competitiveness

## References

1. Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature Machine Intelligence, 1(9), 389–399. https://doi.org/10.1038/s42256-019-0088-2
2. Floridi, L., Cowls, J., Beltrametti, M., et al. (2018). AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. Minds and Machines, 28(4), 689–707. https://doi.org/10.1007/s11023-018-9482-5
3. Wachter, S., Mittelstadt, B., & Floridi, L. (2017). Transparent, explainable, and accountable AI for robotics. Science Robotics, 2(6), eaan6080. https://doi.org/10.1126/scirobotics.aan6080
4. European Commission. (2024). Artificial Intelligence Act. https://artificialintelligenceact.eu/
5. UK Government. (2024). AI Regulation: Pro-innovation approach. https://www.gov.uk/government/publications/ai-regulation-pro-innovation-approach
6. ISO/IEC 42001:2023. Information technology — Artificial intelligence — Management system for AI. https://www.iso.org/standard/81234.html
7. Centre for Data Ethics and Innovation. (2024). AI Governance and Risk Assessment. https://www.gov.uk/government/organisations/centre-for-data-ethics-and-innovation
8. Digital Catapult North East and Cumbria. (2024). AI Innovation in the North. https://www.digit.catapult.org.uk/north-east-and-cumbria/
9. Advanced Manufacturing Research Centre (AMRC). (2024). AI in Advanced Manufacturing. https://www.amrc.co.uk/
10. Greater Manchester AI Alliance. (2024). AI Compliance and Innovation. https://www.gm-ai.org.uk/
11. Leeds Institute for Data Analytics. (2024). AI Governance and Risk Assessment. https://www.lida.ac.uk/
12. Faculty. (2024). AI Governance and Compliance. https://www.faculty.ai/
13. Babylon Health. (2024). AI in Healthcare. https://www.babylonhealth.com/

## Metadata

- **Last Updated**: 2025-11-11
- **Review Status**: Comprehensive editorial review
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

