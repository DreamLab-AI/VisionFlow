-


## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: AnimateDiff

public:: true

- # AnimateDiff
- AnimateDiff is a framework that can generate animated videos from a single static image and a text prompt. It is a powerful tool for creating AI-generated animations and has become very popular in the AI art community.
- ## How it Works
	- AnimateDiff works by adding a motion modelling module to a stable diffusion model. This module is trained on a large dataset of videos and learns to predict the motion between frames. When you provide AnimateDiff with an image and a text prompt, it uses the motion modelling module to generate a sequence of frames that create an animation.
- ## Features
	- **Text-to-Video:** Generate animations from a text prompt and a static image.
	- **Image-to-Video:** Generate animations from a static image.
	- **Video-to-Video:** Transfer the style of one video to another.
	- **ControlNet:** Use ControlNet to guide the animation and create more complex movements.
	- **LoRA:** Use LoRA to fine-tune the model and create specific styles.
- ## Resources
	- ### GitHub Repositories
		- [guoyww/animatediff](https://github.com/guoyww/animatediff) - A method for creating animation using diffusion models that introduces motion modules integrated into pre-trained text-to-image models, enabling flexible [[computer vision]] and [[machine learning]]-based video generation with customisable [[training]] and fine-tuning capabilities
		- [continue-revolution/sd-webui-animatediff](https://github.com/continue-revolution/sd-webui-animatediff) - Provides a straightforward method for incorporating AnimateDiff into Stable Diffusion web user interfaces, simplifying the generation of looping videos and animated GIFs with easy [[workflow management]], [[user experience]] optimisation, and [[documentation]] for [[troubleshooting]] common issues
		- [ArtVentureX/comfyui-animatediff](https://github.com/ArtVentureX/comfyui-animatediff) - Integrates the AnimateDiff motion module into ComfyUI's node-based interface, providing a visual workflow for creating animations with support for controlnets, LoRAs, and various Stable Diffusion checkpoints through [[software engineering]] best practices and [[community]] contributions
	- ### Tutorials
		- [Beginner Friendly AI Animation Tutorial #1](https://www.youtube.com/watch?v=WPlUSnLTmfI) - Discusses strategies for effective time management and increased [[productivity]], covering prioritisation, the Pomodoro Technique, workspace [[organisation]], [[project management]] tools, and [[optimization]] techniques to prevent burnout
		- [AnimateDiff Tutorial for Automatic1111](https://www.youtube.com/watch?v=X-zB4-gX3eA) - Summarises how to organise and manage digital photos effectively through folder structures, descriptive naming, metadata tagging, [[cloud computing]] backups, and [[knowledge management]] principles for maintaining a curated archive
	- ### Models and Examples
		- [Hugging Face - AnimateDiff](https://huggingface.co/guoyww/animatediff) - A framework designed to animate static images generated by text-to-image models, providing pre-trained motion modules, [[documentation]], and resources to lower the barrier to entry for creating animated content from text prompts with customisable artistic styles
		- [Civitai - AnimateDiff](https://civitai.com/models/372584/ipivs-morph-img2vid-animatediff-lcm-hyper-sd) - IPIVS Morph model designed to enhance image-to-video generation using Animatediff, LCM, and Hypernetworks for smoother transitions and improved aesthetic quality through [[automation]], [[optimization]], and [[machine learning]] techniques within the [[computer vision]] ecosystem
- ## See Also
	- [[AI Video]] is a broad category encompassing techniques for generating, editing, and manipulating video content using [[artificial intelligence]] and [[deep learning]] methods
	- [[Stable Diffusion]] is a text-to-image [[deep learning]] model that uses diffusion processes to generate high-quality images from textual descriptions, serving as the foundation for many [[computer vision]] applications
	- [[ComfyUI]] is a node-based graphical interface for Stable Diffusion that enables visual workflow [[design thinking]] and simplified [[user experience]] for creating complex AI-generated imagery

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: ComfyUI

public:: true

- #Public page automatically published
-
- It's not the easiest way to use Stable Diffusion.
- https://private-user-images.githubusercontent.com/140084057/368715655-f46f769d-f168-454c-9c7b-ed8bcd727c1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzEwNjY1NTEsIm5iZiI6MTczMTA2NjI1MSwicGF0aCI6Ii8xNDAwODQwNTcvMzY4NzE1NjU1LWY0NmY3NjlkLWYxNjgtNDU0Yy05YzdiLWVkOGJjZDcyN2MxZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwOFQxMTQ0MTFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00MjQzYjkyNDU2MjNjOTRhYjgyM2JkN2E1YzQxMDcxMDAwNTQ5ZjYzZThhMjBhYjM4NGZhYzhiYzg0ZTNhNDYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.eOij_sfuzFqJLnC0tcT-OYnMTJF30B_mnj2xM3qjG-Y
-
- The [GitHub repository](https://github.com/ZHO-ZHO-ZHO/[[ComfyUI]]-[[Gemini]]) integrates Gemini into ComfyUI, offering models like Gemini-pro, Genimi-pro-vision, and Gemini 1.5 Pro for text, image, and file processing tasks. Users can apply for their own API Key to access Gemini API. The repository provides workflow examples, installation instructions, and updates. Contact information includes zhozho3965@gmail.com and a QQ Group (839821928). Social media links to '-Zho-', Bilibili, Twitter, Little Red Book, and support on Bilibili and Aifadian. Credits to ComfyUI_Custom_Nodes_AlekPet. - Users need to apply for a Gemini_API_Key to use Gemini nodes, ensure a stable connection to [[Google]] Gemini's services, and update the dependency 'google-generativeai' to version 0.4.1 for Gemini 1.5 Pro. Installation can be done using ComfyUI Manager or manually by cloning the repository from GitHub and installing requirements. Workflow versions include V3.0 with Gemini 1.5 Pro, V2.0 with a chatbot workflow, and V1.1 with workflows for Gemini-pro and Genimi-pro-vision. Updates include Version 3.0 adding Gemini 1.5 Pro, system instructions, and file uploads, Version 2.1 fixing a bug, and Version 2.0 adding context chat nodes. - The [status history](https://api.status-histroy.com/svg?repos=ZHO-ZHO-ZHO/ConfyUI-Gemini&type=Timeline) for the repository is available.
- The [video](https://www.youtube.com/watch?v=fFdYdTzq7Kg) titled 'Expanding Horizons: Outpainting Mastery in [[ComfyUI]]' explores the artistry of Outpainting with ComfyUI's [[Stable Diffusion]] feature. The video delves into Hyper Expansion, Sketch to Render, and Auto Background Regeneration within the realm of Outpainting. Topics discussed include the use of LORAs, Quick Basic Outpainting, and various techniques for Outpainting with ComfyUI.
- The [Aerial view of the building](https://civitai.com/models/121728/aerial-view-of-the-building) showcases a [[LoRA DoRA etc]] model for urban bird's-eye views, offering high-definition training sets for cityscapes and buildings. The model, based on SD 1.5, has received very positive reviews and was last updated on Aug 3, 2023.
- The [GitHub repository](https://github.com/comfyanonymous/[[ComfyUI]]) showcases ComfyUI, a powerful and modular [[Stable Diffusion]] GUI, API, and backend with a graph/nodes interface. The interface allows users to design and execute advanced stable diffusion pipelines without needing to code. ComfyUI supports various features like SD1.x, SD2.x, SDXL, Stable Video Diffusion, and Stable Cascade. Users can - back_hashcash-denial_2002 - - [[experiment]] with complex workflows, embeddings/textual inversion, Loras, hypernetworks, and more. The repository provides detailed installation instructions for Windows, Linux, AMD GPUs, NVIDIA GPUs, Intel GPUs, [[Apple]] Mac silicon, and DirectML for AMD cards on Windows. Additionally, it offers shortcuts for workflow management, high-quality previews, TLS/SSL setup, and support channels for users. The repository is licenced under GPL-3.0 and has garnered 35.3k stars and 3.8k forks.
- The [video](https://www.youtube.com/watch?v=DROM8vfIYUY) titled 'How 2 Canvas Node' is available on YouTube. The video duration is 14 minutes and 22 seconds.
- The [video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that provides realtime AI assistance to
- ## Unsorted links
	- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)
	- A GitHub repository that provides instructions on using Gemini in ComfyUI.
	- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)
	- A YouTube video tutorial that demonstrates how to master outpainting in ComfyUI.
	- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)
	- This GitHub repository contains a Python front end for ComfyUI, known as ComfyScript.
	- [Aerial view of the buildingÔºàÂª∫Á≠ëÈ∏üÁû∞ÂõæÔºâ
	- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)
	- Civitai provides a stable diffusion LoRA model for generating an aerial view of a building using ComfyUI.
	- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)
	- A GitHub repository that provides nodes-based examples and workflows for ComfyUI.
	- üé¨
		- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)
		- A YouTube video tutorial that explains how to use the Canvas Node in ComfyUI.
		- [This One Simple Plugin Adds Realtime AI Assistance to Krita
		- YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U)
		- A YouTube video that introduces a plugin for Krita that adds realtime AI assistance using ComfyUI.
		- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)
		- A YouTube video tutorial series that covers ComfyUI from the basic principles.
		- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)
		- A YouTube video that showcases a modular workflow in ComfyUI.
		- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)
		- A YouTube channel with detailed tutorials on using ComfyUI.
		- üîß
			- [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)
			- A GitHub repository that attempts to use TensorRT with ComfyUI.
			- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)
			- This GitHub repository provides an experimental usage of stable-fast and TensorRT in ComfyUI.
			- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in Krita. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)
			- A GitHub repository that offers a streamlined interface for generating images with AI in Krita using ComfyUI.
			- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)
			- This GitHub repository contains a workflow for ComfyUI that includes JavaScript files.
		- üåê
			- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)
			- A Reddit post that discusses the Plush-for-ComfyUI style_prompt and its capability to create prompts from images using ChatGPT.
			- [(1) AP Workflow 6.0 for ComfyUI
			- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)
			- A Reddit post about the AP Workflow 6.0 for ComfyUI, which includes various features and enhancements.
			- [UI node packs on civitai](https://civitai.com/tag/comfyui)
			- Civitai provides UI node packs for ComfyUI.
		- üìë
			- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)
			- This GitHub repository contains various resources related to ComfyUI and its applications.
			- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)
			- A comprehensive wiki filled with links to resources, tutorials, and examples related to ComfyUI.
			- [Images / workflows](https://comfyworkflows.com/)
			- ComfyWorkflows offers a collection of images and workflows created using ComfyUI.
		- Using LLMs in ComfyUI
			- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)
		-
		- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)
		- https://arxiv.org/abs/2309.11497
		- TensorRT converter [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)
		- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)
		- https://perilli.com/ai/comfyui/#soon
		-
		- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)
		- Model leaderboard  [SDXL Model Compare
			- Google Sheets](https://docs.google.com/spreadsheets/d/1IYJw4Iv9M_vX507MPbdX4thhVYxOr6-IThbaRjdpVgM/edit#gid=0)
		- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)
		- [chaojie/ComfyUI-MotionCtrl (github.com)](https://github.com/chaojie/ComfyUI-MotionCtrl)
		- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)
		- [Ferniclestix](https://www.youtube.com/@ferniclestix)
			- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)
		- [Nerdy Rodent YouTube](https://www.youtube.com/@NerdyRodent)
		- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in [[Krita]]. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)
			- TODO this needs the live view debugging
			- [This One Simple Plugin Adds Realtime AI Assistance to Krita YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U) [[Courses and Training]]
		- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)
		- [Controlnet auto installer](https://github.com/Fannovel16/comfy_controlnet_preprocessors)
		- [LATENT Tricks Amazing ways to use ComfyUI](https://www.youtube.com/watch?v=OdMtJMzjNLg)
		- [latent consistency model](https://github.com/0xbitches/ComfyUI-LCM#img2img--vid2vid)
		- [UI node packs on civitai](https://civitai.com/tag/comfyui)
		- [(1) AP Workflow 6.0 for ComfyUI
			- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)
		- [Workflows that can be loaded](https://github.com/comfyanonymous/ComfyUI_examples)
		- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)
		- [Impact pack and youtube](https://github.com/ltdrdata/ComfyUI-extension-tutorials/tree/Main/ComfyUI-Impact-Pack/workflow)
		- [youtube](https://www.youtube.com/watch?v=KvZ8ucBqyqw)
		- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)
		- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)
		- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)
		- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)
		- [Prompt free diffusion](https://github.com/SHI-Labs/Prompt-Free-Diffusion)
		- Motion brush [chaojie/ComfyUI-DragNUWA (github.com)](https://github.com/chaojie/ComfyUI-DragNUWA)
		- [reference_only controlnet](https://gist.github.com/comfyanonymous/343e5675f9a2c8281fde0c440df2e2c6#file-workflow-json)
		- [Citivia autoprompt](https://civitai.com/models/123358/sdvn-comfyui-workflow-autoprompt-sdxl)
		- [Typescript client for comfyui](https://github.com/itsKaynine/comfy-ui-client)
		- [Animation workflow](https://www.reddit.com/r/comfyui/comments/15s6lpr/short_animation_img2img_in_comfyui_with/)
		- [Complex workflow tutorials](https://www.youtube.com/@ArchAi3D/videos)
		- [animation](https://www.youtube.com/watch?v=js4JeDF3v4g)
		- [Manual](https://blenderneko.github.io/ComfyUI-docs/)
		- [Turn comfyui to python](https://github.com/pydn/ComfyUI-to-Python-Extension)
		- [Share workflows](https://comfy.icu/)
		- [consistent character creation](https://www.reddit.com/r/comfyui/comments/16ceh10/i_succeeded_to_adapt_the_tutorial_character/)
		- [Edit in another tab](https://www.reddit.com/r/comfyui/comments/16d0wtx/workflow_using_15_scribble_controlnet_to_feed/)
		- [semi automated inpainting](https://www.youtube.com/watch?v=SMOM1bIY5yA)
		- [Canvas editor with layers](https://github.com/Lerc/canvas_tab)
		- [Build custom nodes howto](https://github.com/chrisgoringe/Comfy-Custom-Node-How-To/wiki)
		- [Images / workflows](https://comfyworkflows.com/)
		- [Interpolate everything (openpose)](https://github.com/shockz0rz/ComfyUI_InterpolateEverything)
		- [Autogen inside comfyui](https://github.com/xXAdonesXx/NodeGPT)
		- [autogen tutorial](https://www.youtube.com/watch?v=PUPO2tTyPOo)
		- [lcm consistency lora](https://github.com/0xbitches/ComfyUI-LCM)
		- [touch designer](https://github.com/olegchomp/TDComfyUI)
		- [NimaNzrii/comfyui-photoshop:](github.com)](https://github.com/NimaNzrii/comfyui-photoshop) [[Photoshop]] node inside of ComfyUi, send and get data from Photoshop
			- Reddit post on the matter  [(2) NEW AI NEWS! Photoshop to Comfyui V1 is Finally Released! : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18jygtn/new_ai_news_photoshop_to_comfyui_v1_is_finally/)
		- [Aerial view of the buildingÔºàÂª∫Á≠ëÈ∏üÁû∞ÂõæÔºâ
			- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)
-

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Flux

public:: true

-
- ### Reddit: Flux dev lora training use SimpleTuner
	- [This web link has been automatically summarised](https://www.reddit.com/r/StableDiffusion/comments/1epl454/flux_dev_lora_training_use_simpletuner_it_works/)
	  Title: Flux dev lora training use SimpleTuner, it works with 4090. Detail:
		- [SimpleTuner/documentation/quickstart/FLUX.md at main ¬∑ bghira/SimpleTuner (github.com)](https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/FLUX.md)
	- [Finetuning Flux Dev on a 3090! (Local LoRA Training) - YouTube](https://www.youtube.com/watch?v=HzGW_Kyermg)
- ### Flux+ Detailer: Photorealistic Model Overview
- [This web link has been automatically summarised](https://civitai.com/models/636355/flux-detailer?modelVersionId=712060)
	- **Model Overview**
		- Flux+ Detailer is a photorealism model developed by Black Forest Labs under a non-commercial licence. It excels in deep semantic comprehension, capturing emotions and detailed understanding of paragraphs.
		- The model uses a curated dataset to enhance image generation, providing quality and precision.
	- **Technical Information**
		- Identified as LoRA type with a base model of Flux.1, it underwent 2,500 training steps and one epoch.
		- SafeTensor file format offered at 21.39 MB, confirmed as verified.
	- **Usage and Workflows**
		- Designed for use with ComfyUI and accompanied by workflows to optimise usage.
		- Users are encouraged to redownload the updated versions as both versions are consolidated into one file.
	- **Community and Support**
		- Very positive reviews from 919 users underline its reliability.
		- Acknowledgments to users and testers, highlighting collaborative development efforts.
		  Topics: Deep Learning, Model Optimisation and Performance, Open Generative AI tools
	- [https://cointelegraph.com/news/half-10-most-valuable-companies-making-metaverse-hardware](https://cointelegraph.com/news/half-10-most-valuable-companies-making-metaverse-hardware)
	- [https://medium.com/firebird-technologies/auto-analyst-2-0-the-ai-data-analytics-system-26aec602928e](https://medium.com/firebird-technologies/auto-analyst-2-0-the-ai-data-analytics-system-26aec602928e)
- # ComfyUI's Innovative Flux Inpainting
	- [This web link has been automatically summarised](https://openart.ai/workflows/cgtips/comfyui---flux-inpainting-technique/cto0IyTgL6FpwGD6qbaA)
	- ComfyUI introduces the [[Flux]] , an advanced [[artificial intelligence|AI]] image generation model available in three variants:
		- FLUX.1 [pro] for superior performance;
		- FLUX.1 [dev] for efficient non-commercial applications;
		- FLUX.1 [schnell] for rapid local development.
	- These models are designed to excel in prompt adherence, visual quality, and output diversity, making them ideal for various image generation tasks.
	- Video tutorials are available via the YouTube channel CgTopTips, offering guidance on using these models effectively.
	- The page provides links to essential resources like sample images and detailed node configurations, showcasing the underlying architecture of the [[ComfyUI]] platform.
	- Node details include a range of primitive and custom nodes, such as FluxGuidance, BasicGuider, SamplerCustomAdvanced, and more.
	- The platform supports different operations like image loading and resizing, inpainting model conditioning, and advanced diffusion techniques.
	- Despite the comprehensive offering, there are no user reviews or discussions available at the moment.
	  Topics: artificial intelligence, Flux Inpainting Technique, ComfyUI
- # Training
	- If using the Fp8 dev Flux model, to get good results make sure and use the fp8_e4m3fn version.
	- Use the lora at about strength of 0.7-.75. Higher strengths will increase likelihood of generating the little details better but also increase chances of unwanted artefacts like messy fingers and other unwanted things. Lowering the strength below 0.7 will increase the cohesion of the image.
	- In comfy for the model sampling flux node make sure and use the mas_shift strength of .5 and base_shift at 0.5 respectively.
	- Use Euler as the sampler and Beta as the scheduler with 25 steps minimum.
	- Higher resolutions like 1024x1400 or 1024x1216 seem to produce best results. Also use 2x3 aspect ratio (portrait) for best results.
	- It was trained on 100 images and manual caption pair's all in "cowboy shot" where the subject is seen from thighs up, so the images generated with this lora will be very biased in that camera shot and angle. A person seen from different angles can be generated successfully with good quality but you need to reduce the strength of the lora to prevent mutations and other cohesion issues for other angles, so play around with the strength of the lora for best results in your use case.
	- This lora was trained on an A100 using the simple tuner training script (props to the dev!). The lora was trained on an fp16 dev base flux model, during training it was using about 27gb worth of VRAM for the following settings. The training speeds are about 2.3 sec/it on the A100. We used prodigy with constant, 64 rank and 64 alpha, bf16, gamma 5. No dropout used, batch size of 1 (batch size 1 yields better results versus using any other batch size).
	- It takes quite a while for the concept to converge decently at about 350 steps per image minimum and 650 steps per image for good results. Lots of tests were performed to converge on the best hyperparameters and this is what we settled on (more testing needed for manual hyperparameters as I expect a large speedup with use of adam8w and such..).
	- Some other notes of interest. We trained on an fp8 flux variant and results were just as good as the fp16 flux model at the cost of 2x convergence speed. That means it now took 700 minimum steps to converge on the subject decently and 1400 steps to converge on a good result. Training on an fp8 flux model took about 16.3gb worth of vram with our settings so I don't see a reason training cant happen on any card that has that VRAM, and possibly with some optimizations maybe could even happen on cards with 16gb of vram for fp8 lora training.
- ## Controlnet
	- https://huggingface.co/XLabs-AI/flux-controlnet-collections [[Controlnet and similar]]
		- [[ComfyWorkFlows]] [x-flux-comfyui/workflows at main ¬∑ XLabs-AI/x-flux-comfyui (github.com)](https://github.com/XLabs-AI/x-flux-comfyui/tree/main/workflows)
		-
- ## Resources
	- [(2047) Discord | #üí°-announcement | XLabs AI](https://discord.com/channels/1271080914692341801/1271086905743638591)
	- whatever this mad thing is [[FLUX] Diagram of UNET / DiT and exotic merging methods (v8.01) | Civitai](https://civitai.com/articles/3409/flux-diagram-of-unet-dit-and-exotic-merging-methods-v7)
	- [XLabs-AI/x-flux-comfyui (github.com)](https://github.com/XLabs-AI/x-flux-comfyui) [[Flux]] [[ComfyUI]]
	- https://www.reddit.com/r/StableDiffusion/comments/1er8q13/an_updated_flux_canny_controlnet_released_by/ [[Flux]] [[Stable Diffusion]] [[Controlnet and similar]]
	- https://huggingface.co/kudzueye/boreal-flux-dev-v2 [[Flux]] [[LoRA DoRA etc]]
	- https://github.com/camenduru/comfyui-colab/blob/main/workflow/flux_image_to_image.json [[flux]] [[ComfyWorkFlows]]
	- [Text Guided Flux Inpainting - a Hugging Face Space by Gradio-Community](https://huggingface.co/spaces/Gradio-Community/Text-guided-Flux-Inpainting) [[Segmentation and Identification]]
	- [(17) Post | Feed | LinkedIn](https://www.linkedin.com/feed/update/urn:li:activity:7230251219888316417/) [[KOHYA Dreambooth and similar]] [[Flux]]
	- https://huggingface.co/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Alpha [[Controlnet and similar]] [[Flux]]
	- https://github.com/cocktailpeanut/fluxgym [[Flux]]
	- https://civitai.com/models/731324 [[Flux]] Social Media Image Generator [[Death of the Internet]]
	- [docs/docs/getting-started/env-configuration.md at improve-flux-docs ¬∑ JohnTheNerd/docs (github.com)](https://github.com/JohnTheNerd/docs/blob/improve-flux-docs/docs/getting-started/env-configuration.md) [[Flux]] [[ComfyUI]] [[Open Webui and Pipelines]]
	- https://github.com/camenduru/comfyui-colab/blob/main/workflow/flux_image_to_image.json [[flux]]
	- [city96/ComfyUI-GGUF: GGUF Quantization support for native ComfyUI models (github.com)](https://github.com/city96/ComfyUI-GGUF) [[ComfyUI]] [[Model Optimisation and Performance]] [[Flux]]
		- [Excuse me? GGUF quants are possible on Flux now! : r/StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/1eslcg0/excuse_me_gguf_quants_are_possible_on_flux_now/)
	-
	- https://github.com/comfyanonymous/ComfyUI/commit/d0b7ab88ba0f1cb4ab16e0425f5229e60c934536 [[Flux]] [[Model Optimisation and Performance]]
	- https://medium.com/@furkangozukara/ultimate-flux-lora-training-tutorial-windows-and-cloud-deployment-abb72f21cbf8 [[Flux]] [[LoRA]]
	- https://github.com/ToTheBeginning/PuLID [[Face Swap]] [[Flux]] [[style transfer]]
		- https://huggingface.co/spaces/yanze/PuLID-FLUX
		- ![image.png](../assets/image_1726490585592_0.png)
		-
- https://www.reddit.com/r/StableDiffusion/comments/1fkeei6/a_simple_flux_pipeline_workflow/
- [dagthomas/comfyui_dagthomas: ComfyUI SDXL Auto Prompter (github.com)](https://github.com/dagthomas/comfyui_dagthomas) [[flux]] [[ComfyUI]] [[Prompt Engineering]]
- https://www.reddit.com/r/StableDiffusion/comments/1fkdp6j/flux_stability_video_how_to_automate_short_videos/ [[AI Video]]
- https://huggingface.co/kudzueye/boreal-flux-dev-v2
- https://openart.ai/workflows/tenofas/flux-detailer-with-latent-noise-injection/TzQXKBjYhIKI75ctU209
- https://openart.ai/workflows/civet_fine_1/tenofas-flux-workflow-v33---txt2img-img2img-and-llm-prompt-facedetailer-faceswap-and-upscaler/mC53ge31vojzvo1gZrJs
- [ComfyUI ‚Äî Flux Advanced - v5-OC | Stable Diffusion Workflows | Civitai](https://civitai.com/models/643835) [[ComfyWorkFlows]]
- https://huggingface.co/XLabs-AI/flux-ip-adapter
- https://huggingface.co/kudzueye/boreal-flux-dev-v2
- https://www.reddit.com/r/StableDiffusion/comments/1f92b9m/first_attempt_at_flux_lora_dreambooth_full/
- https://www.reddit.com/r/StableDiffusion/comments/1f2e1xp/hyper_flux_8_steps_lora_released/
- https://www.reddit.com/r/FluxAI/comments/1f1uhnm/new_flux_controlnet_union_model_just_dropped/
- https://www.reddit.com/r/comfyui/comments/1es8r9o/flux_controlnet_depth_canny_comfyui/?utm_source=diffusiondigest.beehiiv.com&utm_medium=referral&utm_campaign=trump-ai-pics-procreate-says-human-only-this-week-in-ai-art
- https://www.reddit.com/r/comfyui/comments/1ezlzsp/flux_controlnets_3d_scenes_in_playbook_web_editor/ [[visionflow]]
- https://huggingface.co/jbilcke-hf/flux-dev-panorama-lora-2
	- https://www.reddit.com/r/StableDiffusion/comments/1esz4hj/flux_hdr_seamless_panorama_links_in_comment/
- https://www.reddit.com/r/StableDiffusion/comments/1ex64jj/i_made_an_fp8_implementation_of_flux_which_gets/
- https://www.reddit.com/r/FluxAI/comments/1esyy3u/flux_dev_workflow_v20_for_loras_face_detailer_and/
- https://huggingface.co/spaces/Gradio-Community/Text-guided-Flux-Inpainting
- https://github.com/camenduru/comfyui-colab/blob/main/workflow/flux_image_to_image.json [[ComfyWorkFlows]]
- https://comfyanonymous.github.io/ComfyUI_examples/flux/
- https://www.reddit.com/r/StableDiffusion/comments/1emcblr/flux_updates_realism_lora_cannynet_and_finetuning/
-
- # Training LoRA and Fine Tuning
	- The Flux 1D fine-tuning discussion reveals a rapidly evolving landscape of techniques and challenges. Here's a distilled summary of the best options and tips from the community, prioritizing newer information:
	
	  Best Fine-Tuning Options:
	
	  LoRA (Low-Rank Adaptation): Remains the most popular and accessible method due to lower VRAM requirements and good results. Ranks of 16, 32, and even as low as 4 or 2 are being used successfully, depending on the task. Alpha typically matches the rank.
	
	  Full Fine Tuning (FFT): Offers potentially superior results, especially for complex concepts and preventing overfitting, but demands significantly more VRAM (around 24GB or more, even with optimizations). 2kpr's method (integrated into Kohya's sd-scripts) allows FFT within 24GB using BF16, stochastic rounding, and fused backpass, with optional block swapping for even lower VRAM.
	
	  Key Training Considerations and Tips:
	
	  LR (Learning Rate): For LoRA, 1e-4 seems a good starting point, with some finding success at 4e-4 or even higher depending on rank and optimiser. For FFT, significantly lower LRs are necessary (around 1e-5 to 1e-6 or even lower).
	
	  Optimizer: AdamW and Prodigy are both used for LoRA, with Prodigy often converging faster but offering less control. Adafactor with stochastic rounding is crucial for FFT with 2kpr's method. CAME is also being explored.
	
	  Captions: While some early advice suggested minimal or no captions for Flux, the consensus now leans towards detailed, natural language captions, especially for complex subjects and preventing overfitting. Using an LLM like CogVLM or Florence2 is recommended. Avoid overly long, "word salad" captions. Concise and descriptive captions targeting the specific learning objective seem to work best. For style training, include the type of art (painting, photo, etc.) and the style name in the caption. For characters, caption diverse images and avoid overfitting on specific outfits or backgrounds.
	
	  Dataset: High-quality images are crucial. Flux is sensitive to artefacts, so clean your dataset. For likeness, 12-20 varied images are sufficient. For style, aim for diversity of content, pose, and lighting within the style. For characters, include variations in pose, expression, clothing, and background to maximise flexibility. Too similar images can lead to overfitting. Background removal can be helpful for characters and some styles. Avoid including famous faces in your dataset if you don't intend to train them specifically.
	
	  Data Augmentation: Flipping is generally safe. Cropping can be helpful, but avoid scaling if possible as it can introduce artefacts.
	
	  Multi-Resolution Training: While initially recommended, the community now seems divided. It might be helpful for some tasks but can degrade detail and introduce artefacts in others, especially styles. Consider generating only at the highest resolution you plan to use.
	
	  Regularization: Crucial for preventing overfitting and concept bleed, especially in multi-concept training and FFT. Current methods aren't as effective as with previous models. Using a combination of captioned and uncaptioned regularisation images with varied styles is suggested. More research is needed in this area.
	
	  T5 Training: Still experimental and resource-intensive. May be useful for enhancing specific concepts or prompt understanding, but requires careful tuning. Combining natural language captions with tags is being explored.
	
	  CLIP Training: Has become a recommended practice alongside training the transformer (Unet in other models). It seems crucial for good results, especially with more complex concepts and styles.
	
	  Validation: Validation images generated during training can be misleading with Flux, often looking very different from inference results. Rely on thorough testing with a variety of prompts and styles during and after training.
	
	  ComfyUI Updates: Keeping ComfyUI up to date is essential as new features and bug fixes (particularly related to LoRA strength and weight dtype) are frequently released.
	
	  Tools: SimpleTuner, Ostris' ai-toolkit, and Kohya's sd-scripts are the main training tools. Kohya's scripts now include 2kpr's FFT method. Kijai's ComfyUI-FluxTrainer provides ComfyUI nodes for Kohya's scripts.
	
	  Current Issues and Unsolved Challenges:
	
	  Concept Bleed: A persistent problem, particularly in multi-concept training and FFT. Regularization techniques are being explored, but a definitive solution is yet to be found.
	
	  Validation Image Accuracy: Validation images don't reliably predict inference results.
	
	  T5 Training Optimization: Still resource-intensive and requires further research.
	
	  Style Training: Flux seems to favour realism and can be more challenging to train for stylized outputs than previous models. Detailed captions and lower guidance values seem helpful.
	
	  Overfitting: Can happen quickly, especially with smaller datasets and larger ranks. Careful dataset curation, captioning, and monitoring are crucial.
	
	  This space is rapidly changing. Stay tuned to the community for the latest developments and optimised training practices.
	
	
	  User
	  extend and expand. Give me settings and software suggestions where possible
	
	
	  Model
	  111.2s
	  Let's extend and expand on Flux 1D fine-tuning, providing concrete settings and software suggestions:
	
	  1. LoRA Training:
	
	  Software: SimpleTuner, Ostris' ai-toolkit, Kohya's sd-scripts (via ComfyUI nodes with ComfyUI-FluxTrainer)
	
	  Rank/Alpha: Start with 32/32 or 16/16. Experiment with lower ranks (4/4, even 2/2) for styles where subtle changes are desired.
	
	  LR: 1e-4 is a good default. If training stalls, try 4e-4. If it overfits quickly, try lower values.
	
	  Optimizer: AdamW is generally recommended. Prodigy can be used for faster convergence.
	
	  Steps: 2000-3000 steps for likeness, style and character training. Experiment with fewer steps for small, focused datasets. For style training, test at multiple checkpoints as results can oscillate.
	
	  Captions: Concise and descriptive. Include the type of art and style name. For characters, vary pose, expression, clothing, and background. Example: "portrait photo of ohwx woman in a red dress, standing in a garden"
	
	  Dataset: 12-20 images for likeness, 300+ for style or character, focusing on diversity.
	
	  Example config.env (SimpleTuner):
	
	  export FLUX=true
	  export MODEL_NAME="black-forest-labs/FLUX.1-dev"
	  export MAX_NUM_STEPS=3000
	  export LEARNING_RATE=1e-4
	  export TRAIN_BATCH_SIZE=1 # Or higher if VRAM allows
	  export GRADIENT_ACCUMULATION_STEPS=1
	  export LR_SCHEDULE="constant"
	  export CAPTION_DROPOUT_PROBABILITY=0.05 # Or lower, experiment
	  export OPTIMIZER="adamw_bf16" # or "prodigy"
	  export MIXED_PRECISION="bf16"
	  export TRAINER_EXTRA_ARGS="--lora_rank=32 --lora_alpha=32 --keep_vae_loaded --clip_skip=2"
	  Use code with caution.
	  ComfyUI Workflow for Inference: Use the Load/Save Lora and Model Sampling Flux nodes. Adjust the Lora strength and guidance scale according to the trained LoRA. Consider using the Adaptive Guidance V2 node to control guidance more precisely.
	
	  2. Full Fine Tuning (FFT):
	
	  Software: 2kpr's trainer (when released), Kohya's sd-scripts (with caveats regarding stochastic rounding)
	
	  LR: Much lower than for LoRA, start with 1e-5 or 1e-6 and adjust as needed.
	
	  Optimizer: Adafactor with stochastic rounding is essential with 2kpr's method.
	
	  Steps: Potentially fewer steps needed than LoRA due to "overkill" effect. Start with 500-1000 and monitor progress.
	
	  Captions: Similar to LoRA, detailed and natural language.
	
	  Dataset: Similar to LoRA, prioritize quality and diversity.
	
	  Example train.toml (2kpr's trainer - illustrative):
	
	  model_path = "path/to/your/flux1-dev.safetensors"
	  training_data_path = "path/to/your/training/data"
	  output_dir = "path/to/your/output/directory"
	
	  batch_size = 1
	  gradient_accumulation_steps = 1
	  learning_rate = 1e-5  # Lower for FFT
	  optimizer_type = "adafactor"
	  scheduler_type = "constant"
	  max_train_steps = 1000
	  mixed_precision = "bf16"
	  stochastic_rounding = true
	  gradient_checkpointing = false # If VRAM allows
	  blocks_to_swap = 0, # If VRAM allows
	- # Text Encoder settings (CLIP and T5):
	  train_clip_l = true # or false
	  train_t5 = true # or false
	  clip_learning_rate = 1e-6 # Usually lower than unet LR
	  t5_learning_rate = 1e-6 # Usually lower than unet LR
	  Use code with caution.
	  Toml
	  ComfyUI Workflow for Inference: Use the resulting .safetensors file like the base Flux model. You can also extract LoRAs from the FFT checkpoint with 2kpr's extraction script (or equivalent).
	
	  3. Training Text Encoders (CLIP and T5):
	
	  Status: Still experimental and needs careful consideration. Some find it beneficial for improving concept separation and flexibility. Others find it makes little difference or degrades results, depending on the dataset, captions, and task.
	
	  Software: Currently enabled in Kohya's sd-scripts and Kijai's ComfyUI nodes. 2kpr's trainer will also offer this functionality.
	
	  LR: Generally much lower than the Unet/transformer LR. Start with 1e-6 for CLIP and even lower for T5 (1e-7 or less). Separate LRs for CLIP and T5 are often required.
	
	  ComfyUI workflow: Use Kijai's Flux Train node, enabling Train Clip L and/or Train T5. Provide appropriate learning rates in the respective inputs.
	
	  4. LoRA Merging and Pruning:
	
	  Merging in ComfyUI: Use two (or more) Lora Loader nodes, followed by a Lora Merge node.
	
	  Pruning in ComfyUI: Use Kijai's Block Lora Loader node to selectively load and adjust the strength of individual blocks from a LoRA. This can be used to identify which blocks are responsible for desired (or undesired) effects and effectively prune the LoRA by setting unimportant block strengths to 0. Currently, no direct save functionality exists for the pruned LoRA, but merging it with the base model is possible.
	
	  Merging in Kohya: Use merge_lora.py
	
	  Alternative Strategy: Fine-tune a LoRA, then merge it into the base model, then fine-tune again.
	
	  Scaling LoRAs: Use 2kpr's scale_lora.py script to adjust the overall strength of a LoRA. This can be helpful to normalise LoRAs from different trainers or training runs.
	
	  5. Other Key Tools and Techniques:
	
	  Captioning: TagGUI locally supports various VLMs (including CogVLM) for improved captioning. Prompt engineering for captioning is important.
	
	  Background Removal: Use BRIA, InspyreNet, SAM2, or Photoroom Pro for higher-quality masks.
	
	  Upscaling: Ultimate SD Upscale can be used. Consider generating at your desired resolution directly, as Flux handles high resolutions well.
	
	  Inpainting: ComfyUI's Inpaint node with appropriate mask generation.
	
	  Regularization: Experiment with different ratios of captioned vs. uncaptioned and style-varied regularization images. Further research is needed in this area for Flux.
	
	  Debugging and Monitoring: Use ComfyUI's built-in logging, Tensorboard, and 2kpr's trainer's periodic loss charts.
	
	  This expanded information should provide a more concrete starting point for fine-tuning Flux 1D. Remember to experiment, document your findings, and share your results with the community. The field is still under active development, so continuous learning and adaptation are key.
	
	
	  User
	  can you add in any tips and tricks and prompts?
	
	
	  Model
	  83.7s
	  Let's add some prompts, tips, and tricks to fine-tune Flux 1D effectively:
	
	  Prompts for Captioning (using an LLM like CogVLM):
	
	  General: "A detailed, objective description of this image, focusing on visual elements only. Avoid subjective interpretations or artistic descriptions."
	
	  Character Focus: "A detailed description of this character, including their physical attributes, clothing, pose, and expression. The character's name is [character name]."
	
	  Style Focus: "Describe the artistic style of this image, focusing on lines, colours, composition, and textures. The type of art is [painting/photo/illustration/etc.]. The style is [style name]."
	
	  Specific Details: You can combine the above with instructions for specific details: "Describe this image. Focus on the character's facial features. The character's name is [character name]."
	
	  Tips and Tricks for Fine-Tuning:
	
	  Start Simple, Then Scale: Begin with LoRA and small datasets before moving to FFT and larger datasets. This helps develop intuition and find good starting parameters.
	
	  Test Extensively: Use a variety of prompts, styles, and resolutions during and after training. Don't rely solely on validation images. Pay close attention to details like anatomy, coherence, and concept bleed.
	
	  Iterative Approach: Fine-tuning is an iterative process. Train, test, adjust parameters, and retrain. Don't be afraid to experiment.
	
	  Document Everything: Keep track of your datasets, captions, parameters, and results. This allows for better analysis and reproduction. Version your LoRAs and checkpoints.
	
	  Community Resources: Follow the Flux fine-tuning communities (Discord, Reddit, etc.) for the latest developments, tips, and shared experiences.
	
	  Pre-trained Models: Explore existing LoRAs and checkpoints on Civitai and Hugging Face for inspiration and as starting points for your own fine-tuning.
	
	  Seed Exploration: Even with Flux, seed variation can have a noticeable effect on outputs. Try different seeds to see the range of possibilities with your fine-tuned model.
	
	  Guidance Scale Tweaking: Experiment with lower guidance scales (2-3) during inference, especially for artistic styles.
	
	  LoRA Weight Adjustment: Fine-tune the LoRA weight during inference to control its strength and balance it with other LoRAs or concepts.
	
	  Prompts for Testing Generalization and Flexibility:
	
	  Likeness:
	
	  Simple Prompts: "[character name], portrait photo"
	
	  Style Variations: "[character name], painting by [artist name]" , "[character name], anime style"
	
	  Complex Prompts: "[character name] riding a unicorn in a cyberpunk city, photorealistic"
	
	  Negative Prompts: While not directly supported in Flux training, test with negative prompts during inference to see if they work as expected.
	
	  Style:
	
	  Basic Prompts: "a woman in a dress, [style name] artstyle"
	
	  Different Subjects: "a cat, [style name] artstyle", "a spaceship, [style name] artstyle"
	
	  Complex Scenes: "A group of people in a medieval market, [style name] painting"
	
	  Multi-Concept: Combine prompts that test each concept individually and together. For example, if you trained a LoRA with a character and a specific object, test with prompts that include only the character, only the object, and both.
	
	  Specific Prompting Tips for Flux:
	
	  Two-Word Classes: For style and character LoRAs, use a two-word class to provide more context. Example: "ohwx character name, woman elf" or "ohwx style name, painting artstyle"
	
	  Descriptive Captions: For complex subjects or scenes, provide short, clear, natural language descriptions focusing on the elements you want Flux to learn.
	
	  Object and Background Specificity: For styles and characters, if you don't want certain objects or backgrounds to be associated with the LoRA, include them in the captions without the trigger word. This helps prevent overfitting and bleeding.
	
	  "Unpromptable" Concepts: Flux can struggle with some artistic styles or concepts that are hard to describe with prompts. In these cases, using an IPAdapter (when available) might be a more effective approach.
	
	  Prompt Engineering: Experiment with different phrasing and prompt structures to see how Flux interprets them. Some concepts might require more specific or descriptive language than others.
	
	  Leverage Prior Knowledge: Flux has extensive prior knowledge. Start with simple prompts and gradually add detail as needed. See if you can get the desired results without explicitly mentioning every detail.
	
	  By combining these tips, tricks, and prompts, you'll be well on your way to mastering the art of Flux 1D fine-tuning. Remember, experimentation and community engagement are your best allies in this rapidly evolving field.

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: IPAdapter

public:: true

- #Public page
	 - automatically published
- From [ip-adapter.github.io¬†/](https://ip-adapter.github.io/)
- # IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models
	- Hu Ye Jun Zhang Sibo Liu Xiao Han Wei Yang Tencent AI Lab4-5 minutes
	
	  ---
	
	  ![](https://ip-adapter.github.io/assets/fig0.jpg){:height 430, :width 1159}
- ### *Various image synthesis with our proposed IP-Adapter applied on the pretrained text-to-image diffusion model and additional structure controller.*

  [[Paper]](https://arxiv.org/abs/2308.06721)¬†¬†¬†¬†¬†¬†[[Code]](https://github.com/tencent-ailab/IP-Adapter)¬†¬†¬†¬†¬†¬†[[BibTeX]](https://ip-adapter.github.io/files/bibtex.txt)
- ## Abstract

  Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. But, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes: "an image is worth a thousand words". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fine-tuned image prompt model. As we freeze the pretrained duffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to accomplish multimodal image generation.
- ## Approach

  The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt. The proposed IP-Adapter consists of two parts: a image encoder to extract image features from image prompt, and adapted modules with decoupled cross-attention to embed image features into the pretrained text-to-image diffusion model.

  ![](https://ip-adapter.github.io/assets/fig1.png)
- ## Comparison with Existing Methods

  The comparison of our proposed IP-Adapter with other methods conditioned on different kinds and styles of images.

  ![](https://ip-adapter.github.io/assets/result1.jpg)
- ## More Results

  **Generalizable to Custom Models**

  Once the IP-Adapter is trained, it can be directly reusable on custom models fine-tuned from the same base model.

  ![](https://ip-adapter.github.io/assets/result2.jpg)

  **Structure Control**

  The IP-Adapter is fully compatible with existing controllable tools, e.g., ControlNet and T2I-Adapter.

  ![](https://ip-adapter.github.io/assets/result3.jpg)

  Our method not only outperforms other methods in terms of image quality, but also produces images that better align with the reference image.

  ![](https://ip-adapter.github.io/assets/result4.jpg)

  **Image-to-Image and Inpainting**

  Image-guided image-to-image and inpainting can be also achieved by simply replacing text prompt with image prompt.

  ![](https://ip-adapter.github.io/assets/result5.jpg)

  **Multimodal Prompt**

  Due to the decoupled cross-attention strategy, image prompt can work together with text prompt to realise multimodal image generation.

  ![](https://ip-adapter.github.io/assets/result6.jpg)

  Compared with other existing methods, our method can generate superior results in both image quality and alignment with multimodal prompts.

  ![](https://ip-adapter.github.io/assets/result7.jpg)
-

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Open Webui and Pipelines

# OpenWebUI
	- [[WebDev and Consumer Tooling]] [[Large language models]] WebUI for [[Large language models]] formally [[Ollama]] [open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI) (github.com)](https://github.com/open-webui/open-webui)
	- [Open WebUI](https://openwebui.com/)
	- [Codewriter Modelfile | OpenWebUI Community](https://openwebui.com/m/vianch/codewriter:latest) [[Coding support]]
	- [[Open Webui and Pipelines]] corporate integrations [feat: allow authenticating with a trusted email header by cheahjs ¬∑ Pull Request #1347 ¬∑ open-webui/open-webui (github.com)](https://github.com/open-webui/open-webui/pull/1347)
- [win4r/GraphRAG4OpenWebUI: GraphRAG4OpenWebUI integrates Microsoft's GraphRAG technology into Open WebUI, providing a versatile information retrieval API. It combines local, global, and web searches for advanced Q&A systems and search engines. This tool simplifies graph-based retrieval integration in open web environments. (github.com)](https://github.com/win4r/GraphRAG4OpenWebUI) [[Open Webui and Pipelines]] [[Knowledge Graphing]] [[Knowledge Graphing]]
- [docs/docs/getting-started/env-configuration.md at improve-flux-docs ¬∑ JohnTheNerd/docs (github.com)](https://github.com/JohnTheNerd/docs/blob/improve-flux-docs/docs/getting-started/env-configuration.md) [[Flux]] [[ComfyUI]] [[Open Webui and Pipelines]]
- [pipelines/examples/pipelines/rag/llamaindex_ollama_github_pipeline.py at main ¬∑ open-webui/pipelines](https://github.com/open-webui/pipelines/blob/main/examples/pipelines/rag/llamaindex_ollama_github_pipeline.py) [[Knowledge Graphing]] [[Open Webui and Pipelines]] [[Projects]] [[SHOULD]]
- [[Home Assistant]] [[Open Webui and Pipelines]] [Home Assistant Filter Pipeline by atgehrhardt ¬∑ Pull Request #95 ¬∑ open-webui/pipelines (github.com)](https://github.com/open-webui/pipelines/pull/95)
- [Firefunction-v2: Function calling capability on par with GPT4o at 2.5x the speed and 10% of the cost (fireworks.ai)](https://fireworks.ai/blog/firefunction-v2-launch-post) [[MUST]] [[Open Webui and Pipelines]] [[Projects]]

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Open Generative AI tools

public:: true

	- # Open (ish) tooling
	- ## Opensource vs Freeware in AI:
		- This is a hot, and also seemingly endless debate that has been going on for years.
		- Open-source AI allows users to access, modify, and distribute the source code **and training methods** for free, promoting collaboration and community-driven development. Popular AI frameworks like TensorFlow and PyTorch fall under this category.
		- Free-to-use, on the other hand, is copyrighted software distributed without charge, but with limited rights to modify or distribute. Meta Llama 2 falls into that catagory.
		- Feel free to get right into the weeds with the [Hannibal046/Awesome-LLM: Awesome-LLM: a curated list of Large Language Model (github.com)](https://github.com/Hannibal046/Awesome-LLM)
	- ## Large Language models:
		- ### **"AI is the high interest credit card of product development"**
			- There's SO much activity. [Thousands and thousands](https://llm.extractum.io/) of merges and models and LoRAs oh my...
				- Some of these are from legit labs are are fabulous.
				- [[Deepseek]] (Chinese) [[Falcon]] (UAE) [[Mistral]] (France) [[Red Pyjama]] [[Microsoft Models]] [[Poro]] (Finland) and more.
			- It's confusing because people are "gaming" the evaluation tools, so nobody really knows what's good.
			- Best to ask people who know, and accept you're going to be changing the back end of your system a lot.
			- You can pick a size and utility of model and get a long way, but do you need to?
			- Low code [flowise](https://flowiseai.com/) demo which you probably saw earlier.
				- It is multi-modal, can generate images like OpenAI, and use audio bi-driectionally, like OpenAI.
				- It is a drop in replacement, so crucially it can serve as a BACKUP
				- This is doable, but probably don't do it.
	- this loads up my local LLM sandbox [<]iframe src="http://192.168.0.51:3000/canvas/b9738eeb-4fa2-41a0-9535-549638a958f5" style="width: 100%; height: 600px"></iframe>
	- [[ComfyUI]] live demo (not here for now)
	- #### Demo: Running UK Company [[Stable Diffusion]] (SDXL) with a cutting edge French language model creating the prompts in real-time, completely privately on local hardware
	- this is a local demo [<]iframe src="http://192.168.0.51:8188" style="width: 100%; height: 600px"></iframe>
	- {{video https://www.youtube.com/watch?v=kN8jdvRQvzA}}
	- {{video https://www.youtube.com/watch?v=AF2VyqSApjA}}
	- You can play with all of these on [Rundiffusion](https://app.rundiffusion.com/)
	- ![image.png](../assets/image_1705158589247_0.png)
	- [[3D and 4D]]
	- {{embed ((65a5024d-11bb-4737-aef3-af73103fa218))}}
-

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: AnimateDiff

public:: true

- # AnimateDiff
- AnimateDiff is a framework that can generate animated videos from a single static image and a text prompt. It is a powerful tool for creating AI-generated animations and has become very popular in the AI art community.
- ## How it Works
	- AnimateDiff works by adding a motion modelling module to a stable diffusion model. This module is trained on a large dataset of videos and learns to predict the motion between frames. When you provide AnimateDiff with an image and a text prompt, it uses the motion modelling module to generate a sequence of frames that create an animation.
- ## Features
	- **Text-to-Video:** Generate animations from a text prompt and a static image.
	- **Image-to-Video:** Generate animations from a static image.
	- **Video-to-Video:** Transfer the style of one video to another.
	- **ControlNet:** Use ControlNet to guide the animation and create more complex movements.
	- **LoRA:** Use LoRA to fine-tune the model and create specific styles.
- ## Resources
	- ### GitHub Repositories
		- [guoyww/animatediff](https://github.com/guoyww/animatediff) - A method for creating animation using diffusion models that introduces motion modules integrated into pre-trained text-to-image models, enabling flexible [[computer vision]] and [[machine learning]]-based video generation with customisable [[training]] and fine-tuning capabilities
		- [continue-revolution/sd-webui-animatediff](https://github.com/continue-revolution/sd-webui-animatediff) - Provides a straightforward method for incorporating AnimateDiff into Stable Diffusion web user interfaces, simplifying the generation of looping videos and animated GIFs with easy [[workflow management]], [[user experience]] optimisation, and [[documentation]] for [[troubleshooting]] common issues
		- [ArtVentureX/comfyui-animatediff](https://github.com/ArtVentureX/comfyui-animatediff) - Integrates the AnimateDiff motion module into ComfyUI's node-based interface, providing a visual workflow for creating animations with support for controlnets, LoRAs, and various Stable Diffusion checkpoints through [[software engineering]] best practices and [[community]] contributions
	- ### Tutorials
		- [Beginner Friendly AI Animation Tutorial #1](https://www.youtube.com/watch?v=WPlUSnLTmfI) - Discusses strategies for effective time management and increased [[productivity]], covering prioritisation, the Pomodoro Technique, workspace [[organisation]], [[project management]] tools, and [[optimization]] techniques to prevent burnout
		- [AnimateDiff Tutorial for Automatic1111](https://www.youtube.com/watch?v=X-zB4-gX3eA) - Summarises how to organise and manage digital photos effectively through folder structures, descriptive naming, metadata tagging, [[cloud computing]] backups, and [[knowledge management]] principles for maintaining a curated archive
	- ### Models and Examples
		- [Hugging Face - AnimateDiff](https://huggingface.co/guoyww/animatediff) - A framework designed to animate static images generated by text-to-image models, providing pre-trained motion modules, [[documentation]], and resources to lower the barrier to entry for creating animated content from text prompts with customisable artistic styles
		- [Civitai - AnimateDiff](https://civitai.com/models/372584/ipivs-morph-img2vid-animatediff-lcm-hyper-sd) - IPIVS Morph model designed to enhance image-to-video generation using Animatediff, LCM, and Hypernetworks for smoother transitions and improved aesthetic quality through [[automation]], [[optimization]], and [[machine learning]] techniques within the [[computer vision]] ecosystem
- ## See Also
	- [[AI Video]] is a broad category encompassing techniques for generating, editing, and manipulating video content using [[artificial intelligence]] and [[deep learning]] methods
	- [[Stable Diffusion]] is a text-to-image [[deep learning]] model that uses diffusion processes to generate high-quality images from textual descriptions, serving as the foundation for many [[computer vision]] applications
	- [[ComfyUI]] is a node-based graphical interface for Stable Diffusion that enables visual workflow [[design thinking]] and simplified [[user experience]] for creating complex AI-generated imagery

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: ComfyUI

public:: true

- #Public page automatically published
-
- It's not the easiest way to use Stable Diffusion.
- https://private-user-images.githubusercontent.com/140084057/368715655-f46f769d-f168-454c-9c7b-ed8bcd727c1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzEwNjY1NTEsIm5iZiI6MTczMTA2NjI1MSwicGF0aCI6Ii8xNDAwODQwNTcvMzY4NzE1NjU1LWY0NmY3NjlkLWYxNjgtNDU0Yy05YzdiLWVkOGJjZDcyN2MxZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwOFQxMTQ0MTFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00MjQzYjkyNDU2MjNjOTRhYjgyM2JkN2E1YzQxMDcxMDAwNTQ5ZjYzZThhMjBhYjM4NGZhYzhiYzg0ZTNhNDYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.eOij_sfuzFqJLnC0tcT-OYnMTJF30B_mnj2xM3qjG-Y
-
- The [GitHub repository](https://github.com/ZHO-ZHO-ZHO/[[ComfyUI]]-[[Gemini]]) integrates Gemini into ComfyUI, offering models like Gemini-pro, Genimi-pro-vision, and Gemini 1.5 Pro for text, image, and file processing tasks. Users can apply for their own API Key to access Gemini API. The repository provides workflow examples, installation instructions, and updates. Contact information includes zhozho3965@gmail.com and a QQ Group (839821928). Social media links to '-Zho-', Bilibili, Twitter, Little Red Book, and support on Bilibili and Aifadian. Credits to ComfyUI_Custom_Nodes_AlekPet. - Users need to apply for a Gemini_API_Key to use Gemini nodes, ensure a stable connection to [[Google]] Gemini's services, and update the dependency 'google-generativeai' to version 0.4.1 for Gemini 1.5 Pro. Installation can be done using ComfyUI Manager or manually by cloning the repository from GitHub and installing requirements. Workflow versions include V3.0 with Gemini 1.5 Pro, V2.0 with a chatbot workflow, and V1.1 with workflows for Gemini-pro and Genimi-pro-vision. Updates include Version 3.0 adding Gemini 1.5 Pro, system instructions, and file uploads, Version 2.1 fixing a bug, and Version 2.0 adding context chat nodes. - The [status history](https://api.status-histroy.com/svg?repos=ZHO-ZHO-ZHO/ConfyUI-Gemini&type=Timeline) for the repository is available.
- The [video](https://www.youtube.com/watch?v=fFdYdTzq7Kg) titled 'Expanding Horizons: Outpainting Mastery in [[ComfyUI]]' explores the artistry of Outpainting with ComfyUI's [[Stable Diffusion]] feature. The video delves into Hyper Expansion, Sketch to Render, and Auto Background Regeneration within the realm of Outpainting. Topics discussed include the use of LORAs, Quick Basic Outpainting, and various techniques for Outpainting with ComfyUI.
- The [Aerial view of the building](https://civitai.com/models/121728/aerial-view-of-the-building) showcases a [[LoRA DoRA etc]] model for urban bird's-eye views, offering high-definition training sets for cityscapes and buildings. The model, based on SD 1.5, has received very positive reviews and was last updated on Aug 3, 2023.
- The [GitHub repository](https://github.com/comfyanonymous/[[ComfyUI]]) showcases ComfyUI, a powerful and modular [[Stable Diffusion]] GUI, API, and backend with a graph/nodes interface. The interface allows users to design and execute advanced stable diffusion pipelines without needing to code. ComfyUI supports various features like SD1.x, SD2.x, SDXL, Stable Video Diffusion, and Stable Cascade. Users can - back_hashcash-denial_2002 - - [[experiment]] with complex workflows, embeddings/textual inversion, Loras, hypernetworks, and more. The repository provides detailed installation instructions for Windows, Linux, AMD GPUs, NVIDIA GPUs, Intel GPUs, [[Apple]] Mac silicon, and DirectML for AMD cards on Windows. Additionally, it offers shortcuts for workflow management, high-quality previews, TLS/SSL setup, and support channels for users. The repository is licenced under GPL-3.0 and has garnered 35.3k stars and 3.8k forks.
- The [video](https://www.youtube.com/watch?v=DROM8vfIYUY) titled 'How 2 Canvas Node' is available on YouTube. The video duration is 14 minutes and 22 seconds.
- The [video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that provides realtime AI assistance to
- ## Unsorted links
	- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)
	- A GitHub repository that provides instructions on using Gemini in ComfyUI.
	- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)
	- A YouTube video tutorial that demonstrates how to master outpainting in ComfyUI.
	- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)
	- This GitHub repository contains a Python front end for ComfyUI, known as ComfyScript.
	- [Aerial view of the buildingÔºàÂª∫Á≠ëÈ∏üÁû∞ÂõæÔºâ
	- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)
	- Civitai provides a stable diffusion LoRA model for generating an aerial view of a building using ComfyUI.
	- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)
	- A GitHub repository that provides nodes-based examples and workflows for ComfyUI.
	- üé¨
		- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)
		- A YouTube video tutorial that explains how to use the Canvas Node in ComfyUI.
		- [This One Simple Plugin Adds Realtime AI Assistance to Krita
		- YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U)
		- A YouTube video that introduces a plugin for Krita that adds realtime AI assistance using ComfyUI.
		- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)
		- A YouTube video tutorial series that covers ComfyUI from the basic principles.
		- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)
		- A YouTube video that showcases a modular workflow in ComfyUI.
		- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)
		- A YouTube channel with detailed tutorials on using ComfyUI.
		- üîß
			- [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)
			- A GitHub repository that attempts to use TensorRT with ComfyUI.
			- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)
			- This GitHub repository provides an experimental usage of stable-fast and TensorRT in ComfyUI.
			- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in Krita. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)
			- A GitHub repository that offers a streamlined interface for generating images with AI in Krita using ComfyUI.
			- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)
			- This GitHub repository contains a workflow for ComfyUI that includes JavaScript files.
		- üåê
			- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)
			- A Reddit post that discusses the Plush-for-ComfyUI style_prompt and its capability to create prompts from images using ChatGPT.
			- [(1) AP Workflow 6.0 for ComfyUI
			- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)
			- A Reddit post about the AP Workflow 6.0 for ComfyUI, which includes various features and enhancements.
			- [UI node packs on civitai](https://civitai.com/tag/comfyui)
			- Civitai provides UI node packs for ComfyUI.
		- üìë
			- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)
			- This GitHub repository contains various resources related to ComfyUI and its applications.
			- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)
			- A comprehensive wiki filled with links to resources, tutorials, and examples related to ComfyUI.
			- [Images / workflows](https://comfyworkflows.com/)
			- ComfyWorkflows offers a collection of images and workflows created using ComfyUI.
		- Using LLMs in ComfyUI
			- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)
		-
		- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)
		- https://arxiv.org/abs/2309.11497
		- TensorRT converter [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)
		- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)
		- https://perilli.com/ai/comfyui/#soon
		-
		- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)
		- Model leaderboard  [SDXL Model Compare
			- Google Sheets](https://docs.google.com/spreadsheets/d/1IYJw4Iv9M_vX507MPbdX4thhVYxOr6-IThbaRjdpVgM/edit#gid=0)
		- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)
		- [chaojie/ComfyUI-MotionCtrl (github.com)](https://github.com/chaojie/ComfyUI-MotionCtrl)
		- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)
		- [Ferniclestix](https://www.youtube.com/@ferniclestix)
			- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)
		- [Nerdy Rodent YouTube](https://www.youtube.com/@NerdyRodent)
		- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in [[Krita]]. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)
			- TODO this needs the live view debugging
			- [This One Simple Plugin Adds Realtime AI Assistance to Krita YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U) [[Courses and Training]]
		- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)
		- [Controlnet auto installer](https://github.com/Fannovel16/comfy_controlnet_preprocessors)
		- [LATENT Tricks Amazing ways to use ComfyUI](https://www.youtube.com/watch?v=OdMtJMzjNLg)
		- [latent consistency model](https://github.com/0xbitches/ComfyUI-LCM#img2img--vid2vid)
		- [UI node packs on civitai](https://civitai.com/tag/comfyui)
		- [(1) AP Workflow 6.0 for ComfyUI
			- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)
		- [Workflows that can be loaded](https://github.com/comfyanonymous/ComfyUI_examples)
		- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)
		- [Impact pack and youtube](https://github.com/ltdrdata/ComfyUI-extension-tutorials/tree/Main/ComfyUI-Impact-Pack/workflow)
		- [youtube](https://www.youtube.com/watch?v=KvZ8ucBqyqw)
		- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)
		- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)
		- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)
		- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)
		- [Prompt free diffusion](https://github.com/SHI-Labs/Prompt-Free-Diffusion)
		- Motion brush [chaojie/ComfyUI-DragNUWA (github.com)](https://github.com/chaojie/ComfyUI-DragNUWA)
		- [reference_only controlnet](https://gist.github.com/comfyanonymous/343e5675f9a2c8281fde0c440df2e2c6#file-workflow-json)
		- [Citivia autoprompt](https://civitai.com/models/123358/sdvn-comfyui-workflow-autoprompt-sdxl)
		- [Typescript client for comfyui](https://github.com/itsKaynine/comfy-ui-client)
		- [Animation workflow](https://www.reddit.com/r/comfyui/comments/15s6lpr/short_animation_img2img_in_comfyui_with/)
		- [Complex workflow tutorials](https://www.youtube.com/@ArchAi3D/videos)
		- [animation](https://www.youtube.com/watch?v=js4JeDF3v4g)
		- [Manual](https://blenderneko.github.io/ComfyUI-docs/)
		- [Turn comfyui to python](https://github.com/pydn/ComfyUI-to-Python-Extension)
		- [Share workflows](https://comfy.icu/)
		- [consistent character creation](https://www.reddit.com/r/comfyui/comments/16ceh10/i_succeeded_to_adapt_the_tutorial_character/)
		- [Edit in another tab](https://www.reddit.com/r/comfyui/comments/16d0wtx/workflow_using_15_scribble_controlnet_to_feed/)
		- [semi automated inpainting](https://www.youtube.com/watch?v=SMOM1bIY5yA)
		- [Canvas editor with layers](https://github.com/Lerc/canvas_tab)
		- [Build custom nodes howto](https://github.com/chrisgoringe/Comfy-Custom-Node-How-To/wiki)
		- [Images / workflows](https://comfyworkflows.com/)
		- [Interpolate everything (openpose)](https://github.com/shockz0rz/ComfyUI_InterpolateEverything)
		- [Autogen inside comfyui](https://github.com/xXAdonesXx/NodeGPT)
		- [autogen tutorial](https://www.youtube.com/watch?v=PUPO2tTyPOo)
		- [lcm consistency lora](https://github.com/0xbitches/ComfyUI-LCM)
		- [touch designer](https://github.com/olegchomp/TDComfyUI)
		- [NimaNzrii/comfyui-photoshop:](github.com)](https://github.com/NimaNzrii/comfyui-photoshop) [[Photoshop]] node inside of ComfyUi, send and get data from Photoshop
			- Reddit post on the matter  [(2) NEW AI NEWS! Photoshop to Comfyui V1 is Finally Released! : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18jygtn/new_ai_news_photoshop_to_comfyui_v1_is_finally/)
		- [Aerial view of the buildingÔºàÂª∫Á≠ëÈ∏üÁû∞ÂõæÔºâ
			- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)
-

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Flux

public:: true

-
- ### Reddit: Flux dev lora training use SimpleTuner
	- [This web link has been automatically summarised](https://www.reddit.com/r/StableDiffusion/comments/1epl454/flux_dev_lora_training_use_simpletuner_it_works/)
	  Title: Flux dev lora training use SimpleTuner, it works with 4090. Detail:
		- [SimpleTuner/documentation/quickstart/FLUX.md at main ¬∑ bghira/SimpleTuner (github.com)](https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/FLUX.md)
	- [Finetuning Flux Dev on a 3090! (Local LoRA Training) - YouTube](https://www.youtube.com/watch?v=HzGW_Kyermg)
- ### Flux+ Detailer: Photorealistic Model Overview
- [This web link has been automatically summarised](https://civitai.com/models/636355/flux-detailer?modelVersionId=712060)
	- **Model Overview**
		- Flux+ Detailer is a photorealism model developed by Black Forest Labs under a non-commercial licence. It excels in deep semantic comprehension, capturing emotions and detailed understanding of paragraphs.
		- The model uses a curated dataset to enhance image generation, providing quality and precision.
	- **Technical Information**
		- Identified as LoRA type with a base model of Flux.1, it underwent 2,500 training steps and one epoch.
		- SafeTensor file format offered at 21.39 MB, confirmed as verified.
	- **Usage and Workflows**
		- Designed for use with ComfyUI and accompanied by workflows to optimise usage.
		- Users are encouraged to redownload the updated versions as both versions are consolidated into one file.
	- **Community and Support**
		- Very positive reviews from 919 users underline its reliability.
		- Acknowledgments to users and testers, highlighting collaborative development efforts.
		  Topics: Deep Learning, Model Optimisation and Performance, Open Generative AI tools
	- [https://cointelegraph.com/news/half-10-most-valuable-companies-making-metaverse-hardware](https://cointelegraph.com/news/half-10-most-valuable-companies-making-metaverse-hardware)
	- [https://medium.com/firebird-technologies/auto-analyst-2-0-the-ai-data-analytics-system-26aec602928e](https://medium.com/firebird-technologies/auto-analyst-2-0-the-ai-data-analytics-system-26aec602928e)
- # ComfyUI's Innovative Flux Inpainting
	- [This web link has been automatically summarised](https://openart.ai/workflows/cgtips/comfyui---flux-inpainting-technique/cto0IyTgL6FpwGD6qbaA)
	- ComfyUI introduces the [[Flux]] , an advanced [[artificial intelligence|AI]] image generation model available in three variants:
		- FLUX.1 [pro] for superior performance;
		- FLUX.1 [dev] for efficient non-commercial applications;
		- FLUX.1 [schnell] for rapid local development.
	- These models are designed to excel in prompt adherence, visual quality, and output diversity, making them ideal for various image generation tasks.
	- Video tutorials are available via the YouTube channel CgTopTips, offering guidance on using these models effectively.
	- The page provides links to essential resources like sample images and detailed node configurations, showcasing the underlying architecture of the [[ComfyUI]] platform.
	- Node details include a range of primitive and custom nodes, such as FluxGuidance, BasicGuider, SamplerCustomAdvanced, and more.
	- The platform supports different operations like image loading and resizing, inpainting model conditioning, and advanced diffusion techniques.
	- Despite the comprehensive offering, there are no user reviews or discussions available at the moment.
	  Topics: artificial intelligence, Flux Inpainting Technique, ComfyUI
- # Training
	- If using the Fp8 dev Flux model, to get good results make sure and use the fp8_e4m3fn version.
	- Use the lora at about strength of 0.7-.75. Higher strengths will increase likelihood of generating the little details better but also increase chances of unwanted artefacts like messy fingers and other unwanted things. Lowering the strength below 0.7 will increase the cohesion of the image.
	- In comfy for the model sampling flux node make sure and use the mas_shift strength of .5 and base_shift at 0.5 respectively.
	- Use Euler as the sampler and Beta as the scheduler with 25 steps minimum.
	- Higher resolutions like 1024x1400 or 1024x1216 seem to produce best results. Also use 2x3 aspect ratio (portrait) for best results.
	- It was trained on 100 images and manual caption pair's all in "cowboy shot" where the subject is seen from thighs up, so the images generated with this lora will be very biased in that camera shot and angle. A person seen from different angles can be generated successfully with good quality but you need to reduce the strength of the lora to prevent mutations and other cohesion issues for other angles, so play around with the strength of the lora for best results in your use case.
	- This lora was trained on an A100 using the simple tuner training script (props to the dev!). The lora was trained on an fp16 dev base flux model, during training it was using about 27gb worth of VRAM for the following settings. The training speeds are about 2.3 sec/it on the A100. We used prodigy with constant, 64 rank and 64 alpha, bf16, gamma 5. No dropout used, batch size of 1 (batch size 1 yields better results versus using any other batch size).
	- It takes quite a while for the concept to converge decently at about 350 steps per image minimum and 650 steps per image for good results. Lots of tests were performed to converge on the best hyperparameters and this is what we settled on (more testing needed for manual hyperparameters as I expect a large speedup with use of adam8w and such..).
	- Some other notes of interest. We trained on an fp8 flux variant and results were just as good as the fp16 flux model at the cost of 2x convergence speed. That means it now took 700 minimum steps to converge on the subject decently and 1400 steps to converge on a good result. Training on an fp8 flux model took about 16.3gb worth of vram with our settings so I don't see a reason training cant happen on any card that has that VRAM, and possibly with some optimizations maybe could even happen on cards with 16gb of vram for fp8 lora training.
- ## Controlnet
	- https://huggingface.co/XLabs-AI/flux-controlnet-collections [[Controlnet and similar]]
		- [[ComfyWorkFlows]] [x-flux-comfyui/workflows at main ¬∑ XLabs-AI/x-flux-comfyui (github.com)](https://github.com/XLabs-AI/x-flux-comfyui/tree/main/workflows)
		-
- ## Resources
	- [(2047) Discord | #üí°-announcement | XLabs AI](https://discord.com/channels/1271080914692341801/1271086905743638591)
	- whatever this mad thing is [[FLUX] Diagram of UNET / DiT and exotic merging methods (v8.01) | Civitai](https://civitai.com/articles/3409/flux-diagram-of-unet-dit-and-exotic-merging-methods-v7)
	- [XLabs-AI/x-flux-comfyui (github.com)](https://github.com/XLabs-AI/x-flux-comfyui) [[Flux]] [[ComfyUI]]
	- https://www.reddit.com/r/StableDiffusion/comments/1er8q13/an_updated_flux_canny_controlnet_released_by/ [[Flux]] [[Stable Diffusion]] [[Controlnet and similar]]
	- https://huggingface.co/kudzueye/boreal-flux-dev-v2 [[Flux]] [[LoRA DoRA etc]]
	- https://github.com/camenduru/comfyui-colab/blob/main/workflow/flux_image_to_image.json [[flux]] [[ComfyWorkFlows]]
	- [Text Guided Flux Inpainting - a Hugging Face Space by Gradio-Community](https://huggingface.co/spaces/Gradio-Community/Text-guided-Flux-Inpainting) [[Segmentation and Identification]]
	- [(17) Post | Feed | LinkedIn](https://www.linkedin.com/feed/update/urn:li:activity:7230251219888316417/) [[KOHYA Dreambooth and similar]] [[Flux]]
	- https://huggingface.co/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Alpha [[Controlnet and similar]] [[Flux]]
	- https://github.com/cocktailpeanut/fluxgym [[Flux]]
	- https://civitai.com/models/731324 [[Flux]] Social Media Image Generator [[Death of the Internet]]
	- [docs/docs/getting-started/env-configuration.md at improve-flux-docs ¬∑ JohnTheNerd/docs (github.com)](https://github.com/JohnTheNerd/docs/blob/improve-flux-docs/docs/getting-started/env-configuration.md) [[Flux]] [[ComfyUI]] [[Open Webui and Pipelines]]
	- https://github.com/camenduru/comfyui-colab/blob/main/workflow/flux_image_to_image.json [[flux]]
	- [city96/ComfyUI-GGUF: GGUF Quantization support for native ComfyUI models (github.com)](https://github.com/city96/ComfyUI-GGUF) [[ComfyUI]] [[Model Optimisation and Performance]] [[Flux]]
		- [Excuse me? GGUF quants are possible on Flux now! : r/StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/1eslcg0/excuse_me_gguf_quants_are_possible_on_flux_now/)
	-
	- https://github.com/comfyanonymous/ComfyUI/commit/d0b7ab88ba0f1cb4ab16e0425f5229e60c934536 [[Flux]] [[Model Optimisation and Performance]]
	- https://medium.com/@furkangozukara/ultimate-flux-lora-training-tutorial-windows-and-cloud-deployment-abb72f21cbf8 [[Flux]] [[LoRA]]
	- https://github.com/ToTheBeginning/PuLID [[Face Swap]] [[Flux]] [[style transfer]]
		- https://huggingface.co/spaces/yanze/PuLID-FLUX
		- ![image.png](../assets/image_1726490585592_0.png)
		-
- https://www.reddit.com/r/StableDiffusion/comments/1fkeei6/a_simple_flux_pipeline_workflow/
- [dagthomas/comfyui_dagthomas: ComfyUI SDXL Auto Prompter (github.com)](https://github.com/dagthomas/comfyui_dagthomas) [[flux]] [[ComfyUI]] [[Prompt Engineering]]
- https://www.reddit.com/r/StableDiffusion/comments/1fkdp6j/flux_stability_video_how_to_automate_short_videos/ [[AI Video]]
- https://huggingface.co/kudzueye/boreal-flux-dev-v2
- https://openart.ai/workflows/tenofas/flux-detailer-with-latent-noise-injection/TzQXKBjYhIKI75ctU209
- https://openart.ai/workflows/civet_fine_1/tenofas-flux-workflow-v33---txt2img-img2img-and-llm-prompt-facedetailer-faceswap-and-upscaler/mC53ge31vojzvo1gZrJs
- [ComfyUI ‚Äî Flux Advanced - v5-OC | Stable Diffusion Workflows | Civitai](https://civitai.com/models/643835) [[ComfyWorkFlows]]
- https://huggingface.co/XLabs-AI/flux-ip-adapter
- https://huggingface.co/kudzueye/boreal-flux-dev-v2
- https://www.reddit.com/r/StableDiffusion/comments/1f92b9m/first_attempt_at_flux_lora_dreambooth_full/
- https://www.reddit.com/r/StableDiffusion/comments/1f2e1xp/hyper_flux_8_steps_lora_released/
- https://www.reddit.com/r/FluxAI/comments/1f1uhnm/new_flux_controlnet_union_model_just_dropped/
- https://www.reddit.com/r/comfyui/comments/1es8r9o/flux_controlnet_depth_canny_comfyui/?utm_source=diffusiondigest.beehiiv.com&utm_medium=referral&utm_campaign=trump-ai-pics-procreate-says-human-only-this-week-in-ai-art
- https://www.reddit.com/r/comfyui/comments/1ezlzsp/flux_controlnets_3d_scenes_in_playbook_web_editor/ [[visionflow]]
- https://huggingface.co/jbilcke-hf/flux-dev-panorama-lora-2
	- https://www.reddit.com/r/StableDiffusion/comments/1esz4hj/flux_hdr_seamless_panorama_links_in_comment/
- https://www.reddit.com/r/StableDiffusion/comments/1ex64jj/i_made_an_fp8_implementation_of_flux_which_gets/
- https://www.reddit.com/r/FluxAI/comments/1esyy3u/flux_dev_workflow_v20_for_loras_face_detailer_and/
- https://huggingface.co/spaces/Gradio-Community/Text-guided-Flux-Inpainting
- https://github.com/camenduru/comfyui-colab/blob/main/workflow/flux_image_to_image.json [[ComfyWorkFlows]]
- https://comfyanonymous.github.io/ComfyUI_examples/flux/
- https://www.reddit.com/r/StableDiffusion/comments/1emcblr/flux_updates_realism_lora_cannynet_and_finetuning/
-
- # Training LoRA and Fine Tuning
	- The Flux 1D fine-tuning discussion reveals a rapidly evolving landscape of techniques and challenges. Here's a distilled summary of the best options and tips from the community, prioritizing newer information:
	
	  Best Fine-Tuning Options:
	
	  LoRA (Low-Rank Adaptation): Remains the most popular and accessible method due to lower VRAM requirements and good results. Ranks of 16, 32, and even as low as 4 or 2 are being used successfully, depending on the task. Alpha typically matches the rank.
	
	  Full Fine Tuning (FFT): Offers potentially superior results, especially for complex concepts and preventing overfitting, but demands significantly more VRAM (around 24GB or more, even with optimizations). 2kpr's method (integrated into Kohya's sd-scripts) allows FFT within 24GB using BF16, stochastic rounding, and fused backpass, with optional block swapping for even lower VRAM.
	
	  Key Training Considerations and Tips:
	
	  LR (Learning Rate): For LoRA, 1e-4 seems a good starting point, with some finding success at 4e-4 or even higher depending on rank and optimiser. For FFT, significantly lower LRs are necessary (around 1e-5 to 1e-6 or even lower).
	
	  Optimizer: AdamW and Prodigy are both used for LoRA, with Prodigy often converging faster but offering less control. Adafactor with stochastic rounding is crucial for FFT with 2kpr's method. CAME is also being explored.
	
	  Captions: While some early advice suggested minimal or no captions for Flux, the consensus now leans towards detailed, natural language captions, especially for complex subjects and preventing overfitting. Using an LLM like CogVLM or Florence2 is recommended. Avoid overly long, "word salad" captions. Concise and descriptive captions targeting the specific learning objective seem to work best. For style training, include the type of art (painting, photo, etc.) and the style name in the caption. For characters, caption diverse images and avoid overfitting on specific outfits or backgrounds.
	
	  Dataset: High-quality images are crucial. Flux is sensitive to artefacts, so clean your dataset. For likeness, 12-20 varied images are sufficient. For style, aim for diversity of content, pose, and lighting within the style. For characters, include variations in pose, expression, clothing, and background to maximise flexibility. Too similar images can lead to overfitting. Background removal can be helpful for characters and some styles. Avoid including famous faces in your dataset if you don't intend to train them specifically.
	
	  Data Augmentation: Flipping is generally safe. Cropping can be helpful, but avoid scaling if possible as it can introduce artefacts.
	
	  Multi-Resolution Training: While initially recommended, the community now seems divided. It might be helpful for some tasks but can degrade detail and introduce artefacts in others, especially styles. Consider generating only at the highest resolution you plan to use.
	
	  Regularization: Crucial for preventing overfitting and concept bleed, especially in multi-concept training and FFT. Current methods aren't as effective as with previous models. Using a combination of captioned and uncaptioned regularisation images with varied styles is suggested. More research is needed in this area.
	
	  T5 Training: Still experimental and resource-intensive. May be useful for enhancing specific concepts or prompt understanding, but requires careful tuning. Combining natural language captions with tags is being explored.
	
	  CLIP Training: Has become a recommended practice alongside training the transformer (Unet in other models). It seems crucial for good results, especially with more complex concepts and styles.
	
	  Validation: Validation images generated during training can be misleading with Flux, often looking very different from inference results. Rely on thorough testing with a variety of prompts and styles during and after training.
	
	  ComfyUI Updates: Keeping ComfyUI up to date is essential as new features and bug fixes (particularly related to LoRA strength and weight dtype) are frequently released.
	
	  Tools: SimpleTuner, Ostris' ai-toolkit, and Kohya's sd-scripts are the main training tools. Kohya's scripts now include 2kpr's FFT method. Kijai's ComfyUI-FluxTrainer provides ComfyUI nodes for Kohya's scripts.
	
	  Current Issues and Unsolved Challenges:
	
	  Concept Bleed: A persistent problem, particularly in multi-concept training and FFT. Regularization techniques are being explored, but a definitive solution is yet to be found.
	
	  Validation Image Accuracy: Validation images don't reliably predict inference results.
	
	  T5 Training Optimization: Still resource-intensive and requires further research.
	
	  Style Training: Flux seems to favour realism and can be more challenging to train for stylized outputs than previous models. Detailed captions and lower guidance values seem helpful.
	
	  Overfitting: Can happen quickly, especially with smaller datasets and larger ranks. Careful dataset curation, captioning, and monitoring are crucial.
	
	  This space is rapidly changing. Stay tuned to the community for the latest developments and optimised training practices.
	
	
	  User
	  extend and expand. Give me settings and software suggestions where possible
	
	
	  Model
	  111.2s
	  Let's extend and expand on Flux 1D fine-tuning, providing concrete settings and software suggestions:
	
	  1. LoRA Training:
	
	  Software: SimpleTuner, Ostris' ai-toolkit, Kohya's sd-scripts (via ComfyUI nodes with ComfyUI-FluxTrainer)
	
	  Rank/Alpha: Start with 32/32 or 16/16. Experiment with lower ranks (4/4, even 2/2) for styles where subtle changes are desired.
	
	  LR: 1e-4 is a good default. If training stalls, try 4e-4. If it overfits quickly, try lower values.
	
	  Optimizer: AdamW is generally recommended. Prodigy can be used for faster convergence.
	
	  Steps: 2000-3000 steps for likeness, style and character training. Experiment with fewer steps for small, focused datasets. For style training, test at multiple checkpoints as results can oscillate.
	
	  Captions: Concise and descriptive. Include the type of art and style name. For characters, vary pose, expression, clothing, and background. Example: "portrait photo of ohwx woman in a red dress, standing in a garden"
	
	  Dataset: 12-20 images for likeness, 300+ for style or character, focusing on diversity.
	
	  Example config.env (SimpleTuner):
	
	  export FLUX=true
	  export MODEL_NAME="black-forest-labs/FLUX.1-dev"
	  export MAX_NUM_STEPS=3000
	  export LEARNING_RATE=1e-4
	  export TRAIN_BATCH_SIZE=1 # Or higher if VRAM allows
	  export GRADIENT_ACCUMULATION_STEPS=1
	  export LR_SCHEDULE="constant"
	  export CAPTION_DROPOUT_PROBABILITY=0.05 # Or lower, experiment
	  export OPTIMIZER="adamw_bf16" # or "prodigy"
	  export MIXED_PRECISION="bf16"
	  export TRAINER_EXTRA_ARGS="--lora_rank=32 --lora_alpha=32 --keep_vae_loaded --clip_skip=2"
	  Use code with caution.
	  ComfyUI Workflow for Inference: Use the Load/Save Lora and Model Sampling Flux nodes. Adjust the Lora strength and guidance scale according to the trained LoRA. Consider using the Adaptive Guidance V2 node to control guidance more precisely.
	
	  2. Full Fine Tuning (FFT):
	
	  Software: 2kpr's trainer (when released), Kohya's sd-scripts (with caveats regarding stochastic rounding)
	
	  LR: Much lower than for LoRA, start with 1e-5 or 1e-6 and adjust as needed.
	
	  Optimizer: Adafactor with stochastic rounding is essential with 2kpr's method.
	
	  Steps: Potentially fewer steps needed than LoRA due to "overkill" effect. Start with 500-1000 and monitor progress.
	
	  Captions: Similar to LoRA, detailed and natural language.
	
	  Dataset: Similar to LoRA, prioritize quality and diversity.
	
	  Example train.toml (2kpr's trainer - illustrative):
	
	  model_path = "path/to/your/flux1-dev.safetensors"
	  training_data_path = "path/to/your/training/data"
	  output_dir = "path/to/your/output/directory"
	
	  batch_size = 1
	  gradient_accumulation_steps = 1
	  learning_rate = 1e-5  # Lower for FFT
	  optimizer_type = "adafactor"
	  scheduler_type = "constant"
	  max_train_steps = 1000
	  mixed_precision = "bf16"
	  stochastic_rounding = true
	  gradient_checkpointing = false # If VRAM allows
	  blocks_to_swap = 0, # If VRAM allows
	- # Text Encoder settings (CLIP and T5):
	  train_clip_l = true # or false
	  train_t5 = true # or false
	  clip_learning_rate = 1e-6 # Usually lower than unet LR
	  t5_learning_rate = 1e-6 # Usually lower than unet LR
	  Use code with caution.
	  Toml
	  ComfyUI Workflow for Inference: Use the resulting .safetensors file like the base Flux model. You can also extract LoRAs from the FFT checkpoint with 2kpr's extraction script (or equivalent).
	
	  3. Training Text Encoders (CLIP and T5):
	
	  Status: Still experimental and needs careful consideration. Some find it beneficial for improving concept separation and flexibility. Others find it makes little difference or degrades results, depending on the dataset, captions, and task.
	
	  Software: Currently enabled in Kohya's sd-scripts and Kijai's ComfyUI nodes. 2kpr's trainer will also offer this functionality.
	
	  LR: Generally much lower than the Unet/transformer LR. Start with 1e-6 for CLIP and even lower for T5 (1e-7 or less). Separate LRs for CLIP and T5 are often required.
	
	  ComfyUI workflow: Use Kijai's Flux Train node, enabling Train Clip L and/or Train T5. Provide appropriate learning rates in the respective inputs.
	
	  4. LoRA Merging and Pruning:
	
	  Merging in ComfyUI: Use two (or more) Lora Loader nodes, followed by a Lora Merge node.
	
	  Pruning in ComfyUI: Use Kijai's Block Lora Loader node to selectively load and adjust the strength of individual blocks from a LoRA. This can be used to identify which blocks are responsible for desired (or undesired) effects and effectively prune the LoRA by setting unimportant block strengths to 0. Currently, no direct save functionality exists for the pruned LoRA, but merging it with the base model is possible.
	
	  Merging in Kohya: Use merge_lora.py
	
	  Alternative Strategy: Fine-tune a LoRA, then merge it into the base model, then fine-tune again.
	
	  Scaling LoRAs: Use 2kpr's scale_lora.py script to adjust the overall strength of a LoRA. This can be helpful to normalise LoRAs from different trainers or training runs.
	
	  5. Other Key Tools and Techniques:
	
	  Captioning: TagGUI locally supports various VLMs (including CogVLM) for improved captioning. Prompt engineering for captioning is important.
	
	  Background Removal: Use BRIA, InspyreNet, SAM2, or Photoroom Pro for higher-quality masks.
	
	  Upscaling: Ultimate SD Upscale can be used. Consider generating at your desired resolution directly, as Flux handles high resolutions well.
	
	  Inpainting: ComfyUI's Inpaint node with appropriate mask generation.
	
	  Regularization: Experiment with different ratios of captioned vs. uncaptioned and style-varied regularization images. Further research is needed in this area for Flux.
	
	  Debugging and Monitoring: Use ComfyUI's built-in logging, Tensorboard, and 2kpr's trainer's periodic loss charts.
	
	  This expanded information should provide a more concrete starting point for fine-tuning Flux 1D. Remember to experiment, document your findings, and share your results with the community. The field is still under active development, so continuous learning and adaptation are key.
	
	
	  User
	  can you add in any tips and tricks and prompts?
	
	
	  Model
	  83.7s
	  Let's add some prompts, tips, and tricks to fine-tune Flux 1D effectively:
	
	  Prompts for Captioning (using an LLM like CogVLM):
	
	  General: "A detailed, objective description of this image, focusing on visual elements only. Avoid subjective interpretations or artistic descriptions."
	
	  Character Focus: "A detailed description of this character, including their physical attributes, clothing, pose, and expression. The character's name is [character name]."
	
	  Style Focus: "Describe the artistic style of this image, focusing on lines, colours, composition, and textures. The type of art is [painting/photo/illustration/etc.]. The style is [style name]."
	
	  Specific Details: You can combine the above with instructions for specific details: "Describe this image. Focus on the character's facial features. The character's name is [character name]."
	
	  Tips and Tricks for Fine-Tuning:
	
	  Start Simple, Then Scale: Begin with LoRA and small datasets before moving to FFT and larger datasets. This helps develop intuition and find good starting parameters.
	
	  Test Extensively: Use a variety of prompts, styles, and resolutions during and after training. Don't rely solely on validation images. Pay close attention to details like anatomy, coherence, and concept bleed.
	
	  Iterative Approach: Fine-tuning is an iterative process. Train, test, adjust parameters, and retrain. Don't be afraid to experiment.
	
	  Document Everything: Keep track of your datasets, captions, parameters, and results. This allows for better analysis and reproduction. Version your LoRAs and checkpoints.
	
	  Community Resources: Follow the Flux fine-tuning communities (Discord, Reddit, etc.) for the latest developments, tips, and shared experiences.
	
	  Pre-trained Models: Explore existing LoRAs and checkpoints on Civitai and Hugging Face for inspiration and as starting points for your own fine-tuning.
	
	  Seed Exploration: Even with Flux, seed variation can have a noticeable effect on outputs. Try different seeds to see the range of possibilities with your fine-tuned model.
	
	  Guidance Scale Tweaking: Experiment with lower guidance scales (2-3) during inference, especially for artistic styles.
	
	  LoRA Weight Adjustment: Fine-tune the LoRA weight during inference to control its strength and balance it with other LoRAs or concepts.
	
	  Prompts for Testing Generalization and Flexibility:
	
	  Likeness:
	
	  Simple Prompts: "[character name], portrait photo"
	
	  Style Variations: "[character name], painting by [artist name]" , "[character name], anime style"
	
	  Complex Prompts: "[character name] riding a unicorn in a cyberpunk city, photorealistic"
	
	  Negative Prompts: While not directly supported in Flux training, test with negative prompts during inference to see if they work as expected.
	
	  Style:
	
	  Basic Prompts: "a woman in a dress, [style name] artstyle"
	
	  Different Subjects: "a cat, [style name] artstyle", "a spaceship, [style name] artstyle"
	
	  Complex Scenes: "A group of people in a medieval market, [style name] painting"
	
	  Multi-Concept: Combine prompts that test each concept individually and together. For example, if you trained a LoRA with a character and a specific object, test with prompts that include only the character, only the object, and both.
	
	  Specific Prompting Tips for Flux:
	
	  Two-Word Classes: For style and character LoRAs, use a two-word class to provide more context. Example: "ohwx character name, woman elf" or "ohwx style name, painting artstyle"
	
	  Descriptive Captions: For complex subjects or scenes, provide short, clear, natural language descriptions focusing on the elements you want Flux to learn.
	
	  Object and Background Specificity: For styles and characters, if you don't want certain objects or backgrounds to be associated with the LoRA, include them in the captions without the trigger word. This helps prevent overfitting and bleeding.
	
	  "Unpromptable" Concepts: Flux can struggle with some artistic styles or concepts that are hard to describe with prompts. In these cases, using an IPAdapter (when available) might be a more effective approach.
	
	  Prompt Engineering: Experiment with different phrasing and prompt structures to see how Flux interprets them. Some concepts might require more specific or descriptive language than others.
	
	  Leverage Prior Knowledge: Flux has extensive prior knowledge. Start with simple prompts and gradually add detail as needed. See if you can get the desired results without explicitly mentioning every detail.
	
	  By combining these tips, tricks, and prompts, you'll be well on your way to mastering the art of Flux 1D fine-tuning. Remember, experimentation and community engagement are your best allies in this rapidly evolving field.

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: IPAdapter

public:: true

- #Public page
	 - automatically published
- From [ip-adapter.github.io¬†/](https://ip-adapter.github.io/)
- # IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models
	- Hu Ye Jun Zhang Sibo Liu Xiao Han Wei Yang Tencent AI Lab4-5 minutes
	
	  ---
	
	  ![](https://ip-adapter.github.io/assets/fig0.jpg){:height 430, :width 1159}
- ### *Various image synthesis with our proposed IP-Adapter applied on the pretrained text-to-image diffusion model and additional structure controller.*

  [[Paper]](https://arxiv.org/abs/2308.06721)¬†¬†¬†¬†¬†¬†[[Code]](https://github.com/tencent-ailab/IP-Adapter)¬†¬†¬†¬†¬†¬†[[BibTeX]](https://ip-adapter.github.io/files/bibtex.txt)
- ## Abstract

  Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. But, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes: "an image is worth a thousand words". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fine-tuned image prompt model. As we freeze the pretrained duffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to accomplish multimodal image generation.
- ## Approach

  The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt. The proposed IP-Adapter consists of two parts: a image encoder to extract image features from image prompt, and adapted modules with decoupled cross-attention to embed image features into the pretrained text-to-image diffusion model.

  ![](https://ip-adapter.github.io/assets/fig1.png)
- ## Comparison with Existing Methods

  The comparison of our proposed IP-Adapter with other methods conditioned on different kinds and styles of images.

  ![](https://ip-adapter.github.io/assets/result1.jpg)
- ## More Results

  **Generalizable to Custom Models**

  Once the IP-Adapter is trained, it can be directly reusable on custom models fine-tuned from the same base model.

  ![](https://ip-adapter.github.io/assets/result2.jpg)

  **Structure Control**

  The IP-Adapter is fully compatible with existing controllable tools, e.g., ControlNet and T2I-Adapter.

  ![](https://ip-adapter.github.io/assets/result3.jpg)

  Our method not only outperforms other methods in terms of image quality, but also produces images that better align with the reference image.

  ![](https://ip-adapter.github.io/assets/result4.jpg)

  **Image-to-Image and Inpainting**

  Image-guided image-to-image and inpainting can be also achieved by simply replacing text prompt with image prompt.

  ![](https://ip-adapter.github.io/assets/result5.jpg)

  **Multimodal Prompt**

  Due to the decoupled cross-attention strategy, image prompt can work together with text prompt to realise multimodal image generation.

  ![](https://ip-adapter.github.io/assets/result6.jpg)

  Compared with other existing methods, our method can generate superior results in both image quality and alignment with multimodal prompts.

  ![](https://ip-adapter.github.io/assets/result7.jpg)
-

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Proprietary Image Generation

public:: true

#Public page

	- automatically published
- ## Closed Source Image Generation: id:: 659a9229-ed15-4932-a207-eb2daa96786e
	- ### MidJourney v5&6
		- [twitter link to the render loading below](https://twitter.com/LibertyRPF/status/1737848545657618873)
		  {{twitter https://twitter.com/LibertyRPF/status/1737848545657618873}}
		- Super popular San Francisco-based MidJourney, Inc.
			- **Cost**: Starts at $10/month with various subscription plans.
			- **User Interface**: Primarily through Discord, with a website under development.
			- **Strengths**: Excellent image quality, new prompt following is far better, V6 does great humans and locations.
			- **Weaknesses**: Discord not user-friendly for image generation, prompting rules change per model.
			- **Skill Level**: Suitable for beginners and intermediates, the mainstay of creatives.
			- [TechHalla on X: "üÜï Version 6 of MidJourney is here! üò± What's new? How can I use it? All the updates here with prompt examples üëá #PromptShare #MidJourney #AIArtCommunity https://t.co/ZJJCxXkIpb" / X (twitter.com)](https://twitter.com/techhalla/status/1737761999981838379)
	- ### Adobe
		- Developed by Adobe, Firefly claims ethical training practices.
			- **Cost**: "Generative cred* system starting from $4.99/month for 100 credits.
			- **User Interface**: Clean, simple web interface, now integrated fully into Photoshop for free.
			- **Strengths**: User-friendly, good for everyday photorealistic images, legal protection.
			- {{video https://www.youtube.com/watch?v=VNs1lW8t4hw}}
	- ### DALL¬∑E 3
		- From [[OpenAI]] , integrated with ChatGPT pro, [integrated with Bing](https://www.bing.com/images/create), built into Swiftkey [[android]] keyboard.
			- ![image.png](../assets/image_1704230683869_0.png){:height 700, :width 300}
			- **Cost**: Free on Microsoft‚Äôs Bing Image Creator; some free credits. $20 pcm for ChatGPT.
			- **User Interface**: Simple, via Bing Image Creator. Can iteratively build though ChatGPT interface.
			- **Strengths**: Easy to use, understands complex prompts, versatile in image types.
			- **Weaknesses**: Aesthetic quality subjectively different to MidJourney, censorship issues, limited free credits.
			- **Skill Level**: Beginner-friendly.
			- **Some ChatGPT images:** this prompting complexity is extremely hard elsewhere.
			  ChatGPT Dall-E is iterative; Tell it to *keep the same seed"
			- .#.v-gallery-col3
				- ![433f2bd0-ae29-4661-b542-c28fa54c09ff_1702324038829_0.webp](../assets/433f2bd0-ae29-4661-b542-c28fa54c09ff_1702324038829_0_1705315472745_0.webp)
				- ![spider.png](../assets/spider_1705315548949_0.png)
				- ![slices.png](../assets/slices_1705349018973_0.png)
			- ![image.png](../assets/image_1704791221902_0.png){:width 600}
	- ### Magnific AI
		- [Magnific AI ‚Äî The magic image Upscaler & Enhancer](https://magnific.ai/)
		- {{tweet https://twitter.com/LinusEkenstam/status/1744431726443614539?}}
		- [[Landscape]] contains some maps of more products.
- This is a [[presentation]] slide and the next slide is [[Proprietary Video]]

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Stable Diffusion

public:: true

- #Public page automatically published
- Stable¬†Diffusion¬†has¬†emerged¬†as¬†a¬†transformative¬†force¬†in¬†generative¬†AI,¬†mainly¬†for¬†text to image¬†synthesis. This¬†open source¬†model was developed¬†by¬†UK company 'Stability¬†AI', has¬†democratised¬†access¬†to¬†high quality image workflows,¬†empowering¬†artists,¬†creatives, and¬†professionals.
- # Why Stable Diffusion?
  id:: 66408f9e-30e0-442b-9aba-9eb51e36a739
	- ## Image, Video and 3D
		- [[Stable Diffusion]] and [[Stable Video Diffusion]] allow a lot of control, but at a cost of complexity.
		  collapsed:: true
		- ## Stable Diffusion 1.5, XL, and 3
			- UK company with global impact. It is likely now winding up it's operations after difficulty generating revenue in the hyper competitive GenAI market.
				- **Introduction**: Open-source model by StabilityAI
					- **Cost**: Free to run on own hardware; nominal fee for online tools.
					- **User Interface**: User-friendly through platforms like Leonardo.AI.
					- **Strengths**: Unlimited control, good image quality, no censorship.
					- **Weaknesses**: Requires decent hardware, steep learning curve. Questions about Stability business.
					- **Skill Level**: Intermediate to advanced.
			- ### Text-to-Image Generation
				- Stable¬†Diffusion¬†generates¬†realistic¬†and¬†imaginative¬†images¬†from¬†descriptive¬†text¬†prompts.¬†This¬†core¬†functionality¬†allows¬†users¬†to¬†translate¬†their¬†creative¬†visions¬†into¬†visual¬†form¬†with¬†remarkable¬†accuracy¬†and¬†detail.¬†Whether¬†it's¬†a¬†photorealistic¬†portrait,¬†a¬†surreal¬†landscape,¬†or¬†an¬†abstract¬†concept,¬†Stable¬†Diffusion¬†can¬†bring¬†your¬†ideas¬†to¬†life¬†with¬†just¬†a¬†few¬†words.
				- A lot of the products you see on the market are either wrappers for the big AI companies, or else leveraging Stability models on rented cloud compute.
					- ![ComfyUI_temp_exgja_00013_.png](../assets/ComfyUI_temp_exgja_00013_1702592236908_0.png){:width 800}
					- ![Rife_00027.mp4](../assets/Rife_00027_1702831574413_0.mp4){:width 300}
	- ## Open¬†Source
		- Stable¬†Diffusion's¬†open-source¬†nature¬†sets¬†it¬†apart¬†from¬†many¬†other¬†generative¬†AI¬†models.
		- Users¬†have¬†free¬†access¬†to¬†the¬†model's¬†weights and a lot of modular¬†code,¬†allowing¬†them¬†to¬†modify,¬†distribute,¬†and¬†build¬†upon¬†it.
		- This¬†openness¬†fosters¬†collaboration,¬†innovation,¬†and¬†community driven¬†development.
		- Ensures¬†that¬†the¬†technology¬†is¬†not¬†controlled¬†by¬†a¬†select¬†few¬†entities.
		- For brands and private companies this allows private development of digital assets.
	- ## User Friendly¬†Interfaces
		- Platforms¬†like¬†[Leonardo.AI](https://leonardo.ai/), [RunDiffusion](https://rundiffusion.com/) and¬†[Automatic1111's¬†WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)¬†provide¬†intuitive¬†and¬†user friendly¬†interfaces¬†for¬†interacting¬†with¬†Stable¬†Diffusion.
		- ### Rundiffusion
			- <iframe src="https://rundiffusion.com/" style="width: 800px; height: 600px"></iframe>
		- These¬†interfaces¬†offer¬†a¬†range¬†of¬†options¬†for¬†customising¬†parameters,¬†fine tuning¬†models,¬†and¬†experimenting¬†with¬†different¬†artistic¬†styles.
	- ### Customisation
		- Stable¬†Diffusion's¬†flexibility¬†extends¬†to¬†its¬†ability¬†to¬†be¬†fine-tuned¬†on¬†custom¬†datasets.
		- Techniques¬†like¬† [[KOHYA Dreambooth and similar]]¬†and¬† [[LoRA DoRA etc]] training  ¬†allow¬†users¬†to¬†tailor¬†the¬†model¬†to¬†their¬†specific¬†needs¬†and¬†generate¬†images¬†that¬†align¬†with¬†their¬†unique¬†artistic¬†visions¬†or¬†domain-specific¬†requirements.
		  :LOGBOOK:
		  CLOCK: [2024-05-12 Sun 11:12:30]--[2024-05-12 Sun 11:12:31] =>  00:00:01
		  :END:
			- ![ComfyUI_temp_ayipz_00012_.png](../assets/ComfyUI_temp_ayipz_00012_1702330298489_0.png){:width 300, :height 402}
		- This¬†opens¬†up¬†a¬†world¬†of¬†possibilities¬†for¬†creating¬†personalised¬†images,
			- Generating¬†images¬†of¬†specific¬†objects¬†or¬†individuals,
			- Developing¬†models¬†for¬†specialised¬†domains¬†like¬† [[Fashion]] ¬†or¬†architectural¬†design.
	- ### Community¬†Support
		- One¬†of¬†Stable¬†Diffusion's¬†greatest¬†strengths¬†is¬†its¬†vibrant¬†and¬†active¬†community.
		- Much of this happens on Discord and Reddit
			- [(1832) Discord | #ad_resources | banodoco](https://discord.com/channels/1076117621407223829/1149372684220768367)
			- ![image.png](../assets/image_1715715718817_0.png){:width 800}
			- [comfyui (reddit.com)](https://www.reddit.com/r/comfyui/)
			- ![image.png](../assets/image_1715715792127_0.png){:width 800}
			- The¬†[StableDiffusion¬†subreddit](https://www.reddit.com/r/StableDiffusion/)
			- The¬†[Stability¬†AI¬†Discord](https://discord.gg/stabilityai)¬†serve¬†as¬†hubs¬†for¬†sharing¬†creations,¬†resources,¬†and¬†tutorials.
			- This¬†collaborative¬†environment¬†fosters¬†learning,¬†inspiration,¬†and¬†rapid¬†innovation
		- <iframe src="https://openaijourney.com/comfyui-guide/" style="width: 800px; height: 600px"></iframe>
		- <iframe src="https://comfyworkflows.com" style="width: 900px; height: 600px"></iframe>
	- ## Core¬†Models
		- ~~Stable Diffusion 1.4~~
		- ### Stable Diffusion 1.5
			- [Available¬†on¬†GitHub](https://github.com/CompVis/stable-diffusion),¬†this¬†model¬†is¬†optimised¬†for¬†speed¬†and¬†efficiency,
			- Suitable¬†for¬†generating¬†images¬†quickly,¬†especially¬†on¬†less¬†powerful¬†hardware.
			- Highest model diversity
		- ~~Stable Diffusion 2.1~~
		- ### SDXL
			- Higher resolution, better prompt control
			- Will often mess up human bodies due to constrained training
			- More resource intensive
			- Less compatible extensions
		- ### CosXL
			- Likely the last update from the team, most of whom have left following the departure of founder [Emad Mostaque](https://en.wikipedia.org/wiki/Emad_Mostaque).
			- This is a "best practice" update to SDXL which allows higher contrast.
		- ### Zero123 & SV3D
			- {{video https://youtu.be/gl1s7f0mV2E?t=677}}
		- ### ~~Stable Cascade~~
			- Only a partial release.
			- Not great adoption.
			- Better prompt adherence.
		- ### Stable Diffusion 3
			- [Temporary Stable Diffusion 3 Ban | Civitai](https://civitai.com/articles/5732/temporary-stable-diffusion-3-ban)
			- Might be ok in the end.
			- Whole new architecture.
			- Excellent prompt following.
			- Terrible human anatomy.
	- # Community models
		- Models and inspiration from CivitAI, which is very often "not safe for work" so do exercise caution.
			- <iframe src="https://civitai.com/images" style="width: 100%; height: 600px"></iframe>
- ### Prompt¬†Engineering:¬†The¬†Art¬†of¬†Guiding¬†AI¬†Creativity
	- Effective¬†prompt¬†engineering¬†is¬†crucial¬†for¬†unlocking¬†the¬†full¬†potential¬†of¬†Stable¬†Diffusion. Different models demand different styles
	- Here¬†are¬†some¬†tips¬†to¬†enhance¬†your¬†prompts:
		- #### Specificity:
			- Use¬†specific¬†keywords¬†and¬†descriptive¬†phrases¬†to¬†clearly¬†convey¬†your¬†desired¬†image¬†to¬†the¬†AI¬†model.
			- The¬†more¬†precise¬†and¬†detailed¬†your¬†prompt,¬†the¬†better¬†the¬†model¬†can¬†understand¬†your¬†intent¬†and¬†generate¬†images¬†that¬†match¬†your¬†vision.
		- #### Negative¬†Prompts:
			- Utilise¬†negative¬†prompts¬†to¬†exclude¬†unwanted¬†elements¬†or¬†styles¬†from¬†the¬†generated¬†image.
			- This¬†allows¬†you¬†to¬†refine¬†the¬†output¬†and¬†avoid¬†generating¬†images¬†with¬†undesirable¬†features.
		- #### Compositional¬†Control:
			- Employ¬†prompt¬†scheduling¬†and¬†area¬†prompting¬†to¬†create¬†complex¬†compositions¬†and¬†focus¬†on¬†specific¬†details.
			- These¬†techniques¬†allow¬†you¬†to¬†control¬†the¬†timing¬†and¬†location¬†of¬†different¬†elements¬†within¬†the¬†image,¬†resulting¬†in¬†more¬†intricate¬†and¬†visually¬†compelling¬†outputs.
		- #### Extensions:
			- Leverage¬†extensions¬†like¬†"Test¬†My¬†Prompt"¬†to¬†understand¬†the¬†impact¬†of¬†each¬†word¬†in¬†your¬†prompt¬†and¬†refine¬†your¬†wording¬†for¬†better¬†results.¬†This¬†extension¬†helps¬†you¬†analyse¬†how¬†the¬†model¬†interprets¬†different¬†words¬†and¬†phrases,¬†allowing¬†you¬†to¬†optimise¬†your¬†prompts¬†for¬†the¬†desired¬†outcome.
		- #### Experimentation:
			- Don't¬†be¬†afraid¬†to¬†experiment¬†with¬†different¬†models,¬†fine tuning¬†techniques,¬†and¬†prompt¬†styles¬†to¬†discover¬†new¬†possibilities¬†and¬†achieve¬†your¬†desired¬†artistic¬†outcomes.
			- The¬†beauty¬†of¬†Stable¬†Diffusion¬†lies¬†in¬†its¬†flexibility¬†and¬†the¬†endless¬†creative¬†potential¬†it¬†offers.
- ### Applications¬†Across¬†Industries:
	- Stable¬†Diffusion's¬†versatility¬†has¬†led¬†to¬†its¬†adoption¬†across¬†various¬†industries:
		- #### Digital¬†Art¬†Creation:
			- Artists¬†are¬†using¬†Stable¬†Diffusion¬†to¬†create¬†stunning¬†and¬†innovative¬†digital¬†artworks,¬†pushing¬†the¬†boundaries¬†of¬†artistic¬†expression¬†and¬†exploring¬†new¬†creative¬†frontiers.
			  Concept¬†Visualization:
		- #### Designers¬†and¬†engineers
			- Use¬†Stable¬†Diffusion¬†to¬†quickly¬†generate¬†visual¬†representations¬†of¬†their¬†ideas,¬†facilitating¬†rapid¬†prototyping¬†and¬†concept¬†development.¬†This¬†allows¬†for¬†faster¬†iteration¬†and¬†improved¬†communication¬†within¬†design¬†teams.
			  Character¬†Design:
		- #### Game¬†developers¬†and¬†animators
			- Leverage¬†Stable¬†Diffusion¬†to¬†create¬†unique¬†and¬†memorable¬†characters,¬†streamlining¬†the¬†design¬†process¬†and¬†reducing¬†the¬†time¬†and¬†resources¬†required¬†for¬†character¬†creation.
			  Illustration:
		- #### Illustrators
			- Can¬†use¬†Stable¬†Diffusion¬†to¬†generate¬†high-quality¬†illustrations¬†for¬†books,¬†magazines,¬†and¬†other¬†media,¬†offering¬†a¬†faster¬†and¬†more¬†efficient¬†way¬†to¬†produce¬†visually¬†compelling¬†artwork.
		- #### Virtual¬†Production:
			- Filmmakers¬†and¬†VFX¬†artists¬†can¬†use¬†Stable¬†Diffusion¬†to¬†generate¬†realistic¬†backgrounds¬†and¬†environments¬†for¬†virtual¬†production¬†shoots,¬†offering¬†a¬†cost-effective¬†and¬†efficient¬†alternative¬†to¬†traditional¬†green¬†screen¬†techniques.
- ### Addressing¬†Hardware¬†Limitations:
  While¬†Stable¬†Diffusion¬†requires¬†a¬†decent¬†GPU¬†for¬†optimal¬†performance,¬†several¬†solutions¬†are¬†emerging¬†to¬†address¬†hardware¬†limitations:
  Cloud-based¬†Solutions:¬†Platforms¬†like¬†[RunDiffusion](https://app.rundiffusion.com/)
- https://www.forbes.com/sites/iainmartin/2024/03/20/key-stable-diffusion-researchers-leave-stability-ai-as-company-flounders/
  Stable diffusion is a company that specializes in developing advanced artificial intelligence models. They are known for their expertise in creating generative models, which are capable of producing high-quality and realistic outputs in various domains such as image synthesis, language generation, and music composition. Stable Diffusion's cutting-edge research and innovative approaches have made significant contributions to the field of generative AI.
- ## Stable diffusion
	- is a company that specializes in developing advanced artificial intelligence models. They are known for their expertise in creating generative models, which are capable of producing high-quality and realistic outputs in various domains such as image synthesis, language generation, and music composition. Stable Diffusion's cutting-edge research and innovative approaches have made significant contributions to the field of generative AI.
	- [Illustrated overview](https://jalammar.github.io/illustrated-stable-diffusion/)
	- Stable diffusion XL muse GPT [Stable Diffusion Muse SDXL GPT Prompt Generator | Civitai](https://civitai.com/articles/2928/stable-diffusion-muse-sdxl-gpt-prompt-generator)
	- [Automatic1111 GUI and user guide](https://www.thosesixfaces.com/post/stable-diffusion-getting-started-windows)
	- [citivia browser](https://github.com/Vetchems/sd-civitai-browser)
	- [Automatic WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
	- Vlads next SD
	- [InvokeAI simple interface](https://invoke-ai.github.io/InvokeAI/)
	- ### Prompt engineering links
		- https://phraser.tech/
		- [Artist keywords that are known to work](https://docs.google.com/document/d/1SaQx1uJ9LBRS7c6OsZIaeanJGkUdsUBjk9X4dC59BaA/edit#)
		- https://promptomania.com/stable-diffusion-prompt-builder/
		- https://www.krea.ai/
		- [Lexica](https://lexica.art/)
		- [Dall-E prompt engineering](https://docs.google.com/document/d/11WlzjBT0xRpQhP9tFMtxzd0q6ANIdHPUBkMV-YB043U/edit#)
		- [public prompts guy](https://publicprompts.art/)
		- [Promptimize testing suite for prompts](https://github.com/preset-io/promptimize)
		- [Photoshop plugin](https://christiancantrell.com/#ai-ml)
	- ### Dreambooth retraining for faces
		- [windows instructions](https://pastebin.com/xcFpp9Mr)
		- [Discord server](https://discord.com/channels/1023277529424986162/)
		- [dreambooth for SD2](https://github.com/nitrosocke/dreambooth-training-guide/blob/main/README.md#how-to-fine-tune-stable-diffusion-20)
	- ### Birme image resizer
		- [2 hour tutorial](https://www.youtube.com/watch?v=Bdl-jWR3Ukc&t=34)
		- [inject your face into any model (dreambooth)](https://www.youtube.com/watch?v=s25hcW4zq4M)
		- [Guide for dreambooth](https://github.com/nitrosocke/dreambooth-training-guide)
		- [Shivram](https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth)
		- [Progen photorealism Miro guide](https://miro.com/app/board/uXjVPzJyAtU=/)
		- [rare dreambooth tokens](https://github.com/2kpr/dreambooth-tokens)
		- [Multi subject tokens](https://medium.com/@yushantripleseven/using-captions-with-dreambooth-joepenna-dreambooth-716f5b9e9866)
		- [tag editor](https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor)
		- [SDXL dreambooth](https://medium.com/@yushantripleseven/dreambooth-training-sdxl-using-kohya-ss-windows-7d2491460608)
		- [Lora guide](https://civitai.com/articles/1771)
		- [stable swarm distributed comfyui](https://github.com/Stability-AI/StableSwarmUI)
		- [Textual inversion](https://www.reddit.com/r/StableDiffusion/comments/10gs4s2/new_expert_tutorial_for_textual_inversion_text/)
		- [Img2Img guide from reddit for face mapping](https://www.reddit.com/r/StableDiffusion/comments/xgurs3/testing_img2img_batch_processing_i_convert_this/)
		- [textual inversion cheaper training](https://github.com/rinongal/textual_inversion)
		- [CIO blog post](https://danieljeffries.substack.com/p/the-turning-point-for-truly-open?sd=pf)
		- [google stable diffusion](https://www.youtube.com/watch?v=lHcPtbZ0Mnc)
		- [Cross attention replace named items](https://github.com/bloc97/CrossAttentionControl)
		- [256 x faster speedup](https://the-decoder.com/stable-diffusion-could-soon-generate-images-much-faster/)
		- [VoltaML acceleration](https://github.com/VoltaML/voltaML-fast-stable-diffusion)
		- [Depth map into blender from SD2](https://www.youtube.com/watch?v=AeDngG9kQNI)
		- [midjourney tweaks](https://www.reddit.com/r/StableDiffusion/comments/z622mp/trained_midjourney_embedding_on_stable_diffusion/)
		- [and another](https://civitai.com/models/1253/anthro)
		- [Updates Pastebin](https://rentry.org/sdupdates3)
		- [Game development using SD](https://www.heroo.ai/)
		- [Wildcard manager using ChatGPT](https://github.com/mattjaybe/sd-wildcards)
		- [Depth2Img for text](https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/)
		- [train chat GPT to write prompts](https://dreamlike.art/guides/using-openai-chat-gpt-to-write-stable-diffusion-prompts)
		- [non destructive image manipulation using seeds](https://www.reddit.com/r/StableDiffusion/comments/10no6tp/non_destructive_image_variation_in_text2image/)
		- [Instruct pix2pix](https://www.reddit.com/r/StableDiffusion/comments/10l74sl/instruct_pix2pix_is_amazing_inpaintingimg2img/)
		- [reddit post](https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/)
		- [Attention heatmap for prompts (youtube)](https://www.youtube.com/watch?v=XiKyEKJrTLQ)
		- [enormous link roundup](https://rentry.org/RentrySD/)
		- [Prompt master variations management](https://github.com/hoblin/prompt-master)
		- [panoramic world builder](https://huggingface.co/congazverse/worldBuilder)
		- [GitHub AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin: A user-friendly plug-in that makes it easy to generate stable diffusion images inside Photoshop using Automatic1111-sd-webui as a backend.](https://github.com/abdullahalfaraj/auto-photoshop-stablediffusion-plugin)
		- [GitHub ashawkey/stable-dreamfusion: A pytorch implementation of text-to-3D dreamfusion, powered by stable diffusion.](https://github.com/ashawkey/stable-dreamfusion)
		- [Fine tune stable diffusion](https://github.com/nitrosocke/dreambooth-training-guide/blob/main/readme.md#how-to-fine-tune-stable-diffusion-20)
		- [GitHub Sanster/lama-cleaner: Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures.](https://github.com/sanster/lama-cleaner)
		- [holovolo immersive volumetric VR180 videos and photos, and 3D stable diffusion, for Quest and WebVR](https://holovolo.tv)
		- [The Illustrated Stable Diffusion Jay Alammar Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-stable-diffusion/)
		- [reddit educational links](https://www.reddit.com/r/StableDiffusion/comments/116ki29/comment/j97jac3/)
		- [Negative prompt hack tip](https://www.reddit.com/r/StableDiffusion/comments/11pcsxe/just_discovered_a_useful_trick_for_getting_good/)
		- [Modify images with text](https://github.com/justinpinkney/stable-diffusion/blob/main/notebooks/imagic.ipynb)
		- [Photorealism](https://www.reddit.com/r/StableDiffusion/comments/11u2p0u/lazy_guide_to_photorealistic_images/)
		- [sdtools image v 1.6](https://www.reddit.com/r/StableDiffusion/comments/127gck9/sdtools_v16/)
		- [Character plugin](https://github.com/alexv0iceh/AutoChar)
	- [[Checkpoints]]
		- [Stability specific tools](https://sdtools.org/)
		- Arible Prompt Database https://www.arible.co/prompts
		- [[Guide] Make your own Loras, easy and free | Stable Diffusion Other | Civitai: You don't need to download anything, this is a guide with online tools. Click "Show more" below.](https://civitai.com/models/22530)
		- [sdxl lora training](https://github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/How-To-Install-And-Use-Kohya-GUI-And-Do-Ultra-Realistic-SDXL-Training-Tutorial.md)
		- [dylora scripts](https://github.com/facebookresearch/dadaptation/issues/24)
		- [kohya fork with scripts](https://github.com/bmaltais/kohya_ss#about-sdxl-training)
		- [lora of loras (compressed sets)](https://huggingface.co/FFusion/400GB-LoraXL/tree/main)
		- [chart of print size aspect ratios](https://www.reddit.com/r/StableDiffusion/comments/10wqv7r/when_it_comes_to_printing_converting_resolutions/)
		- [SDXL native text lora](https://civitai.com/models/176555/harrlogos-xl-finally-custom-text-generation-in-sd)
		- [SDXL lcm motion lora](https://huggingface.co/latent-consistency/lcm-lora-sdxl)
		- SDXL universal negative prompt
			- text, watermark, low-quality, signature, moir√© pattern, downsampling, aliasing, distorted, blurry, glossy, blur, jpeg artefacts, compression artefacts, poorly drawn, low-resolution, bad, distortion, twisted, excessive, exaggerated pose, exaggerated limbs, grainy, symmetrical, duplicate, error, pattern, beginner, pixelated, fake, hyper, glitch, overexposed, high-contrast, bad-contrast
		- [SDXL prodigy training guide](https://civitai.com/articles/1022)
		- [Lora training interface for windows](https://github.com/bmaltais/kohya_ss)
		- [Refined model](https://civitai.com/models/8392/refined)
		- [Fine tuning with captioning and other fine tuning tricks, followfox](https://substack.com/profile/110613456-followfoxai)
		- [Negative embedding textual inversion for hands etc](https://huggingface.co/datasets/Nerfgun3/bad_prompt)
		- [GitHub kpthedev/ez-text2video: Easily run text-to-video diffusion with customised video length, fps, and dimensions on 4GB video cards, as well as on CPU.](https://github.com/kpthedev/ez-text2video)
		- [Gligen grounding capability for sd1.5](https://gligen.github.io/)
		- [This repository contains a ComfyUI Extension for Automated Text Generation. The extension provides nodes which can be used to automate the text generation process. The goal is to build a node-based Automated Text Generation AGI. This extension should ultimately combine all of the features of the existing text generation tools into one tool.](https://github.com/xXAdonesXx/NodeGPT)
		- [[R] Text-to-image Diffusion Models in Generative AI: A Survey: r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/12ehcez/r_texttoimage_diffusion_models_in_generative_ai_a/)
		- [Tutorial: Creating a Consistent Character as a Textual Inversion Embedding](https://github.com/BelieveDiffusion/tutorials/discussions/3)
		- [Segment anything webui](https://www.reddit.com/r/StableDiffusion/comments/12hkdy8/sd_webui_segment_everything/)
		- [segment anything training](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SAM)
		- [Nvidia stable diffusion segment through clip](https://github.com/NVlabs/ODISE)
		- [Overriding iphone footage with SD characters using controlnet](https://www.reddit.com/r/StableDiffusion/comments/12lg8mn/override_more_experiments_overriding_the_original/)
		- [Interactive photo manipulation GAN](https://huggingface.co/papers/2305.10973)
		- [3d plugin for Automatic1111](https://github.com/jtydhr88/sd-webui-3d-editor)
		- [Face replace plugin for automatic](https://github.com/Gourieff/sd-webui-roop-nsfw)
		- ## Images
			- [Colour palette extraction](https://github.com/mattdesl/gifenc)
			- [Text based real time image manipulation](https://arxiv.org/abs/2210.09276)
			- [Sketch guided text to image inference](https://sketch-guided-diffusion.github.io/)
			- [Google prompt to prompt image remodeller](https://www.youtube.com/watch?v=lHcPtbZ0Mnc)
			- [github](https://github.com/google/prompt-to-prompt)
			- [Img2Prompt](https://replicate.com/methexis-inc#)
			- [eDiffi nvidia text to image](https://deepimagination.cc/eDiffi/)
			- [Image to caption](https://laion.ai/blog/laion-coco/)
			- [lama image cleanup](https://github.com/Sanster/lama-cleaner)
			- [upscalers](https://upscale.wiki/wiki/Model_Database)
			- [upscayl](https://github.com/upscayl/upscayl)
			- [Google Muse](https://www.infoq.com/news/2023/01/google-muse-text-to-image/)
			- [Flair generate photo shoots of products](https://flair.ai/)
			- [Vector graphics from text](https://illustroke.com/)
			- [Simple stock image generator](https://stockimg.ai/)
			- [Patterned: Generates royalty-free patterns.](https://www.patterned.ai/)
			- [Cleanup.picture: Removes objects, defects, people or text from your images.](https://cleanup.pictures/)
			- [Looka: Generates brand names and logos.](https://looka.com/)
			- [CLIP interrogator and prompt engineering colab](https://github.com/pharmapsychotic/clip-interrogator)
			- [Prompt management engine (local and cloud) (promptlayer)](https://magniv.notion.site/PromptLayer-Docs-db0e6f50cacf4564a6d09824ba17a629)
			- [Composer stable diffusion TYPE model](https://github.com/damo-vilab/composer)
			- [Multi-diffusion panoramas](https://multidiffusion.github.io/)
			- [coherent panoramas paper](https://syncdiffusion.github.io/)
			- [UX design AI](https://www.usegalileo.ai/)
			- [pix2pix-3D: 3D-aware Conditional Image Synthesis](http://www.cs.cmu.edu/~pix2pix3D/)
			- [HuggingFace Demo for /ELITE: new fine-tuning technique that can be trained in less than a second/ now available: r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/11mzxyu/huggingface_demo_for_elite_new_finetuning/)
			- [GIGAgan](https://mingukkang.github.io/GigaGAN/)
			- [implementation](https://github.com/lucidrains/gigagan-pytorch)
			- [GitHub danielgatis/rembg: Rembg is a tool to remove images background (other)](https://github.com/danielgatis/rembg)
			- Other. The text is a description of a new product called the "Meta 2" which is a headset that allows users to interact with a computer using their hands.
			- [GitHub kanewallmann/Dreambooth-Stable-Diffusion: Implementation of Dreambooth with Stable Diffusion (tweaks focused on training faces)](https://github.com/kanewallmann/dreambooth-stable-diffusion)
			- [GitHub sedthh/pyxelate: Python class that generates pixel art from images (other)](https://github.com/sedthh/pyxelate)
			- [GitHub upscayl/upscayl: Free and Open Source AI Image Upscaler for Linux, MacOS and Windows built with Linux-First philosophy. (other)](https://github.com/upscayl/upscayl)
			- [GitHub YuxinWenRick/hard-prompts-made-easy: Contribute to YuxinWenRick/hard-prompts-made-easy development by creating an account on GitHub.](https://github.com/YuxinWenRick/hard-prompts-made-easy)
		- This repository contains a tool for gradient-based discrete optimization, which can be used to find the optimal solution for a given problem. The tool is designed to be easy to use, and includes a number of features to make the process of finding the optimal solution easier.
			- [Civitai Helper: SD Webui Civitai Extension | Stable Diffusion Other | Civitai: Now, we finally have a Civitai SD webui extension!! Update: 1.5.7 is here, if you're using localization extension, like Asian language UI, you need ...](https://civitai.com/models/16768/civitai-helper-sd-webui-civitai-extension)
		- The Civitai Helper is a Civitai extension that allows for stable diffusions of other Civitai extensions. It also includes an animation which rotates and scales the extension icon.
		- [GitHub YuxinWenRick/hard-prompts-made-easy: Contribute to YuxinWenRick/hard-prompts-made-easy development by creating an account on GitHub.](https://github.com/YuxinWenRick/hard-prompts-made-easy)
		- This repository contains code for a gradient-based discrete optimization method. The method is designed to make it easy to find hard prompts, which are useful for training machine learning models.
		- [StableSam meta segmentation plus SD inpainting](https://twitter.com/abhi1thakur/status/1645669023726592007)
		- New Feature: "ZOOM ENHANCE" for the A111 WebUI. Automatically fix small details like faces and hands! : r/StableDiffusion [https://www.reddit.com/r/StableDiffusion/comments/11pyiro/new_feature_zoom_enhance_for_the_a111_webui/](https://www.reddit.com/r/StableDiffusion/comments/11pyiro/new_feature_zoom_enhance_for_the_a111_webui/)
		- [Realtime scribble](https://github.com/houseofsecrets/SdPaint)
		- [latent labs 360 images lora](https://civitai.com/models/10753/latentlabs360)
		- Kandinsky model
			- [finetuned 2.1](https://www.reddit.com/r/StableDiffusion/comments/13hgpo2/kandinsky_21_fine_tune/)
			- [QR codes](https://www.youtube.com/watch?v=IntRn96C4l4)
			- [DragGan image editing through drag points](https://github.com/XingangPan/DragGAN)
			- [Faster CPP clip](https://github.com/monatis/clip.cpp)
			- [animateDiff](https://github.com/guoyww/animatediff/)
			- [AnimatediffSDXL lora](https://www.reddit.com/r/StableDiffusion/comments/17stnug/sdxl_animatediff_motion_lora_released/)
			- [diffbar image sharpen](https://github.com/XPixelGroup/DiffBIR?ref=aiartweekly)
			- [SD model mixer](https://github.com/wkpark/sd-webui-model-mixer)
			- Textual Inversion character creation [tutorials/consistent_character_embedding/README.md at main ¬∑ BelieveDiffusion/tutorials (github.com)](https://github.com/BelieveDiffusion/tutorials/blob/main/consistent_character_embedding/README.md)
			- [%3 e](https://github.com/nitrosocke/dreambooth-training-guide/blob/main/README.md#how-to-fine-tune-stable-diffusion-20%22/%3E)
			- [AI Creating 'Art' Is An Ethical And [[Copyright]] Nightmare](https://kotaku.com/ai-art-dall-e-midjourney-stable-diffusion-[[copyright]]-1849388060)
			- [CompVis/stable-diffusion: A latent text-to-image diffusion model](https://github.com/CompVis/stable-diffusion)
			- [Consistency in Stable Diffusion Definitive Guide to Having Multiple Faces of the Same Character](https://www.youtube.com/watch?v=Ig1S2guCfKM%22%2F%3E)
			- [Consistent character embedding#readme%22](https://github.com/BelieveDiffusion/tutorials/tree/main/consistent_character_embedding#readme%22)
			- [Consistent character embedding#readme}{walkthrough](https://github.com/BelieveDiffusion/tutorials/tree/main/consistent_character_embedding#readme}{walkthrough)
			- [Controlnet for DensePose v1.0 | Stable Diffusion Controlnet | Civitai](https://civitai.com/models/120149/controlnet-for-densepose%22/%3E)
			- [From the StableDiffusion community on Reddit: New Feature: "ZOOM ENHANCE" for the A111 WebUI. Automatically fix small details like faces and hands!](https://www.reddit.com/r/StableDiffusion/comments/11pyiro/new_feature_zoom_enhance_for_the_a111_webui)
			- [From the StableDiffusion community on Reddit](https://www.reddit.com/r/StableDiffusion/comments/132rcou/30_stable_diffusion_tutorials_automatic1111_web)
			- [How to Inject Your Trained Subject e.g. Your Face Into Any Custom Stable Diffusion Model By Web UI](https://www.youtube.com/watch?v=s25hcW4zq4M%22%2F%3E)
			- [Imagic: Text-Based Real Image Editing with Diffusion Models](https://buff.ly/3VLGMzo)
			- [RODIN Diffusion](https://3d-avatar-diffusion.microsoft.com/?amp%3Butm_medium=email&amp%3Butm_source=Revue+newsletter#/%22/%3E)
			- [Readme](https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#tensorrt-text2image-stable-diffusion-pipeline)
			- [Readme](https://github.com/nitrosocke/dreambooth-training-guide/blob/main/README.md#how-to-fine-tune-stable-diffusion-20%22)
			- [Spirited Away General Model (1.5) @Spirited | Stable Diffusion Checkpoint | Civitai](https://civitai.com/models/5378/spirited-away-general-model-15%22)
			- [Style-Info: An embedding for infographic style art 1.0 | Stable Diffusion Embedding | Civitai](https://civitai.com/models/5271/style-info-an-embedding-for-infographic-style-art%22/%3E)
			- [THE DECODER](https://the-decoder.com/stable-diffusion-could-soon-generate-images-much-faster/%7D%7B256)
			- [Tutorial: Creating a Consistent Character as a Textual Inversion Embedding ¬∑ BelieveDiffusion tutorials ¬∑ Discussion #3](https://github.com/BelieveDiffusion/tutorials/discussions/3%22%3E)
			- [Ultimate Guide to Upscale Images with AI in Stable Diffusion](https://onceuponanalgorithm.org/ultimate-guide-to-upscale-images-with-ai-in-stable-diffusion)
			- [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models#classifier-free-guidance}{here}.)
			- [Wojak SDXL v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/128046/wojak-sdxl%22)
			- https://www.reddit.com/r/StableDiffusion/comments/145d6by/scannable_cat_qr_art_with_ai_my_recent_attempt)
			- [https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/%22%3E%3Crichcontent](https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/%22%3E%3Crichcontent)
			- [wl-zhao/UniPC: [NeurIPS 2023] UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models](https://github.com/wl-zhao/UniPC)
			- [‰∏áË±°ÁÜîÁÇâ | Anything V5/Ink ink | Stable Diffusion Checkpoint | Civitai](https://civitai.com/models/9409/anything-v5-or-anything-diffusion-original%22)
			- [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](https://buff.ly/41FgQrb%22)
			- [Anthro v1 | Stable Diffusion Embedding | Civitai](https://civitai.com/models/1253/anthro%22)
			- [Become A Stable Diffusion Prompt Master By Using DAAM Attention Heatmap For Each Used Token Word](https://www.youtube.com/watch?v=XiKyEKJrTLQ%7D%7BAttention)
			- [Consistent AI Characters with Different Poses Angles CharTurner Stable Diffusion](https://www.youtube.com/watch?v=-iwPVUzAWzk%22)
			- [From the StableDiffusion community on Reddit: Advanced advice for model training / fine-tuning and captioning](https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and)
			- [From the StableDiffusion community on Reddit](https://www.reddit.com/r/StableDiffusion/comments/11mulj6/quality_improvements_to%22)
			- [Google's prompt-to-prompt AI for Stable Diffusion tutorial!](https://www.youtube.com/watch?v=lHcPtbZ0Mnc%7D%7BGoogle)
			- [Home](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Developing-extensions%22)
			- [How to Make 360 VR Environments for Quest with AI Stable Diffusion and Blender Tutorial 2023](https://www.youtube.com/watch?v=t9zzcRsf0IA%22)
			- [Open Source AI and Stable Diffusion with Emad Mostaque](https://open.spotify.com/episode/3PCboPPIdkicl9eyW5Eaux?si=u6sUA8WdR9Wyw3NPLMdAXQ)
			- [Reddit Prove your humanity](https://www.reddit.com/r/StableDiffusion/comments/10no6tp/non_destructive_image_variation_in_text2image/%7D%7Bnon)
			- [Refined Refined v11 | Stable Diffusion Checkpoint | Civitai](https://civitai.com/models/8392/refined%22)
			- [Spirited Away General Model (1.5) @Spirited | Stable Diffusion Checkpoint | Civitai](https://civitai.com/models/5378/spirited-away-general-model-15%7D%7Bspirited)
			- [Stable Diffusion Outpainting Colab Tutorial](https://m.youtube.com/watch?list=PLpdmBGJ6ELUJOuL83tQKJm-ty4IJficHc&v=-8jmBGgGj2E)
			- [Style-Info: An embedding for infographic style art 1.0 | Stable Diffusion Embedding | Civitai](https://civitai.com/models/5271/style-info-an-embedding-for-infographic-style-art%7D%7BInfographic)
			- [Tutorial: Creating a Consistent Character as a Textual Inversion Embedding ¬∑ BelieveDiffusion tutorials ¬∑ Discussion #3](https://github.com/BelieveDiffusion/tutorials/discussions/3%22)
			- [Zero To Hero Stable Diffusion DreamBooth Tutorial By Using Automatic1111 Web UI Ultra Detailed](https://www.youtube.com/watch?t=34%7D%7B2&v=Bdl-jWR3Ukc%5C)
			- [altryne/awesome-ai-art-image-synthesis: A list of awesome tools, ideas, prompt engineering tools, colabs, models, and helpers for the prompt designer playing with aiArt and image synthesis. Covers Dalle2, MidJourney, StableDiffusion, and open source tools.](https://github.com/altryne/awesome-ai-art-image-synthesis)
			- [diStyApps/Stable-Diffusion-Pickle-Scanner-GUI: Pickle Scanner GUI](https://github.com/diStyApps/Stable-Diffusion-Pickle-Scanner-GUI)
			- [https://www.reddit.com/r/StableDiffusion/comments/100tp0v/protogenx34_has_absolutely_amazing_detail/%22](https://www.reddit.com/r/StableDiffusion/comments/100tp0v/protogenx34_has_absolutely_amazing_detail/%22)
			- [https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/%22](https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/%22)
			- [https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/%7D%7BDepth2Img](https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/%7D%7BDepth2Img)
			- [https://www.reddit.com/r/StableDiffusion/comments/10gs4s2/new_expert_tutorial_for_textual_inversion_text/%7D%7BTextual](https://www.reddit.com/r/StableDiffusion/comments/10gs4s2/new_expert_tutorial_for_textual_inversion_text/%7D%7BTextual)
			- [https://www.reddit.com/r/StableDiffusion/comments/10l74sl/instruct_pix2pix_is_amazing_inpaintingimg2img/%7D%7BInstruct](https://www.reddit.com/r/StableDiffusion/comments/10l74sl/instruct_pix2pix_is_amazing_inpaintingimg2img/%7D%7BInstruct)
			- [https://www.reddit.com/r/StableDiffusion/comments/10no6tp/non_destructive_image_variation_in_text2image/%22](https://www.reddit.com/r/StableDiffusion/comments/10no6tp/non_destructive_image_variation_in_text2image/%22)
			- [https://www.reddit.com/r/StableDiffusion/comments/10rr99t/mocap_unreal_engine_warpfusion/%7D%7BMoCap](https://www.reddit.com/r/StableDiffusion/comments/10rr99t/mocap_unreal_engine_warpfusion/%7D%7BMoCap)
			- [https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/%22](https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/%22)
			- [https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/%7D%7Breddit](https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/%7D%7Breddit)
			- [https://www.reddit.com/r/StableDiffusion/comments/1148x38/tencent_ai_just_release_their_method_and_code/%7D%7BTencent](https://www.reddit.com/r/StableDiffusion/comments/1148x38/tencent_ai_just_release_their_method_and_code/%7D%7BTencent)
			- [https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/%7D%7BAdvanced](https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/%7D%7BAdvanced)
			- [https://www.reddit.com/r/StableDiffusion/comments/114zmh3/controlnet_and_ebsynth_make_incredible_temporally/%22](https://www.reddit.com/r/StableDiffusion/comments/114zmh3/controlnet_and_ebsynth_make_incredible_temporally/%22)
			- [Stable Assistant ‚Äî Stability AI](https://stability.ai/stable-assistant) [[Stable Diffusion]]
			- [[Controlnet and similar]] [[Stable Diffusion]] [xinsir/controlnet-union-sdxl-1.0 ¬∑ Hugging Face]
			- [[AI Video]] [(1865) Discord | "Steerable Motion 1.4 - now with unlimited input frames! (+ minor optimisations)" | banodoco](https://discord.com/channels/1076117621407223829/1234280186892259368) [[Stable Diffusion]] [[Stable Video Diffusion]]
			- (https://huggingface.co/xinsir/controlnet-union-sdxl-1.0)

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: ollama

public:: true

- The [[Ollama]] repository on GitHub is a comprehensive, MIT-licenced platform for utilising and managing large language models (LLMs) such as Llama 3.1, Mistral, Gemma 2, and others. It offers a straightforward way to run these models on various operating systems, including macOS, Windows, and Linux, either through manual installation or via Docker.
-
- [ComfyUI (image generation) and AllTalk (TTS) ¬∑ open-webui/open-webui ¬∑ Discussion #1221 (github.com)](https://github.com/open-webui/open-webui/discussions/1221)
- /parse https://twitter.com/0xfoobar/status/1591142748207583235
- https://twitter.com/0xfoobar/status/1591142748207583235
- [[Knowledge Graphing]] MindsDB integration with