- ### OntologyBlock
  id:: recurrent-neural-network-ontology
  collapsed:: true
	- ontology:: true
	- term-id:: AI-0033
	- preferred-term:: Recurrent Neural Network
	- source-domain:: ai
	- status:: active
	- public-access:: true
	- definition:: A Recurrent Neural Network (RNN) is a [[neural network]] architecture in which outputs from both the previous layer and the previous processing step are fed into the current layer. RNNs maintain internal state (memory), making them suitable for processing [[sequential data]] such as [[time series]], [[natural language]], and [[speech recognition]].
	- maturity:: mature
	- owl:class:: ml:RecurrentNeuralNetwork
	- owl:physicality:: ConceptualEntity
	- owl:role:: Algorithm
	- belongsToDomain:: [[Machine Learning Domain]]
	- **[Updated 2025]**: RNNs remain foundational for sequential data processing despite the rise of [[transformer]] architectures

## About Recurrent Neural Network

### Primary Definition [Updated 2025]
- A **[[Recurrent Neural Network]] (RNN)** is a [[neural network]] architecture featuring feedback connections that enable the processing of sequential data by maintaining [[hidden state]] across time steps.
- Unlike [[feedforward neural networks]], RNNs can capture temporal dependencies and process variable-length sequences through recurrent connections that create internal memory.
- **Core Characteristics**:
	- **Temporal processing**: Processes sequences one element at a time
	- **Internal memory**: Maintains [[hidden state]] across time steps
	- **Parameter sharing**: Same weights applied across all time steps
	- **Variable-length input**: Can handle sequences of arbitrary length
	- **Sequential dependencies**: Captures temporal relationships in data

### Historical Development [Updated 2025]
- **1980s**: Foundational concepts introduced by [[John Hopfield]] and [[David Rumelhart]]
- **1997**: [[Long Short-Term Memory]] (LSTM) developed by [[Sepp Hochreiter]] and [[JÃ¼rgen Schmidhuber]] to address [[vanishing gradient problem]]
- **2014**: [[Gated Recurrent Unit]] (GRU) introduced as simplified LSTM variant by [[Kyunghyun Cho]] et al.
- **2017-2025**: Hybrid architectures combining RNNs with [[attention mechanisms]] and [[transformers]] emerge for optimal efficiency-performance balance

## Architecture and Components [Updated 2025]

### Core Architecture
- **Input Layer**: Receives sequential input at each time step
- **Recurrent Hidden Layer**:
	- Maintains [[hidden state]] vector $h_t$
	- Computes: $h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$
	- Passes information both forward and through time
- **Output Layer**: Generates predictions based on hidden state
- **Recurrent Connections**: Feedback loops enabling temporal dependencies

### Variants and Extensions [Updated 2025]
- **[[Long Short-Term Memory]] (LSTM)**:
	- Addresses [[vanishing gradient problem]] through gating mechanisms
	- Includes forget gate, input gate, and output gate
	- Maintains separate cell state and hidden state
- **[[Gated Recurrent Unit]] (GRU)**:
	- Simplified LSTM with fewer parameters
	- Combines forget and input gates into update gate
	- Faster training with comparable performance
- **[[Bidirectional RNN]]**:
	- Processes sequences in both forward and backward directions
	- Captures both past and future context
	- Common in [[natural language processing]] tasks
- **[[Deep RNN]]**:
	- Multiple stacked recurrent layers
	- Enables hierarchical feature learning
	- Requires careful initialization to avoid gradient issues

## Current Landscape (2025) [Updated 2025]

### Industry Applications
- **[[Natural Language Processing]]**:
	- [[Language modelling]]: Character and word-level generation
	- [[Machine translation]]: Sequence-to-sequence models (though increasingly replaced by transformers)
	- [[Sentiment analysis]]: Temporal text classification
	- [[Named entity recognition]]: Sequential tagging tasks
- **[[Speech Processing]]**:
	- [[Speech recognition]]: Acoustic modelling for voice assistants
	- [[Speaker identification]]: Temporal voice pattern analysis
	- [[Speech synthesis]]: Prosody and timing generation
- **[[Time Series Analysis]]**:
	- [[Financial forecasting]]: Stock price and market trend prediction
	- [[Anomaly detection]]: Sequential pattern deviation identification
	- [[Energy demand prediction]]: Grid load forecasting
	- [[Weather forecasting]]: Temporal meteorological modelling

### Technical Status [Updated 2025]
- **Performance Context**:
	- RNNs excel in resource-constrained environments where transformer computational costs are prohibitive
	- Preferred for real-time streaming applications requiring low latency
	- Effective for shorter sequences (<100 time steps) where transformers show minimal advantage
- **vs. Transformers**:
	- **RNN Advantages**: Lower memory footprint, sequential processing efficiency, inherent temporal inductive bias
	- **Transformer Advantages**: Parallel training, better long-range dependencies, attention mechanisms
	- **Hybrid Approaches**: Combining RNN efficiency with transformer expressiveness

## See Also
- [[Neural Networks]]
- [[Deep Learning]]
- [[Machine Learning]]
- [[Natural Language Processing]]
- [[Time Series Analysis]]
- [[LSTM]]
- [[GRU]]
- [[Transformers]]
