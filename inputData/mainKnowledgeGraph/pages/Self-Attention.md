- ### OntologyBlock
  id:: self-attention-ontology
  collapsed:: true
	- ontology:: true
	- term-id:: AI-0197
	- preferred-term:: Self Attention
	- source-domain:: metaverse
	- status:: draft
	- definition:: An attention mechanism where every token in a sequence attends to every other token in the same sequence, allowing the model to capture intra-sequence dependencies.


### Relationships
- is-subclass-of:: [[AttentionMechanism]]

## Characteristics

- **Intra-Sequence Dependencies**: Each token computes attention over all other tokens in the same sequence
- **Query-Key-Value Framework**: Uses query, key, and value representations for attention computation
- **Permutation Invariance**: Requires positional encoding as self-attention is inherently permutation-invariant
- **Computational Complexity**: O(n²) complexity with sequence length, challenging for very long sequences

## Academic Foundations

**Primary Source**: Vaswani et al., "Attention Is All You Need", arXiv:1706.03762 (2017)

**Key Innovation**: Demonstrated that self-attention alone, without recurrence, could model long-range dependencies effectively.

## Technical Context

Self-attention models compute representations by allowing each token to attend to all other tokens using query-key compatibility scores. This mechanism enables capturing complex dependencies without the sequential constraints of recurrent architectures.

## Ontological Relationships

- **Broader Term**: Attention Mechanism
- **Related Terms**: Multi-Head Attention, Scaled Dot-Product Attention, Cross-Attention
- **Component Of**: Transformer Architecture

## Usage Context

"Self-attention models compute representations by allowing each token to attend to all other tokens using query-key compatibility scores."

## OWL Functional Syntax

```clojure
(Declaration (Class :SelfAttention))
(AnnotationAssertion rdfs:label :SelfAttention "Self-Attention"@en)
(AnnotationAssertion rdfs:comment :SelfAttention
  "An attention mechanism where every token in a sequence attends to every other token in the same sequence."@en)
(AnnotationAssertion :hasSource :SelfAttention
  "Vaswani et al., 'Attention Is All You Need', arXiv:1706.03762 (2017)"@en)

;; Taxonomic relationships
(SubClassOf :SelfAttention :AttentionMechanism)

;; Mechanism components
(SubClassOf :SelfAttention
  (ObjectSomeValuesFrom :usesComponent :QueryKeyValueFramework))
(SubClassOf :SelfAttention
  (ObjectSomeValuesFrom :requires :PositionalEncoding))

;; Computational properties
(SubClassOf :SelfAttention
  (ObjectSomeValuesFrom :computesAttentionOver :InputSequence))
(SubClassOf :SelfAttention
  (ObjectSomeValuesFrom :produces :ContextualisedRepresentation))

;; Architectural characteristics
(DataPropertyAssertion :hasComplexityClass :SelfAttention "O(n²)"^^xsd:string)
(DataPropertyAssertion :isPermutationInvariant :SelfAttention "true"^^xsd:boolean)
(DataPropertyAssertion :isBidirectional :SelfAttention "true"^^xsd:boolean)
(DataPropertyAssertion :capturesLongRangeDependencies :SelfAttention "true"^^xsd:boolean)

;; Related mechanisms
(DisjointClasses :SelfAttention :CrossAttention)

;; Used in architectures
(SubClassOf :TransformerArchitecture
  (ObjectSomeValuesFrom :implementsMechanism :SelfAttention))
```

## References

- Vaswani, A., et al. (2017). "Attention Is All You Need". arXiv:1706.03762
- Bahdanau, D., et al. (2014). "Neural Machine Translation by Jointly Learning to Align and Translate". arXiv:1409.0473

---

*Ontology Term managed by AI-Grounded Ontology Working Group*
*UK English Spelling Standards Applied*
	- maturity:: draft
	- owl:class:: mv:SelfAttention
	- owl:physicality:: ConceptualEntity
	- owl:role:: Concept
	- belongsToDomain:: [[MetaverseDomain]]
