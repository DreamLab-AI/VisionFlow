- ## Key Concepts
	- **Transfer Learning:** The process of adapting a pre-trained model to a new task.
	- **Fine-tuning:** The process of further training a pre-trained model on a smaller, task-specific dataset.
	- **Prompt Engineering:** The process of designing prompts to elicit the desired output from a language model.
- ## Popular Models
	- ### [GPT-4](https://openai.com/research/gpt-4)
		- A large multimodal model from OpenAI that can accept image and text inputs and produce text outputs.
	- ### [Claude 3](https://www.anthropic.com/news/claude-3-family)
		- A family of models from Anthropic that are designed to be helpful, harmless, and honest.
	- ### [Gemini](https://deepmind.google/technologies/gemini/)
		- A family of models from Google that are designed to be multimodal and can understand and generate text, code, and images.
	- ### [Llama](https://ai.meta.com/llama/)
		- A family of open-source models from Meta AI.
	- ### [Mistral](https://mistral.ai/)
		- A family of open-source models from Mistral AI.
	- ### [Falcon](https://falconllm.tii.ae/)
		- A family of open-source models from the Technology Innovation Institute (TII).
- ## Tools and Platforms
	- ### [Hugging Face](https://huggingface.co/)
		- A platform for sharing and using pre-trained models.
	- ### [OpenWebUI](https://openwebui.com/)
		- A user-friendly web interface for interacting with large language models.
	- ### [LangChain](https://www.langchain.com/)
		- A framework for developing applications powered by language models.
	- ### [Pinecone](https://www.pinecone.io/)
		- A vector database for AI applications.
- ## Research and Papers
	- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
	- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
	- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
	- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- ## See Also
	- [[Large language models]]
	- [[Artificial Intelligence]]
	- [[Machine Learning]]
	- [[Deep Learning]]

### Relationships
- is-subclass-of:: [[MachineLearning]]


## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Proprietary Large Language Models

public:: true

- #Public page
	- automatically published
- ## Closed source Large Language Models: and [[AI Companies]]
	- ![1712680210067.jpeg](../assets/1712680210067_1712686278817_0.jpeg)
	- [LinkedIn post from Peter Gostev](https://www.linkedin.com/posts/peter-gostev_we-are-seeing-some-clear-categories-emerge-activity-7183501457684365314-iihT?)
	-
	- ### OpenAI & ChatGPT
		- ðŸŸ¢ ChatGPT mobile [app revenue suggests](https://techcrunch.com/2023/10/09/chatgpts-mobile-app-hit-record-4-58m-in-revenue-last-month-but-growth-is-slowing/) around 250,000 users of their pro service **globally**. That is much lower than I thought... Let's assume only one in ten paying users install the app. That's still 2.5M users, which is about 0.003% of the eligible population.
		- They have released a [Prompt engineering
		- OpenAI API](https://platform.openai.com/docs/guides/prompt-engineering) guide.
		- I have a [[Prompt Engineering]] section too.
		- The GPT "store" / app experience.
		- A note about GPTs. They really are quite powerful. Think of them as an app builder, containing an AI agent, in a box, with [bidirectional internet](https://medium.com/@michaelev3/connecting-custom-gpts-to-google-apis-726dc2cdb54d), and the ability to build code. (such as which is an excellent coding assistant [Grimoire](https://chat.openai.com/g/g-n7Rs0IK86-grimoire)). - ðŸŸ¢ They are the most advantage you can get for $20 a month, if you have tasks that you repeat, and you're not a coder. **
		- Note they now want $25 if you want to keep your data out of their training set.**
			- {{{tweet https://twitter.com/ConsensusNLP/status/1724872225780625419}}}
		- This is called "Actions" and is only in the GPTs or via the API (or both)
		- ðŸŸ¢ Microsoft integrates OpenAI right across their suites under the [[Microsoft CoPilot]] brand.
		  id:: 659a922a-e819-4baa-b323-c07b3cf85290
		- This is pure speculation, but it feels like Microsoft might eventually effectively take over, being more experienced, mature, and canny.
		- The novel structural reason for OpenAI existing the way it did (a non-profit with a "fuse" for runaway AI) has been broken.
		- Keep an eye out for the remaining canary in the coal mine which is OpenAI declaring [[Artificial Intelligence]], isolating Microsoft from that element of their models. They might pretend [[Artificial Superintelligence]] for commercial reasons.
	- ### Anthropic
		- Smaller, unsure how much smaller, it's a fork of people from OpenAI, but has radically exceeded earning expectations.
		- [[Constitutional AI]], like Asimov's laws of robotics
		- ðŸŸ¢ [anthropics/sleeper-agents-paper: Contains random samples referenced in the paper "Sleeper Agents: Training Robustly Deceptive LLMs that Persist Through Safety Training". (github.com)](https://github.com/anthropics/sleeper-agents-paper)
		- {{twitter https://twitter.com/natfriedman/status/1777739863678386268/photo/1}}
	- ### Grok from Musk is pretty bad, but..
		- People haven't appreciated the strength of the business model Musk has
		- His is the only unified language and vision company in the world at this scale that can handle real world interactions.
	- ### Google Gemini
		- Incredible million token context and [[Multimodal]]
		- Not as capable as Claude3 but writes more human readable text than ChatGPT
		- [Gemini 1.5 and Googleâ€™s Nature â€“ Stratechery by Ben Thompson](https://stratechery.com/2024/gemini-1-5-and-googles-nature/)
		- For me the interesting one is Nano, which is natively multimodal (4 bit 1-3B) and fits in a phone.
		- I think we'll look back at Nano as having signalled the future.
	- ### Salesforce
		- Slack. Don't discount Salesforce. Again, if you use slack, stick with this for now. **All** of the tools are coming to **all** of the platforms.
- # Actionable LLM advice:
	- Stick with the tools in the ecosystem you have already bought, until you need more. If your data is in Google use Google. If it's in Microsoft use Microsoft. If you use Runway / Mid journey then please do continue.
	- The edge isn't between vendors, it's a deep understanding of what problems you need it to solve, and the disposition of your data, or business technical edge.
	- Importance of unique business strategies over proprietary technology. [Harvard Business Review](https://hbr.org/2023/12/strategy-not-technology-is-the-key-to-winning-with-genai).
		- Necessity for firms to create value beyond technical features.
		- Focus on building strong brands and exceptional customer experiences.
		- Significance of strategic human resource management, including talent retention and skill development.
	- Sort your data management protocols out. Nobody wants to, but things are getting confusing. For me that's as simple as logseq and github.
		- Nearly 10,000 data points that reach well into active code and research. (graph).
	- The OpenAI kerfuffle last year has alerted companies to the fragility of single providers.
	- Consider how business critical the tooling is becoming over time for your business. Have a **NOW / NEXT / LATER** plan. Have and understand options but don't waste too much time exploring them, focus is king.
	- There are now many cloud middleware solutions that will handle the prompt realignment, load balancing, pricing etc across the whole [[Proprietary Large Language Models]] [[landscape]].
	- I think open source will win in the end because SO many people in the world will be forced and/or want not to use these few hyper centralised providers. **This is a contentious opinion**.
	- {{embed ((659fe0be-a52a-42ef-8f50-73695a802945))}}
	- https://www.reddit.com/r/StableDiffusion/comments/18tqyn4/midjourney_v60_vs_sdxl_exact_same_prompts_using/
	- If your business needs custom **models** then still do as much with off the shelf as you can. You need to be mindful of ethics and the law. This is non-trivial. The team here can help.
	- Regardless of the scale and technical proficiency of your team, these tools, especially the open source ones, can provide a rapid way to ask your customers **"is this what you mean?"**. People are bad at specifying, but good at instinctive validation. You can then go and manufacture a properly optimised and legally compliant toolchain.
- This is a [[presentation]] side, and the next slide is [[Proprietary Image Generation]]

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: multimodal

public:: true

- #Public page automatically published
- # OpenAI ChatGPT-4o (omni)
  id:: 66446c0e-93be-431d-93d4-1e5fa36848c5
	- Free to use, for everyone! Not private by default.
	- True multi modality across video, images, and audio.
	- The first of the true publicly accessible models trained without compromise for multi-modality.
	- Multi-lingual across 50 languages, supporting image input and output, real time video input, text to 3D.
	- Empathetic voice to voice with very low latency.
	- [Min Choi on X: "I used GPT-4o to create STL file for 3D model in ~ 20 seconds on my phone. Pretty remarkable what you can generate with AI and simple prompt now. https://t.co/2fbObrpPol" / X (twitter.com)](https://twitter.com/minchoi/status/1790396782200987662)
	- {{twitter https://twitter.com/minchoi/status/1790396782200987662}}
- # Google DeepMind Gemini
	- Gemini is a multimodal LLM capable of inputting and outputting text, understanding images, and generating images.
	- While specific architecture details are scarce, it represents a leap in LLMs interacting with multiple data types.
- ### Multi-Modal Large Language Models (LLMs)
	- **Introduction:**
		- [[Large language models]] are adept at generating coherent text sequences, predicting word probabilities and co-occurrences.
		- Multimodal models extend LLMs capabilities to not just output text, but images and understand multimodal inputs.
	- **Core Concepts:**
		- **LLMs for Text:**
			- LLMs process prompts and generate replies one token at a time, acting as a multiclass classifier.
		- **Image Generation:**
			- Traditional pixel-by-pixel image generation is intractable; hence, a different approach is needed.
			- The solution is treating image generation as a language generation problem, akin to ancient hieroglyphics.
	- **Techniques in Multi-Modal LLMs:**
		- **Autoencoders:**
			- Compress images into a lower-dimensional latent space and then regenerate them, learning crucial properties.
		- **[[Variational Autoencoders]] (VAE) & VQ-VAE:**
			- VAEs add a generative aspect by allowing for new image generation from random latent embeddings.
			- VQ-VAE further discretizes this process, creating a vocabulary of image "words" or tokens.
	- **Implementation:**
		- **Vector Quantization:**
			- Creates a discrete set of embedding vectors forming the vocabulary for our image-based language.
		- **Encoding and Decoding:**
			- Images are encoded to these discrete codes and decoded back to form new or reconstructed images.
	- **Training and Inference:**
		- A mixed sequence of embeddings (words and image tokens) is created for training.
		- The model learns to generate image tokens, forming a coherent sequence with the text, allowing for the generation of images corresponding to text descriptions.
	- **Challenges and Developments:**
		- The importance of quality data over quantity, especially for large, complex models.
		- Ongoing efforts focus on refining data quality, applying safety measures, and improving model transparency.
-
- ```mermaid
  flowchart LR
  A[Text Input] -->|Processed by LLM| B[Text Tokens]
  B -->|Alongside Image Tokens| D[Mixed Embeddings]
  C[Image Input] -->|Encoded via VQ-VAE| E[Image Tokens]
  E --> D
  D -->|Next Token Prediction| F[Generated Sequence]
  F -->|Decoded| G[Output Image & Text]
  ```
-
- Some random links
	- Apple Ferret is a [[Multimodal]] [[Large language models]] from [[Apple]] that can understand and ground anything at any granularity [apple/ml-ferret (github.com)](https://github.com/apple/ml-ferret)
	- [THUDM/CogVLM: a state-of-the-art-level open visual language model | å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡åž‹ (github.com)](https://github.com/THUDM/CogVLM)
	- [moondream
		- a tiny vision language model](https://moondream.ai/)
	- Key Papers
		- [Variational Autoencoder (VAE)](https://arxiv.org/abs/1312.6114)
		- [Vector Quantized Variational Autoencoder (VQ-VAE)](https://arxiv.org/abs/1711.00937)
		- [Vector Quantized Generative Adversarial Network (VQ-GAN)](https://compvis.github.io/taming-transformers/)
		- [Gemini](https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/rJRW6x8V4P0g/v0)
		- [Parti](https://sites.research.google/parti/)
		- [DallE](https://arxiv.org/pdf/2102.12092.pdf)
		- [2304.06939.pdf (arxiv.org)](https://arxiv.org/pdf/2304.06939.pdf) C4 model
		- [huggingface/OBELICS: Code used for the creation of OBELICS, an open, massive and curated collection of interleaved image-text web documents, containing 141M documents, 115B text tokens and 353M images. (github.com)](https://github.com/huggingface/OBELICS?tab=readme-ov-file)
		-
		-

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Multi Agent RAG scrapbook

public:: true

- Lit survey for [[PEOPLE]] [[David Tully]] [[MUST]] In here for now.
- {{video https://www.youtube.com/watch?v=LhWtpV-ZEeI}}
- [chat-Sure thing! Below, were going to methodically construct a series of diagrams as code using Mermaid syntax and detailed technical explanations. Well walk through the entire pipeline, from data ingestion to user .txt](../assets/chat-Sure_thing!_Below,_were_going_to_methodically_construct_a_series_of_diagrams_as_code_using_Mermaid_syntax_and_detailed_technical_explanations._Well_walk_through_the_entire_pipeline,_from_data_ingestion_to_user_1716930774582_0.txt)
- # Distilling Social Complexity: A Knowledge Graph and Ontology Approach for Immersive Environments
- ## Introduction
	- Capturing complex social dynamics in real-time immersive environments is a novel research area
	- Combines knowledge graphs, ontologies, and multi-modal Large Language Models (LLMs)
	- Aims to distil and bound complexity to constrain errors in deep search by naive multi-modal models
- ## Defining the Scope and Ontology
	- Identify the specific type of social interactions being modelled (e.g., professional events, casual gatherings, online communities)
	- Develop a formal ontology capturing core concepts:
		- Actors: Individuals, groups, organizations
		- Relationships: Friend, colleague, family, competitor, influencer
		- Interactions: Conversation, gesture, post, like, share
		- Context: Location, time, event, shared activities
		- Social Signals: Proximity, eye contact, tone of voice, facial expressions
	- Define properties and attributes to describe these concepts in detail
- ## Knowledge Graph Construction and Real-Time Updates
- ### Data Ingestion & Knowledge Extraction
  ```mermaid
  graph LR
      subgraph Data Ingestion & Knowledge Extraction
          direction LR
          subgraph A["User Data"]
              direction TB
              A1["Social Media"] --> A2["Parser (e.g., Beautiful Soup)"]
              A3["Event Registration"] --> A2
              A4["User-Provided Bios"] --> A2
          end
          subgraph B["Immersive Space Data"]
              direction TB
              B1["Location Tracking"] --> B2["Sensor Fusion (e.g., ROS)"]
              B2["Proximity Sensors"] --> B2
              B3["Wearable Biometrics"] --> B2
              B4["Audio/Video Feeds"] --> B5["Speech/Vision APIs (e.g., Google Cloud Vision, AssemblyAI)"]
          end
          A2 --> C["Knowledge Graph Database (e.g., Neo4j, TigerGraph)"]
          B2 --> C
          B5 --> D["Natural Language Processing (e.g., spaCy, Hugging Face Transformers)"]
          D --> C
          subgraph E["Ontology Engineering"]
              direction TB
              E1["Ontology Editor (e.g., ProtÃ©gÃ©, WebProtÃ©gÃ©)"] --> E2["Ontology (OWL/RDF)"]
              E2 --> C
          end
      end
  ```
- ### Knowledge Graph Construction Flow
  ```mermaid
  graph TB
      subgraph Knowledge Graph Construction
          direction TB
          A["Formal Ontology (OWL/RDF)"] --> B1["Entity Resolution"]
          B1 --> C["Graph Population"]
          subgraph Data Ingestion
              direction LR
              D[Social Media] -->|Beautiful Soup| B1
              E[Event Registration] -->|Custom Connectors| B1
              F[Immersive Data] -->|ROS| B1
          end
          C --> G["Graph Database (Neo4j, TigerGraph)"]
      end
      subgraph Real-Time Processing
          direction TB
          H[Sensor Fusion] --> I[Fusion Data]
          I --> J[Graph Updates]
          J --> G
      end
  ```
- ## Constrained Multi-Modal Retrieval Augmented Generation
- ### Retrieval Augmented Generation Flow
  ```mermaid
  graph LR
      subgraph Multi-Modal Retrieval Augmented Generation
          direction LR
          A[User/System Queries] --> B["Query Decomposition<br>(spaCy, Rasa)"]
          B --Ontology--> C[Ontology-Guided Search]
          B --Vectors--> D[Vector Search<br>(Pinecone, Weaviate)]
          C --> E[Relevant Knowledge Subgraph]
          D --> E
          E --> F["Constrained Response Generation<br>(GPT-3/4 with Prompt Engineering)"]
          F --> G["Response Validation<br>(Fact-Checking APIs, Rules)"]
          G --> H[User Interface<br>(Immersive Environment)]
      end
  ```
- ## Applications and Ethical Considerations
- ### Applications Overview
  ```mermaid
  graph TD
      A["Enhanced Social<br>Awareness"] -->|Insights| B[User Interaction]
      B --> C["Personalized<br>Recommendations"]
      A --> D["Social<br>Simulations"]
      subgraph Ethical Considerations
          E[Privacy and Consent]
          F[Bias Mitigation]
          G[Transparency]
          H[Security Measures]
          E & F & G & H --> I[Policy Compliance]
      end
      subgraph Applications
          I1["Networking<br>Events"] --> B
          I2["Social<br>Gatherings"] --> B
          I3["Online<br>Communities"] --> B
          I4["Virtual<br>Labs"] --> D
      end
  ```
- ### Ethical Design and Deployment
	- Establish clear guidelines for data collection, storage, and usage
	- Ensure user privacy and agency
	- Address potential biases in data sources, models, and algorithms
	- Promote fair and inclusive social environments
	- Make the system's reasoning and recommendations understandable to users
	- Foster trust and accountability
- ## Challenges and Research Directions
	- Efficiently process and integrate large-scale, heterogeneous data streams from the immersive environment
	- Accurately recognise and interpret subtle social cues from multi-modal data
		- Account for cultural differences and individual variations
	- Adapt the ontology over time to accommodate evolving social contexts and norms
	- Prioritize user well-being, privacy, and autonomy throughout the system's development and deployment
- ## Conclusion
	- Ambitious undertaking with profound implications
	- Combines knowledge graphs, ontologies, and constrained multi-modal LLMs
	- Creates truly immersive and insightful social experiences
	- Requires careful design, continuous refinement, and strong ethical foundations
	
	  The mermaid diagrams should render correctly inline, providing visual representations of the key components and their interactions within this metaverse ecosystem. The document maintains the technical detail, nuance, tool choices, and buildout advice from the original, while integrating the best aspects of the mermaid diagrams and restructuring the content into a clear narrative arc using Logseq markdown.

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable