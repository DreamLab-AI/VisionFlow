- ## Introduction
	- There may be an inflection point in the organisational topology of the internet, because of trust abuses by the incumbent providers. This moment has been calling itself Web3, but the moniker is fraught with problems, and somewhat meaningless. The drivers are real.
	- The word metaverse was coined by the author Neal Stephenson in his 1992 novel Snowcrash. It started popping up soon after in [news articles](https://www.newscientist.com/article/mg14819994-000-how-to-build-a-metaverse/) and research papers \[[158](https://arxiv.org/html/2207.09460v11/#bib.bibx158)\], but in the last five years it has been finding a new life within a silicon valley narrative. Perhaps in response to this Stephenson is now working with a company called [Lamina1](https://www.lamina1.com/), so perhaps we have been on the right track.
	- There were clear precursors to modern social VR, such as [VRML in the 1990's](https://www.howtogeek.com/778554/remembering-vrml-the-metaverse-of-1995/) which laid much of the groundwork for 3D content over networked computers.
	- It might seem that there would be a clear path from there to now, in terms of a metaverse increasingly meaning connected social virtual spaces, but this has not happened. Instead interest in metaverse as a concept waned, MMORG (described later) filled in the utility, and then recently an entirely new definition emerged. Park and Kim surveyed dozens of different historical interpretations of the word, and the generational reboot they describe makes it even less clear \[[159]. The concept of the Metaverse is extremely plastic at this time.
	- Perhaps is is closer to  'Cyberspace' as described by William Gibson in Neuromancer \[[160](https://arxiv.org/html/2207.09460v11/#bib.bibx160)\] ["A global domain within the information environment consisting of the interdependent network of information systems infrastructures including the Internet, telecommunications networks, computer systems, and embedded processors and controllers."]
	- `The Metaverse' is coming, in some form, at some point. Everyone is positioning in case it's "soon". It's not at all clear what it is, or if people want it, but the best of the emergent narrative looks like the older field of "digital society" and that obviously should not be dismissed lightly.
	- Park and Kim identify the generational inflection point which has led to the resurgence of the concept of Metaverse \[[159](https://arxiv.org/html/2207.09460v11/#bib.bibx159)\]: ["Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offine selves are not different."]
	- Large scale `social' & immersive metaverse is suffering poor adoption, failing as it has in the past. It's likely that the market need has been overstated. More advanced and popular (closed) games based solutions do not serve societal or business needs.
	- Brett Leonard, writer director of Lawnmower Man talks about the pressing need to get out in front of moral questions in the development of metaverse applications. He stressed that wellbeing will be a crucial underpinning of the technology because of the inherent intimacy of immersion in virtual spaces. He suggests that emotional engagement with storied characters is needed to satisfy the human need for narrative, and that this should be utopian by design to stave off the worst of dystopian emergent characteristics of the technology.
	- Adoption and convergence requires low friction communication and economic activity, within groups, at a global scale. Cryptography and distributed software can assist us with globally 'true' persistence of digital data, so we will look to integrate this with our social XR. This focus on persistence, value, and trust means it's most appropriate to focus on business uses as there is more opportunity for value creation which will be important to bootstrap this technology.
	- I think that with sufficiently informed guiding constraints in place, and smaller group sizes (ie, not a large scale social metaverse), that there is a path forward.
	- This "visual capitalist" hosted Gartner article cuts to the core of the issue. As always with such things, the numbers are likely guessed by someone with strong priors, but Gartners aren't terrible at this stuff. The best thing about it is the extent to which it shows that the metaverse doesn't meaningfully exist, even after ALL the hype. Metaverse is basically [[Roblox]] at this time. Hyundai having built an isolated 'metaverse' being a headline item is terribly exposing.
		- <iframe src="https://www.visualcapitalist.com/sp/life-in-the-metaverse/" style="width: 100%; height: 600px"></iframe>
- ![Figure 7.2: Elon Musk agrees with this on Twitter. It's notable that Musk is now Twitters' [biggest shareholder](https://twitter.com/paraga/status/1511320953598357505), and has been vocal about web censorship on the platform.](../assets/muskWeb3.png)
-
- ## [](https://github.com/flossverse#current-contenders-in-the-metaverse) Current contenders in the Metaverse
	- The closest contenders at this time are [[Roblox]] for social and play, VRChat for more serious users, and Nvidia [[Omniverse]] for high end business to business metaverse.
	- From a business perspective metaverse is the worst of the remote collaboration tool-kits, and undermines flow, productivity, and interpersonal trust. This hasn't stopped Apple, Microsoft, and Meta's heavy investment in their business technologies and marketing. Metaverse is probably technology for technologies sake at this time.
	- With that said Hennig-Thurau says the following in a [LinkedIn post](https://www.linkedin.com/feed/update/urn:li:activity:7020679507141361664/):
		- [Our research finds that the performance of social interactions in the VR metaverse varies for different outcomes and settings, with productivity and creativity being on par with Zoom (not higher, but also not lower) for the two experimental settings in which we studied these constructs. Thus, as of today, meeting in VR does not overcome all the limitations that we are facing when using Zoom or Teams. But most importantly (to us), we find clear evidence that when people get together in the metaverse via VR, it creates SUBSTANTIALLY higher levels of social presence among group members across ALL FIVE STUDY CONTEXTS, from idea generation to joint movie going. This is the main insight from our study and the stuff we believe future uses of social virtual reality can (and should) build on. We also explain that the effectiveness of VR meetings can be further increased, and also how this can be done (by selecting the most appropriate settings, people, avatars, hardware, environments etc.).] \[[161]
- ## [](https://github.com/flossverse#digital-society) Digital Society
	- Digital society may be a more tangible and less hyped term to build around, and extends out into the more compelling spatial and augmented reality technologies, web, and digital money and trust.
	- Emerging markets, less developed nations, indeed much of the world is excluded from many of the tools that are taken for granted in `Western' digital society. They do not necessarily have the identification, banking rails, or compute power to engage fully. Our focus is on Africa and India.
- ## [](https://github.com/flossverse#crypto-and-blockchain) Crypto and Blockchain
	- Uniting these attempts, with portable (transferable) "goods" across digital society possibly requires a global ledger (blockchain), indeed this is the basis of the Web3 interpretation. Crypto has undergone it's own boom and bust cycle on this topic, and is still seeing adoption both inside out outside of the metaverse context. There are other potential options based around simpler PKI infrastructure.
	- Representations of dollars and pounds can ride securely on top of such networks as stablecoins, and this is getting easier to integrate, though there are risks. This has the potential to open up global collaborative working practices, inclusive of emerging markets.
	- Crypto is a nightmare; rife with scams, poor technology choices, limited life, and incorrect assumptions. The only thing blockchain / crypto can do well is "money like networks", which is a cornerstone of human interaction, and the killer application. We believe that Bitcoin is the signal, and crypto is the noise, but even this is a risky proposition.
- ## [](https://github.com/flossverse#open-digital-society) Open Digital Society
	- Industry has noted the risk, and failures of Meta across both metaverse, and digital currency, and have latched onto "open metaverse" as a narrative, to de-risk their interest. The current open metaverse is muddy and confused.
	- A truth seems to have been missed; that open metaverse should mean open source metaverse. There are some options, but they are under developed. We would like to contribute to this by applying our decades of telecollaboration research.
	- The UK seems to be endorsing significant controls and restrictions on internet usage including metaverse applications. This compliance overhead will price small companies out of large scale social experiences. Company walled gardens are less impacted (as per the slack service model), and this is an opportunity if tied to real business use cases.
- ## [](https://github.com/flossverse#augmented-intelligence) Augmented Intelligence
	- AI & machine learning and especially generative art is further blurring these boundaries. A better term for AI/ML is supported creativity and/or augmented intelligence. While current models such as GPT3.5 and LAION based generative systems are already causing a global stir, and large language models are already forcing global debate about general AI.
	- Trust, accessibility, governance, and safeguarding, are hard problems, and made more complex by unrecorded social flow in immersive social VR. The challenge is to build a topologically flat, inclusive, permissionless, federated, and open metaverse, with economically empowered ML and AI actors, which can mediate governance issues, transparently, according to well constructed custom schemas, between cryptographically verifiable economic users (human or AI).
	- New open source [supported creativity, augmented intelligence] tooling from Stability and Llama potentially removes many of the problems with accessibility, creativity, language barriers, safeguarding, and governance. This is a huge, complex, and fast moving area, but tremendously exciting. Using new image generation ML it may be possible to build new kind of collaborative global networks for creative industries, ideating in simplistic immersive spaces while instantly creating scenes which can be stylised using verbal commands in real-time. This may open up and enfranchise fresh ideas from a wider cultural pool.
	- Such teams could be far more ad-hoc by experimenting with the designs outlined in this book. This kind of genuine digital society use case is something sorely lacking in large scale attempts such as Meta Horizons. It need not be complex or large scale, but it must be secure, trusted, and task appropriate. We think we can deliver this and conversations with the industry suggest that there is excent and cautious appetite.
- ## Toward an open metaverse
	- The push toward open standards is being joined (somewhat late) by credible and established bodies [like the IEEE](https://spectrum.ieee.org/laying-foundation-for-extended-reality). It's such a fast moving and under explored set of problems that this movement toward standards will take a long time to even find it's feet. Hopefully it's clear to the reader that this kind of development guides the work here. In the wider "real-time social VR" various companies have attempted to build closed ecosystems, for years. These now look more like attempts at digital society, but are closer to isolated metaverses, or more usefully isolated digital ecosystems. This is still happening.
	- There's every chance that the Apple Vision Pro will remain closed, as this tends to be their business model. Theo Priestly, CEO at Metanomics [points out](https://www.linkedin.com/feed/update/urn:li:activity:6977366421034967040/) that Chinese Giant Tencent are doing similar, and he cited this image of building a closed but tightly linked suite of businesses into something that looks like a metaverse.
	- The levels of investment which are being hung under the metaverse moniker [are mind blowing](https://www.scmp.com/tech/policy/article/3194092/chinas-iphone-production-hub-henan-bets-its-future-metaverse), but that is not what we want to discuss as an end point for this book.
	- ![Figure 7.1: [McCormick attempts to guess the Tencent metaverse](https://www.notboring.co/p/tencents-dreams)](../assets/tencent.png)
	- For our purposes in this product design the interface between the previous chapter (NFTs) and this metaverse chapter is crucial. Punk6529 is a pseudonymous twitter account and thought leader in the "crypto" space. The text below encapsulates much of the reasoning that led to this book and product exploration, and is paraphrased [from this thread](https://twitter.com/punk6529/status/1536046831045685248) for our purposes.
		- Bit by bit, the visualization layer of the internet will get better until it is unrecognisably better (+/10 years). As the visualization layer of the internet gets better, digital objects will become more useful and more important. Avatars (2D and 3D), art, schoolwork, work work, 3D virtual spaces and hundreds of other things. Not only will the objects themselves become more important, they will lead to different emergent behaviours. We see this already with avatars and mixed eponymous/pseudonymous/anonymous communities. Yes, it is the internet plumbing underneath, but just like social media changed human behaviour on the internet, metaverse type experiences will further change it. NFT Twitter + Discord + various virtual worlds is a form of early metaverse. I feel like I am entering a different world here, not just some websites. The most important question for the health of the internet/metaverse/human society in the 2030s will be decided now. And that question is: "who stores the definitive ownership records of those digital objects". There are two answers: a company's database OR a blockchain. If we end up with "a company's database" we will end up with all the web dysfunctions, but worse. SMTP is an open protocol that anyone can use so we don't have societal level fights on "who is allowed to use email". Short messaging online ended up becoming Twitter. So we end up having the most absurd, surreal discussions on the topic of "who is allowed to use short-messaging" being dependant on "who is the CEO of Twitter". There is no way this is the correct architecture for our progressively more digital economy.... If this is your first time around here, we are fighting for an open metaverse."
	- It seems that industry shares much of this opinion regarding an open metaverse. The proposal of a persistent interactive digital universe online is [so] vast that major players recognise that they will not be able to monopolise this space, though Facebook/Meta are clearly attempting to. The [Metaverse Standards Forum](https://metaverse-standards.org/news/press-releases/leading-standards-organizations-and-companies-unite-to-drive-open-metaverse-interoperability/) is clearly an attempt by the other industry players to catch up and then get out ahead of Meta in this regard. It's also possible to view this as just another land grab, but through the vehicle of a standards body. Time will tell. They say:
		- "Announced today, The Metaverse Standards Forum brings together leading standards organizations and companies for industry-wide cooperation on interoperability standards needed to build the open metaverse. The Forum will explore where the lack of interoperability is holding back metaverse deployment and how the work of Standards Developing Organizations (SDOs) defining and evolving needed standards may be coordinated and accelerated. Open to any organization at no cost, the Forum will focus on pragmatic, action-based projects such as implementation prototyping, hackathons, plugfests, and open-source tooling to accelerate the testing and adoption of metaverse standards, while also developing consistent terminology and deployment guidelines."
- {{embed ((65cbd03c-03a4-4ca3-8e9d-984692e29412))}}
- There is a lot of work for the creative and technical industries to do to integrate human narrative creativity this nascent metaverse, and it’s not even completely
  clear that this is possible, or even what people want.
-
- Collaborative mixed reality---------------------------
- ### Toward an open metaverse
- The Openstand principles are a great starting place for what an openmetaverse might mean. [Theyare](https://open-stand.org/about-us/principles/):
-
	- Cooperation: Respectful cooperation between standards organizations, whereby each respects the autonomy, integrity, processes, and intellectual property rules of the others.
-
	- Adherence to Principles: Adherence to the five fundamental principles of standards development:
-
	- Due process. Decisions are made with equity and fairness among        participants. No one party dominates or guides standards        development. Standards processes are transparent and        opportunities exist to appeal decisions. Processes for periodic        standards review and updating are well defined.
-
	- Broad consensus. Processes allow for all views to be considered        and addressed, such that agreement can be found across a range        of interests.
-
	- Transparency. Standards organizations provide advance public        notice of proposed standards development activities, the scope        of work to be undertaken, and conditions for participation.        Easily accessible records of decisions and the materials used in        reaching those decisions are provided. Public comment periods        are provided before final standards approval and adoption.
-
	- Balance. Standards activities are not exclusively dominated by        any particular person, company or interest group.
-
	- Openness. Standards processes are open to all interested and        informed parties.
-
	- Collective Empowerment: Commitment by affirming standards organizations and their participants to collective empowerment by striving for standards that:
-
	- are chosen and defined based on technical merit, as judged by        the contributed expertise of each participant;
-
	- provide global interoperability, scalability, stability, and        resiliency;
-
	- enable global competition;
-
	- serve as building blocks for further innovation;
-
	- contribute to the creation of global communities, benefiting        humanity.
-
	- Availability: Standards specifications are made accessible to all for implementation and deployment. Affirming standards organizations have defined procedures to develop specifications that can be implemented under fair terms. Given market diversity, fair terms may vary from royalty-free to fair, reasonable, and non-discriminatory terms (FRAND).
-
	- Voluntary Adoption: Standards are voluntarily adopted and success is determined by the market.
- The push toward open standards is being joined (somewhat late) bycredible and established bodies [like theIEEE](https://spectrum.ieee.org/laying-foundation-for-extended-reality).It’s such a fast moving and under explored set of problems that thismovement toward standards will take a long time to even find it’s feet.Hopefully it’s clear to the reader that this kind of development guidesthe work here. In the wider “real-time social VR” various companies haveattempted to build closed ecosystems, for years. These now look morelike attempts at digital society, but are closer to isolated metaverses,or more usefully isolated digital ecosystems. This is still happening.There’s every chance that when Apple make their augmented reality playthis year or next they will keep their system closed off as this tendsto be their business model. Theo Priestly, CEO at Metanomics [pointsout](https://www.linkedin.com/feed/update/urn:li:activity:6977366421034967040/)that Chinese Giant Tencent are doing similar, and he cited Figure<a href="#fig:tencent" data-reference-type="ref" data-reference="fig:tencent">7.1</a>;building a closed but tightly linked suite of businesses into somethingthat looks like a metaverse. The levels of investment which are beinghung under the metaverse moniker [are mindblowing](https://www.scmp.com/tech/policy/article/3194092/chinas-iphone-production-hub-henan-bets-its-future-metaverse),but that is not what we want to discuss as an end point for this book.
  ![]./assets/e63f7f380108db361a71cb5cb9351e68f7c23a21.png
  <a href="https://www.notboring.co/p/tencents-dreams">McCormick attempts to guess the Tencent metaverse</a>
- For our purposes in this product design the interface between theprevious chapter (NFTs) and this metaverse chapter is crucial. Punk6529is a pseudonymous twitter account and thought leader in the “crypto”space. The text below encapsulates much of the reasoning that led tothis book and product exploration, and is paraphrased [from thisthread](https://twitter.com/punk6529/status/1536046831045685248) for ourpurposes.
- itBit by bit, the visualization layer of the internet will get betteruntil it is unrecognisably better (+/- 10 years). As the visualizationlayer of the internet gets better, digital objects will become moreuseful and more important. Avatars (2D and 3D), art, schoolwork, workwork, 3D virtual spaces and hundreds of other things. Not only will theobjects themselves become more important, they will lead to differentemergent behaviours. We see this already with avatars and mixedeponymous/pseudonymous/anonymous communities. Yes, it is the internetplumbing underneath, but just like social media changed human behaviouron the internet, metaverse type experiences will further change it. NFTTwitter + Discord + various virtual worlds is a form of early metaverse.I feel like I am entering a different world here, not just somewebsites. The most important question for the health of theinternet/metaverse/human society in the 2030s will be decided now. Andthat question is: "who stores the definitive ownership records of thosedigital objects". There are two answers: a company’s database OR ablockchain. If we end up with "a company’s database" we will end up withall the web dysfunctions, but worse. SMTP is an open protocol thatanyone can use so we don’t have societal level fights on "who is allowedto use email". Short messaging online ended up becoming Twitter. So weend up having the most absurd, surreal discussions on the topic of "whois allowed to use short-messaging" being dependant on "who is the CEO ofTwitter". There is no way this is the correct architecture for ourprogressively more digital economy.... If this is your first time aroundhere, we are fighting for an open metaverse.”
- It seems that industry shares much of this opinion regarding an openmetaverse. The proposal of a persistent interactive digital universeonline is **so** vast that major players recognise that they will not beable to monopolise this space, though Facebook/Meta are clearlyattempting to. The [Metaverse StandardsForum](https://metaverse-standards.org/news/press-releases/leading-standards-organizations-and-companies-unite-to-drive-open-metaverse-interoperability/)is clearly an attempt by the other industry players to catch up and thenget out ahead of Meta in this regard. It’s also possible to view this asjust another land grab, but through the vehicle of a standards body.Time will tell. They say:
- it“Announced today, The Metaverse Standards Forum brings togetherleading standards organizations and companies for industry-widecooperation on interoperability standards needed to build the openmetaverse. The Forum will explore where the lack of interoperability isholding back metaverse deployment and how the work of StandardsDeveloping Organizations (SDOs) defining and evolving needed standardsmay be coordinated and accelerated. Open to any organization at no cost,the Forum will focus on pragmatic, action-based projects such asimplementation prototyping, hackathons, plugfests, and open-sourcetooling to accelerate the testing and adoption of metaverse standards,while also developing consistent terminology and deployment guidelines.”
- This looks like it will be a useful project and community for thepurposes outlined in this book, but the technology is young enough (inthat it doesn’t really exist) for multiple approaches to be trailed.
- Europe is making metaverse a priority with [The Virtual and AugmentedReality IndustrialCoalition](https://digital-strategy.ec.europa.eu/en/policies/virtual-and-augmented-reality-coalition).President von der Leyen’s State of the Union [letter of intentsays](https://state-of-the-union.ec.europa.eu/system/files/2022-09/SOTEU_2022_Letter_of_Intent_EN_0.pdf):“We will continue looking at new digital opportunities and trends, suchas the metaverse.”
- #### Primitives
- OpenAI identified the following 5 points about metaverse, in response tothe query “What are 5 key points I should know when studying metaverse?”
-
	- Metaverse is a virtual reality platform that allows users to interact with each other and with digital objects in a virtual space.
-
	- Metaverse is a decentralized platform, meaning that there is no central authority or server that controls the platform.
-
	- Metaverse is an open platform, meaning that anyone can develop applications for the platform.
-
	- Metaverse is a secure platform, meaning that all data and transactions are encrypted and secure.
-
	- Metaverse is a scalable platform, meaning that it can support a large number of users and a large number of transactions.
- This is an unexpectedly great answer, probably the cleanest we havefound. The [Metaverse Standard Forum](https://metaverse-standards.org/)highlights the following, which reads like the output from a brainstormbetween academia and industry stakeholders.
-
	- collaborative spatial computing
-
	- interactive 3D graphics
-
	- augmented and virtual reality
-
	- photorealistic content authoring
-
	- geospatial systems
-
	- end-user content tooling
-
	- digital twins
-
	- real-time collaboration
-
	- physical simulation
-
	- online economies
-
	- multi-user gaming
-
	- new levels of scale and immersiveness.
- It’s not a useless list by any means, but it lacks the kind of productfocus we need for detailed exploration of value and trust transfer.
- Mystakidis identifies the following:[[mystakidis2022metaverse]]
-
	- Principles
-
	- Interoperable
-
	- Open
-
	- Hardware agnostic
-
	- Network
-
	- Technologies
-
	- Virtual reality
-
	- Augmented reality
-
	- Mixed reality
-
	- Affordances
-
	- Immersive
-
	- Embodiment
-
	- Presence
-
	- Identity construction
-
	- Challenges
-
	- Physical well-being
-
	- Psychology
-
	- Ethics
-
	- Privacy
- This is quite an academic list. A lot of these words will be explored inthe next section which is more of an academic literature review.
- Nevelsteen attempted to identify key elements for a ‘virtual work’ in2018 and these are relevant now, and described rigorously in theappendix of his paper:[[nevelsteen2018virtual]]
-
	- Shared Temporality, meaning that the distributed users of the virtual world share the same frame of time.
-
	- Real time which he defines as “not turn based”.
-
	- Shared Spatiality, which he says can include an ‘allegory’ of a space, as in text adventures. It seems this might extend to a spoken interface to a mixed reality metaverse.
-
	- ONE Shard is a description of the WLAN network architecture, and conforms to servers in a connected open metaverse.
-
	- Many human agents simply means that more than one person can be represented in the virtual world and corresponds to ‘social’ in our description.
-
	- Many Software Agents corresponds to AI actors in our descriptions. Non playing characters would be the gaming equivalent.
-
	- Virtual Interaction pertains to any ability of a user to interact actively with the persistent virtual scene, and is pretty much a given these days.
-
	- Nonpausable isn’t even a word, but is pretty self explanatory.
-
	- Persistence means that if human participants leave then the data of the virtual world continues. This applies to the scenes, the data representing actions, and objects and actors in the worlds.
-
	- Avatar is interesting as it might seem that having avatar representations of connected human participants is a given. In fact the shared spaces employed by Nvidia for digital engineering do not.
- Turning to industry; John Riccitiello, CEO of Unity Technologies saysthat metaverse is it
- “The next generation of the internet that is:
-
	- always real-time
-
	- mostly 3D
-
	- mostly interactive
-
	- mostly social
-
	- mostly persistent”
- Expanding this slightly we will us the following primitives of what wethink are important for a metaverse:
-
	- Fusing of digital and real life
-
	- Social first
-
	- Real time interactive 3d graphics first
-
	- Persistent
-
	- Supports ownership
-
	- Supports user generated content[[ondrejka2004escaping]]
-
	- Open and extensible
-
	- Low friction economic actors and actions
-
	- Trusted / secure
-
	- Convergence of film and games
-
	- Blurring of IP boundaries
-
	- Blurring of narrative flow
-
	- Multimodal and hardware agnostic
-
	- Mobile first experiences
-
	- Safeguarding, and governance
- There is a **lot** of work for the creative and technical industries todo to integrate human narrative creativity this nascent metaverse, andit’s not even completely clear that this is possible, or even whatpeople want.
- ### History
- The word metaverse was coined by the author Neal Stephenson in his 1992novel Snowcrash. It started popping up soon after in [newsarticles](https://www.newscientist.com/article/mg14819994-000-how-to-build-a-metaverse/)and research papers,[[mclellan1993avatars]] but in the last five years ithas been finding a new life within a silicon valley narrative. Perhapsin response to this Stephenson is now working with a company called[Lamina1](https://www.lamina1.com/) which actually looks a lot like therest of this book, so perhaps we have been on the right track.
- There were clear precursors to modern social VR, such as [VRML in the1990’s](https://www.howtogeek.com/778554/remembering-vrml-the-metaverse-of-1995/)which laid much of the groundwork for 3D content over networkedcomputers.
- It might seem that there would be a clear path from there to now, interms of a metaverse increasingly meaning connected social virtualspaces, but this has not happened. Instead interest in metaverse as aconcept waned, MMORG (described later) filled in the utility, and thenrecently an entirely new definition emerged. Park and Kim surveyeddozens of different historical interpretations of the word, and thegenerational reboot they describe makes it even lessclear.[[park2022metaverse]] The concept of the Metaverse is extremelyplastic at this time (Figure<a href="#fig:muskWeb3" data-reference-type="ref" data-reference="fig:muskWeb3">7.2</a>).
- It’s arguable that what will be expanding in this chapter is moreappropriately ‘Cyberspace’ as described by William Gibson inNeuromancer[[gibson2019neuromancer]] it“A global domain within theinformation environment consisting of the interdependent network ofinformation systems infrastructures including the Internet,telecommunications networks, computer systems, and embedded processorsand controllers.”
- Park and Kim identify the generational inflection point which has led tothe resurgence of the concept of Metaverse:[[park2022metaverse]]it“Unlike previous studies on the Metaverse based on Second Life, thecurrent Metaverse is based on the social value of Generation Z thatonline and offine selves are not different.”
- Brett Leonard, writer director of Lawnmower Man talks about the pressingneed to get out in front of moral questions in the development ofmetaverse applications. He stressed that wellbeing will be a crucialunderpinning of the technology because of the inherent intimacy ofimmersion in virtual spaces. He suggests that emotional engagement withstoried characters is needed to satisfy the human need for narrative,and that this should be utopian by design to stave off the worst ofdystopian emergent characteristics of the technology.
- The book will aim to build toward an understanding of metaverse as auseful social mixed reality, that allows low friction communication andeconomic activity, within groups, at a global scale. Cryptography anddistributed software can assist us with globally ‘true’ persistence ofdigital data, so we will look to integrate this with our social XR. Thisfocus on persistence, value, and trust means it’s most appropriate tofocus on business uses as there is more opportunity for value creationwhich will be important to bootstrap this technology.
- Elsewhere in the book we state that metaverse is the worst of thetele-collaboration tool-kits, and in general we ‘believe’ this to betrue at this time. With that said Hennig-Thurau says the following in a[LinkedInpost](https://www.linkedin.com/feed/update/urn:li:activity:7020679507141361664/):itOur research finds that the performance of social interactions in theVR metaverse varies for different outcomes and settings, withproductivity and creativity being on par with Zoom (not higher, but alsonot lower) for the two experimental settings in which we studied theseconstructs. Thus, as of today, meeting in VR does not overcome all thelimitations that we are facing when using Zoom or Teams. But mostimportantly (to us), we find clear evidence that when people gettogether in the metaverse via VR, it creates SUBSTANTIALLY higher levelsof social presence among group members across ALL FIVE STUDY CONTEXTS,from idea generation to joint movie going. This is the main insight fromour study and the stuff we believe future uses of social virtual realitycan (and should) build on. We also explain that the effectiveness of VRmeetings can be further increased, and also how this can be done (byselecting the most appropriate settings, people, avatars, hardware,environments etc.).[[hennig2022social]]
- We agree that with sufficiently informed guiding constraints in place,and smaller group sizes (ie, not a large scale social metaverse), thatthere is a path forward.
- This chapter will first attempt to frame the context for telepresence(the academic term for communicating through technology), and thenexplain the increasingly polarised options for metaverse. It’s useful toprecisely identify the primitives of the product we would like to seehere, so this chapter is far more a review of academic literature in thefield, culminating in a proposed framework.
  ![]./assets/05c60abfdb5138796c3e168be7f5b9653d60edbb.png
  Elon Musk agrees with this on Twitter. It’s notable that Musk is now Twitters’ <a href="https://twitter.com/paraga/status/1511320953598357505">biggest shareholder</a>, and has been vocal about web censorship on the platform.
- ### Video conferencing, the status quo
- This section has been adapted and updated for open source release, fromthe authors PhD thesis, with the permission of the University ofSalford.
- Video-conferencing has become more popular as technology improves, as itgets better integrated with ubiquitous cloud business support suites,and as a function of the global pandemic and changing work patterns.There is obviously increasing demands for real-time communication acrossgreater distances.
- The full effects of video-conferencing on human communication are stillbeing explored, as seen in the experimental [“TogetherMode”](https://news.microsoft.com/innovation-stories/microsoft-teams-together-mode/)within Microsoft Teams. Video-conferencing is presumed to be a somewhatricher form of communication than email and telephone, but not quite asinformative as face-to-face communication.
- In this section we look at the influence of eye contact on communicationand how video-conferencing mediates both verbal and non-verbalinteractions. Facilitation of eye contact is a challenge that must beaddressed so that video-conferencing can approach the rich interactionsof face-to-face communication. This is an even bigger problem in theemerging metaverse systems, so it’s important that we examine thehistory and trajectory.
- There is a tension emerging for companies who do not necessarily need toemploy remote meeting technology, but also cannot afford to ignore thecompetitive advantages that such systems bring. In an experimentpreformed well before the 2020 global pandemic at CTrip, Bloom et aldescribe how home working led to a 13% performance increase, of whichabout 9% was from working more minutes per shift (fewer breaks andsick-days) and 4% from more calls per minute (attributed to a quieterworking environment).[[Bloom2015]] Home workers also reported improvedwork satisfaction and experienced less turnover, but their promotionrate conditional on performance fell. This speaks to a lack ofmanagement capability with such systemic change. It’s clearly a complexand still barely understood change within business and management.
- Due to the success of the experiment, CTrip rolled-out the option towork from home to the whole company, and allowed the experimentalemployees to re-select between the home or office. Interestingly, overhalf of them switched, which led to the gains almost doubling to 22%.This highlights the benefits of learning and selection effects whenadopting modern management practices like working from home.Increasingly this is becoming a choice issue for prospective employees,and an advantage for hiring managers to be able to offer it.
- More recent research by Barrero, Bloom and Davies found that workingfrom home is likely to be “sticky.”[[barrero2021working]] They found:
-
	- better-than-expected WFH experiences,
-
	- new investments in physical and human capital that enable WFH,
-
	- greatly diminished stigma associated with WFH,
-
	- lingering concerns about crowds and contagion risks,
-
	- a pandemic-driven surge in technological innovations that support WFH.
- More recently Enterprise Collaboration Systems (ECS) provide richdocument management, sharing, and collaboration functionality across anorganisation. The enterprise ECS system may integrate collaborativevideo.[[prakash2020characteristic]] This is for instance the case withMicrosoft Teams / Sharepoint. This integration of ECS should beconsidered when thinking about social VR systems which wish to supportbusiness, value, and trust. It is very much the case that largetechnology providers are attempting to integrate their ‘business backend’ systems into their emerging metaverse systems. Open sourceequivalents are currently lacking.
- #### Pandemic drives adoption
- The ongoing global COVID-19 pandemic is [changing how peoplework](https://blog.yelp.com/news/the-future-of-work-is-remote/), towarda new global ‘normal’. Some ways of working are overdue transformation,and will be naturally disrupted. In the UK at least it seems that theremay be real appetite to shift away from old practises. This upheavalwill inevitably present both challenges and opportunities.
- Highly technical workforces, especially, can [operate fromanywhere](https://globalworkplaceanalytics.com/telecommuting-statistics).The post pandemic world seems to have stronger national border controls,with a resultant shortage of highly technical staff. This has forced thehand of global business toward [internationally distributedteams](https://www.lifeatspotify.com/being-here/work-from-anywhere).
- If only a small percentage of companies allow the option of remoteworking, then they gain a structural advantage, enjoying benefits ofreduced travel, lower workplace infection risk across all disease, andglobal agility for the personnel. Building and estate costs willcertainly be reduced. More diversity may be possible. Issues such assexual harassment and bullying may be reduced. With reduced overheadsproduct quality may increase. If customers are happier with theirservices, then over time this ‘push’ may mean an enormous shift awayfrom centralised working practises toward distributed working.
- Technologies which support this working style were still in theirinfancy at the beginning of the pandemic. The rush to ‘Zoom’, apreviously relatively unknown and insecure[[aiken2020zooming]] webmeeting product, shows how naive businesses were in this space.
- Connection of multiple users is now far better supported, with Zoom and[MircosoftTeams](https://www.microsoft.com/en-us/Investor/earnings/FY-2021-Q1/press-release-webcast)alone supporting hundreds of millions of chats a day. This is a 20xincrease on market leader Skype’s 2013 figure of [280million](https://www.microsoft.com/en-us/Investor/earnings/FY-2013-Q1/press-release-webcast)connections per month. Such technologies extend traditional telephony toprovide important multi sensory cues. However, these technologiesdemonstrate shortfalls compared to a live face-to-face meeting, which isgenerally agreed to be optimal for human-human interaction.[[Wolff2008]]
- ## KPMG CEO Survey Highlights
- [KPMG](https://fortune.com/company/kpmg/) surveyed U.S. CEOs from companies with over $500 million in revenue, finding that only [one-third anticipate a full office return](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2024/KPMG-2024-US-CEO-Outlook-Pulse-Survey.pdf) in the next three years.
- A drastic shift from last year's outlook where 62% believed remote work would end by 2026, as detailed in this [Fortune article](https://fortune.com/2023/10/05/ceo-rto-kpmg-research-return-to-work-full-time-5-days/).
- The trend towards hybrid work models is growing, with nearly half of the CEOs supporting this flexible arrangement, up from 34% last year. ## Corporate Resistance and Work Model Evolution
- Deutsche Bank and [Amazon](https://fortune.com/company/amazon-com/) faced significant resistance to their return-to-office mandates, with Deutsche Bank staff voicing displeasure internally, and 30,000 Amazon employees [signing a petition](https://www.businessinsider.com/amazons-hr-boss-rejects-employee-remote-work-petition-2023-3) against the in-office requirements.
- Research indicates that nearly half of companies enforcing office returns observed increased employee attrition, supporting a shift towards hybrid models as a middle ground solution.
- ## Future Workforce Dynamics
- Insights from Amrit Sandhar of &Evolve and Lewis Maleh of Bentley Lewis highlight the ongoing transition to more flexible work arrangements. Maleh notes a significant rise in job postings for remote or hybrid roles, underscoring a broader acceptance of flexibility in work environments as crucial for attracting and retaining talent.
- While the research community and business are learning how to adaptworking practises to web based telepresence,[[oeppen2020human]] thereremains little technology support for ad-hoc serendipitous meetingsbetween small groups. It’s possible that Metaverse applications can helpto fill this gap, by gamification of social spaces, but the underdiscussed problems with video conferencing are likely to be even worsein such systems.
- Chris Herd of “FirstBase” (who admittedly have a bias) provides somefascinating speculations:
- it
- “I’ve spoken to 2,000+ companies with 40M+ employees about remote workin the last 12 months A few predictions of what will happen before 2030:
-
	- Rural Living: World-class people will move to smaller cities, have a lower cost of living & higher quality of life.
-
	- These regions must innovate quickly to attract that wealth. Better schools, faster internet connections are a must.
-
	- Async Work: Offices are instantaneous gratification distraction factories where synchronous work makes it impossible to get stuff done.
-
	- Tools that enable asynchronous work are the most important thing globally remote teams need. A lot of startups will try to tackle this.
-
	- Hobbie Renaissance: Remote working will lead to a rise in people participating in hobbies and activities which link them to people in their local community.
-
	- This will lead to deeper, more meaningful relationships which overcome societal issues of loneliness and isolation.
-
	- Diversity & Inclusion: The most diverse and inclusive teams in history will emerge rapidly Companies who embrace it have a first-mover advantage to attract great talent globally. Companies who don’t will lose their best people to their biggest competitors.
-
	- Output Focus: Time will be replaced as the main KPI for judging performance by productivity and output.
-
	- Great workers will be the ones who deliver what they promise consistently
-
	- Advancement decisions will be decided by capability rather than who you drink beer with after work.
-
	- Private Equity: The hottest trend of the next decade for private equity will see them purchase companies, make them remote-first The cost saving in real-estate at scale will be eye-watering. The productivity gains will be the final nail in the coffin for the office Working Too Much: Companies worry that the workers won’t work enough when operating remotely.
-
	- The opposite will be true and become a big problem.
-
	- Remote workers burning out because they work too much will have to be addressed.
-
	- Remote Retreats: Purpose-built destinations that allow for entire companies to fly into a campus for a synchronous week.
-
	- Likely staffed with facilitators and educators who train staff on how to maximise effectiveness.
-
	- Life-Work Balance: The rise of remote will lead to people re-prioritizing what is important to them.
-
	- Organizing your work around your life will be the first noticeable switch. People realising they are more than their job will lead to deeper purpose in other areas.
-
	- Bullshit Tasks: The need to pad out your 8 hour day will evaporate, replaced by clear tasks and responsibilities.
-
	- Workers will do what needs to be done rather than wasting their trying to look busy with the rest of the office
- ”
- #### Point to Point Video Conferencing
- O’Malley et al. showed that face-to-face and video mediated employedvisual cues for mutual understanding, and that addition of video to theaudio channel aided confidence and mutual understanding. However, videomediated did not provide the clear cues of beingco-located.[[OMalley1996]]
- Dourish et al. make a case for not using face-to-face as a baseline forcomparison, but rather that analysis of the efficacy of remotetele-collaboration tools should be made in a wider context of connectedmultimedia tools and ‘emergent communicative practises.’[[Dourish1996]]While this is an interesting viewpoint it does not necessarily map wellto a recreation of the ad-hoc meeting.
- There is established literature on human sensitivity to eye contact inboth 2D and 3D VC,[[Criminisi2003; @Van_Eijk2010]] with an acceptedminimum of 5-10 degrees before observers can reliably sense they are notbeing looked at.[[Chen2002]] Roberts et al. suggested that at the limitof social gaze distance ( 4m) the maximum angular separation betweenpeople standing shoulder to shoulder in the real world would be around 4degrees[[Roberts2013]].
- Sellen found limited impact on turn passing when adding a visual channelto audio between two people when using Hydra, an early system whichprovided multiple video conference displays in an intuitive spatialdistribution[[Sellen1992]]. She did however, find that the design of thevideo system affected the ability to hold multi-partyconversations.[[Sellen1995]]
- Monk and Gale describe in detail experiments which they used forexamining gaze awareness in communication which is mediated andunmediated by technology. They found that gaze awareness increasedmessage understanding.[[Monk2002]]
- Both Kuster et al. and Gemmel et al. have successfuly demonstratedsoftware systems which can adjust eye gaze to correct for off axiscapture in real time video systems[[Gemmell2000; @Kuster2012]].
- Shahid et al. conducted a study on pairs of children playing games withand without video mediation and concluded that the availability ofmutual gaze affordance enriched social presence and fun, while itsabsence dramatically affects the quality of the interaction. They usedthe ‘Networked Minds’, a social presence questionnaire.
- #### Triadic and Small Group
- Early enthusiasm in the 1970’s for video conferencing, as a medium forsmall group interaction quickly turned to disillusionment. It was agreedafter a flurry of initial research that the systems at the time offeredno particular advantage over audio only communication, and atconsiderable cost.[[Williams1977]]
- Something in the breakdown of normal visual cues seems to impact theability of the technology to support flowing group interaction.Nonetheless, some non-verbal communication is supported in VC withlimited success.
- Additional screens and cameras can partially overcome the limitation ofno multi-party support (that of addressing a room full of people on asingle screen) by making available more bidirectional channels. Forinstance, every remote user can be a head on a screen with acorresponding camera. The positioning of the screens must thennecessarily match the physical organization of the remote room.
- Egido provides an early review of the failure of VC for group activity,with the “misrepresentation of the technology as a substitute forface-to-face" still being valid today.[[Edigo1988]]
- Commercial systems such as Cisco Telepresence Rooms cluster theircameras above the centre screen of three for meetings using theirtelecollaboration product, while admitting that this only works well forthe central seat of the three screens. They also group multiple peopleon a single screen in what Workhoven et al. dub a “non-isotropic"configuration.[[Pejsa2016]] They maintain that this is a suitable tradeoff as the focus of the meeting is more generally toward the importantcontributor in the central seat. This does not necessarily follow forless formal meeting paradigms.
- In small groups, it is more difficult to align non-verbal cues betweenall parties, and at the same time, it is more important because thehand-offs between parties are more numerous and important in groups. Abreakdown in conversational flow in such circumstances is harder tosolve. A perception of the next person to talk must be resolved for allparties and agreed upon to some extent.
- However, most of the conventional single camera, and expensive multicamera VC systems, suffer a fundamental limitation in that the offsetbetween the camera sight lines and the lines of actual sight introduceincongruities that the brain must compensate for.[[Wolff2008]]
- #### Other Systems to Support Business
- There have been many attempts to support group working and rich datasharing between dispersed groups in a business setting. So called ’smartspaces’ allow interaction with different displays for differentactivities and add in some ability to communicate with remote or evenmobile collaborators on shared documents,[[Bardram2012]] with additionalchallenges for multi-disciplinary groups who are perhaps less familiarwith one or more of the technology barriers involved.[[Adamczyk2007]]
- Early systems like clearboard[[Ishii1993]] demonstrated the potential forsmart whiteboards with a webcam component for peer-to-peer collaborativeworking. Indeed it is possible to support this modality with Skype and asmartboard system (and up to deployments such as Accessgrid). Theyremain relatively unpopular however.
- #### Mona Lisa Type Effects
- Almost all traditional group video meeting tools suffer from theso-called Mona Lisa effect which describes the phenomenon where theapparent gaze of a portrait or 2 dimensional image always appears tolook at the observer regardless of the observer’sposition.[[Vishwanath2005; @Anstis1969; @Wollaston1824]] This situationmanifests when the painted or imaged subject is looking into the cameraor at the eyes of the painter.[[Loomis2008; @Fullwood2006]]
- Single user-to-user systems based around bidirectional video implicitlyalign the user’s gaze by constraining the camera to roughly the samelocation as the display. When viewed away from this ideal axis, itcreates the feeling of being looked at regardless of where this observeris,[[Moubayed2012; @Vishwanath2005; @Anstis1969; @Wollaston1824]] or the“collapsed view effect”[[Nguyen2005]] where perception of gazetransmitted from a 2 dimensional image or video is dependent on theincidence of originating gaze to the transmission medium.
- Multiple individuals using one such channel can feel as if they arebeing looked at simultaneously, leading to a breakdown in the normalnon-verbal communication which mediates turn passing.[[Vertegaal2002]]There is research investigating this sensitivity when the gaze ismediated by a technology, finding that “disparity between the opticalaxis of the camera and the looking direction of a looker should be atmost 1.2 degrees in the horizontal direction, and 1.7 degrees invertical direction to support eye contact".[[Van_Eijk2010; @Bock2008]] Itseems that humans assume that they are being looked at unless they aresure that they are not.[[Chen2002]]
- To be clear, there are technological solutions to this problem, but it’suseful in the context of discussing metaverse to know that this problemexists. It’s known that there are cognitive dissonances around panes ofvideo conference images, but it seems that the effect is truely limitedto 2D surfaces. A 3D projection surface (a physical model of a human)designed to address this problem completely removed the Mona Lisaeffect.[[Moubayed2012]]
- Metaverse then perhaps offers the promise of solving this, making morenatural interaction possible, but it’s clearly a long way fromdelivering on those promises right now. We need to understand what’simportant and try to map these into a metaverse product.
- ### What’s important for human communication
- #### Vocal
- The ubiquitous technology to mediate conversation is, of course, thetelephone. The [2021 Ericsson mobilityreport](https://www.ericsson.com/en/reports-and-papers/mobility-report/reports/november-2021)states that there are around 8 billion mobile subscriptions globally.More people have access to mobile phones than to working toilets[according toUNICEF](https://www.unicef.org/innovation/stories/more-cellphones-toilets).
- Joupii and Pan designed a system which focused attention on spatiallycorrect high definition audio. They found “significant improvement overtraditional audio conferencing technology, primarily due to theincreased dynamic range and directionality..[[Jouppi2002]] Aoki et al.also describe an audio only system with support for spatialcues.[[Aoki2003]]
- In the following sections we will attempt to rigorously identify justwhat is important for our proposed application of business centriccommunication, supportive of trust, and thereby value transfer.
- In his book ‘Bodily Communication’[[Argyle1988]] Michael Argyle dividesvocal signals into the following categories:
- 1.  Verbal
- 2.  Non-Verbal Vocalisations
- 1.  Linked to Speech
- 1.  Prosodic
- 2.  Synchronising
- 3.  Speech Disturbances
- 2.  Independent of Speech
- 1.  Emotional Noises
- 2.  Paralinguistic (emotion and interpersonal attitudes)
- 3.  Personal voice and quality of accent
- Additional to the semantic content of verbal communication there is arich layer of meaning in pauses, gaps, and overlaps[[Heldner2010]] whichhelp to mediate who is speaking and who is listening in multi-partyconversation. This mediation of turn passing, to facilitate flow, is byno means a given and is highly dependent on context and otherfactors.[[Kleinke1986]] Interruptions are also a major factor in turnpassing.
- This extra-verbal content[[Ting-Toomey2012]] extends into physical cues,so-called ‘nonverbal’ cues, and there are utterances which link theverbal and non-verbal.[[Otsuka2005]] This will be discussed later, but toan extent, it is impossible to discuss verbal communication withoutregard to the implicit support which exists around the words themselves.
- In the context of all technology-mediated conversation the extra-verbalis easily compromised if technology used to support communication over adistance does not convey the information, or conveys it badly. This canintroduce additional complexity.[[Otsuka2005]]
- These support structures are pretty much lacking in metaverse XRsystems. The goal then here perhaps is to examine the state-of-the-art,and remove as many of the known barriers as possible. Such a processmight better support trust, which might better support the kind ofeconomic and activity we seek to engineer.
- When examining just verbal / audio communication technology it can beassumed that the physical non-verbal cues are lost, though notnecessarily unused. In the absence of non-verbal cues it falls to timelyvocal signals to take up the slack when framing and organising the turnpassing. For the synchronising of vocal signals between the parties tobe effective the systemic delays must remain small. System latency, theinherent delays added by the communication technology, can allow slipsor a complete breakdown of ’flow’.[[katagiri2007aiduti]] This problem canbe felt in current social VR platforms, though people don’t necessarilyidentify the cause of the breakdown correctly. In the main they feel tothe users like a bad “audio-only” teleconference.
- With that said, the transmission of verbal / audio remains the mostcritical element for interpersonal communication as the most essentialmeaning is encoded semantically. There is a debate about ratios of howmuch information is conveyed through the various humanchannels,[[Loomis2012]] but it is reasonable to infer from its ubiquitythat support for audio is essential for meaningful communication over adistance. We have seen that it must be timely, to prevent a breakdown offraming, and preferably have sufficient fidelity to convey sub-vocalutterances.
- For social immersive VR for business users, a real-time network such aswebsockets, RTP, or UDP seems essential, much better microphones areimportant, and the system should support both angular spatialisation,and respond to distance between interlocutors.
- #### Nonverbal
- We have already seen that verbal exchanges take place in a wider contextof sub vocal and physical cues. In addition, the spatial relationshipbetween the parties, their focus of attention, their gestures andactions, and the wider context of their environment all play a part incommunication.[[Goodwin2000]] These are identified as follows by Gilliesand Slater[[Gillies2005]] in their paper on virtual agents.
-
	- Posture and gesture
-
	- Facial expression
-
	- Gaze
-
	- Proxemics
-
	- Head position and orientation
-
	- Interactional synchrony
- This is clearly important for our proposed collaborative mixed realityapplication. Below we will examine these six areas by looking across thewider available research.
- ##### Gaze
- Of particular importance is judgement of eye gaze which is normallyfast, accurate and automatic, operating at multiple levels of cognitionthrough multiplecues.[[Argyle1988; @Argyle1976; @Argyle1965; @Argyle1976; @Argyle1969; @Kendon1967; @Monk2002]]
- Gaze in particular aids smooth turn passing[[Hedge1978]][[Novick1996]] andlack of support for eye gaze has been found to decrease the efficiencyof turn passing by 25%.[[Vertegaal2000]]
- There are clear patterns to eye gaze in groups, with the person talking,or being talked to, probably also being lookedat[[Vertegaal2001]][[Langton2000]]. To facilitate this groups will tend toposition themselves to maximally enable observation of the gaze of theother parties.[[Kendon1967]] This intersects with proxemics which will bediscussed shortly. In general people look most when they are listening,with short glances of 3-10 seconds.[[Argyle1965]] Colburn et al. suggestthat gaze direction and the perception of the gaze of others directlyimpacts social cognition[[Colburn2000]] and this has been supported in afollow up study.[[Macrae2002]]
- The importance of gaze is clearly so significant in evolutionary termsthat human acuity for eye direction is considered high at  30 secarc[[Symons2004]] with straight binocular gaze judged more accuratelythan straight monocular gaze,[[Kluttz2009]] when using stereo vision.
- Regarding the judgement of the gaze of others, Symons et al. suggestedthat “people are remarkably sensitive to shifts in a person’s eye gaze”in triadic conversation.[[Symons2004]] This perception of the gaze ofothers operates at a low level and is automatic. Langton et al. citeresearch stating that the gaze of others is “able to trigger reflexiveshifts of an observer’s visual attention” and further discuss the deepbiological underpinnings of gaze processing.[[Langton2000]]
- When discussing technology-mediated systems, Vertegaal & Ding suggestedthat understanding the effects of gaze on triadic conversation is“crucial for the design of teleconferencing systems and collaborativevirtual environments,”[[Vertegaal2002]] and further found correlationbetween the amount of gaze, and amount of speech. Vertegaal & Slagtersuggest that “gaze function(s) as an indicator of conversationalattention in multiparty conversations.”[[Vertegaal2001]] It seems like iswe are to have useful markets within social immersive environments thensupport for natural gaze effects should be a priority.
- Wilson et al. found that subjects can “discriminate gaze focused onadjacent faces up to \[3.5m\].”[[Wilson2000]] This perhaps gives us atestable benchmark within a metaverse application which is eye gazeenabled. In this regard Schrammel et al. investigated to what extentembodied agents can elicit the same responses in eye gazedetection.[[Schrammel2007]]
- Vertegaal et al. found that task performace was 46% better when gaze wassynchronised in their telepresence scenario. As they point out, gazesynchonisation (temporal and spatial) is ‘commendable’ in all such groupsituations, but the precise utility will depend upon thetask.[[Vertegaal2002]]
- There has been some success in the automatic detection of the focus ofattention of participants in multi partymeetings.[[Stiefelhagen2001; @Stiefelhagen2002]] More recently, eyetracking technologies allow the recording and replaying of accurate eyegaze information[[Steptoe2009]] alongside information about pupildilation toward determination of honesty and socialpresence.[[Steptoe2010]] It seems there are trust and honesty issuesconflated with how collaborants in a virtual space are represented.
- In summary, gaze awareness does not just mediate verbal communicationbut rather is a complex channel of communication in its own right.Importantly, gaze has a controlling impact on those who are involved inthe communication at any one time, including and excluding even beyondthe current participants. Perhaps the systems we propose in this bookneed to demand eye gaze support, but it is clear that it should berecommended, and that the software selected should support thetechnology integration in principle.
- ##### Mutual Gaze
- Aygyle and Cook established early work around gaze and mutual gaze, withtheir seminal book of the same title,[[Argyle1976]] additionallydetailing confounding factors around limitations and inaccuracies inobservance of gaze and how this varies withdistance.[[Argyle1969; @Argyle1988; @Cook1977]]
- Mutual gaze is considered to be the most sophisticated form of gazeawareness with significant impact on dyadic conversationespecially.[[Cook1977; @Kleinke1986; @Fagel2010]] The effects seem moreprofound than just helping to mediate flow and attention, with mutualeye gaze aiding in memory recall and the formation ofimpressions.[[Bohannon2013]]
- While reconnection of mutual eye gaze through a technology boundary doesnot seem completely necessary it is potentially important, with impacton subtle elements of one-to-one communication, and thereforediscrimination of eye gaze direction should be bi-directional ifpossible, and if possible have sufficient accuracy to judge direct eyecontact. In their review Bohannon et al. said that the issue ofrejoining eye contact must be addressed in order to fully realise therichness of simulating face-to-face encounters.[[Bohannon2013]]
- Mutual gaze is a challenging affordance as bi-directional connection ofgaze is not a trivial problem. It’s perhaps best to view this as at the‘edge’ of our requirements for a metaverse.
- ##### Mutual Gaze in Telepresence
- We have seen that transmission of attention can broadly impactcommunication in subtle ways, impacting empathy, trust, cognition, andco-working patterns. Mutual gaze (looking into one another’s eyes), iscurrently the high water mark for technology-mediated conversation.
- Many attempts have been made to re-unite mutual eye gaze when usingtele-conferencing systems. In their 2015 review of approachesRegenbrecht and Langlotz found that none of the methods they examinedwere completely ideal.[[Regenbrecht2015]] They found most promise in 2Dand 3D interpolation techniques, which will be discussed in detaillater, but they opined that such systems were very much ongoing researchand lacked sufficient optimisation.
- A popular approach uses the so called ’Peppers Ghost’phenomenon,[[Steinmeyer2013]] where a semi silvered mirror presents animage to the eye of the observer, but allows a camera to view throughfrom behind the angled mirror surface. The earliest example of this isRosental’s two way television system in 1947,[[Rosenthal1947]] thoughBuxton et al. ‘Reciprocal Video Tunnel’ from 1992 is more oftencited.[[Buxton1992]] This optical characteristic isn’t supported byretroreflective projection technology, and besides requires carefulcontrol of light levels either side of the semi-silvered surface.
- The early GAZE-2 system (which makes use of Pepper’s ghost) is novel inthat it uses an eye tracker to select the correct camera from severaltrained on the remote user. This ensures that the correct returned gaze(within the ability of the system) is returned to the correct user onthe other end of the network.[[Vertegaal2003]] Mutual gaze capability islater highlighted as an affordance supported or unsupported by keyresearch and commercial systems.
- ##### Head Orientation
- Orientation of the head (judged by the breaking of bilateral symmetryand alignment of nose) is a key factor when judging attention.Perception of head orientation can be judged to within a couple ofdegrees.[[Wilson2000]]
- It has been established that head gaze can be detected all the way outto the extremis of peripheral vision, with accurate eye gaze assessmentonly achievable in central vision.[[Loomis2008]] This is less of use forour metaverses at this time, because user field of view is almost alwaysrestricted in such systems. More usefully, features of illumination canalter the apparent orientation of the head.[[Troje1998]]
- Head motion over head orientation is a more nuanced propostion and canbe considered a micro gesture.[[Boker2011]] Head tracking systems withinhead mounted displays can certainly detect these tiny movements, butit’s clear that not all of this resolution is passed into shared virtualsettings through avatars. It would be beneficial to be able to fine tunethis feature within any software selected.
- It is possible that 3D displays are better suited to perception of headgaze since it is suggested that they are more suitable for “shapeunderstanding tasks”[[St_John2001]]
- Bailenson, Baell, and Blascovich found that giving avatars rendered headmovements in a shared virtual environment decreased the amount oftalking, possibly as the extra channel of head gaze was opened up. Theyalso reported that subjectively, communication wasenhanced.[[Bailenson2002]]
- Clearly head orientation is an important indicator of the direction ofattention of members of a group and can be discerned even in peripheralvision. This allows the focus of several parties to be followedsimultaneously and is an important affordance to replicate on anymulti-party communication system.
- ##### Combined Head and Eye Gaze
- Rienks et al. found that head orientation alone does not provide areliable cue for identification of the speaker in a multipartysetting.[[Rienks2010]] Stiefelhagen & Zhu found “that head orientationcontributes 68.9% to the overall gaze direction onaverage,”[[Stiefelhagen2002]] though head and eye gaze seem to be judgedinterdependently.[[Kluttz2009]] Langton noted that head and eye gaze are“mutually influential in the analysis of socialattention,”[[Langton2000]] and it is clear that transmission of ‘headgaze’ by any mediating system, enhances rather than replaces timelydetection of subtle cues. Combined head and eye gaze give the best ofboth worlds and extend the lateral field of view in which attention canbe reliably conveyed to others.[[Loomis2008]]
- ##### Other Upper Body: Overview
- While it is well evidenced that there are advantages to accurateconnection of the gaze between conversationalpartners,[[Argyle1969; @Kleinke1986]] there is also a body of evidencethat physical communication channels extend beyond theface[[Kleinke1986; @Nguyen2009]] and include both micro (shrugs, handsand arms), and macro movement of the upper body.[[Ekman1993]]Goldin-Meadow suggests that gesturing aids conversational flow byresolving mismatches and aiding cognition.[[Goldin-Meadow1999]]
- In their technology-mediated experiment which compared face to upperbody and face on a flat screen, Nguyen and Canny found that “upper-bodyframing improves empathy measures and gives results not significantlydifferent from face-to-face under several empathymeasures.”[[Nguyen2009]]
- The upper body can be broken up as follows:
- bfFacial  Much emotional context can be described by facial expression (display)alone,[[Ekman1993; @Chovil1991]] with smooth transition betweenexpressions seemingly important.[[schiano2004]] This suggests thatmediating technologies should support high temporal resolution, or atleast that there is a minimum resolution between which transitionsbetween expressions become too ’categorical’. Some aspects ofconversational flow appear to be mediated in part by facialexpression.[[ohba1998]] There are gender differences in the perception offacial affect.[[Hofmann2006]]
- bfGesturing  (such as pointing at objects) paves the way for more complex channels ofhuman communication and is a basic and ubiquitous channel.[[Iverson2005]]Conversational hand gestures provide a powerful additional augmentationto verbal content.[[Krauss1996]]
- bfPosture  Some emotions can be conveyed through upper body configurations alone.Argyle details some of these[[Argyle1988]] and makes reference to theposture of the body and the arrangement of the arms (i.e. folded acrossthe chest). These are clearly important cues. Kleinsmith andBianchi-Berthouze assert that "some affective expressions may be bettercommunicated by the body than the face".[[Kleinsmith2013]]
- bfBody Torque  In multi-party conversation, body torque, that is the rotation of thetrunk from front facing, can convey aspects of attention andfocus.[[Schegloff1998]]
- In summary, visual cues which manifest on the upper body and face canconvey meaning, mediate conversation, direct attention, and augmentverbal utterances.
- ##### Effect of Shared Objects on Gaze
- Ou et al. detail shared task eye gaze behaviour “in which helpers seekvisual evidence for workers’ understanding when they lack confidence ofthat understanding, either from a shared, or commonvocabulary.”[[Ou2005]]
- Murray et al. found that in virtual environments, eye gaze is crucialfor discerning what a subject is looking at.[[Murray2009]] This work isshown in Figure<a href="#fig:murrayeyegaze" data-reference-type="ref" data-reference="fig:murrayeyegaze">7.3</a>.
- It is established that conversation around a shared object or task,especially a complex one, mitigates gaze between parties[[Argyle1976]]and this suggests that in some situations around shared tasks inmetaverses it may be appropriate to reduce fidelity of representation ofthe avatars.
- <span class="image">Eye tracked eye gaze awareness in VR. Murray et al.used immersive and semi immersive systems alongside eye trackers toexamine the ability of two avatars to detect the gaze awareness of asimilarly immersed collaborator.</span>
- ##### Tabletop and Shared Task
- In early telepresence research Buxton and William argued throughexamples that “effective telepresence depends on quality sharing of bothperson and task space.[[Buxton1992]]
- In their triadic shared virtual workspace Tang et al. found difficultyin reading shared text using a ‘round the table’ configuration, a markedpreference for working collaboratively on the same side of the table.They also found additional confusion as to the identity of remoteparticipants.[[Tang2010]] Tse et al. found that pairs can work well overa shared digital tabletop, successfully overcoming a single userinterface to interleave tasks.[[Tse2007]]
- Tang et al. demonstrate that collaborators engage and disengage around agroup activity through several distinct, recognizable mechanisms withunique characteristics.[[Tang2006]] They state that tabletop interfacesshould offer a variety of tools to facilitate this fluidity.
- Camblend is a shared workspace with panoramic high resolution video. Itmaintains some spatial cues between locations by keeping a shared objectin the video feeds.[[Norris2013; @Norris2012]] Participants successfullyresolved co-orientation within the system.
- The t-room system implemented by Luff et al. surrounds co-locatedparticipants standing at a shared digital table with life sized body andhead video representations of remote collaborators[[Luff2011]] but foundthat there were incongruities in the spatial and temporal matchingbetween the collaborators which broke the flow of conversation.Tuddenham et al. found that co-located collaborators naturally devolved’territory’ of working when sharing a task space, and that this did nothappen the same way with a tele-present collaborator.[[Tuddenham2009]]Instead remote collaboration adapted to use a patchwork of ownership ofa shared task. It seems obvious to say that task ownership is a functionof working space, but it is interesting that the research found nomeasurable difference in performance when the patchwork coping strategywas employed.
- The nature of a shared collaborative task and/or interface directlyimpacts the style of interaction between collaborators. This will have abearing on the choice of task forexperimentation.[[Jamil2011; @Jetter2011]]
- ### Psychology of Technology-Mediated Interaction
- #### Proxemics
- Proxemics is the formal study of the regions of interpersonal spacebegun in the late 50’s by Hall and Sommers and building toward TheHidden Dimension,[[Hall1969]] which details bands of space (Figure<a href="#fig:proxemics" data-reference-type="ref" data-reference="fig:proxemics">7.4</a>)that are implicitly and instinctively created by humans and which have adirect bearing on communication.
  ![]./assets/93e5e31635612a45cbb73bc9a52a19c538eeea0c.png
  Bands of social space around a person Image CC0 <a href="https://en.wikipedia.org/wiki/Proxemics">from wikipedia</a>.
- Distance between conversational partners, and affiliation, also have abearing on the level of eye contact[[Argyle1965]] with a natural distanceequilibrium being established and developed throughout, through both eyecontact and a variety of subtle factors. Argyle & Ingham provide levelsof expected gaze and mutual gaze against distance.[[Argyle1969]] Theseboundaries are altered by ethnicity[[Watson1966; @Argyle1988]] andsomewhat by gender,[[Bruno2013]] and age.[[Slessor2008; @Hofmann2006]]
- Even with significant abstraction by communication systems (such asSecondLife) social norms around personal spacepersist.[[Yee2007; @Bailenson2001; @Bailenson2003]] Bailenson &Blascovich found that even in Immersive Collaborative VirtualEnvironments (ICVE’s) “participants respected personal space of thehumanoid representation”[[Bailenson2001]] implying that this is a deeplyheld ’low-level’ psychophysical reaction.[[Blascovich2002]] The degree towhich this applies to non-humanoid avatars seems under explored.
- Maeda et al.[[Maeda2004]] found that seating position impacts the levelof engagement in teleconferencing. Taken together with the potential forreconfiguration within the group as well as perhaps signalling for theattention of participants outside of the confines of the group in anopen business metaverse setting.
- When considering the attention of engaging with people outside theconfines of a meeting Hager et al. found that gross expressions can beresolved by humans over long distances.[[Hager1979; @Argyle1988]] Itseems that social interaction begins around 7.5m in the so-called‘public space.’[[Hall1969]] Recreating this affordance in a metaversewould be a function of the display resolution, and seems another‘stretch goal’ rather than a core requirement.
- #### Attention
- The study of attention is a discrete branch of psychology. It is thestudy of cognitive selection toward a subjective or objective sub focus,to the relative exclusion of other stimulae. It has been defined as “arange of neural operations that selectively enhance processing ofinformation.”[[Carlston2013]] In the context of interpersonalcommunication it can be refined to apply to selectively favouring aconversational agent or object or task above other stimuli in thecontextual frame.
- Humans can readily determine the focus of attention of others in theirspace[[Stiefelhagen2001]] and preservation of the spatial cues whichsupport this are important for technology-mediatedcommunication[[Sellen1992]][[Stiefelhagen2002]].
- The interplay between conversational partners, especially the reciprocalperception of attention, is dubbed the perceptualcrossing.[[Deckers2013; @Gibson1963]]
- This is a complex field of study with gender, age, and ethnicity allimpacting the behaviour of interpersonalattention.[[Bente1998; @Slessor2008; @Argyle1988; @Hofmann2006; @Pan2008]]Vertegaal has done a great deal of work on awareness and attention intechnology-mediated situations and the work of his group is citedthroughout this chapter.[[Vertegaal1997]] As an example it is still sucha challenge to “get” attention through mediated channels ofcommunication, that some research[[Fels2000; @Sellen1992]] and manycommercial systems such as ‘blackboard collaborate’, Zoom, and Teams usetell tale signals (such as a microphone icon) to indicate when aparticipant is actively contributing. Some are automatic, but many arestill manual, requiring that a user effectively hold up a virtual handto signal their wish to communicate.
- Langton et al. cite research stating that the gaze of others is “able totrigger reflexive shifts of an observer’s visual attention”.
- Regarding the attention of others, Fagal et el demonstrated that eyevisibility impacts collaborative task performance when considering ashared task.[[Fagel2010]] Novick et al. performed analysis on taskhand-off gaze patterns which is useful for extension into shared taskproduct design.[[Novick1996]]
- #### Behaviour
- Hedge et al. suggested that gaze interactions between strangers andfriends may be different which could have an impact on the kinds ofinteractions a metaverse might best support.[[Hedge1978]] Voida et al.elaborate that prior relationships can cause “internal fault lines” ingroup working.[[Voida2012]] When new relationships are formed the“primary concern is one of uncertainty reduction or increasingpredictability about the behaviour of both themselves and others in theinteraction.”[[Berger1975]] This concept of smoothness in theconversation is a recurring theme, with better engineered systemsintroducing less extraneous artefacts into the communication, and sodisturbing the flow less. Immersive metaverse are rife with artefacts.
- In a similar vein the actor-observer effect describes the mismatchbetween expectations which can creep into conversation. Conversationsmediated by technology can be especially prone to diverging perceptionsof the causes of behaviour.[[Jones1971]] Basically this meansmisunderstandings happen, and are harder to resolve with more mediatingtechnology.
- Interacting subjects progress conversation through so-called‘perception-action’ loops which are open to predictive modelling throughdiscrete hidden Markov models.[[Mihoub2015]] This might allow product OKRtesting of the effectiveness of engineered systems.[[doerr2018measure]]
- It may be that the perception-behaviour link where unconscious mirroringof posture bolsters empathy between conversational partners, especiallywhen working collaboratively,[[Chartrand1999]] and the extent to whichposture is represented through a communication medium may be important.
- Landsberger posited the Hawthorne effect.[[Parsons1974]] Put simply thisis a short term increase in productivity that may occur as a result ofbeing watched or appreciated. The impression of being watched changesgaze patterns during experimentation, with even implied observationthrough an eye tracker modifying behaviour.[[Risko2011]]
- There are also some fascinating findings around the neural correlates ofgratitude, which turn out not to be linked to gratitude felt by aparticipant, but rather the observation of gratitude received within asocial context.[[fox2015neural]] These findings have potentially usefulimplications for the behaviours of AI actors and avatars within animmersive social scene.
- There is much historic work describing “the anatomy ofcooperation",[[Kollock1998]] and this might better inform how educationalor instructional tasks are built in metaverse applications.
- Cuddihy and Walters defined an early model for assessing desktopinteraction mechanisms for social virtual environments.[[Cuddihy2000]]
- ##### Perception Of Honesty
- Hancock et al. state that we are most likely to lie, and to be lied to,on the telephone.[[Hancock2004]] Technology used for communicationimpacts interpersonal honesty. It seems that at some level humans knowthis; lack of eye contact leads to feelings of deception, impactingtrust.[[Holm2010]] This has a major impact on immersive social XR, whichoften does not support mutual gaze. Trust is crucial for businessinteractions.
- Further there are universal expressions, micro-expressions, and blinkrate which can betray hidden emotions,[[Porter2008]] though the effectsare subtle and there is a general lack of awareness by humans of theirabilities in this regard.[[Holm2010]] Absence of support for suchinstinctive cues inhibits trust.[[Roberts2015]] Support for these rapidand transient facial features demands high resolution reproduction inboth resolution and time domains. There is detectable difference in aparticipant’s ability to detect deception when between video conferencemediated communication and that mediated by avatars.[[Steptoe2010]]Systems should aim for maximally faithful reproduction.
- #### Presence, Co-presence, and Social Presence
- Presence is a heavily cited historic indicator of engagement in virtualreality, though the precise meaning has been interpreted differently bydifferent specialisms.[[Beck2011; @Schuemie2001]] It is generally agreedto be the ’sense of being’ in a virtual environment.[[Slater1999]] Slaterextends this to include the “extent to which the VE becomes dominant".
- Beck et al. reviewed 108 articles and synthesised an ontology ofpresence[[Beck2011]] which at its simplest is as follows:
- 1.  Sentient presence
- 1.  Physical interaction
- 2.  Mental interaction
- 2.  Non-sentient
- 1.  Physical immersion
- 2.  Mental immersion = psychological state
- When presence is applied to interaction it may be split intoTelepresence, and Co/Social presence.[[Heeter1992; @Biocca1997]]Co-presence and/or social presence is the sense of “being there withanother", and describes the automatic responses to complex socialcues.[[Fulk1987; @Haythornthwaite1995]] Social presence (and co-presence)refers in this research context to social presence which is mediated bytechnology (even extending to text based chat[[Gunawardena1997]]), andhas its foundations in psychological mechanisms which engender mutualismin the ‘real’. This is analysed in depth by Nowak.[[Nowak2001]] Anexamination of telepresence, co-presence and social presence necessarilyrevisits some of the knowledge already elaborated.
- The boundaries between the three are blurred in research withconflicting results presented.[[Bulu2012]] Biocca et al. attempted toenumerate the different levels and interpretations surrounding thesevague words,[[Biocca2003]] and to distill them into a more robust theorywhich better lends itself to measurement. They suggest a solidunderstanding of the surrounding psychological requirements which needsupport in a mediated setting, and then a scope that is detailed andlimited to the mediated situation.
- Since ‘social presence’ has been subject to varieddefinitions[[Biocca2003]] it is useful here to consider a singledefinition from the literature which defines it as “the ability ofparticipants in the community of inquiry to project their personalcharacteristics into the community, thereby presenting themselves to theother participants as real people..”[[Garrison1999; @Beck2011]] Similarlyto specifically define co-presence for this research it is taken to bethe degree to which participants in a virtual environment are“accesible, available, and subject to one another".[[Biocca2003]]
- Social presence has received much attention and there are establishedquestionnaires used in the field for measurement of the levels ofperceived social presence yet the definitions here also remain broad,with some confusion about what is being measured.[[Biocca2003]]
- Telepresence meanwhile is interaction with a different (usually remote)environment which may or may not be virtual, and may or may not containa separate social/co-presence component.
- Even in simple videoconferencing Bondareva and Bouwhuis stated (as partof an experimental design) that the following determinants are importantto create social presence.[[Bondareva2004; @Jouppi2002]]
- 1.  Direct eye contact is preserved
- 2.  Wide visual field
- 3.  Both remote participants appear life size
- 4.  Possibility for participants to see the upper body of the interlocutor
- 5.  High quality image and correct colour reproduction
- 6.  Audio with high S/N ratio
- 7.  Directional sound field
- 8.  Minimization of the video and audio signal asynchrony
- 9.  Availability of a shared working space.
- Bondareva et al. went on to describe a person-to-person telepresencesystem with a semi-silvered mirror to reconnect eye gaze, which theyclaimed increased social presence indicators. Interestingly they chose achecklist of interpersonal interactions which they used againstrecordings of conversations through the system.[[Bondareva2004]]
- The idea of social presence as an indicator of the efficacy of thesystem, suggests the use of social presence questionnaires in theevaluation of the system.[[Biocca2003]] Subjective questionnaires arehowever troublesome in measuring effectiveness of virtual agents andembodiments, with even nonsensical questions producing seemingly validresults.[[Slater2004]] Usoh et al. found that ’the real’ produced onlymarginally higher presence results than the virtual.[[Usoh2000]] It wouldbe difficult to test products this way.
- Nowak states that “A satisfactory level of co-presence with another mindcan be achieved with conscious awareness that the interaction ismediated" and asserts that while the mediation may influence the degreeof co-presence it is not a prohibiting factor.[[Nowak2001]]
- Baren and IJsselsteijn[[Van_Baren2004; @Harms2004]] list 20 usefulpresence questionnaires in 2004 of which “Networked Minds" seemed mostappropriate for the research. Hauber et al. employed the “NetworkedMinds" Social Presence questionnaire experimentally and found that whilethe measure could successfully discriminate between triadic conversationthat is mediated or unmediated by technology, it could not find adifference between 2D and 3D mediatedinterfaces.[[Hauber2005; @Gunawardena1997]]
- In summary, social presence and co-presence are important historicmeasures of the efficacy of a communication system. Use of the term inliterature peaked between 1999 and 2006 according to Google’s ngramviewer and has been slowly falling off since. The questionnairemethodology has been challenged in recent research and while moreobjective measurement may be appropriate, the networked minds questionsseem to be able to differentiate real from virtualinteractions.[[Harms2004]]
- ### Other Systems to Support Business
- There have been many attempts to support group working and rich datasharing between dispersed groups in a business setting. So called ’smartspaces’ allow interaction with different displays for differentactivities and add in some ability to communicate with remote or evenmobile collaborators on shared documents,[[Bardram2012]] with additionalchallenges for multi-disciplinary groups who are perhaps less familiarwith one or more of the technology barriers involved.[[Adamczyk2007]]
- Early systems like clearboard[[Ishii1993]] demonstrated the potential forsmart whiteboards with a webcam component for peer to peer collaborativeworking. Indeed it is possible to support this modality with Skype and asmartboard system (and up to deployments such as Accessgrid). Theyremain relatively unpopular however.
- Displays need not be limited to 2 dimensional screens and can beenhanced in various ways.
- Stereoscopy allows an illusion of depth to be added to a 2D image byexploiting the stereo depth processing characteristics of the humanvision system. This technical approach is not perfect as it does notfully recreate the convergence and focus expected by the eyes and brain.
- There are multiple approaches to separating the left and right eyeimages, these primarily being active (where a signal selectively blanksthe input to left then right eyes in synchronicity with the display),passive, where either selective spectrum or selective polarisation oflight allow different portions of a display access to different eyes, orphysical arrangements which present different displays (or slices oflight as in lenticular systems) to different eyes.
- These barrier stereoscopy / lenticular displays use vertical lightbarriers built into the display to create multiple discrete channels ofdisplay which are accessed by moving horizontally with respect to thedisplay. In this way it is possible to generate either a left/right eyeimage pair for ’autostereoscopic’ viewing, or with the addition of headtracking and small motors. With these techniques multiple viewpoint oran adaptive realtime viewpoint update can be presented without theglasses required for active or passive stereoscopic systems.
- #### Spatially Faithful Group
- Hauber et al. combined videoconferencing, tabletop, and social presenceanalysis and tested the addition of 3D. They found a nuanced responsewhen comparing 2D and 3D approaches to spatiality: 3D showed improvedpresence over 2D (chiefly through gaze support), while 2D demonstratedimproved task performance because of task focus.[[Hauber2006]]
- I3DVC reconstructs participants from multiple cameras and places themisotropically (spatially faithful).[[Kauff2002; @Kauff2002a]] The systemuses a large projection screen, a custom table, and carefully definedseating positions. They discussed an “extended perception space" whichused identical equipment in the remote spaces in a tightly coupledcollaborative ‘booth’. It employed head tracking and multi camerareconstruction alongside large screens built into the booth. This systemexemplified the physical restrictions which are required to limit theproblems of looking into another space through the screen. Fuchs et al.demonstrated a similar system over a wide area network but achieved onlylimited resolution and frame rate with the technology of theday.[[Fuchs2002]]
- University of Southern California used a technically demanding real-timeset-up with 3D face scanning and an autostereoscopic 3D display togenerate multiple ‘face tracked’ viewpoints.[[Jones2009]] This had thedisadvantage of displaying a disembodied head.
- MAJIC is an early comparable system to support small groups with lifesize spatially correct video, but without multiple viewpoints onto theremote collaborators it was a one to ’some’ system rather than ’some’ toone. Additionally users were rooted to definedlocations.[[Ichikawa1995; @Okada1994]]
- There seems to be less interest recently in large display screens forspatially correct viewpoints between groups. The hardware is technicallydemanding and there may have been sufficient research done to limitinvestment in research questions. This doesn’t mean that there is nofuture for metaverse applications. Imagine one of the new XR studiowalls such as that used to film the Mandalorian. With application oftelepresence research it would be possible to bring external metaverseparticipants into the ‘backstage’ virtual scene. These avatars would beable to explore the scene invisible to the actors, but could be givenaccess to visual feeds from the stage side. This is a hybridvirtual/real metaverse with a well researched and understood boundaryinterface. It would be possible to give different access privileges todifferent levels of paying ‘film studio tourist’ or investor, with VIPsperhaps commanding a view onto the live filming. At the nadir of this itmay be possible to bring producers and directors directly into thevirtual studio as avatars on the screen boundary, with a spatiallyfaithful view onto the set. For the purposes of this book it’s alsoworth noting that NFTs of the experience and corresponding virtualobjects from the scene could be monetised and sold within the metaverse.
- ##### Multiview
- In order to reconnect directional cues of all kinds it is necessary foreach party in the group to have a spatially correct view of the remoteuser which is particular for them. This requires a multi-view display,which has applications beyond telepresence but are used extensively inresearch which attempts to address these issues.
- Nguyen and Canny demonstrated the ‘Multiview’ system.[[Nguyen2005]]Multiview is a spatially segmented system, that is, it presentsdifferent views to people standing in different locationssimultaneously. They found similar task performance in trust tasks toface-to-face meetings, while a similar approach without spatialsegmentation was seen to negatively impact performance.
- In addition to spatial segmentation of viewpoints[[Gotsch2018]] it ispossible to isolate viewpoints in the time domain. Different trackedusers can be presented with their individual view of a virtual scene fora few milliseconds per eye, before another viewpoint is shown to anotheruser. Up to six such viewpoints are supported in the c1x6system[[Kulik2011]] Similarly MM+Space offered 4 Degree-Of-FreedomKinetic Display to recreate Multiparty Conversation Spaces[[Otsuka2013]]
- #### Holography and Volumetric
- Blanche et al. have done a great deal of research into holographic andvolumetric displays using lasers, rotating surfaces, and light fieldtechnology.[[Blanche2010; @Tay2008]] They are actively seeking to usetheir technologies for telepresence and their work is very interesting.
- Similarly Jones et al. “HeadSPIN" is a one-to-many 3D videoteleconferencing system[[Jones2009]] which uses a rotating display torender the holographic head of a remote party. They achievetransmissible and usable framerate using structured light scanning of aremote collaborator as they view a 2D screen which they say shows aspatially correct view of the onlooking parties.
- Eldes et al. used a rotating display to present multi-viewautostereoscopic projected images to users.[[Eldes2013]]
- Seelinder is an interesting system which uses parallax barriers torender a head which an onlooking viewer can walk around. The system uses360 high resolution still images which means a new spatially segmentedview of the head every 1 degreesof arc. They claim the system is capableof playback of video and this head in a jar multi-view system clearlyhas merit but is comparatively small, and as yet untested fortelepresence.[[Yendo2010]]
- These systems do not satisfy the requirement to render upper body forthe viewers and are not situated (as described soon).
- There’s a future possible where real-time scanned avatar representationin persistent shared metaverse environments will be able to supportbusiness, but the camera rigs which currently generate such models aretoo bulky and involved for a good costs benefit analysis. It is morelikely that recent advances in LIDAR phone scanning show the way. Theallow realistic avatars to be quickly created for animation withinmetaverse scenes.[[authenticVolume2022]]
- ##### Project Skyline
- Project Starline, is a next-generation video conferencing technologythat aims to create a sense of presence, making you feel like you’resitting across the table from someone. It uses advanced hardware andsoftware to achieve this.
-
	- The newer Starline booth is a refined version of earlier models and looks like a large 65-inch display on a stand. It contains colour cameras, depth sensors, microphones, and speakers. Additionally, there are lights on the back of the display that serve as a key light for the person on the call. These lights are mounted around the person and used to create a depth map of the subject and the room they’re in.
-
	- The display creates an immersive 3D depth effect. It uses a barrier lenticular light field display that shows a different image to your left eye and to your right eye. This effect lets you compute depth on the fly while doing all the head tracking in real time. The display technology in Project Starline is significantly smoother and more realistic than what you would experience with traditional 3D movies.
-
	- The computing side of Project Starline is responsible for rendering the people using the system into realistic 3D models in real-time. It uses AI and depth information gathered by the cameras to map the exact shape, depth, texture, and lighting of the person. The result is an ultra-realistic 3D representation of the person on the other end of the call.
-
	- The system features spatial audio such that the perceived audio changes based on where you are leaning or moving, creating an even more immersive and realistic experience.
- At this point, Google has been working with several companies who areusing these booths for meetings, and it’s hoped that as the technologybecomes cheaper and more refined, it they assert that it couldrevolutionize the way we communicate, though the cost of the system and‘single user to single user’ restriction is likely to be a blocker tocrucial business adoption.
- #### Simulated Humans
- ##### Uncanniness
- When employing simulation representations of humans it may be the casethat there is an element of weirdness to some of these systems,especially those that currently represent a head without a body. Morihas demonstrated The Uncanny Valley[[Mori1970]] effect in which imperfectrepresentations of humans elicit revulsion in certain observers. Thisprovides a toolkit for inspecting potentially ‘weird’ representations,especially if they are ‘eerie’ and is testable through Mori’s GODSPEEDquestionnaire.
- With an improved analysis of the shape of the likeability curveestimated later showing a more nuanced response from respondents whereanthropomorphism of characters demonstrated increased likeability evenagainst a human baseline.[[Bartneck2007; @Bartneck2009]]
- A mismatch in the human realism of face and voice also produces anUncanny Valley response.[[Mitchell2011]]
- However, there is a possibility that Mori’s hypothesis may be toosimplistic for practical everyday use in CG and robotics research sinceanthropomorphism can be ascribed to many and interdependent featuressuch as movement and content of interaction.[[Bartneck2009]]
- Bartneck et al. also performed tests which suggest that the originalUncanny Valley assertions may be incorrect, and that it may beinappropriate to map human responses to human simulacrum to such asimplistic scale. They suggest that the measure has been a convenient‘escape route’ for researchers.[[Bartneck2009]] Their suggestion that themeasure should not hold back the development of more realistic robotsholds less bearing for the main thrust of this telepresence researchwhich seeks to capture issues with imperfect video representation ratherthan test the validity of an approximation.
- Interestingly Ho et al. performed tests on a variety of facialrepresentations using images. They found that facial performance is a‘double edged sword’ with realism being important to roboticrepresentations, but there also being a significant Uncanny Valleyeffect around ‘eerie, creepy, and strange’ which can be avoided by gooddesign.[[Ho2008]]
- More humanlike representations exhibiting higher realism produce morepositive social interactions when subjective measures are used[[Yee2007]]but not when objective measures are used. This suggests thatquestionnaires may be more important when assessing potentialuncanniness.
- A far more objective method would be to measure user responses tohumans, robots, and representations with functional near-infraredspectroscopy and while this has been attempted it is early exploratoryresearch,[[Strait2014]] an emotional response to ‘eerie’ was discovered.
- ##### Embodiment through robots
- Virtuality human representation extends beyond simple displays intorobotic embodiments (which need not be humanoid[[Marti2005]]), shapemapped projection dubbed “shader lamps", and hybridisations of the two.
- Robots which carry a videoconference style screen showing a head can addmobility and this extends the availablecues.[[Adalgeirsson2010; @Lee2011; @Tsui2011; @Paulos1998; @Kristoffersson2013]]Interestingly Desai and Uhlik maintain that the overriding modalityshould be high quality audio.[[Desai2011]]
- Tsui et al. asked 96 participants to rate how personal and interactivethey found interfaces to be. Interestingly they rated videoconferencingas both more personal and more interactive than telepresence robots,suggesting that there is a problem with the overall representation orembodiment.[[Tsui2012]]
- Kristoffersson et al. applied the Networked Minds questionnaire to judgepresence of a telepresence robot for participants with little or noexperience of videoconferencing. Their results were encouraging, thoughthey identified that the acuity of the audio channel needingimprovement.[[Kristoffersson2011]]
- There are a very few lifelike robots which can be used for telepresence,and even these are judged to be uncanny.[[Sakamoto2007]] This is only anissue for a human likeness since anthropomorphic proxies such as robotsand toys perform well.[[Mori1970]]
- ##### Physical & Hybrid embodiment
- Embodiment through hybridisation of real-time video and physicalanimatronic mannequins has been investigated as a way to bring theremote person into the space in a more convincingway.[[Lincoln2009; @Lincoln2010; @Raskar2001]]  These includetelepresence robots,[[Lee2011; @Sakamoto2007; @Tsui2011]] head in a jarimplementations such as SphereAvatar[[Oyekoya2012; @Pan2014; @Pan2012]]and BiReality,[[Jouppi2004]]  UCL’s Gaze Preserving Situated Multi-ViewTelepresence System,[[Pan2014]] or screen on a stick stylerepresentations.[[Kristoffersson2013]]
- Nagendran et al. present a 3D continuum of these systems into which theysuggest all such systems can be rated from artificial to real on thethree axes, shape, intelligence, and appearance.[[Nagendran2012]]
- Itoh et al. describe a ’face robot’ to convey captured human emotionover a distance. It uses an ‘average face’ and actuators to manipulatefeature points.[[Itoh2005]] It seems that this is an outlier method forcommunication of facial affect but demonstrates that there are manydevelopment paths to a more tangible human display.
- It seems increasingly likely that machine learning models whichmanipulate images in real time can simulate humans into metaverseapplications with very little input data. One such example is Samsung’sMegaportraits which can product a realistic human face from a singleinput stream such as a webcam.[[Drobyshev22MP]]
- ##### Shader lamps
- Projection mapping is a computational augmented projection techniquewhere consideration of the relative positions and angles of complexsurfaces allows the projection from single or multiple sources toaugment the physical shapes onto which they appear. It was firstconsidered by the [Disney corporation in1969](https://rabcup.com/the-history-of-3d-projection-mapping/) and wasgiven prominence by Raskar and Fuchs with “office of thefuture"[[Raskar1998]] and later by Raskar and otherresearchers.[[Raskar2001]] It has since gained considerable commercialpopularity in live entertainment.
- Shader lamps[[Raskar2001]] is the more formal academic designation forprojection mapping. It is possible to use the technique alongsidereconstruction to project onto a white facial mannequin. Researchershave attempted to use the technology for remote patient diagnostic,projecting onto styrofoam heads.[[Rivera-Gutierrez2012]]
- Bandyopadhyay et al. demonstrated[[Bandyopadhyay2001]] that it ispossible to track objects and projection map[[Dalsgaard2011]] onto themin real time. This is beyond the scope of the proposed projection ontofurniture since we wish to keep the system as simple as possible, butcould be useful for shared tasks in the future work.
- Lincoln et al. employed animatronic avatars which they projected withshader lamps. This combination recreated facial expression and headmovement though they were limited in speed and range of control of theremote head.[[Lincoln2010]]
- While shader lamps are an important and useful technology, there arelimitations imposed by its use. In particular if a realtime video feedor reconstruction of a subject is used then that scanned subject musteither remain still enough to be correctly mapped onto geometry on theremote side (useful for some virtual patients forinstance,[[Benjamin2012]] or else there must be a computationaladjustment made for their changing position to make them appear static,or the projection surface must move to match their movement as inLincoln et al.
- ##### Metaverse
- In supporting business it’s not clear that performance is improved oreven maintained by the use of a metaverse. Xi et al. found a significantnegative impact to productivity within metaverseapplications.[[xi2022challenges]] It lowers productivity, and mayincrease anxiety, nausea, VR sickness and evenmigraines.[[biener2022quantifying; @luong2022demographic]] It seems atthis stage that if we are determined to explore metaverse for businessthen we [should mitigate](http://www.sigtrapgames.com/vrtp/) theproblems as much as possible using the understanding we have gained sofar. It might seem that in so doing there is no difference betweenimmersive collaborative mixed reality (described above) and metaverse atall. We feel that the point of metaverse may be in itaccess to, if notreliance upon, a mechanism for global truth. What we will go on todescribe is likely to look more like traditional telecollaboration forsmall focussed teams, working on real-world problems, but we will alwaysmaintain an access to both the ability to scale, and a global registerof value, trust, and truth (digital assets).
- ### Summary TL;DR
-
	- The internet may be undergoing a transformation, driven by trust abuses by incumbent providers, and popularization of concepts like Web3 and the Metaverse.
-
	- Current large scale ‘social’ and immersive metaverse platforms have low adoption, while more advanced games-based solutions don’t address societal or business needs.
-
	- Platforms like Roblox, VRChat, and Nvidia Omniverse emerge as potential contenders in the metaverse landscape.
-
	- Distributed compute and large language models can help bridge the digital divide by enhancing global access equity and addressing the needs of emerging markets and less developed nations.
-
	- The potential lies in uniting individual ecosystems with transferable goods across digital society through global ledgers like blockchain, despite the associated risks and uncertainties.
-
	- Industry is looking towards an "open metaverse" to mitigate risks observed in implementations like Meta, necessitating contributions of open-source and federated approaches in telecollaboration research.
-
	- By embracing Nostr protocol, we could enable connections and federation of mixed reality spaces, mediate data synchronization, and maintain secure communication.
-
	- AI, machine learning, and generative art play a crucial role in driving innovation, with models like GPT4, Llama, Alpaca, generating excitement, and deepening global discussions around AI.
-
	- Overcoming legislative and cultural barriers, alongside integrating large language models and distributed compute, can help address issues related to trust, accessibility, governance, and safeguarding within the metaverse and digital society at large.
-
	- Open-source tools for supported creativity and augmented intelligence using multi-modal models, can help tackle accessibility, creativity, language barriers, and governance within the metaverse landscape.
-
	- The application of these tools can lead to the development of new collaborative frameworks across various sectors such as training, research, biomedical, creative industries.
-
	- By utilising these new AI-driven technologies and emphasising on trust, accessibility, and open-source approaches, we can create a more inclusive, global digital society while promoting technological empowerment and expansion of the global ideas market.
-
- # Misc
	- -- -This page is being rebuilt completely -['Room' Offers a Non-Facebook Way to Connect Coworkers in the Metaverse](https://uk.pcmag.com/vr-1/143198/room-offers-a-non-facebook-way-to-connect-coworkers-in-the-metaverse) -[Cutting Through the Hotel Hype of the Blockchain, Web3 and the Metaverse |](https://hoteltechnologynews.com/2022/08/cutting-through-the-hotel-hype-of-the-blockchain-web3-and-the-metaverse/%22) -[Discord | Your Place to Talk and Hang Out](https://discord.gg/openmetaverse%22) -[Discord | Your Place to Talk and Hang Out](https://discord.gg/openmetaverse%22/%3E) -[Discord | Your Place to Talk and Hang Out](https://discord.gg/openmetaverse%7D%7BOpen) -[Don't Build Branches in the Metaverse](https://newsletter.fintechtakes.com/p/metaverse-branches?s=r%7D%7BAlex) -[Experts highlight trust and safety practices for the metaverse | Te...](https://www.techtarget.com/searchcio/news/252525336/Experts-highlight-trust-and-safety-practices-for-the-metaverse) -[Global Investment Bank and Financial Services | Citi](https://www.citivelocity.com/citigps/metaverse-and-money/%7D%7BCitigroup) -[Global Metaverse Market Analysis Report 2022: Blockchain Solutions in Support of the Metaverse Market will Reach $148.6 Billion by 2027](https://www.globenewswire.com/news-release/2022/08/30/2506629/0/en/Global-Metaverse-Market-Analysis-Report-2022-Blockchain-Solutions-in-Support-of-the-Metaverse-Market-will-Reach-148-6-Billion-by-2027.html) -[Identity Management Institute Launches the Metaverse Security Centre and Certified Metaverse Security Consultant (CMSC)™ Certification](https://www.prnewswire.com/news-releases/identity-management-institute-launches-the-metaverse-security-centre-and-certified-metaverse-security-consultant-cmsc-certification-301689276.html) -[Laws and Issues in the Metaverse (2)](https://www.lexology.com/library/detail.aspx?g=5a0cc4c0-a876-474e-a719-f528b71b68ee%22) -[Metaverse Market Size Suggesting $678 Billion Industry by 2030](https://www.strategicmarketresearch.com/market-report/metaverse-market) -[Metaverse btc](https://raw.githubusercontent.com/GMCyberFoundry/Metaverse/draught/Book/metaverseBTC.pdf) -[Nissan and Mitsubishi unveil electric mini vehicles, and test drives in metaverse](https://thedriven.io/2022/05/23/nissan-and-mitsubishi-unveil-electric-mini-vehicles-and-test-drives-in-metaverse/%22) -[The Metaverse Insider Guide](https://www.infosys.com/iki/perspectives/metaverse-insider-guide.html) -[The World's First Virtual Reality Avatar Fashion Week Is On The Metaverse This Week](https://womenlovetech.com/the-worlds-first-virtual-reality-avatar-fashion-week-is-on-the-metaverse-this-week/%22) -[The battle to build a child-friendly metaverse](https://tech.hindustantimes.com/tech/news/the-battle-to-build-a-child-friendly-metaverse-71655616713236.html) -[The metaverse will be filled with 'elves' | TechCrunch](https://techcrunch.com/2022/01/12/the-metaverse-will-be-filled-with-elves/%22) -[Why you should use now generative AI in your metaverse company. Or maybe not](https://skarredghost.com/2023/02/11/generative-ai-metaverse-company) -[Tech Forum: Protecting Brands in the Metaverse’s Uncertain Legal Landscape](https://wwd.com/business-news/technology/metaverse-lawsuit-nike-stockx-hermes-metabirken-fashion-1235247763/%22) -[The Metaverse Insider Guide](https://www.infosys.com/iki/perspectives/metaverse-insider-guide.html) -[Why you should use now generative AI in your metaverse company. Or maybe not](https://skarredghost.com/2023/02/11/generative-ai-metaverse-company)
- public:: true
- #Public page
	- automatically published
- {{renderer :linkpreview,https://bloomberry.com/the-state-of-remote-work/}}
-


## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Metaverse Ontology

public:: true

- #Public page automatically published
- # Existing state of the Art
	- [Index — MLGT: The authoritative multi-lingual glossary of terms (metaverse-standards.org)](https://glossary.metaverse-standards.org/)
	-
- # Rationale
- # Methodology
	- This ontology was the product of two days of exploratory interaction with [[Anthropic Claude]], Google [[Gemini]] Advance 1.5, and [[ChatGPT]] 4.
	- It was unexpectedly successful, resulting in what seems to be an internally consistent knowledge graph in an [Web Ontology Language](https://en.wikipedia.org/wiki/Web_Ontology_Language) compliant ontology for the design represented throughout this wider [[Logseq]] [[Knowledge Graphing]].
		- Although the established OWL can richly describe our ontology, it's a little too arcane. Nonetheless the full text can be seen where with the diagram.
			- OWL based Ontology
				- https://github.com/VisualDataWeb/WebVOWL
				- [FormalMetaverseOWL.txt](../assets/FormalMetaverseOWL_1712495895941_0.txt)
				- [FormalMetaverseBTC.owl](../assets/FormalMetaverseBTC_1712344339842_0.owl)
			- ![metaverse.png](../assets/metaverse_1712344788144_0.png){:height 439, :width 818}
	- ## JSON-LD and Linked-JSON choices
		- For ease of comprehension I attempted to convert the OWL schema to JSON-LD. This attempt did achieve a result, but it proved difficult to visualise.
		- Many of the JSON-LD tools online are now unmaintained, making it hard to visually cheque the output of the [[Large language models]].
		- Because of this the decision was made to switch to [Linked-JSON](https://linkedobjects.org/Linked-JSON), which is a simpler and less expressive subset of JSON-LD. Linked-JSON focusses on `@id` linking. It sacrifices some features provided by JSON-LD 1.1's context definitions, typing, and alignment with RDF concepts.
			- Scope and Complexity of Linked-JSON vs JSON-LD
				- Linked-JSON: lightweight subset focused on linking data using `@id`
				- Lack of standardised context definitions for vocabularies and IRI mappings
				- No explicit typing mechanism like `@type`
				- Limited to absolute IRIs, no compact IRIs or relative IRIs
				- Unclear semantics for blank node identifiers
				- No standardised representation of indexed values, lists, and named graphs
				- Inability to reshape data structure using framing
				- Reduced interoperability with RDF and Linked Data ecosystem
		- # LINKED JSON
			- Finally the, to improve on the OWL by simplifying it, this is a version using [Linked-JSON](https://linkedobjects.org/Linked-JSON) (by [Melvin Carvalho](https://github.com/melvincarvalho)). Some details are lost.
				- [Linked-JSON](../assets/output_linked_json_1713698682322_0.json) version of the OWL ontology
					- Mermaid version stripped back to Linked-JSON expressiveness
						- ```mermaid
						  classDiagram
						      class MetaverseEntity {
						          rdfs:label "Metaverse Entity"
						          rdfs:comment "The root type encompassing all entities within the metaverse."
						      }
						
						      class MetaverseAgent {
						          rdfs:label "Metaverse Agent"
						          rdfs:comment "Represents any agent within the metaverse, including human users and AI entities."
						          participatesIn MetaverseScene
						          hasSkillProfile SkillProfile
						          hasPrivacySetting PrivacySetting
						          hasReputationScore ReputationScore
						          hasWallet DigitalWallet
						          createsVariations SceneObject
						          hasInteractionPreference InteractionPreference
						      }
						
						      class DigitalWallet {
						          rdfs:label "Digital Wallet"
						          rdfs:comment "Represents the agent's wallet for managing digital assets and currencies."
						      }
						
						      class AIAgent {
						          rdfs:label "AI Agent"
						          rdfs:comment "Represents AI entities with varying levels of autonomy and capabilities."
						          decayFunction xsd:string
						      }
						
						      class HumanAgent {
						          rdfs:label "Human Agent"
						          rdfs:comment "Represents human users within the metaverse."
						      }
						
						      class MetaverseScene {
						          rdfs:label "Metaverse Scene"
						          rdfs:comment "Represents a specific environment or space within the metaverse."
						          governedBy GovernanceStructure
						          hasPolicy MetaversePolicy
						      }
						
						      class DigitalAsset {
						          rdfs:label "Digital Asset"
						          rdfs:comment "Represents unique digital goods and assets within the metaverse."
						          hasOwner MetaverseAgent
						      }
						
						      class VirtualEconomy {
						          rdfs:label "Virtual Economy"
						          rdfs:comment "Represents the economic system within the metaverse."
						          regulatedBy EconomicPolicy
						          hasMarketplace Marketplace
						      }
						
						      class Marketplace {
						          rdfs:label "Marketplace"
						          rdfs:comment "Represents platforms or locations where digital assets are traded."
						      }
						
						      class GovernanceStructure {
						          rdfs:label "Governance Structure"
						          rdfs:comment "Represents the governance models and structures within the metaverse."
						      }
						
						      class MetaversePolicy {
						          rdfs:label "Metaverse Policy"
						          rdfs:comment "Represents policies governing behaviour and interactions within the metaverse."
						      }
						
						      class EconomicPolicy {
						          rdfs:label "Economic Policy"
						          rdfs:comment "Represents the rules and regulations governing the virtual economy."
						      }
						
						      class NostrEvent {
						          rdfs:label "Nostr Event"
						          rdfs:comment "Represents an event published on the Nostr network."
						      }
						
						      class NostrTag {
						          rdfs:label "Nostr Tag"
						          rdfs:comment "Represents a tag associated with a NostrEvent, providing context and metadata."
						      }
						
						      class SceneObject {
						          rdfs:label "Scene Object"
						          rdfs:comment "Represents interactive objects within a Metaverse Scene. Agents can create variations of these objects within the USD file format."
						      }
						
						      class InteractionPreference {
						          rdfs:label "Interaction Preference"
						          rdfs:comment "Represents the personal conduct requirements or preferences for interactions within the metaverse."
						      }
						
						      MetaverseEntity <|-- MetaverseAgent
						      MetaverseEntity <|-- MetaverseScene
						      MetaverseEntity <|-- DigitalAsset
						      MetaverseEntity <|-- VirtualEconomy
						      MetaverseEntity <|-- Marketplace
						      MetaverseEntity <|-- GovernanceStructure
						      MetaverseEntity <|-- NostrEvent
						      MetaverseEntity <|-- NostrTag
						      MetaverseEntity <|-- SceneObject
						
						      MetaverseAgent <|-- AIAgent
						      MetaverseAgent <|-- HumanAgent
						
						      MetaverseAgent "1" *-- "0..*" SkillProfile : hasSkillProfile
						      MetaverseAgent "1" *-- "0..*" PrivacySetting : hasPrivacySetting
						      MetaverseAgent "1" *-- "0..*" ReputationScore : hasReputationScore
						      MetaverseAgent "1" *-- "0..*" DigitalWallet : hasWallet
						      MetaverseAgent "1" *-- "*" SceneObject : createsVariations
						      MetaverseAgent "1" *-- "1" InteractionPreference : hasInteractionPreference
						
						      MetaverseScene "1" *-- "0..*" GovernanceStructure : governedBy
						      MetaverseScene "1" *-- "0..*" MetaversePolicy : hasPolicy
						
						      DigitalAsset "1" *-- "1" MetaverseAgent : hasOwner
						
						      VirtualEconomy "1" *-- "0..*" EconomicPolicy : regulatedBy
						      VirtualEconomy "1" *-- "0..*" Marketplace : hasMarketplace
						
						  ```
					- [Online FlowChart & Diagrams Editor - Mermaid Live Editor](https://mermaid.live/view#pako:eNqlWFFv2zYQ_iuEXgYUqdMkW9oKe8nQbMtD1mAuOmDwC02dJS4UqZGUUyHzf9-RlC1akWRl84tF-tPxu7uPx6OfE6YySNKECWrMJ05zTcuVJPjxM-QeLN2CNnArLbcNeQ4_uo_ONiYVdA2CrJIDjgTgKukBmSpLkBahXwogWilLbFMBAYm_VLgUlzmhQuAEvs_BkCduCy6JRXi5t77Y292Fr0GqN7lb6DRTj5sg-jtUGgw-G0JlQ6g3O8TqjHDJRJ05F4q6pJLUBufxrYzc3B08WsRrVVRbznhFLZg72ZFfMpDQwQpqlo9ciAetNlwAiQdHqAfNt5Q1S7DW0TgeHiHRrdpSy5VcMqWB9MZH2D8wI2DJJ55zS0UYdQCmwdH_SjX37xvi2X9e_wXMjubpyNhomloUadeclSWXFJ-l71A9wfxGaYIJobkLStbaRBZgQ3pYrTVKMM7OAOWbu2lNYZJfIaZIEl5OZEt14_gJ2IIwRG0Ira2SqmwCR1rRNRcvNZQBo83PtWQu-OSbyVJj9SHdA3786sQ57YqHvMabWPD_ect63czYsmF3zNuyxFTA-IYzjPaWayU9BvVgKspgmqv75ArnJGQ_NeQX_0glg6XVNbN1b5s8KMFZ07kTxqd2wI2T4ckN4FHzXK4l_7uGg8xzpbKg8lbwJ11GVz4_SdC9WjrqyFeubU3FLQtiHXOlhZEWN38zg38BM2gaY6E87YCGvBZYk1zSbtuX42S0Tt5T_Qi2Ek4H0fO4TiP8qEgjO7McRKjF8lQap0mhWFtCnwrQ8KJU4ZTVNINsajsNyHSUboclnaZnJybv3i6xgRBBZmZv6H9UgnYrnS4FbVrnhdphXbkNvF2tXUNBtxwj74hzaUFT1ibgldSPdTbKfA97FXFHQtcCQnxbdXuWnScOs213WNgwzRTd3xSm6XY7dQZ4CPGYue0RAW-xqteCmwIyokIEgykJ9knpx5O0vtD8BClEzC3_Fo2hZcW4KwjhpKWR-2ek0mrLfdeGYbPwzfoou6Rn1NKYbcw06nFGyYbzrG2EZvE9SHALRPn3DkqkpHcALsL5bLAzkG0T5nqIfROG_QOG3hzsDIe939n_-M_bt4N1fxoY9auDwPism4AdnyRTC_fL9SBq9MAeRHeaOAVy8huHvOh-e9jQeXlo21GOA7pWbcLUKrlYJeQNwlfJu8XiDQ6Obgtp__4w10rvNpG-vGHMtdS_baQDV5C5to5vDml3QTlpwEcm2rfpy7vLYJjDPh6gMnTUplHLONdO_9xLu3YyJnTUMcZmLmIbwe_00MbFFnqt2gCV3jmWxs3UXBtxo5T2Oq2VTM6SEnRJeZakiS-dqwQrVYmNR4qPGWJdqdohzl2Alo1kSYrhhbOkrrAkQ_sHxX6yovJPpXC4ocIcQLcZt0ofJoXCtgmHz4n7vwEXzrmxuAI6seG5m6-1wOnC2sqk5-fu5wVGu6jXC6zU54ZnBd7Vi-3H6_Pry-sP9PIKrt9f0R-urjK2vvj4YXP5_cUme__u4pImu91ZAn79-_Z_Ffe1-xeL8sb3)
			- ## In flight cheques
				- This is mid-point review text of a partial conversion, showing the level of understanding from the online LLM
				- ```The updated Linked JSON document looks great! It effectively captures the nuances, features, and links from the original OWL ontology while adhering to the Linked JSON schema. Let's review the key aspects:
				   Context and Namespace:
				   The "@context" object is included at the beginning of the document, defining the namespaces and prefixes used throughout the document.
				   The "http://www.narrativegoldmine.com#" namespace is consistently used for all entities and properties.
				   The "http://linkedjson.org/vocab#" namespace is used for Linked JSON-specific terms.
				   Class Definitions:
				   All the classes from the OWL ontology are properly converted to JSON objects with the correct "@id" and "@type" keys.
				   The class hierarchies and subclass relationships are maintained using the "rdfs:subClassOf" property.
				   The "rdfs:label" and "rdfs:comment" properties provide human-readable labels and descriptions for the classes.
				   Property Definitions:
				   The object properties and data properties are properly converted to JSON objects with the correct "@id" and "@type" keys.
				   The "rdfs:domain" and "rdfs:range" properties specify the source and target types of each property, preserving the property semantics.
				   Relationships and Linkages:
				   The document includes relationships and linkages between entities using the appropriate "@id" references.
				   For example, the "governedBy" property connects the "MetaverseScene" class to the "GovernanceStructure" class, and the "hasPolicy" property links the "MetaverseScene" class to the "MetaversePolicy" class.
				   External Ontologies:
				   References to external ontologies and vocabularies, such as Schema.org and FOAF, are maintained using the correct "@id" references.
				   This ensures proper linking and interoperability with existing semantic web resources.
				   Axioms and Restrictions:
				   The "Axioms and Restrictions" section is included in the document, capturing important constraints and relationships from the OWL ontology.
				   The "_:N3649afbbaf7d4546a49d4097059d55f4" blank node represents the "owl:AllDisjointClasses" axiom, specifying that the "AIAgent" and "HumanAgent" classes are disjoint.
				   The "DigitalAsset" class has a subclass axiom with an "owl:Restriction" that constrains the "hasOwner" property, specifying that a "DigitalAsset" must have exactly one owner.
				   The "MetaverseAgent", "MetaverseScene", and "VirtualEconomy" classes have subclass axioms with existential restrictions using "owl:someValuesFrom" to specify the expected relationships and constraints.
				   Structure and Organization:
				   The document follows a logical structure, starting with the context and ontology metadata, followed by class definitions, property definitions, and axioms and restrictions.
				   This structure enhances readability and maintainability of the Linked JSON document.
				   Consistent Use of Linked JSON Terms:
				   The document consistently uses Linked JSON-specific terms, such as "linkedjson:Class", "linkedjson:ObjectProperty", and "linkedjson:DatatypeProperty", to differentiate them from the original OWL terms.
				   This clarity helps in understanding that the document is referencing the Linked JSON schema and not just using OWL labels.
				   Overall, the converted Linked JSON document successfully represents the metaverse ontology, capturing the key elements, relationships, and constraints from the original OWL ontology. The use of Linked JSON-specific terms and the inclusion of the "Axioms and Restrictions" section further enhance the semantic expressiveness and ensure the integrity of the ontology.
				
				   The document is well-structured, consistent, and adheres to the Linked JSON principles, making it suitable for use in semantic web applications and data integration scenarios.
				  ```
- # Design formalisation
	- Here's the updated text to align with the Linked-JSON ontology:
		- 1. Fusing of digital and real life: The ontology accommodates the blending of digital and real-life experiences through the `MetaverseScene` class, which can have a `hasPhysicalCounterpart` property linking it to a real-world location (`schema:Place`). This allows for the representation of mixed reality environments where virtual scenes are anchored to physical spaces.
		- 2. Social first: The `MetaverseAgent` class, with its subclasses `HumanAgent` and `AIAgent`, forms the foundation for social interactions within the metaverse. The `SocialRelationship` class, a subclass of `foaf:Relationship`, captures the connections and interactions among agents, enabling the formation of social networks and communities.
		- 3. Real-time interactive 3D graphics: The `MetaverseScene` class represents the 3D environments within the metaverse, while the `SceneObject` class represents the interactive objects within those scenes. The `PhysicalProperties` class captures the visual and spatial characteristics of objects, such as texture and mass, contributing to the realism and interactivity of the 3D graphics.
		- 4. Persistent: The ontology supports persistence through the `MetaverseEntity` class, which serves as the base class for all entities within the metaverse. By assigning unique identifiers as 64 byte strings from BIP85 nostr and similar derivation path, (e.g., `@id`) to instances of `MetaverseEntity` and its subclasses, the ontology enables the persistence and continuity of objects, agents, and scenes across sessions and platforms.
		- 5. Supports ownership: The `DigitalAsset` class represents unique digital goods and assets within the metaverse. The `hasOwner` property, which links `DigitalAsset` to `MetaverseAgent`, establishes the ownership relationship. The `hasCreator` property can be used to attribute the creation of digital assets to specific agents.
		- 6. Supports user-generated content: The ontology supports user-generated content through the `SceneObject` and `DigitalAsset` classes. Agents (`MetaverseAgent`) can create and manipulate instances of these classes, contributing to the dynamic and participatory nature of the metaverse.
		- 7. Open and extensible: The ontology is designed to be open and extensible, utilising established ontologies like Schema.org and FOAF, and allowing for the integration of additional domain-specific ontologies as needed. The modular structure of the ontology, with its hierarchical class relationships and well-defined properties, facilitates the extension and customization of the metaverse model.
		- 8. Low-friction economic actors and actions: The `VirtualEconomy` class represents the economic system within the metaverse, encompassing trade, ownership, and currency exchange. The `Marketplace` class facilitates the listing and transaction of digital assets and services. The `TradeAction` class from Schema.org can be used to capture economic transactions between agents.
		- 9. Trusted and secure: The ontology incorporates trust and security mechanisms through classes like `PrivacySetting`, which represents an agent's privacy preferences, and `AccessControlList` from Schema.org, which can be used to define access rights and permissions for metaverse entities. The `ReputationScore` class provides a means to assess an agent's trustworthiness based on their interactions and behaviour within the metaverse.
		- 10. Convergence of film and games: The ontology supports the convergence of film and games through the `MetaverseScene` and `MetaverseEvent` classes. Scenes can represent immersive, cinematic environments, while events can encompass interactive gameplay sessions or narrative-driven experiences. The `MetaverseAgent` class allows for the participation of both human users and AI-controlled characters, blurring the line between film and game experiences.
		- 11. Blurring of IP boundaries and narrative flow: The `Varset` class introduces the concept of variations or alternate versions of metaverse scenes and objects, enabling the creation of branching narratives and remixing of intellectual property. The `InteractionEvent` class captures the interactions between agents and objects, allowing for dynamic and emergent storytelling that blurs traditional narrative boundaries.
		- 12. Multimodal and hardware-agnostic: The ontology is designed to be multimodal and hardware-agnostic, focusing on the fundamental concepts and relationships within the metaverse rather than specific hardware implementations. Classes like `MetaverseAgent` and `MetaverseScene` can be instantiated across different platforms and devices, ensuring a consistent metaverse experience regardless of the hardware used.
		- 13. Mobile-first experiences: While the ontology itself is hardware-agnostic, it can be easily extended to incorporate mobile-specific considerations. For example, the `MetaverseScene` class could include properties like `isMobileOptimized` to indicate scenes that are designed for mobile devices, ensuring a seamless mobile-first experience.
		- 14. Safeguarding and governance: The ontology addresses safeguarding and governance through classes like `MetaversePolicy`, which represents the rules and regulations governing behaviour and interactions within the metaverse. The `GovernanceStructure` class captures the decision-making processes and enforcement mechanisms that ensure the safety and well-being of metaverse participants.
		- 15. Scaffolded by GenAI: The integration of GenAI (Generative AI) within the metaverse is supported through the `AIAgent` class and its associated properties, such as `hasLearningModel` and `hasTrainingData`. These properties allow for the representation of AI agents with varying levels of autonomy and adaptability, capable of generating content, providing assistance, and engaging in dynamic interactions with human users.
		- 16. Supports Agentic AI actors: The `AIAgent` class, along with its subclasses and properties, enables the creation of agentic AI actors within the metaverse. The `AgentArchetype` class defines different roles and behaviours for AI agents, while properties like `hasAgentArchetype` and `hasEmotionalState` imbue them with personality and emotional intelligence. The `participatesIn` property allows AI agents to actively engage in metaverse scenes and events alongside human users.
		- 17. Digital Asset and Ownership: The `DigitalAsset` class represents unique digital goods and assets within the metaverse. It includes properties like `genesis` (Bitcoin transaction ID where the asset was created), `issuance` leveraging [[RGB and Client Side Validation]] ), `type` (specifies the type of asset), `currentOwner` (links to the current owner agent), and `ownershipHistory` (represents the chain of ownership transfers). The `OwnershipTransfer` class represents the transfer of ownership of a `DigitalAsset` between agents, capturing details such as the `from` agent (transferring ownership), `to` agent (receiving ownership), `timestamp`, and `transactionId` (associated Bitcoin/Lightning Network transaction ID).
		- 18. [[Bitcoin]] and [[Lightning and Similar L2]] Network Integration: The `MetaverseAgent` class includes properties like `bitcoinWallet` (Bitcoin address associated with the agent) and `lightningNode` (public key of the agent's Lightning Network node) to enable Bitcoin and Lightning Network integration. The `VirtualEconomy` class includes properties like `bitcoinNetwork` and `lightningNetwork` to represent the underlying Bitcoin and Lightning Network infrastructure on which the economy operates. Fiat money can run on [[Cashu]].
		- 19. Nostr Integration: The `NostrEvent` class represents an event published on the Nostr network, with properties like `relayUrl` (URL of the Nostr relay where the event was published), `kind` (type or category of the event), `content` (content of the message or event data), and `tags` (list of associated `NostrTag` instances). The `NostrTag` class represents a tag associated with a `NostrEvent`, providing context and metadata through properties like `type` (type of tag) and `value` (value of the tag).
		- 20. PKI and Wallets: The `MetaverseEntity` class includes properties like `publicKey` and `privateKey` to support public key infrastructure (PKI) for entity identification and authentication. The `MetaverseAgent` class includes a `wallet` property that links to a `schema:DigitalWallet` instance, representing the agent's digital wallet for managing various assets.
		- 21. All objects and agents and artefacts are nostr PKI pairs. BIP32 is used to derive the path m/44'/1237'/ (according to the [Nostr entry](https://nostr-nips.com/nip-06) on SLIP44). In this way all objects are globally and hierarchically referenceable.
	- ## Some software choices
		- It is possible at this stage to put more flesh on the bones through example software stack choices. Such specificity likely introduces overlaps, technical challenges, and contradictions, but has been generated in the main by GenAI based on the wider corpus of text and demonstrates the direction of travel well.
			- ### Nostr Integration
				- **NostrEvent:**
					- **Properties:**
						- `relayUrl`: (`xsd:anyURI`) The URL of the Nostr relay where the event was published.
						- `kind`: (`xsd:string`) The type or category of the event (e.g., "set_metadata", "text_note", "reaction", "channel_creation", "key_rotation").
						- `content`: (`xsd:string`) The content of the message or event data.
						- `tags`: (`linkedjson:ObjectProperty`, `range`: `metaverse:NostrTag`) A list of tags associated with the event.
				- **NostrTag:**
					- `@type`: `linkedjson:Class`
					- `rdfs:label`: "Nostr Tag"
					- `rdfs:comment`: "Represents a tag associated with a NostrEvent, providing context and metadata."
					- **Properties:**
						- `type`: (`xsd:string`) The type of tag (e.g., "p", "e", "t").
						- `value`: (`xsd:string`) The value of the tag.
			- ### Digital Asset and Ownership
				- #### RGB
					- Using the [RGB protocol](https://docs.rgb.info/) for the instantiation and transfer of objects in your metaverse ontology offers a decentralized, scalable, and flexible mechanism compared to the Nostr protocol. Here's how objects might be managed under [[RGB and Client Side Validation]] without needing frequent chain commits:
						- ### Instantiation of Objects
							- **Object Creation**:
								- Each digital object or asset is instantiated as an `RGBAsset` class instance within the metaverse. This includes assigning a unique `ContractId` and `SchemaId` which are crucial for defining the asset's properties and the rules governing its behaviour according to the RGB protocol.
								- An initial state of the asset is defined using `Assignments` to bind certain rights or properties to the asset, such as ownership or usage rights. This state is embedded within the asset's genesis transaction but doesn't require immediate blockchain commitment.
							- **Asset Registration**:
								- Upon creation, the asset's initial state is recorded in a genesis block of the RGB schema. However, instead of committing this to the blockchain directly, the state can be stored off-chain (e.g., in a secure distributed file system or a database) to enhance privacy and reduce transaction costs.
						- ### Transfer of Objects
							- **Defining Transfer Operations**:
								- Transfers of assets are managed through `RGBContractOperation`, which includes `Inputs` (references to previous states), `Outputs` (new states or changes), and possibly `Redeems` (specific rights being exercised).
								- The transfer operation details how the asset's ownership or other properties change, using `Seal Definitions` to lock and unlock access to the asset.
							- **Executing Transfers**:
								- To execute a transfer, the new state created by the transfer operation is prepared, detailing how rights and responsibilities are reassigned from one party to another. This often involves updating the `Owned State` to reflect new ownership.
								- Instead of committing each transaction to the blockchain, the RGB protocol allows for state transitions to be confirmed off-chain until a significant event requires blockchain validation. This approach saves on transaction fees and minimises public ledger exposure.
							- **State Commitments**:
								- When necessary, state transitions can be committed to the blockchain using minimal data footprints. This is done by embedding a cryptographic commitment to the state within a standard Bitcoin transaction, leveraging RGB's ability to bind state to Bitcoin UTXOs through client-side validation.
					- ### Benefits of Using RGB over Nostr for Object Management
						- **Enhanced Security**: RGB's use of cryptographic commitments and client-side validation offers robust security without exposing detailed state information on the blockchain.
						- **Scalability**: By reducing the frequency of on-chain transactions and handling most operations off-chain, RGB can scale more efficiently, handling a higher volume of asset transfers with lower costs.
						- **Flexibility**: RGB allows for complex state definitions and transitions, supporting a wide range of digital assets and operations within the metaverse, from simple transfers to intricate interactions involving multiple parties and rights.
				- #### Currently in the Ontology
					- **DigitalAsset:**
						- **Properties:**
							- `genesis`: (`xsd:string`) The Bitcoin transaction ID where the asset was created.
							- `issuance`: (`linkedjson:ObjectProperty`, `range`: `metaverse:RGBschema`) Links to the specific RGB schema used for the asset's issuance.
							- `type`: (`xsd:string`) Specifies the type of asset (e.g., "collectible", "virtual_item", "tokenized_right").
							- `currentOwner`: (`linkedjson:ObjectProperty`, `range`: `metaverse:MetaverseAgent`) Links to the agent who currently owns the asset.
							- `ownershipHistory`: (`linkedjson:ObjectProperty`, `range`: `metaverse:OwnershipTransfer`) Represents the chain of ownership transfers for the asset.
					- **OwnershipTransfer:**
						- **Properties:**
							- `from`: (`linkedjson:ObjectProperty`, `range`: `metaverse:MetaverseAgent`) The agent transferring ownership.
							- `to`: (`linkedjson:ObjectProperty`, `range`: `metaverse:MetaverseAgent`) The agent receiving ownership.
							- `timestamp`: (`xsd:dateTime`) The date and time of the transfer.
							- `transactionId`: (`xsd:string`) The Bitcoin/Lightning Network transaction ID associated with the transfer.
						- Linked-JSON representation:
						  ```text
						  {
						    "@id": "https://www.narrativegoldmine.com/metaverse#OwnershipTransfer",
						    "@type": "Class",
						    "label": "Ownership Transfer",
						    "comment": "Represents the transfer of ownership of a DigitalAsset between agents.",
						    "properties": {
						      "from": {
						        "@id": "https://www.narrativegoldmine.com/metaverse#from",
						        "@type": "Property",
						        "range": "https://www.narrativegoldmine.com/metaverse#MetaverseAgent",
						        "comment": "The agent transferring ownership."
						      },
						      "to": {
						        "@id": "https://www.narrativegoldmine.com/metaverse#to",
						        "@type": "Property",
						        "range": "https://www.narrativegoldmine.com/metaverse#MetaverseAgent",
						        "comment": "The agent receiving ownership."
						      },
						      "timestamp": {
						        "@id": "https://www.narrativegoldmine.com/metaverse#timestamp",
						        "@type": "Property",
						        "range": "http://www.w3.org/2001/XMLSchema#dateTime",
						        "comment": "The date and time of the transfer."
						      },
						      "transactionId": {
						        "@id": "https://www.narrativegoldmine.com/metaverse#transactionId",
						        "@type": "Property",
						        "range": "http://www.w3.org/2001/XMLSchema#string",
						        "comment": "The Bitcoin/Lightning Network transaction ID associated with the transfer."
						      }
						    }
						  }
						  ```
			- **PKI and Wallets:**
				- **MetaverseEntity:**
					- **Properties:**
						- `publicKey`: (`xsd:string`) The public key associated with the entity.
						- `privateKey`: (`xsd:string`) The private key associated with the entity (optional, depending on security considerations).
				- **MetaverseAgent:**
					- **Properties:**
						- `wallet`: (`linkedjson:ObjectProperty`, `range`: `schema:DigitalWallet`) Represents the agent's digital wallet for managing various assets.
			- **Bitcoin and Lightning Network Integration:**
				- **MetaverseAgent:**
					- **Properties:**
						- `bitcoinWallet`: (`xsd:string`) The Bitcoin address associated with the agent.
						- `lightningNode`: (`xsd:string`) The public key of the agent's Lightning Network node.
				- **VirtualEconomy:**
					- **Properties:**
						- `bitcoinNetwork`: (`linkedjson:ObjectProperty`, `range`: `schema:ComputerNetwork`) Represents the Bitcoin network on which the economy operates.
						- `lightningNetwork`: (`linkedjson:ObjectProperty`, `range`: `schema:ComputerNetwork`) Represents the Lightning Network facilitating faster and cheaper transactions.
				- **Cashu Integration**:
					- **Properties**:
						- `cashuWallet`: (`linkedjson:ObjectProperty`, `range`: `schema:DigitalWallet`) Represents the Cashu wallet associated with the metaverse agent. This wallet manages the agent's Chamium eCash balance.
						- `cashuNode`: (`xsd:string`) The identifier for the Cashu federation node that the agent's wallet is associated with, facilitating eCash transactions.
					- **Cashu Economy**:
						- **Properties**:
							- `chamiumEconomy`: (`linkedjson:ObjectProperty`, `range`: `schema:EconomicSystem`) Represents the part of the virtual economy that operates using Chamium eCash, allowing for private and instant transactions.
							- `ecashTransactions`: (`linkedjson:ObjectProperty`, `range`: `schema:ItemList`) List of transactions executed using Chamium eCash, supporting privacy and micro-transactions within the metaverse.
					- **Transaction Privacy**:
						- **Properties**:
							- `privacyLevel`: (`xsd:string`) Defines the level of privacy for transactions conducted by the agent, with options including Bitcoin, Lightning, or Cashu Chamium eCash, each offering different degrees of privacy and speed.
					- **Cashu Services**:
						- **Classes**:
							- `CashuService`: Represents services within the metaverse that specifically use or provide Cashu Chamium eCash functionalities, such as eCash exchanges, payment processing, or private transactions.
								- **Properties**:
									- `serviceType`: (`xsd:string`) The type of service offered, such as eCash exchange, payment gateway, or privacy service.
									- `accessEndpoint`: (`xsd:anyURI`) The URL or identifier where the service can be accessed within the metaverse.
			- **NVIDIA Omniverse:**
				- **MetaverseScene:**
					- **Properties:**
						- `omniverseNucleusUrl`: (`xsd:anyURI`) The URL of the Omniverse Nucleus server hosting the scene.
						- `usdFile`: (`xsd:anyURI`) The URL or reference to the USD file defining the scene's content.
				- **USD Variance:** Define properties or subclasses within `SceneObject` to represent USD variations and the conditions under which they are activated.
				- **Scene Schema Scaffolding:** Exploring Linked-JSON structures to emulate the more expressive OWL `owl:oneOf` and `owl:someValuesFrom`.
					- Linked-JSON representation for `SceneType`:
					  ```text
					  {
					    "@id": "metaverse:SceneType",
					    "@type": "linkedjson:Class",
					    "rdfs:label": "Scene Type",
					    "rdfs:comment": "Represents different types of scenes in the metaverse.",
					    "metaverse:sceneTypeEnum": [
					      {
					        "@id": "metaverse:InteriorScene"
					      },
					      {
					        "@id": "metaverse:ExteriorScene"
					      },
					      {
					        "@id": "metaverse:MixedRealityScene"
					      }
					    ]
					  }
					  ```
					- Linked-JSON representation for `MetaverseScene`:
					  ```text
					  {
					    "@id": "metaverse:MetaverseScene",
					    "@type": "linkedjson:Class",
					    "rdfs:label": "Metaverse Scene",
					    "rdfs:comment": "Represents a specific environment or space within the metaverse.",
					    "metaverse:hasSceneObject": {
					      "@type": "linkedjson:ObjectProperty",
					      "rdfs:range": "metaverse:SceneObject",
					      "metaverse:minCardinality": 1
					    }
					  }
					  ```
						- In this example, the custom property `metaverse:minCardinality` is used to indicate that a `MetaverseScene` must have at least one `SceneObject`.
			- ## TODO
				- **AI Agent Capabilities:** Expand the capabilities property of `AIAgent` to include specific actions and functions related to Bitcoin, RGB, and Nostr, such as "create_digital_asset", "transfer_ownership", "publish_nostr_event", etc.
				- **Event Logging and Attestation:** Consider adding mechanisms for logging significant events and generating cryptographic attestations, which could be used for dispute resolution or auditing purposes. This would operate on an automated threshold trigger system mediated by LLM, and would wrap the recent interactions between parties in pubkey encrypted data blobs, sending them to both parties alongside a report of the trigger event. This would potentially allow action by the parties in their jurisdictions. The data would then be deleted from the metaverse.
	- ### Visualising
		- http://owlgred.lumii.lv/online_visualization/4qge#
		- [[Some legacy Linked-JSON]]
- This more specific and expanded metaverse ontology featuring a relay based communication protocol, URIs, blockchain wallets, and NVIDIA omniverse, offers a comprehensive and extensible framework generated with the help of GenAI. It offers a glimpse of the potential for automating ontological descriptions for emergent and novel social, digital, collaborative spaces.
- By incorporating a wide range of classes, properties, and relationships, the ontology starts to enable formalisation of:
	- Agents: Human users and AI entities, their attributes, skills, and relationships.
	- Scenes and Objects: Virtual environments, their characteristics, and interactive elements.
	- Digital Assets and Economy: Creation, ownership, and exchange of virtual goods and currencies.
	- Events and Interactions: Social gatherings, communication, and collaborative activities.
	- Governance and Policies: Decision-making processes, rules, and regulations within the metaverse.
	- Infrastructure and Technology: Hardware, software, and networking components enabling the metaverse.
	-
- # AI Agent Refactor
- ```mermaid
  classDiagram

  %% -----------------------------------------------------------------------
  %% 1. CORE: Agent-Oriented Ontology
  %% -----------------------------------------------------------------------
      class AgentOntologyEntity {
          rdfs:label "Agent Ontology Entity"
          rdfs:comment "Root class for all concepts in the agent-focused ontology."
      }

      class Agent {
          rdfs:label "Agent"
          rdfs:comment "A general actor capable of autonomous or semi-autonomous behaviour, including humans and AIs."
          hasSkillProfile SkillProfile
          hasPrivacySetting PrivacySetting
          hasReputationScore ReputationScore
          hasWallet DigitalWallet
          hasInteractionPreference InteractionPreference
      }

      class AIAgent {
          rdfs:label "AI Agent"
          rdfs:comment "An agent driven by AI models, with specialized capabilities for autonomy and decision-making."
          hasAIModel AIModel
          decayFunction xsd:string
      }

      class HumanAgent {
          rdfs:label "Human Agent"
          rdfs:comment "A real human user. May be linked to a WebID for decentralized identity and authentication."
      }

      class AIModel {
          rdfs:label "AI Model"
          rdfs:comment "The underlying neural network or algorithm powering an AI Agent's intelligence."
      }

      class SkillProfile {
          rdfs:label "Skill Profile"
          rdfs:comment "Describes the abilities or competencies an agent (human or AI) can exhibit."
      }

      class PrivacySetting {
          rdfs:label "Privacy Setting"
          rdfs:comment "Rules and preferences regarding data usage, sharing, and retention for an agent."
      }

      class ReputationScore {
          rdfs:label "Reputation Score"
          rdfs:comment "Metric or rating reflecting an agent's trust or standing in a community."
      }

      class DigitalWallet {
          rdfs:label "Digital Wallet"
          rdfs:comment "Holds digital assets, cryptocurrencies, or tokens owned by an agent."
      }

      class InteractionPreference {
          rdfs:label "Interaction Preference"
          rdfs:comment "Specifies how an agent prefers to interact, moderate content, or engage with others."
      }


  %% -----------------------------------------------------------------------
  %% 2. WEBID SUBCATEGORY: Decentralized Identity & Events
  %% -----------------------------------------------------------------------
      class WebIDEntity {
          rdfs:label "WebID Entity"
          rdfs:comment "A conceptual parent for all objects in the WebID ecosystem."
      }

      class Person {
          rdfs:label "Person"
          rdfs:comment "Represents a decentralized identity and profile using WebID."
          webid xsd:anyURI "A unique WebID URI identifying this individual"
          name xsd:string "A handle or short name (distinct from display_name)"
          display_name xsd:string "A more user-facing display name"
          about xsd:string "Short bio/about text"
          picture xsd:anyURI "URI of the user’s profile image"
          banner xsd:anyURI "URI of the user’s banner image"
          website xsd:anyURI "User’s personal website"
      }

      class WebIDEvent {
          rdfs:label "WebID Event"
          rdfs:comment "A generic event referencing a WebID identity, with timestamps, content, and possible signatures."
          authorWebID xsd:anyURI "The WebID of the event author"
          created_at xsd:dateTime "Timestamp of creation"
          content xsd:string "Event content or message"
          signature xsd:string "Digital signature (e.g., for verifiability)"
      }

      class WebIDService {
          rdfs:label "WebID Service"
          rdfs:comment "A decentralized or semi-decentralized service endpoint that stores or routes WebID events."
          serviceURI xsd:anyURI "The endpoint (e.g., HTTPS, WebSocket) providing service functionality"
      }


  %% -----------------------------------------------------------------------
  %% 3. IMMERSIVE REAL-TIME SUBCATEGORY
  %% -----------------------------------------------------------------------
      class ImmersiveRealTimeEntity {
          rdfs:label "Immersive Real-Time Entity"
          rdfs:comment "Base class for objects and concepts within immersive real-time environments (3D spaces, XR, etc.)."
      }

      class ImmersiveScene {
          rdfs:label "Immersive Scene"
          rdfs:comment "A real-time interactive environment or 'scene' supporting multi-agent presence."
          governedBy GovernanceStructure
          hasPolicy ImmersivePolicy
      }

      class GovernanceStructure {
          rdfs:label "Governance Structure"
          rdfs:comment "Rules, roles, and processes governing an immersive scene or community."
      }

      class ImmersivePolicy {
          rdfs:label "Immersive Policy"
          rdfs:comment "Behavioral and content guidelines within a particular immersive real-time environment."
      }

      class SceneObject {
          rdfs:label "Scene Object"
          rdfs:comment "Any interactive or visible object in an immersive real-time scene."
      }

      class DigitalAsset {
          rdfs:label "Digital Asset"
          rdfs:comment "A unique digital good or token that can be owned or traded in immersive or other digital contexts."
          hasOwner Agent
      }

      class VirtualEconomy {
          rdfs:label "Virtual Economy"
          rdfs:comment "A system enabling trade and currency exchange in immersive or other digital environments."
          regulatedBy EconomicPolicy
          hasMarketplace Marketplace
      }

      class Marketplace {
          rdfs:label "Marketplace"
          rdfs:comment "A platform or location to trade digital assets (NFTs, tokens, etc.)."
      }

      class EconomicPolicy {
          rdfs:label "Economic Policy"
          rdfs:comment "Regulations defining currency issuance, exchange rules, and other economic mechanics."
      }


  %% -----------------------------------------------------------------------
  %% 4. RELATIONSHIPS & INHERITANCE
  %% -----------------------------------------------------------------------

      %% A. Ontology Roots & Subcategories
      AgentOntologyEntity <|-- Agent
      AgentOntologyEntity <|-- WebIDEntity
      AgentOntologyEntity <|-- ImmersiveRealTimeEntity

      %% B. Agent Hierarchy
      Agent <|-- AIAgent
      Agent <|-- HumanAgent

      %% C. WebID
      WebIDEntity <|-- Person
      WebIDEntity <|-- WebIDEvent
      WebIDEntity <|-- WebIDService
      HumanAgent -- Person : "may link to/represent"

      %% D. Immersive Real-Time
      ImmersiveRealTimeEntity <|-- ImmersiveScene
      ImmersiveRealTimeEntity <|-- DigitalAsset
      ImmersiveRealTimeEntity <|-- VirtualEconomy
      ImmersiveRealTimeEntity <|-- Marketplace
      ImmersiveRealTimeEntity <|-- GovernanceStructure
      ImmersiveRealTimeEntity <|-- ImmersivePolicy
      ImmersiveRealTimeEntity <|-- SceneObject

      %% E. Detailed Links
      AIAgent "1" *-- "1" AIModel : hasAIModel
      Agent "1" *-- "0..*" SkillProfile : hasSkillProfile
      Agent "1" *-- "0..*" PrivacySetting : hasPrivacySetting
      Agent "1" *-- "0..*" ReputationScore : hasReputationScore
      Agent "1" *-- "0..*" DigitalWallet : hasWallet
      Agent "1" *-- "1" InteractionPreference : hasInteractionPreference

      %% Example of AI Agents creating scene objects (optional)
      AIAgent "1" *-- "*" SceneObject : canCreateVariations

      ImmersiveScene "1" *-- "0..*" GovernanceStructure : governedBy
      ImmersiveScene "1" *-- "0..*" ImmersivePolicy : hasPolicy

      DigitalAsset "1" *-- "1" Agent : hasOwner

      VirtualEconomy "1" *-- "0..*" EconomicPolicy : regulatedBy
      VirtualEconomy "1" *-- "0..*" Marketplace : hasMarketplace

  ```
-
- # Mycelium Experiment
	- This idea is developed further here:
		- [[Agentic Mycelia]]
		- This is an [[Ontology conversation with AIs]]

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

## Related Content: Mixed reality

public:: true

#Public page

	- automatically published
- # Spatial and Augment Reality
- ## Meta Orion
	- {{video https://www.youtube.com/watch?v=G0eKzU_fV00}}
	-
- ## Apple Vision Pro
	- [This thing is mind-blowing 🤯 This demo is better than Apple's actual ads for the Vision Pro. Apple launched the Vision Pro a few days ago,… | Instagram](https://www.instagram.com/reel/C2-G8WgsLSj/?igsh=am93cXF4OWppa29l)
	-
	- This review (for me) asks the right questions, finally, of "spatial".
		- https://www.theverge.com/24054862/apple-vision-pro-review-vr-ar-headset-features-price
	- Is using the Vision Pro so good that I’m willing to mess up my hair every time I put it on?
	- Is it so good that I want to lug it around in its giant carrying case instead of my laptop bag?
	- Is it so good that I want to look at the world around me through screens instead of with my own eyes?
	- Do I prefer using a computer in there rather than out here.
	
	  I stopped working in VR when I tore down my venerable 17 year old augmented virtuality system. That was the right mix, for me, but untenably expensive. Most people will just continue to regard the Apple HMD as an interface that does Apple credit, that they will probably never use.
	
	  They don't know that most of the owners will never use them either, but I do.
- {{video https://www.youtube.com/watch?v=aqTIB_q40bo}}
- [twitter link to the render loading below](https://twitter.com/bilawalsidhu/status/1759060086977143132)
  {{twitter https://twitter.com/bilawalsidhu/status/1759060086977143132}}
	- Meta research paper finally nails down focus with waveguide AR glasses [Waveguide holography for 3D augmented reality glasses | Nature Communications](https://www.nature.com/articles/s41467-023-44032-1)
- [CES (Pt. 2), Sony XR, DigiLens, Vuzix, Solos, EverySight, Mojie, TCL colour µLED – KGOnTech (kguttag.com)](https://kguttag.com/2024/01/24/ces-pt-2-sony-xr-digilens-vuzix-solos-everysight-mojie-tcl-colour-%c2%b5led/)
- [Campus](https://techcrunch-com.cdn.ampproject.org/c/s/techcrunch.com/2022/09/09/roblox-rdc-2022/amp/)
- Vircadia
- O3DE
- [server](https://www.reddit.com/r/O3DE/comments/pbovl9/can_i_develop_my_own_dedicated_server_with_o3de/)
- [Global lighting](https://www.co3dex.com/blog/image-based-lighting-1/#/)
- Unreal
- [Technically this might be a decade away since like everything the primay user base will be mobile mixed realitym, which is contingent on 5G](https://www.matthewball.vc/all/forwardtothemetaverseprimer)
- book, the metaverse and how it will revolutise everything
	- ball2022metaverse
	- challenges
- bandwidth
	- latency
	- global shared truth
	- form factor
	- gpu processing
- [[Humans, Avatars , Character]]
- [the many challenges of XR hardware](https://www.matthewball.vc/all/why-vrar-gets-farther-away-as-it-comes-into-focus)
- HCI
- MoveAI
- [Meta's wrist reader](https://www.from-the-interface.com/wrist-interfaces/)
- [Touch music interface](https://scitechdaily.com/groundbreaking-new-technology-allows-people-to-listen-to-music-through-touch/)
- Interface and tracking
- Pose estimations
- [Standable](https://www.standablevr.com/)
- [Dense face fields from Microsoft](https://microsoft.github.io/DenseLandmarks/)
- Viveverse web3 nonsense
- [Meetungs ARE the work](https://medium.com/@ElizAyer/meetings-are-the-work-9e429dde6aa3)
- Identity
- [Strongnode identity article on venturebeat](https://venturebeat.com/virtual/identity-in-the-metaverse-creating-a-global-identity-system/)
- [Solid-lite](https://solid-lite.org/)
- legal / governance / privacy / safeguarding
- legal jeopardy for celebrities
- [Gang sexual assault vice article](https://www.vice.com/en/article/3abpg3/woman-says-she-was-virtually-gang-raped-in-facebooks-metaverse?)
- [not enough training on safety in africa](https://www.thecable.ng/safeguarding-africans-safety-and-freedom-in-the-metaverse/amp)
- [How Regulation Will Apply To The Metaverse](https://www.forbes.com/sites/nisaamoils/2023/03/01/how-regulation-will-apply-to-the-metaverse/?)
- [Podcast on the law](https://www.reply.com/en/metaminutes-s3-e5-legal-challenges-and-regulation-for-the-metaverse)
- [dai.ki blog post](https://dai.ki/navigating-ai-governance-a-comprehensive-look-at-existing-and-new-eu-and-us-ai-regulations/)
- Bio vertical
	- [NGL protein fold model viewer](https://github.com/nglviewer/ngl)
	- [OpenBioML discord](discord.gg/AMRdyPjwBb)
	- [Nanome on quest pro](https://www.youtube.com/watch?v=Q-V5EQ-FBMc)
	- [Openfold github](https://github.com/aqlaboratory/openfold)
	- [Pymol2 open source visualisation](https://github.com/schrodinger/pymol-open-source)
	- Alphafold OpenAI
	- [Biological structure diffusion](https://github.com/RosettaCommons/RFdiffusion)
- The RFdiffusion code allows for the running of RFdiffusion simulations. The code is written in Python and is available on GitHub. The code includes a number of features, such as the ability to run simulations on multiple processors and the ability to output results in a variety of formats.
- [Diagnostics](https://www.amazon.co.uk/AI-Revolution-Medicine-GPT-4-Beyond/dp/0138200130)
- Market research
- [Addidas](https://www.adidas.com/into_the_metaverse/mint)
- [Bubblepunk interiors ML art](https://www.bubblepunk.io/bubblepunk-interiors)
- [What is a chief metaverse officer (bloomberg)](https://www.bloomberg.com/news/articles/2022-09-22/what-is-a-chief-metaverse-officer-and-do-you-need-one)
- [Userbase struggles (coindesk)](https://www.coindesk.com/markets/2022/04/06/metaverse-majors-struggle-as-user-base-falls-short-of-market-expectations/?outputType=amp)
- [Protecting Brands in the Metaverse’s Uncertain Legal Landscape](https://wwd.com/business-news/technology/metaverse-lawsuit-nike-stockx-hermes-metabirken-fashion-1235247763/)
- [Market research global impact](https://www.analysisgroup.com/globalassets/insights/publishing/2022-the-potential-global-economic-impact-of-the-metaverse.pdf)
- [McDonalds in the metaverse](https://www.businessinsider.com/mcdonalds-metaverse-virtual-online-restaurant-trademark-delivers-food-web3-nft-2022-2)
- [Universal music metaverse / web3 team](https://www.musicweek.com/labels/read/universal-music-group-s-digital-strategy-team-creates-key-roles-for-web3-and-the-metaverse/087103)
- Narratives and convergence
- [With the help of generative AI it may be possible to democratise the externalisation of complex narratives, with these new narratives shaping the outcomes of society through the medium of the metaverse](https://www.epsilontheory.com/narrative-and-metaverse-pt-3-the-luther-protocol/#.YjyHbnLIE5k.twitter)
- [A lot of metaverse recently has just been convergence as companies take their existing simulation and repackage it for the moment.](https://thedriven.io/2022/05/23/nissan-and-mitsubishi-unveil-electric-mini-vehicles-and-test-drives-in-metaverse/)
- Games is the main convergence: from globalblock ""More companies are entering the metaverse as global electronics giant Sony has announced their own metaverse push in the latest annual corporate strategy meeting. Sony said this will be a more focused approach, as they aim to use metaverse-inspired experiences to engage users. As Sony owns PlayStation Brands, one of the biggest install userbases in the world, they are in an amazing position to make an impact. They have also revealed that prior investments in Epic, makers of the Fortnite game, and Bungie, another gaming studio, are part of this push."
- -
- [Epsilomn theory thesis on metaverse](https://www.epsilontheory.com/narrative-and-metaverse-pt-3-the-luther-protocol/#.YjyHbnLIE5k.twitter)
- [Epic games programming language for the metaverse](https://www.geekmetaverse.com/epic-games-launches-verse-the-metaverse-programming-language/)
- [Fortnite is the metaverse](https://www.ign.com/articles/how-fortnite-is-the-antidote-to-metaverse-skepticism)
- [epic unreal for fortnite](https://store.epicgames.com/en-US/p/fortnite--uefn?)
- Why you should use now generative AI in your metaverse company. Or maybe not
	- The Ghost Howls https://skarredghost.com/2023/02/11/generative-ai-metaverse-company/
- BlackRock digs further into crypto with metaverse ETF https://financefeeds.com/blackrock-digs-further-into-crypto-with-metaverse-etf/
- China’s iPhone production hub of Henan bets its future on the metaverse | South China Morning Post https://www.scmp.com/tech/policy/article/3194092/chinas-iphone-production-hub-henan-bets-its-future-metaverse
- Cutting Through the Hotel Hype of the Blockchain, Web3 and the Metaverse | https://hoteltechnologynews.com/2022/08/cutting-through-the-hotel-hype-of-the-blockchain-web3-and-the-metaverse/
- Experts highlight trust and safety practices for the metaverse https://www.techtarget.com/searchcio/news/252525336/Experts-highlight-trust-and-safety-practices-for-the-metaverse
- Global Metaverse Market Analysis Report 2022: Blockchain https://www.globenewswire.com/news-release/2022/08/30/2506629/0/en/Global-Metaverse-Market-Analysis-Report-2022-Blockchain-Solutions-in-Support-of-the-Metaverse-Market-will-Reach-148-6-Billion-by-2027.html
- Identity Management Institute Launches the Metaverse Security Centre and Certified Metaverse Security Consultant (CMSC)™ Certification https://www.prnewswire.com/news-releases/identity-management-institute-launches-the-metaverse-security-centre-and-certified-metaverse-security-consultant-cmsc-certification-301689276.html
- Is the metaverse good for business? Why blending the virtual and the real remains a matter of much debate | Fortune https://fortune.com/2022/07/13/business-metaverse-dropbox-brainstorm-tech/
- Laws and Issues in the Metaverse (2)
	- Lexology https://www.lexology.com/library/detail.aspx?g=5a0cc4c0-a876-474e-a719-f528b71b68ee
- Laying the Foundation of the Metaverse, Streaming Video, Social, Gaming, and Broader Digital Advertising Markets to Collectively Clear US$2 Trillion by 2030 https://www.abiresearch.com/press/laying-the-foundation-of-the-metaverse-streaming-video-social-gaming-and-broader-digital-advertising-markets-to-collectively-clear-us2-trillion-by-2030/
- Major crypto exchange announces its arrival in the metaverse https://cointelegraph.com/news/major-crypto-exchange-announces-its-arrival-in-the-metaverse
- Metaverse exploitation and abuse to rise in 2023: Kaspersky https://cointelegraph.com/news/metaverse-exploitation-and-abuse-to-rise-in-2023-kaspersky
- Metaverse Market Size, Share & Industry Report 2020-2030 https://www.strategicmarketresearch.com/market-report/metaverse-market
- Metaverse Real Estate Gets Reality Cheque https://therealdeal.com/2022/08/04/metaverse-real-estate-gets-reality-cheque/
- Nissan and Mitsubishi unveil electric mini vehicles, and test drives in metaverse https://thedriven.io/2022/05/23/nissan-and-mitsubishi-unveil-electric-mini-vehicles-and-test-drives-in-metaverse/
- Nvidia Sees a Metaverse Populated With Lifelike Chatbot Avatars
	- CNET https://www.cnet.com/tech/computing/nvidia-sees-a-metaverse-populated-with-lifelike-chatbot-avatars/
- Room' Offers a Non-Facebook Way to Connect Coworkers in the Metaverse https://uk.pcmag.com/vr-1/143198/room-offers-a-non-facebook-way-to-connect-coworkers-in-the-metaverse
- The Architecture of the Metaverse (So Far) | ArchDaily https://www.archdaily.com/988639/the-architecture-of-the-metaverse-so-far
- The battle to build a child-friendly metaverse | Tech News https://tech.hindustantimes.com/tech/news/the-battle-to-build-a-child-friendly-metaverse-71655616713236.html
- The Metaverse Casino That Wasn’t https://www.coindesk.com/layer2/sinweek/2022/08/29/the-metaverse-casino-that-wasnt/
- The World's First Virtual Reality Avatar Fashion Week Is On The Metaverse This Week https://womenlovetech.com/the-worlds-first-virtual-reality-avatar-fashion-week-is-on-the-metaverse-this-week/
- Top 10 Metaverse Platforms that will Replace Social Media in Future https://www.analyticsinsight.net/top-10-metaverse-platforms-that-will-replace-social-media-in-future/
- Top 15 Metaverse Companies To Watch Out For !! https://www.cryptotimes.io/top-15-metaverse-companies-to-watch-out-for/
- We're Ready for the Metaverse but the Technology Is Not. Here's Why.
	- Decrypt https://decrypt.co/100781/were-ready-for-the-metaverse-but-the-technology-is-not-heres-why
- [The Photorealistic Metaverse | Welcome to Cornerstone, Cornerstone is a gamified metaverse experience distributed to you via the web browser. Create, co-develop, and monetize your creative idea in the new digital realm.](https://cornerstone.land/)
- [Mega Yacht Sold for $650,000 in Metaverse, Becomes Most-Expensive NFT in Sandbox Virtual World | Technology News , A mega yacht has been sold for a whopping $650,000 (roughly Rs. 4.8 crore) in the Sandbox virtual gaming world. The pricey digital asset was released by metaverse developer Republic Realm as part of a luxury NFT series.](https://gadgets.ndtv.com/cryptocurrency/news/mega-yacht-sold-usd-650000-metaverse-most-expensive-nft-sandbox-virtual-world-2630187)
- [Facebook whistleblower warns Metaverse will repeat ‘all the harms’,Frances Haugen says she is worried about privacy and safety inside Meta’s impending Metaverse. Her accusations of profit before safety are still red hot.](https://cointelegraph.com/news/facebook-whistleblower-warns-metaverse-will-repeat-the-harms)
- [Virtual production and the future of generative art](https://virtualproducer.io/generative-ai-and-the-future-of-filmmaking/)
- [Zuckerburg disengages from metaverse](https://www.thestreet.com/technology/mark-zuckerberg-quietly-buries-the-metaverse)
- [Metahouse Could be First of Many
	- Los Angeles Business Journal digital twin of a mansion](https://labusinessjournal.com/featured/metahouse-could-be-first-of-many/)
- [The Future is a Dead Mall
	- Decentraland and the Metaverse: Clickbait Title: I spent three months living in the metaverse and now I'm starvingThe metaverse salespeople have a weird fixation with Animal Crossing, in sp...](https://www.youtube.com/watch?v=EiZhdpLXZ8Q)
- https://www.infosys.com/iki/perspectives/metaverse-insider-guide.html
- [Everyone abandoning metaverse](https://www.reddit.com/r/CryptoCurrency/comments/128hqkw/meta_microsoft_and_disney_are_reversing_their/)
- [The Internet Is Ruined. The Metaverse Can Still Be Saved: In this nascent stage, there are opportunities for virtual worlds to avoid the mistakes of the past.](https://www.wired.com/story/metaverse-ethics/)
- The article discusses how the internet has ruined the Metaverse, and how it can still be saved. It argues that the internet has made the Metaverse less accessible and has made it more difficult to find information. However, it also states that the Metaverse can still be saved if people are willing to work together to make it more accessible and user-friendly.
- Omniverse
- [Free to individuals](https://blogs.nvidia.com/blog/2022/01/04/omniverse-available-free-to-creators/)
- [Full RTX rendering](https://www.youtube.com/watch?v=Jm155QkRjl0&feature=youtu.be)
- [AI assisted blended character plugin](https://blogs.nvidia.com/blog/2023/02/15/blender-alpha-release-omniverse/?ncid=so-link-466434#cid=ov01_so-link_en-us)
- [NVIDIA Unveils Powerful AI, Simulation and Creative Tools for Creators and Developers of Virtual Worlds | NVIDIA Blog](https://blogs.nvidia.com/blog/2022/08/09/omniverse-siggraph/)
- Open metaverse
- [Open metaverse discord from linux foundation](https://discord.gg/openmetaverse)
- [Free 1 Million objects](https://huggingface.co/datasets/allenai/objaverse)
- Usability
- [bridging the real and the virtual like mcdonalds home delivery](https://www.businessinsider.com/mcdonalds-metaverse-virtual-online-restaurant-trademark-delivers-food-web3-nft-2022-2)
- Virtual land
- virtual
- [hybrid land linking real and virtual (including digital twin)](https://labusinessjournal.com/featured/metahouse-could-be-first-of-many/)
- Simple geo-referencing of physical place in mixed reality
- Digital assistants
- [MultiOn digital assistant](https://multion.ai/)
- [LEON open source assistant](https://docs.getleon.ai/)
- [Open source assistant github of issues](https://github.com/LAION-AI/Open-Assistant/issues)
- Avatars
- [Free VRChat Models & Avatars | VRCMods (other)](https://vrcmods.com/)
- [CLIP-Actor Text-Driven Recommendation and Stylization for Animating Human Meshes](https://clip-actor.github.io/)
- Displaytech
- [CREAL lens display](https://creal.com/2023/05/19/creals-breakthrough-ar-display-real-depth-with-a-classic-lens/)
- Autostereoscopic
- [nanomaterials paper](https://www.mdpi.com/2079-4991/12/3/429#)
- [Bubbles and speakers](https://www.youtube.com/watch?v=7VLdMXnM0lU)
- [lightfield labs](https://www.lightfieldlab.com/)
- The Wall Street Journal has reported that Meta Quest, formerly known as Oculus, had over 6 million monthly active users as of October 2022. This news has generated discussion on the r/virtualreality subreddit, with users speculating about the future of VR and Meta Quest's upcoming products. Some users believe that VR needs more content to attract more users, while others express excitement over the release of Undead Citadel and the possibility of a Starfield VR game. Users also discuss the potential of streaming headsets like the Quest working on the PlayStation 5 and the importance of storytelling in VR games. Some users express disappointment in the launch of the remastered edition of San Andreas and its impact on Quest sales, while others joke about the perceived death of VR. https://www.reddit.com/r/virtualreality/comments/12lpsvf/rumor_meta_quest_had_more_than_6_million_monthly/ Meta urged to prevent minors entering the metaverse https://www.bloomberg.com/news/articles/2023-04-14/meta-urged-to-halt-plans-allowing-minors-into-the-metaverse?leadSource=uverify%20wa
- It's a valuable tool for video gaming, film industry, and metaverse applications that require 3D scenes. The article discusses how the internet has ruined the metaverse, and how it can still be saved. It argues that the internet has made the metaverse less accessible and more difficult to navigate, and that this has had a negative impact on its potential. The article suggests that the metaverse can still be saved if we take steps to improve its accessibility and make it easier to use.
   <https://www.wired.com/story/metaverse-ethics/> Exploring Why the Metaverse Hasn't Taken Off as Expected: The metaverse has quickly turned from a profitable utopia into a cash-guzzling dystopia.The text provides an overview of the Metaverse, a virtual world that has not yet taken off as expected. The text describes the potential reasons for this, including the lack of a clear business model and the difficulty of creating an immersive experience.
   <https://www.bbntimes.com/technology/exploring-why-the-metaverse-hasn-t-taken-off-as-expected> Unfortunately, the requested website (mirror-next-hop.forbes.com) is not accessible and the current session has been terminated with an access denied error (403). Further information can be obtained by contacting the website administrators using the provided reference code (217.138.196.24 2023-06-26T16:51:46.121Z). https://www.forbes.com/sites/charliefink/2023/05/07/this-week-in-xr-after-ai-sucks-the-air-out-of-the-metaverse-it-will-remake-xr/ The Building the Metaverse YouTube channel features a range of videos discussing the use of generative AI and large language models (LLMs) in game development and world-building. One video features a discussion with Kayla Comalli, co-founder and CEO of Lovelace Studios, about their platform Nyric, which generates entire worlds from a text prompt using generative AI technologies like ChatGPT. Other videos on the channel cover topics such as generative art assets for games, AI storytelling and narrative, generative graphics workflow for games, and the use of generative AI in game production. The channel also features discussions with individuals in related fields, such as Edward Saatchi of Fable Studio, who talks about virtual beings and simulated worlds, and Adam B. Levine of Blockade Labs, who discusses Skybox AI and game development. Additionally, there are videos discussing the potential applications of AI in other areas, such as defence and longevity. The channel provides a valuable resource for those interested in learning about the intersection of generative AI and gaming, as well as related topics. https://www.youtube.com/watch?v=fsg83BvsXww The article discusses the potential risks and opportunities of motion tracking data in extended reality (XR) and the metaverse. While this data is often presumed to be innocuous, recent studies have shown that it has the potential to profile and deanonymize XR users, posing a significant threat to security and privacy in the metaverse. The article highlights the need for increased awareness and caution regarding the collection and use of motion data in XR and metaverse experiences. https://arxiv.org/abs/2306.06459 Beauty company Coty has created a metaverse for its 11,000 global employees, using technology from Spatial. The virtual campus is based on 3D tech and tools from Spatial, and aims to develop upskilling and future innovation for Coty brands. The campus includes features such as text and vocal chat forums, screen and filesharing, customisable avatars and location exploration and quest fulfilment, as well as a "phygital" rewards system. Coty and Spatial said the campus was a significant milestone in crafting "new models for scalable gamified experiences". https://drugstorenews.com/coty-enters-metaverse-campus-global-workforce Tencent Cloud has announced its commitment to support the development of the Web3 ecosystem at the first global Web3 summit. The cloud business of Tencent has unveiled a development roadmap for a full suite of blockchain API services and its Tencent Cloud Metaverse-in-a-Box offering. The platform will provide technical support for Web3 and work with industry partners to nurture the Web3 ecosystem. Tencent Cloud plans to promote sustainable growth by offering Web3 builders cloud solutions credits, marketing workshops and publicity opportunities. It has also signed a Memorandum of Understanding with Ankr, a Web3 infrastructure provider, to jointly develop a full suite of blockchain API services. The API services will cover security, storage, identity management, middleware, development tools, and data analytics, among other areas. https://www.tencentcloud.com/dynamic/news-details/100437?lang=en&pg=
   <https://www.tencentcloud.com/dynamic/news-details/100437?lang=en&amp;pg=>
- The concept of a "predictive metaverse", an artificial intelligence (AI)-powered advanced form of a virtual world that could predict and anticipate its users' intentions and behaviours, is offering new ways to improve engagement, creativity and personalised learning in [[Education and AI]], according to an article in eSchool News by Roger James Hamilton, Founder and CEO of Genius Group. As virtual reality worlds become increasingly sophisticated and realistic, they are also becoming more intelligent and able to analyse data in real-time to deliver tailored recommendations and feedback to individual users, for example to optimise the virtual marketplace to improve user experience and increase sales. https://www.eschoolnews.com/educational-leadership/2023/04/21/predictive-metaverse-the-future-of-guided-learning/
- Reddit and its partners use cookies and similar technologies to improve the quality of its website, personalize content and advertising, measure advertising effectiveness, and ensure the proper functionality of its platform. By accepting all cookies, users agree to the use of cookies. However, by rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of its platform. Reddit's Cookie Notice and Privacy Policy provide more information about its use of cookies. Recently, Meta, Microsoft, and Disney have reversed their bets on the metaverse. Posts related to this topic on r/CryptoCurrency have been removed by moderators. https://www.reddit.com/r/CryptoCurrency/comments/128hqkw/meta_microsoft_and_disney_are_reversing_their/ Exokit, Adrian's opensource mixed reality toolkit for web <https://github.com/exokitxr/exokit>
- [[Apple]] has acquired Mira, a Los Angeles-based startup that creates AR headsets for various companies and the US military. The acquisition, which was confirmed by [[Apple]], follows the company's launch of its $3,499 mixed reality headset, the Vision Pro. Two former Mira employees said that Jony Ive, [[Apple]]'s former design chief, was an advisor to the startup at one point. Mira's military contracts include a small agreement with the US Air Force and a $702,351 agreement with the Navy while its contract with Nintendo World provides headsets for the Mario Kart ride at its theme parks in Japan and LA's Universal Studios. It is currently unknown if [[Apple]] will continue Mira's military contracts. https://www.theverge.com/2023/6/6/23751350/[[apple]]-mira-ar-headset-startup
- These posts include insights on the development of XR technology, possible applications, and innovations in the field. https://www.linkedin.com/posts/reneschulte_visionpro-ar-vr-activity-7072953336156602369-_2pL?utm_source=share&utm_medium=member_android
- [This text is a brief description of a position paper published by the OMA3 Portaling and Mapping Working Group (PMWG) on the transformative technology of portals in the metaverse. The position paper provides insights into the potential of a universal portal system to reshape digital interaction for consumers, businesses, and platforms. It highlights the development process, strategic approach, and vision of OMA3 in setting new standards for the Web3 universe. The paper invites readers to comment on it by creating an issue or commenting in the Google Doc. It also encourages individuals to join OMA3 and participate in the Portaling and Mapping Working Group if they would like to contribute to the project. The paper is licenced under a Creative Commons Attribution 4.0 International Licence.](https://github.com/oma3dao/portal-position-paper)
- [The Open Metaverse Alliance for Web3 (OMA3) has announced the Inter-World Portaling System (IWPS) project, aimed at creating standards for seamless travel between metaverse platforms. OMA3, based in Zug, Switzerland, is a consortium of top metaverse companies in Web3. The IWPS project will allow users to walk through inter-world portals and travel between metaverse platforms such as Alien Worlds, My Neighbour Alice, and Sandbox. OMA3 believes that IWPS has the potential to enhance accessibility and engagement within the digital realm by bridging disparate metaverse environments. They compare IWPS to the development of transportation technology like railroads and highways in the industrial revolution and the introduction of the HTTP standard in the digital realm, both of which facilitated the free flow of goods, services, and information. OMA3 has released a position paper outlining the importance of IWPS and inviting participation and comments from the Web3 metaverse community. They see the development and standardization of IWPS as the next frontier in the evolution of the metaverse, enabling new levels of connectivity, commerce, and shared experiences.](https://venturebeat.com/games/oma3-offers-way-for-users-to-travel-between-blockchain-gaming-worlds-in-the-metaverse/)
- [Beatoven.ai is a website that uses advanced AI music generation techniques to create unique, mood-based music for videos and podcasts. Users can start by choosing a genre or style that suits their theme and then make cuts to reflect different moods throughout their content. With a rich selection of 16 moods to choose from, users can easily find the right mood for each cut. Once the desired moods have been selected, users can hit compose and let the AI algorithm generate a unique track for them.   The website is useful for various types of content creators, including agency/production houses, YouTube creators, podcast creators, indie game developers, audiobook producers, and web3 and metaverse companies. It offers a range of benefits, such as packing a punch in videos, creating a signature sound for YouTube channels, making intro and outro sections special for podcasts, designing themes and background music for games, elevating audio books with atmospheric music, and providing background music for metaverse experiences.   Beatoven.ai also allows users to customise the length, genre, mood, and instruments of their tracks. The resulting music is production-ready with industry-standard mixing and mastering.   The licencing terms for the music on Beatoven.ai grant users a perpetual licence for usage on their chosen platforms. All copyrights for the music created on the website belong to Beatoven Private Limited.   The website offers a free membership option, allowing users to create and download music for the first 15 minutes of their projects. There is also a premium pricing plan available for unlimited usage.   Overall, Beatoven.ai is a versatile and user-friendly platform that offers customised, royalty-free music for a wide range of content creators.](https://www.beatoven.ai/)
- [A predictive metaverse is an advanced virtual world powered by AI and machine learning algorithms. It can predict and anticipate the actions and behaviours of its users, allowing for personalized recommendations, predictions, and feedback. This concept is becoming increasingly appealing to content creators and educators in the field of education, as it can improve engagement and creativity and create personalized learning programmes.  In a predictive metaverse, AI algorithms can analyse real-time data to understand the preferences, behaviours, and intentions of users. This information can then be used to optimise the virtual world and improve the user experience. For example, an AI algorithm could predict user behaviour in a virtual marketplace, such as what they are likely to buy and when they are likely to buy it. This data can be used to optimise the marketplace and increase sales.  While the term metaverse is often associated with the gaming industry, its potential applications in education are significant. The predictive metaverse can enhance virtual learning by providing personalized guidance and support to students. It can help create immersive learning experiences and improve student engagement and motivation.  As virtual worlds become more sophisticated and realistic, the predictive metaverse holds great promise for the future of guided learning. By harnessing the power of AI and machine learning, educators can create personalized learning experiences that cater to the unique needs and preferences of each student. This technology has the potential to revolutionize education by providing tailored instruction, real-time feedback, and personalized recommendations, ultimately improving student outcomes and overall learning experiences.](https://www.eschoolnews.com/educational-leadership/2023/04/21/predictive-metaverse-the-future-of-guided-learning/)
- [This text is a Reddit post from the r/CryptoCurrency subreddit. The post mentions that Meta (formerly known as Facebook) as well as Microsoft and Disney are reversing their bets on the metaverse. However, the post has been removed by the subreddit moderators. The comments in the post discuss the current hype around artificial intelligence (AI) and the need for companies to hop on that trend. Some users express their opinions that these companies went about their approach to the metaverse in the wrong way. The post also includes comments about the ownership of a bot that has received a high number of moons (a cryptocurrency earned on the Reddit platform) and speculation on the future of meta platforms like Meta. The post is followed by a list of related crypto news articles from various sources, covering topics such as refunds in crypto scams, acquisitions of Bitcoin, changes in cryptocurrency taxes, and the launch of web3 games by Ubisoft.](https://www.reddit.com/r/CryptoCurrency/comments/128hqkw/meta_microsoft_and_disney_are_reversing_their/)
- [Tencent Cloud, the cloud business of global tech company Tencent, has announced its commitment to support the development of the Web3 ecosystem. The company unveiled its development roadmap for a full suite of blockchain API services and its Tencent Cloud Metaverse-in-a-Box offerings. It aims to provide a strong technological foundation for Web3 builders and be the digital enabler for the Web3 industry. Tencent Cloud will collaborate with Web3 partners to accelerate the adoption of Web3. The company also signed a Memorandum of Understanding (MoU) with Web3 infrastructure provider Ankr to jointly develop a full suite of blockchain API services. Additionally, Tencent Cloud announced strategic collaborations with Avalanche, Scroll, and Sui, three other Web3 blockchain partners, to build a stronger foundational infrastructure for global builders. The collaboration with Avalanche will explore blockchain solutions for enterprise customers, while the partnership with Scroll aims to scale Ethereum through an open-sourced zk-Rollup. The collaboration with Sui will optimise the on-chain gaming experience. Furthermore, Tencent Cloud introduced Tencent Cloud Metaverse-in-a-Box, a comprehensive solution that integrates infrastructure, products, SDKs, and low-code solutions. The Metaverse-in-a-Box allows businesses to develop metaverse applications rapidly. Tencent Cloud hosted its first global Web3 summit, Tencent Cloud Web3 Build Day, to discuss the latest blockchain landscape and development trends in Web3 games and social networks.](https://www.tencentcloud.com/dynamic/news-details/100437?lang=enandpg=)
-
- BrahmGAN is a cutting-edge 3D content creation tool that uses advanced technology such as NeRF, GAN, and Blockchain to create 3D content effortlessly, without requiring technical expertise or complex software. The tool is designed for industries such as eCommerce, XR, and Gaming. BrahmGAN's NeRF solutions for VR Services are capable of creating stunning worlds within days. The company is based in Bengaluru and Singapore. Interested parties can contact them via email at info@brahmgan.com or join their mailing list to stay updated on their latest developments. http://www.brahmGAN.ai
- Hallway is an app that empowers creators to tell their stories using avatars instead of their real faces, making it easy for anyone to express their creativity without the limitations of video as a medium. It is a single-camera app that supports a variety of avatars and requires no custom hardware or clunky setups. The app is currently taking VTuber and 2D/3D artist intakes to join the waitlist for early access. Hallway provides a new avenue for creators to express themselves and join the next generation of creators. http://joinhallway.com/
- BrahmGAN is an AI-based 3D content creation platform that uses advanced technologies like NeRF, GAN, and Blockchain for quick and decentralized content creation. BrahmGAN's text-to-3D tools enable users to create 3D content without technical expertise or complex software. BrahmGAN caters to industries like XR, gaming, and eCommerce and uses NeRF for video to 3D, and Blockchain for democratizing 3D content creation globally. BrahmGAN has offices in Bangalore and Singapore and has clients like Taanga Studios. http://www.brahmGAN.ai
- [In this post on Reddit, a user shares their experience developing a C++ library for running Stable Diffusion, an AI image generation model. They explain that the library does not rely on Python and can use the GPU for executing the AI models involved. The user's main motivation for developing this library was to use its image synthesis capabilities in real-time 3D software written in C++.   The user shares their first results, which include a simple library available as an integration-ready MIT licenced Nuget package, capable of running Stable Diffusion models in ONNX format. They note that the code is currently targeting Windows, but only a small portion related to image editing tasks relies on the WinAPI, which can easily be replaced for other platforms.  Several redditors comment on the post, expressing interest in the library and discussing their own experiences with Stable Diffusion and C++ implementations in machine learning. Some users appreciate the user interface design of the library, while others discuss the advantages and disadvantages of using Python for machine learning tasks.  The user also mentions that they are working on an Xbox release and have already generated Stable Diffusion images on the Xbox. There are further discussions on related topics, such as RAID arrays, graphics libraries for Rust, and C++ language support in Xcode 15.  Overall, the post provides an overview of a C++ library for running Stable Diffusion and highlights the user's experiences and progress in developing it.](https://www.reddit.com/r/cpp/comments/143olej/an_open_source_library_for_running_stable/)
- [The text is a Reddit post discussing the process of generating and applying AI-generated images to a 3D model. The poster shares the results of their image generation using AI and provides details about the rendering process. They mention using Blender and a custom UV map to project the generated image onto the face of the 3D model.  The poster explains that they manually removed diffuse and specular reflections during the AI image generation stage and conducted additional tests to bring out more details. They aim to create a colour map that can be used without adjustments.  Other users in the comments ask about the training process and suggest alternative methods for projection mapping. The poster responds, stating that they did additional training with images that suppress shadows, light, and AO. They also mention that the AI-generated image was baked into a custom UV map before rendering.  Some users express interest in learning the process and suggest creating a tutorial. The original poster mentions that the process takes a lot of time and effort and that the results may vary. They recommend referring to their previous test articles for more information on AI image generation.  The post concludes with a list of related posts from other subreddits discussing topics such as computer vision, AI-generated avatars, and visualizations in Stable Diffusion.](https://www.reddit.com/r/StableDiffusion/comments/11ol47u/3d_model_face_color_map_generation_test3/)
- The article discusses Reddit's use of cookies and similar technologies to provide a better experience to users. By accepting all cookies, users agree to Reddit's use of cookies to provide and maintain their services, personalize content and advertising, and measure advertising effectiveness. If users reject non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of the platform. Additionally, the article includes a user-generated post on using Mixamo, a free website with a large library of 3D animations, for character posing. Several commenters offer their own suggestions and recommendations for related tools and workflows on different subreddits. https://www.reddit.com/r/StableDiffusion/comments/11owo31/something_that_might_help_ppl_with_posing/
- Mixed reality, spatial, metaverse and telecollaboration
- [The text discusses the use of cookies and similar technologies on Reddit's platform. By accepting all cookies, users agree to the use of cookies to improve their experience, deliver and maintain services, personalize content and advertising, and measure the effectiveness of advertising. Rejecting non-essential cookies still allows Reddit to use certain cookies for proper platform functionality. The text also mentions that more information can be found in the Cookie Notice and Privacy Policy.  The rest of the text is a Reddit post from the r/StableDiffusion subreddit. The post discusses a generalist model that the OP (original poster) will be releasing soon. The model is capable of creating images with resolutions ranging from 1024 to 1080p, and it is fine-tuned on SD 2.1 768X. The model can generate a variety of images, including photorealism, paintings, and anime. The OP shares some example images generated during the training process and invites others to test prompts for image creation. The post receives several comments and discussions about the model's capabilities, potential uses, and suggestions for improvement.  Additionally, the text includes a list of other posts from various subreddits such as r/3dsmax, r/colorists, and r/StableDiffusion. These posts cover topics related to rendering, colour grading, using AI-generated visuals, and showcasing artistic works.](https://www.reddit.com/r/StableDiffusion/comments/13j78fo/some_examples_of_the_generalist_model_i_will_be/)
- Scan the World is a museum of sculptures, artefacts, and statues made possible through 3D scanning and printing technology. The scans of these historical pieces are free to download for accessibility, educational and cultural heritage purposes. Scan the World uses photogrammetry to capture these high-resolution scans, from digital archaeology to downloadable monuments and buildings, 3D printing enthusiasts will be able to find it at the museum. The museum offers more than 20 categories ranging from Africa to South America and everything in between, and it ranks objects based on popularity, date published, and views. Visitors have instant access to the MyMiniFactory library, which is community-powered with users able to upload their 3D printable designs as well. The ultimate goal of Scan the World is to make historical and artistic objects more widely available to people around the world. It’s the perfect solution for those who want to own an authentic, accurate replica of an important sculpture, artefact, or more. https://www.myminifactory.com/category/scan-the-world
- LinkedIn uses both essential and non-essential cookies to provide, secure, analyse and improve their services, as well as show users relevant ads on and off the platform, according to their Cookie Policy. Users can accept or reject non-essential cookies for this use and can update their choices at any time in their settings. In a post by Rene Schulte, the Head of 3D & Quantum CoPs at Microsoft, he shared resources for Unity developers to prepare for developing for Apple's VisionPro and visionOS. The post sparked conversations and comments from other professionals in the AR/VR industry, including discussions about AR's utility, developments in volumetric video and tracking, and the use of digital twins in building. https://www.linkedin.com/posts/reneschulte_visionpro-ar-vr-activity-7072953336156602369-_2pL?utm_source=share&utm_medium=member_android
- [The text discusses a research paper on a diffusion model called 3DiM for 3D novel view synthesis. The model takes a single input view and generates consistent and sharp completions across many views. It uses a pose-conditional image-to-image diffusion model that takes a source view and pose as inputs and generates a novel view for a target pose as output. The model employs stochastic conditioning, where a random conditioning view is selected from previously generated views at each denoising step, to improve 3D consistency. The paper introduces a new evaluation methodology called 3D consistency scoring to assess the 3D consistency of the generated objects. The model is geometry-free, does not rely on hyper-networks or test-time optimization, and can easily scale to a large number of scenes.  The paper presents samples generated by 3DiM trained on the ShapeNet dataset. The model achieves high fidelity and approximate 3D consistency in generating completions from a single view. It also demonstrates the model's effectiveness by generating 3D objects from in-the-wild images downloaded from the internet. The paper compares 3DiM to prior work on the SRN ShapeNet benchmark and shows that 3DiM outperforms other methods in terms of generating sharp samples. The paper also discusses the technical details of 3DiM, including its generation process using stochastic conditioning and the modifications made to the image-to-image UNet model to achieve high-quality results.  Overall, the paper highlights the effectiveness of diffusion models for 3D novel view synthesis and introduces novel techniques, such as stochastic conditioning and 3D consistency scoring, to improve the quality and consistency of generated views. The proposed model, 3DiM, shows promising results in generating realistic and consistent 3D objects from a single input view.](https://3d-diffusion.github.io/)
- [The paper presents a monocular depth estimation method using denoising diffusion models. The goal is to generate accurate depth maps from single RGB images. The authors address the problem of noisy and incomplete depth maps in the training data by using step-unrolled denoising diffusion, an L1 loss, and depth infilling during training.  To overcome the limited availability of supervised training data, the authors leverage pre-training on self-supervised image-to-image translation tasks. Despite the simplicity of the approach, their model achieves state-of-the-art (SOTA) performance on the indoor NYU dataset and near SOTA results on the outdoor KITTI dataset.  The approach involves infilling missing depth in ground truth depth maps using nearest neighbour interpolation. Then, noise is added to the depth map and a neural network is trained to predict the noise given the RGB image and noisy depth map. During fine-tuning, one step of the forward pass is unrolled and the ground truth depth map is replaced with the model's prediction.  The DepthGen model achieves an absolute relative error of 0.074 on the indoor NYU dataset and a competitive relative error of 0.064 on the outdoor KITTI dataset, demonstrating its accuracy in depth estimation.  The paper also introduces a text-to-3D pipeline that combines DepthGen with off-the-shelf text-to-image and text-conditioned image completion models. This pipeline allows for generating 3D point clouds from text prompts.  In conclusion, the proposed method of monocular depth estimation using diffusion models achieves state-of-the-art performance, even with limited supervised training data. The approach is simple yet effective and can be integrated into a text-to-3D pipeline for generating 3D scenes from text prompts.](https://depth-gen.github.io/)
- [The text provided is a collection of video titles and descriptions related to Blender, AI, and 3D design. The videos cover topics such as creating isometric rooms, using AI in 3D design, Unreal Engine, toon shading in Blender, QR code art, GPT (Generative Pre-trained Transformer) engineering, creating Ghibli-style characters, new features in Blender 3.6, animation in Blender, and adding vegetation in Twinmotion. The videos are created by various individuals and brands, including vertex vendor, Unreal Sensei, Quick QR Art, ENFANT TERRIBLE, Matt Wolfe, Ian Wootten, Brandon's Drawings, Polyfjord, Charlie Barber, and vishal panjeta. The text also mentions a Google company and provides information about cookie usage and privacy settings when using Google services.](https://www.youtube.com/watch?v=GZO7TAlVE_8)
- [WebXR is a device API that allows for VR/AR experiences through web browsers. However, monetization has been a major issue for the platform, with indie creators struggling to capture value. Most WebXR apps appear as prototypes because developers find it difficult to justify investing more resources into the ecosystem. The current ways people pay for WebXR content include purchasing tickets, using cryptocurrency for virtual land, and accessing certain features by login or ownership of bot handles. The process of paying for WebXR content can be made easier and more frictionless by integrating payment methods like Apple Pay or Google Pay while in VR. Artists can get paid through various means such as commissions, Patreon, grants, VC investment, and event tickets. Non-payment based monetization strategies like advertising are also being explored. A list of 101 ideas for WebXR monetization includes platforms like Patreon and Github Sponsors, virtual market stalls, virtual land parcels, and in-world advertising. Other strategies include payment processing integration with platforms like PayPal or Discord, creating virtual actors and performers, storytelling, and podcast sponsorships. Advertisements targeted at 18-44 year old males interested in software, gaming, and VR have shown promising results. A Github repository for WebXR monetization examples is in progress. Despite these efforts, monetization in the WebXR ecosystem is still a work in progress, and more exploration and innovation is needed.](https://hackmd.io/@xr/monetization)
- The paper proposes a system called CLIP-Actor, which animates a 3D human mesh to conform to a text prompt by recommending a motion sequence and optimising mesh style attributes. The system's novelty lies in its ability to recommend motion that conforms to the prompt in a pose-agnostic and temporally-consistent manner while leveraging multi-frame human motion and rejecting poorly rendered views. The authors demonstrate that CLIP-Actor produces plausible and human-recognizable style 3D human mesh in motion with detailed geometry and texture solely from a natural language prompt. The paper's methodology shows that CLIP-Actor is an effective and efficient way to generate plausible results when the pose of an artist-designed mesh does not conform to the text prompt from the beginning. The research has been sponsored by the Korean government's grant funded by the Institute of Information & communications Technology Planning & Evaluation (IITP). https://clip-actor.github.io
- [The paper CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes proposes a system for animating human meshes based on text prompts. The system, called CLIP-Actor, generates motion sequences and optimises mesh style attributes to conform to a given text prompt.  The authors highlight a limitation of previous work, which struggled to produce realistic results when the starting pose of a pre-designed mesh did not align with the text prompt. To address this issue, CLIP-Actor leverages a large-scale human motion dataset with language labels to build a text-driven human motion recommendation system. It suggests a motion sequence that aligns with the given prompt in a coarse-to-fine manner.  In addition, the authors introduce a novel neural style optimization technique that adds detail and texture to the recommended mesh sequence in a temporally-consistent and pose-agnostic manner. They also propose spatio-temporal view augmentation and mask-weighted embedding attention techniques to stabilize the optimization process by incorporating multi-frame human motion and rejecting poorly rendered views.  The results of CLIP-Actor demonstrate its ability to generate plausible and human-recognizable 3D human meshes in motion with detailed geometry and texture solely from natural language prompts.  The paper includes the BibTeX citation for academic referencing and acknowledges the support received from the Institute of Information and Communications Technology Planning and Evaluation (IITP) in Korea for funding the research.  The website containing the paper and code is licenced under a Creative Commons Attribution-ShareAlike 4.0 International Licence. The source code for the system is mainly borrowed from Keunhong Park's Nerfies website, and feedback and questions can be directed to Kim Ji-Yeon.](https://clip-actor.github.io)
- [Magic123 is a two-stage solution for generating high-quality, textured 3D meshes from a single unposed image. The first stage optimises a neural radiance field to create a coarse geometry, while the second stage uses a memory-efficient differentiable mesh representation to produce a high-resolution mesh with realistic textures. The 3D content is learned through reference view supervision and guided by both 2D and 3D diffusion priors. The system includes a tradeoff parameter that controls the balance between exploring novel geometries and achieving precise results. Textual inversion and monocular depth regularization are employed to ensure consistent appearances across views and prevent degenerate solutions. Magic123 outperforms previous image-to-3D techniques, as demonstrated through experiments on synthetic benchmarks and real-world images.  The Magic123 pipeline consists of two stages: coarse and fine. In the coarse stage, an Instant-NGP neural radiance field is optimised to reconstruct a rough geometry. In the fine stage, a DMTet mesh is initialised from the coarse output and optimised to generate a high-resolution mesh with textures. Textural inversion is used in both stages to preserve object geometry and ensure consistent textures across views.  Example generated objects from Magic123 show photo-realistic 3D representations created from single images. The system offers a tradeoff between 2D and 3D priors, allowing for exploration and imagination in geometry generation or precise results with reduced details. By combining both priors and adjusting the tradeoff parameter, Magic123 consistently produces identity-preserving 3D content with fine-grained geometry and visually appealing textures.  Quantitative evaluations on the NeRF4 and RealFusion15 datasets demonstrate the effectiveness of Magic123 compared to previous state-of-the-art approaches. The system achieves top performance across various metrics, showcasing its ability to generate high-quality 3D representations.  The article credits DreamFusion authors for their website templates, and the text is licenced under a Creative Commons Attribution-ShareAlike 4.0 International Licence.](https://guochengqian.github.io/project/magic123/)
- [The paper Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors presents a two-stage approach for generating high-quality, textured 3D meshes from a single unposed image.   In the first stage, a neural radiance field is optimised to create a coarse geometry. In the second stage, a memory-efficient differentiable mesh representation is used to generate a high-resolution mesh with visually appealing texture.   To learn the 3D content, reference view supervision and novel views guided by a combination of 2D and 3D diffusion priors are employed in both stages. A trade-off parameter controls the balance between exploration and exploitation of the generated geometry.   Textual inversion and monocular depth regularization techniques are also used to ensure consistent appearances across views and prevent degenerate solutions.   The approach, called Magic123, outperforms previous image-to-3D techniques according to extensive experiments on synthetic benchmarks and real-world images.   The code, models, and generated 3D assets are available on GitHub.](https://huggingface.co/papers/2306.17843)
- This is a conversation and code change history on the GitHub platform for the "dream-textures" repository. It appears that the main topic of discussion is the addition of a new feature called "Project Dream Texture operator". This feature involves the use of depth-to-image projection to apply a texture to a mesh based on user input. The conversation includes comments and feedback from users who have tested the feature, as well as responses from the repository owner who is implementing the changes. There are also code commits and updates to various files related to the feature and its implementation. https://github.com/carson-katri/dream-textures/pull/409
- This text appears to be a GitHub pull request for the "dream-textures" repository, with the pull request titled "Add Project Dream Texture operator". The pull request adds functionality that allows users to project a texture onto a mesh using a text prompt and depth data. The pull request includes a log of commits and comments from users who have tested the functionality and provided feedback. It seems that the pull request has been approved by at least two reviewers and has been merged into the main branch of the repository. https://github.com/carson-katri/dream-textures/pull/409
         |
       |
-
-
- [SDFStudio is a unified and modular framework for neural implicit surface reconstruction. It provides an implementation of three major implicit surface reconstruction methods: UniSurf, VolSDF, and NeuS. The framework also supports various scene representations, point sampling strategies, and incorporates advances in monocular cues, geometry regularization, and multi-view consistency. The modular implementation of SDFStudio makes it easy to transfer ideas from one method to another. The repository includes documentation, datasets, and examples for users to get started.  To use SDFStudio, users need to set up the environment by installing CUDA and creating a Conda environment. The framework requires Python 3.7 or higher. Users can install the necessary dependencies, including PyTorch and tiny-cuda-nn, using pip. After installing the dependencies, users can clone the SDFStudio repository and install it using pip. Tab completion can be enabled for better user experience.  To train a model, users can download test data and train a model on a specific dataset using the provided commands. SDFStudio supports different models and parameters can be modified to train different models. The training progress can be tracked using visualization tools such as the viewer, Tensorboard, or Weights and Biases.  Once a model is trained, users can export the mesh and render it. The repository provides commands for extracting the mesh and rendering it. Video rendering and customization of the camera path are also supported. Advanced options include training models other than NeuS-facto and modifying the configuration.  SDFStudio is built on top of the Nerfstudio project and incorporates contributions from various developers. If the library is used or the documentation is found useful, the authors request users to consider citation.  The repository includes a comprehensive README file that provides detailed instructions and explanations for using SDFStudio. It also provides information about the contributors, licence, and other resources.](https://github.com/autonomousvision/sdfstudio)
- [The text summarizes a GitHub repository called CLIP-Actor, which is a pytorch implementation for the ECCV 2022 paper, CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. CLIP-Actor is a system for text-driven motion recommendation and neural mesh stylization for human mesh animation. The repository contains code developed on Ubuntu 18.04 with Python 3.7, CUDA 10.2, and PyTorch 1.9.0. The system requirements include Python 3.7, CUDA 10.2, and a single GPU with a minimum of 24 GB RAM. The repository provides instructions for setting up the environment and installing the required dependencies. It also provides instructions for downloading the necessary body models and datasets. The repository includes example commands to generate stylized 4D human avatars based on prompts, such as a scuba diver is scuba diving or Freddie Mercury is dancing. The outputs include final video files, stylized .obj files, colored render views, and screenshots. The repository includes a citation for the paper and acknowledges the prior work that inspired the CLIP-Actor implementation.](https://github.com/youwang-kim/clip-actor)
- [Mixed reality design guidelines from Meta](https://developer.oculus.com/resources/mr-design-guideline/)

## Current Landscape (2025)

- Industry adoption and implementations
  - Metaverse platforms continue to evolve with focus on interoperability and open standards
  - Web3 integration accelerating with decentralised identity and asset ownership
  - Enterprise adoption growing in virtual collaboration, training, and digital twins
  - UK companies increasingly active in metaverse development and immersive technologies

- Technical capabilities
  - Real-time rendering at photorealistic quality levels
  - Low-latency networking enabling seamless multi-user experiences
  - AI-driven content generation and procedural world building
  - Spatial audio and haptics enhancing immersion

- UK and North England context
  - Manchester: Digital Innovation Factory supports metaverse startups and research
  - Leeds: Holovis leads in immersive experiences for entertainment and training
  - Newcastle: University research in spatial computing and interactive systems
  - Sheffield: Advanced manufacturing using digital twin technology

- Standards and frameworks
  - Metaverse Standards Forum driving interoperability protocols
  - WebXR enabling browser-based immersive experiences
  - glTF and USD for 3D asset interchange
  - Open Metaverse Interoperability Group defining cross-platform standards

## Metadata

- **Last Updated**: 2025-11-16
- **Review Status**: Automated remediation with 2025 context
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable