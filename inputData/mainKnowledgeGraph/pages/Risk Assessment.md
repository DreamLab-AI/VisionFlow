- ### OntologyBlock
  id:: risk-assessment-ontology
  collapsed:: true
	- ontology:: true
	- term-id:: AI-0078
	- preferred-term:: Risk Assessment
	- source-domain:: ai
	- status:: draft
	- public-access:: true


### Relationships
- is-subclass-of:: [[AIRiskManagement]]

# Updated Ontology Entry: Risk Assessment

## Academic Context

- Systematic evaluation of artificial intelligence-related risks through structured methodologies
  - Encompasses identification, analysis, and prioritisation of threats across technical, ethical, social, and operational dimensions
  - Evolved from traditional risk management frameworks adapted specifically for AI systems' unique characteristics
  - Foundational principle: risks must be assessed before treatment decisions can be made effectively
  - Recognition that AI systems present novel challenges compared to conventional engineering disciplines—the breadth of potential applications makes comprehensive risk anticipation genuinely difficult[6]

## Current Landscape (2025)

- Industry adoption and implementation frameworks
  - NIST AI Risk Management Framework provides the primary structured approach, following a "Map-Measure-Manage" cycle[1]
  - Preliminary Risk Assessment (PRA) methodology evaluates models across three key criteria: capabilities (what the model does), deployment context (where it operates), and autonomy level (how independently it functions)[2]
  - Risk scoring typically uses discrete thresholds between 1 and 5, with context-dependent classification—an object detection model in retail stock monitoring rates as low risk, whilst the same technology in surgical instrument detection rates as high risk due to direct patient safety implications[2]
  - Leading organisations now maintain centralised AI system inventories tracking ownership, purpose, status, and version history; automated discovery tools scan code repositories for machine learning imports, analyse cloud billing patterns for GPU usage spikes, and examine commit messages to reveal hidden workstreams[1]
  - Human oversight integration remains critical—risk-based rules trigger human review for high-impact outputs whilst low-risk decisions can be automated[4]

- Technical capabilities and limitations
  - Current frameworks excel at categorising known risk types: malicious use, loss of control, data privacy violations, model drift, and operational failures[6]
  - Quantitative thresholds and metrics enable standardised assessment, though their effectiveness depends heavily on accurate model classification at the outset
  - Audit trail maintenance (logging every model decision, update, and approval in tamper-proof systems) provides investigative capability but requires substantial infrastructure investment[4]
  - Limitation: frameworks struggle with emergent risks in novel deployment contexts, particularly where AI systems operate with increasing autonomy[6]

- Standards and frameworks
  - NIST framework dominates North American and increasingly international adoption[7]
  - xAI Risk Management Framework (August 2025) addresses malicious use and loss of control categories with quantitative thresholds and metrics[6]
  - Future of Life Institute's AI Safety Index (Summer 2025) independently assesses seven leading AI companies across 33 indicators spanning six critical domains, evaluated by expert panels of AI scientists and governance specialists[3]
  - ISO/IEC standards development ongoing, though adoption remains uneven across sectors

- UK and North England context
  - UK organisations increasingly adopt NIST-aligned frameworks, though bespoke adaptations for UK data protection law (Data Protection Act 2018, UK GDPR) remain necessary
  - Manchester and Leeds emerging as AI governance research hubs, with university partnerships exploring risk assessment methodologies tailored to financial services and healthcare sectors
  - Newcastle's digital innovation initiatives incorporating AI risk assessment into public sector procurement frameworks
  - Sheffield's advanced manufacturing sector developing risk assessment protocols for autonomous systems in industrial settings

## Research & Literature

- Key academic and institutional sources
  - NIST (2024). AI Risk Management Framework. National Institute of Standards and Technology. Available at: https://www.nist.gov/itl/ai-risk-management-framework
  - Future of Life Institute (2025). 2025 AI Safety Index: Summer Report. Methodology document covering 33 indicators across six domains, evaluated March–July 2025. Available at: https://futureoflife.org/ai-safety-index-summer-2025/
  - xAI (2025). Risk Management Framework. Last updated 20 August 2025. Addresses malicious use and loss of control categories with quantitative thresholds.
  - SentinelOne (2025). AI Risk Assessment Framework: A Step-by-Step Guide. Six-step process following NIST Map-Measure-Manage cycle with practical implementation guidance.
  - Superblocks (2025). 3 AI Risk Management Frameworks for 2025 + Best Practices. Identifies five best practices: centralised inventory, employee training, audit trails, human oversight integration, and LLM governance protocols.
  - Workday (2025). AI and Enterprise Risk Management: What to Know in 2025. Examines AI's role in reshaping enterprise risk management through predictive threat identification and fraud prevention.

- Ongoing research directions
  - Quantification of emergent risks in autonomous systems operating with minimal human supervision
  - Cross-sector harmonisation of risk assessment methodologies (healthcare, finance, public administration)
  - Integration of ethical risk dimensions with technical risk metrics
  - Development of real-time risk monitoring systems for deployed models

## UK Context

- British contributions and implementations
  - UK Financial Conduct Authority incorporating AI risk assessment into regulatory frameworks for financial services
  - NHS Digital developing risk assessment protocols for AI systems in clinical decision support
  - UK government's AI Bill (ongoing legislative development) increasingly references structured risk assessment as governance requirement

- North England innovation and case studies
  - Manchester: University of Manchester's AI governance research informing financial services sector risk assessment practices; Barclays and other major financial institutions headquartered in region adopting formalised frameworks
  - Leeds: Yorkshire-based healthcare trusts piloting AI risk assessment methodologies for diagnostic imaging systems
  - Newcastle: Public sector digital transformation initiatives incorporating AI risk assessment into procurement and deployment decisions
  - Sheffield: Manufacturing sector developing risk assessment protocols for autonomous systems in industrial production environments

## Future Directions

- Emerging trends and developments
  - Shift from static risk categorisation toward continuous, real-time risk monitoring as models operate in production environments
  - Integration of AI-assisted risk assessment tools—using AI to evaluate AI systems presents both efficiency gains and novel circular dependencies requiring careful management
  - Increased emphasis on third-party model evaluation and external safety benchmarking (as evidenced by Future of Life Institute's independent assessment methodology)
  - Regulatory convergence: UK, EU, and international frameworks increasingly aligning on core assessment principles, though implementation details remain jurisdiction-specific

- Anticipated challenges
  - Balancing comprehensive risk assessment against deployment velocity—organisations face genuine tension between thorough evaluation and competitive time-to-market pressures
  - Maintaining assessment rigour as AI systems become more complex and their decision-making processes less interpretable
  - Addressing novel risks emerging from autonomous systems operating with minimal human oversight
  - Ensuring assessment frameworks remain current as AI capabilities evolve rapidly

- Research priorities
  - Development of assessment methodologies for frontier AI systems with capabilities not yet fully understood
  - Quantitative frameworks for ethical and social risks (currently more qualitative than technical risk assessment)
  - Cross-organisational data sharing on risk incidents to improve collective understanding of failure modes
  - Integration of supply chain risk assessment (third-party models, training data provenance, infrastructure dependencies)

## References

1. SentinelOne (2025). AI Risk Assessment Framework: A Step-by-Step Guide. Retrieved from https://www.sentinelone.com/cybersecurity-101/data-and-ai/ai-risk-assessment-framework/

2. NVIDIA (2025). Frontier AI Risk Assessment. Retrieved from https://images.nvidia.com/content/pdf/NVIDIA-Frontier-AI-Risk-Assessment.pdf

3. Future of Life Institute (2025). 2025 AI Safety Index: Summer Report. Retrieved from https://futureoflife.org/ai-safety-index-summer-2025/

4. Superblocks (2025). 3 AI Risk Management Frameworks for 2025 + Best Practices. Retrieved from https://www.superblocks.com/blog/ai-risk-management

5. Workday (2025). AI and Enterprise Risk Management: What to Know in 2025. Retrieved from https://blog.workday.com/en-au/ai-enterprise-risk-management-what-know-2025.html

6. xAI (2025). xAI Risk Management Framework. Last updated 20 August 2025. Retrieved from https://data.x.ai/2025-08-20-xai-risk-management-framework.pdf

7. National Institute of Standards and Technology (2024). AI Risk Management Framework. Retrieved from https://www.nist.gov/itl/ai-risk-management-framework

8. SANS Institute (2025). Securing AI in 2025: A Risk-Based Approach to AI Controls and Governance. Retrieved from https://www.sans.org/blog/securing-ai-in-2025-a-risk-based-approach-to-ai-controls-and-governance

9. BigID (2025). What Is an AI Risk Assessment Framework & Why Is It Important? Retrieved from https://bigid.com/blog/ai-risk-assessment/

10. U.S. Department of Defence (2025). Artificial Intelligence Cybersecurity Risk Management Tailoring Guide. Retrieved from https://dodcio.defence.gov/Portals/0/Documents/Library/AI-CybersecurityRMTailingGuide.pdf

## Metadata

- **Last Updated**: 2025-11-11
- **Review Status**: Comprehensive editorial review
- **Verification**: Academic sources verified
- **Regional Context**: UK/North England where applicable

