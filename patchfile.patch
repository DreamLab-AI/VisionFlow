diff --git a/client/src/api/settingsApi.ts b/client/src/api/settingsApi.ts
index d16d257f..df590bc6 100644
--- a/client/src/api/settingsApi.ts
+++ b/client/src/api/settingsApi.ts
@@ -2,6 +2,7 @@
 import { Settings } from '../features/settings/config/settings';
 import { createLogger } from '../utils/loggerConfig';
 import { unifiedApiClient, isApiError } from '../services/api/UnifiedApiClient';
+import { settingsCacheClient } from '../services/SettingsCacheClient';

 const logger = createLogger('SettingsApi');
 const API_BASE = '/settings';
@@ -233,15 +234,14 @@ const updateManager = new SettingsUpdateManager();

 export const settingsApi = {
   /**
-   * Get a single setting by its dot-notation path
+   * Get a single setting by its dot-notation path with cache support
    * @param path - Dot notation path (e.g., "visualisation.nodes.baseColor")
+   * @param options - Optional cache configuration
    * @returns The setting value
    */
-  async getSettingByPath(path: string): Promise<any> {
+  async getSettingByPath(path: string, options?: { useCache?: boolean }): Promise<any> {
     try {
-      const encodedPath = encodeURIComponent(path);
-      const result = await unifiedApiClient.getData(`${API_BASE}/path?path=${encodedPath}`);
-      return result.value; // Backend returns { value: actualValue }
+      return await settingsCacheClient.get(path, options);
     } catch (error) {
       const errorMessage = isApiError(error)
         ? error.message
@@ -261,51 +261,21 @@ export const settingsApi = {
   },

   /**
-   * Get multiple settings by their paths in a single request using optimized batch endpoint
+   * Get multiple settings by their paths in a single request using optimized batch endpoint with cache support
    * @param paths - Array of dot notation paths
+   * @param options - Optional cache configuration
    * @returns Object mapping paths to their values
    */
-  async getSettingsByPaths(paths: string[]): Promise<Record<string, any>> {
+  async getSettingsByPaths(paths: string[], options?: { useCache?: boolean }): Promise<Record<string, any>> {
     if (!paths || paths.length === 0) {
       return {};
     }

     try {
-      // Use the optimized batch POST endpoint
-      const result = await unifiedApiClient.postData(`${API_BASE}/batch`, { paths });
-      logger.info(`Successfully fetched ${paths.length} settings using batch endpoint`);
-      return result; // Server returns { path: value } mapping
+      return await settingsCacheClient.getBatch(paths, options);
     } catch (error) {
-      logger.warn('Batch endpoint failed, falling back to individual requests:', error);
-
-      // Fallback to individual path requests
-      const result: Record<string, any> = {};
-      const results = await Promise.allSettled(
-        paths.map(async (path) => {
-          try {
-            const value = await this.getSettingByPath(path);
-            return { path, value };
-          } catch (err) {
-            logger.error(`Failed to fetch path ${path}:`, err);
-            return { path, value: undefined };
-          }
-        })
-      );
-
-      // Process results
-      for (const [index, promiseResult] of results.entries()) {
-        if (promiseResult.status === 'fulfilled') {
-          const { path, value } = promiseResult.value;
-          result[path] = value;
-        } else {
-          const path = paths[index];
-          logger.error(`Failed to process path ${path}:`, promiseResult.reason);
-          result[path] = undefined;
-        }
-      }
-
-      logger.info(`Fallback completed: fetched ${Object.keys(result).length}/${paths.length} settings`);
-      return result;
+      logger.error('Failed to fetch settings batch:', error);
+      throw error;
     }
   },

@@ -408,6 +378,30 @@ export const settingsApi = {
   async flushPendingUpdates(): Promise<void> {
     return updateManager.flush();
   },
+
+  /**
+   * Subscribe to settings changes via WebSocket for real-time updates
+   * @param path - Setting path to watch (e.g., "visualisation.nodes.baseColor")
+   * @param callback - Function to call when setting changes
+   * @returns Unsubscribe function
+   */
+  subscribeToSettingChanges(path: string, callback: (path: string, value: any) => void): () => void {
+    return settingsCacheClient.subscribe(path, callback);
+  },
+
+  /**
+   * Get cache performance metrics
+   */
+  getCacheMetrics() {
+    return settingsCacheClient.getPerformanceMetrics();
+  },
+
+  /**
+   * Clear all cached settings
+   */
+  clearCache(): void {
+    settingsCacheClient.clearCache();
+  },

   /**
    * Update setting immediately without debouncing (use sparingly)
diff --git a/client/src/features/help/settingsHelp.ts b/client/src/features/help/settingsHelp.ts
index bc2929ff..0e67ade3 100755
--- a/client/src/features/help/settingsHelp.ts
+++ b/client/src/features/help/settingsHelp.ts
@@ -32,11 +32,11 @@ export const settingsHelpContent: Record<string, HelpContent> = {
     detailedHelp: 'When enabled, settings are saved automatically after each change. Disable this to manually control when settings are persisted.',
   },

-  // Visualization Settings
-  'settings.visualization.theme': {
-    id: 'settings.visualization.theme',
-    title: 'Visualization Theme',
-    description: 'Choose the color theme for the graph visualization',
+  // Visualisation Settings
+  'settings.visualisation.theme': {
+    id: 'settings.visualisation.theme',
+    title: 'Visualisation Theme',
+    description: 'Choose the color theme for the graph visualisation',
     detailedHelp: 'Different themes optimize visibility for various lighting conditions and personal preferences.',
     examples: [
       'Dark theme for reduced eye strain',
@@ -44,14 +44,14 @@ export const settingsHelpContent: Record<string, HelpContent> = {
       'High contrast for better visibility'
     ]
   },
-  'settings.visualization.nodeSize': {
-    id: 'settings.visualization.nodeSize',
+  'settings.visualisation.nodeSize': {
+    id: 'settings.visualisation.nodeSize',
     title: 'Node Size',
     description: 'Adjust the size of nodes in the graph',
     detailedHelp: 'Larger nodes are easier to click but may overlap in dense graphs. Smaller nodes allow viewing more connections at once.',
   },
-  'settings.visualization.linkOpacity': {
-    id: 'settings.visualization.linkOpacity',
+  'settings.visualisation.linkOpacity': {
+    id: 'settings.visualisation.linkOpacity',
     title: 'Link Opacity',
     description: 'Control the transparency of connections between nodes',
     detailedHelp: 'Lower opacity helps see through dense connection networks, while higher opacity makes individual connections clearer.',
diff --git a/client/src/features/settings/components/panels/SettingsPanelRedesign.tsx b/client/src/features/settings/components/panels/SettingsPanelRedesign.tsx
index d5dc60c2..4e097732 100644
--- a/client/src/features/settings/components/panels/SettingsPanelRedesign.tsx
+++ b/client/src/features/settings/components/panels/SettingsPanelRedesign.tsx
@@ -1,323 +1,653 @@
-// Unified Settings Panel - The single control center for all settings
-import React, { useState, useEffect, useMemo } from 'react';
-import { Tabs, TabsContent, TabsList, TabsTrigger } from '../../../design-system/components/Tabs';
-import { Input } from '../../../design-system/components/Input';
-import { Button } from '../../../design-system/components/Button';
-import { ScrollArea } from '../../../design-system/components/ScrollArea';
-import {
-  Settings,
-  Monitor,
-  Palette,
-  Activity,
-  Database,
-  Code,
-  Shield,
-  Headphones,
-  Search,
-  Save,
-  RotateCcw,
-  Download,
-  Upload,
-  Undo2 as Undo,
-  Redo2 as Redo,
-  Brain,
+import React, { useState, useMemo } from 'react';
+import { Tabs, TabsList, TabsTrigger, TabsContent } from '@/features/design-system/components/Tabs';
+import { Card, CardContent, CardDescription, CardHeader, CardTitle } from '@/features/design-system/components/Card';
+import { Button } from '@/features/design-system/components/Button';
+import { SearchInput } from '@/features/design-system/components/SearchInput';
+import {
   Eye,
-  BarChart3,
-  Smartphone
+  Settings,
+  Smartphone,
+  Info,
+  ChevronDown,
+  ChevronUp,
+  Check,
+  Search,
+  Keyboard,
+  User,
 } from 'lucide-react';
-import { useSettingsStore, settingsSelectors } from '../../../../store/settingsStore';
-import { settingsUIDefinition } from '../../config/settingsUIDefinition';
+import { useSettingsStore } from '@/store/settingsStore';
 import { SettingControlComponent } from '../SettingControlComponent';
-import { toast } from '../../../../utils/toast';
-import { createLogger } from '../../../../utils/loggerConfig';
+import { settingsUIDefinition } from '../../config/settingsUIDefinition';
+import { cn } from '@/utils/classNameUtils';
+import { useKeyboardShortcuts } from '@/hooks/useKeyboardShortcuts';
+import { KeyboardShortcutsModal } from '@/components/KeyboardShortcutsModal';
+import { LoadingSpinner, LoadingOverlay } from '@/features/design-system/components/LoadingSpinner';
+import { SkeletonSetting } from '@/features/design-system/components/LoadingSkeleton';
+import { useErrorHandler } from '@/hooks/useErrorHandler';
+import { useToast } from '@/features/design-system/components/Toast';
+import { UndoRedoControls } from '../UndoRedoControls';
+import NostrAuthSection from '../../../auth/components/NostrAuthSection';
+import { useSelectiveSetting, useSettingSetter } from '@/hooks/useSelectiveSettingsStore';
+import { VirtualizedSettingsGroup } from '../VirtualizedSettingsGroup';
+import { GraphSelector } from '../GraphSelector';
+
+interface SettingItem {
+  key: string;
+  path: string;
+  definition: any;
+  isPowerUser?: boolean;
+}

-interface SettingsPanelRedesignProps {
-  isOpen?: boolean;
-  onClose?: () => void;
+interface SettingGroup {
+  title: string;
+  description?: string;
+  items: SettingItem[];
+  isPowerUser?: boolean;
 }

-export const SettingsPanelRedesign: React.FC<SettingsPanelRedesignProps> = ({
-  isOpen = true,
-  onClose
-}) => {
-  const [activeTab, setActiveTab] = useState('visualization');
+export function SettingsPanelRedesign() {
+  const { isPowerUser, initialized } = useSettingsStore();
+  const [expandedGroups, setExpandedGroups] = useState<Set<string>>(new Set(['Node Appearance']));
+  const [savedNotification, setSavedNotification] = useState<string | null>(null);
   const [searchQuery, setSearchQuery] = useState('');
-  const [hasUnsavedChanges, setHasUnsavedChanges] = useState(false);
-
-  // Settings store
-  const {
-    settings,
-    saving,
-    loading,
-    updateSettings,
-    saveSettings,
-    resetSettings,
-    exportToFile,
-    loadFromFile,
-    hasUnsavedChanges: checkUnsavedChanges
-  } = useSettingsStore();
-
-  // Undo/redo support (disabled - not implemented yet)
-  const canUndo = false;
-  const canRedo = false;
-  const undo = () => toast.info('Undo not yet implemented');
-  const redo = () => toast.info('Redo not yet implemented');
-
-  // Check for unsaved changes
-  useEffect(() => {
-    const interval = setInterval(() => {
-      setHasUnsavedChanges(checkUnsavedChanges());
-    }, 1000);
-    return () => clearInterval(interval);
-  }, [checkUnsavedChanges]);
-
-  // Filter settings based on search
-  const filteredUIDefinition = useMemo(() => {
-    if (!searchQuery) return settingsUIDefinition;
-
-    const query = searchQuery.toLowerCase();
-    const filtered: typeof settingsUIDefinition = {};
+  const [showShortcuts, setShowShortcuts] = useState(false);
+  const [loadingSettings, setLoadingSettings] = useState<Set<string>>(new Set());
+  const [isInitializing, setIsInitializing] = useState(true);
+  const [selectedGraph, setSelectedGraph] = useState<'logseq' | 'visionflow'>('logseq');
+  const { handleError } = useErrorHandler();
+  const { toast } = useToast();
+
+  // Dynamically get background and text color from settings
+  const panelBackground: string = useSelectiveSetting('visualisation.rendering.backgroundColor') ?? '#18181b';
+  const panelForeground: string = useSelectiveSetting('visualisation.labels.textOutlineColor') ?? '#fff';
+  const { set: setSetting } = useSettingSetter();
+
+  // Organize settings into logical groups with better structure
+  const settingsStructure = useMemo(() => {
+    // Helper function to filter out items with undefined definitions
+    const filterValidItems = (items: SettingItem[]) =>
+      items.filter(item => item.definition != null);

-    Object.entries(settingsUIDefinition).forEach(([category, categoryDef]) => {
-      const filteredSubsections: typeof categoryDef.subsections = {};
-
-      Object.entries(categoryDef.subsections || {}).forEach(([sectionKey, section]) => {
-        const filteredSettings: Record<string, any> = {};
-
-        Object.entries(section.settings || {}).forEach(([settingKey, setting]) => {
-          if (setting.label.toLowerCase().includes(query) ||
-              setting.path.toLowerCase().includes(query) ||
-              setting.description?.toLowerCase().includes(query)) {
-            filteredSettings[settingKey] = setting;
-          }
+    return {
+    appearance: {
+      label: 'Appearance',
+      icon: <Eye className="h-4 w-4" />,
+      groups: [
+        {
+          title: 'Node Appearance',
+          description: 'Customize how nodes look',
+          items: filterValidItems([
+            { key: 'baseColor', path: `visualisation.graphs.${selectedGraph}.nodes.baseColor`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.baseColor },
+            { key: 'opacity', path: `visualisation.graphs.${selectedGraph}.nodes.opacity`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.opacity },
+            { key: 'metalness', path: `visualisation.graphs.${selectedGraph}.nodes.metalness`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.metalness },
+            { key: 'roughness', path: `visualisation.graphs.${selectedGraph}.nodes.roughness`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.roughness },
+            { key: 'nodeSize', path: `visualisation.graphs.${selectedGraph}.nodes.nodeSize`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.nodeSize },
+            { key: 'enableInstancing', path: `visualisation.graphs.${selectedGraph}.nodes.enableInstancing`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.enableInstancing },
+            { key: 'enableHologram', path: `visualisation.graphs.${selectedGraph}.nodes.enableHologram`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.enableHologram },
+            { key: 'enableMetadataShape', path: `visualisation.graphs.${selectedGraph}.nodes.enableMetadataShape`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.enableMetadataShape },
+            { key: 'enableMetadataVisualisation', path: `visualisation.graphs.${selectedGraph}.nodes.enableMetadataVisualisation`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.enableMetadataVisualisation },
+          ])
+        },
+        {
+          title: 'Edge Appearance',
+          description: 'Customize connection lines',
+          items: filterValidItems([
+            { key: 'color', path: `visualisation.graphs.${selectedGraph}.edges.color`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.color },
+            { key: 'opacity', path: `visualisation.graphs.${selectedGraph}.edges.opacity`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.opacity },
+            { key: 'baseWidth', path: `visualisation.graphs.${selectedGraph}.edges.baseWidth`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.baseWidth },
+            { key: 'enableArrows', path: `visualisation.graphs.${selectedGraph}.edges.enableArrows`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.enableArrows },
+            { key: 'arrowSize', path: `visualisation.graphs.${selectedGraph}.edges.arrowSize`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.arrowSize },
+            { key: 'widthRange', path: `visualisation.graphs.${selectedGraph}.edges.widthRange`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.widthRange },
+            { key: 'enableFlowEffect', path: `visualisation.graphs.${selectedGraph}.edges.enableFlowEffect`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.enableFlowEffect },
+            { key: 'flowSpeed', path: `visualisation.graphs.${selectedGraph}.edges.flowSpeed`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.flowSpeed },
+            { key: 'flowIntensity', path: `visualisation.graphs.${selectedGraph}.edges.flowIntensity`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.flowIntensity },
+            { key: 'glowStrength', path: `visualisation.graphs.${selectedGraph}.edges.glowStrength`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.glowStrength },
+            { key: 'distanceIntensity', path: `visualisation.graphs.${selectedGraph}.edges.distanceIntensity`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.distanceIntensity },
+            { key: 'useGradient', path: `visualisation.graphs.${selectedGraph}.edges.useGradient`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.useGradient },
+            { key: 'gradientColors', path: `visualisation.graphs.${selectedGraph}.edges.gradientColors`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.gradientColors },
+          ])
+        },
+        {
+          title: 'Labels',
+          description: 'Text display settings',
+          items: filterValidItems([
+            { key: 'enableLabels', path: `visualisation.graphs.${selectedGraph}.labels.enableLabels`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Labels`]?.settings?.enableLabels },
+            { key: 'desktopFontSize', path: `visualisation.graphs.${selectedGraph}.labels.desktopFontSize`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Labels`]?.settings?.desktopFontSize },
+            { key: 'textColor', path: `visualisation.graphs.${selectedGraph}.labels.textColor`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Labels`]?.settings?.textColor },
+            { key: 'textOutlineColor', path: `visualisation.graphs.${selectedGraph}.labels.textOutlineColor`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Labels`]?.settings?.textOutlineColor },
+            { key: 'textOutlineWidth', path: `visualisation.graphs.${selectedGraph}.labels.textOutlineWidth`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Labels`]?.settings?.textOutlineWidth },
+            { key: 'textResolution', path: `visualisation.graphs.${selectedGraph}.labels.textResolution`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Labels`]?.settings?.textResolution },
+            { key: 'textPadding', path: `visualisation.graphs.${selectedGraph}.labels.textPadding`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Labels`]?.settings?.textPadding },
+            { key: 'billboardMode', path: `visualisation.graphs.${selectedGraph}.labels.billboardMode`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Labels`]?.settings?.billboardMode },
+          ])
+        },
+        {
+          title: 'Visual Effects',
+          description: 'Bloom and glow effects',
+          items: filterValidItems([
+            { key: 'bloomEnabled', path: 'visualisation.bloom.enabled', definition: settingsUIDefinition.visualisation.subsections.bloom.settings.enabled },
+            { key: 'bloomStrength', path: 'visualisation.bloom.strength', definition: settingsUIDefinition.visualisation.subsections.bloom.settings.strength },
+            { key: 'bloomRadius', path: 'visualisation.bloom.radius', definition: settingsUIDefinition.visualisation.subsections.bloom.settings.radius },
+            { key: 'edgeBloomStrength', path: 'visualisation.bloom.edgeBloomStrength', definition: settingsUIDefinition.visualisation.subsections.bloom.settings.edgeBloomStrength },
+            { key: 'environmentBloomStrength', path: 'visualisation.bloom.environmentBloomStrength', definition: settingsUIDefinition.visualisation.subsections.bloom.settings.environmentBloomStrength },
+            { key: 'nodeBloomStrength', path: 'visualisation.bloom.nodeBloomStrength', definition: settingsUIDefinition.visualisation.subsections.bloom.settings.nodeBloomStrength },
+            { key: 'threshold', path: 'visualisation.bloom.threshold', definition: settingsUIDefinition.visualisation.subsections.bloom.settings.threshold },
+          ])
+        },
+        {
+          title: 'Lighting & Rendering',
+          description: 'Control lighting and background',
+          items: filterValidItems([
+            { key: 'ambientLightIntensity', path: 'visualisation.rendering.ambientLightIntensity', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.ambientLightIntensity },
+            { key: 'directionalLightIntensity', path: 'visualisation.rendering.directionalLightIntensity', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.directionalLightIntensity },
+            { key: 'environmentIntensity', path: 'visualisation.rendering.environmentIntensity', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.environmentIntensity },
+            { key: 'backgroundColor', path: 'visualisation.rendering.backgroundColor', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.backgroundColor },
+            { key: 'enableAmbientOcclusion', path: 'visualisation.rendering.enableAmbientOcclusion', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.enableAmbientOcclusion, isPowerUser: true },
+            { key: 'enableShadows', path: 'visualisation.rendering.enableShadows', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.enableShadows, isPowerUser: true },
+            { key: 'shadowMapSize', path: 'visualisation.rendering.shadowMapSize', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.shadowMapSize, isPowerUser: true },
+            { key: 'shadowBias', path: 'visualisation.rendering.shadowBias', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.shadowBias, isPowerUser: true },
+            { key: 'context', path: 'visualisation.rendering.context', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.context },
+          ])
+        },
+        {
+          title: 'Hologram Effect',
+          description: 'Control hologram visualization',
+          items: filterValidItems([
+            { key: 'ringCount', path: 'visualisation.hologram.ringCount', definition: settingsUIDefinition.visualisation.subsections.hologram.settings.ringCount },
+            { key: 'ringColor', path: 'visualisation.hologram.ringColor', definition: settingsUIDefinition.visualisation.subsections.hologram.settings.ringColor },
+            { key: 'ringOpacity', path: 'visualisation.hologram.ringOpacity', definition: settingsUIDefinition.visualisation.subsections.hologram.settings.ringOpacity },
+            { key: 'sphereSizes', path: 'visualisation.hologram.sphereSizes', definition: settingsUIDefinition.visualisation.subsections.hologram.settings.sphereSizes },
+            { key: 'ringRotationSpeed', path: 'visualisation.hologram.ringRotationSpeed', definition: settingsUIDefinition.visualisation.subsections.hologram.settings.ringRotationSpeed },
+            { key: 'globalRotationSpeed', path: 'visualisation.hologram.globalRotationSpeed', definition: settingsUIDefinition.visualisation.subsections.hologram.settings.globalRotationSpeed },
+          ])
+        },
+        {
+          title: 'Animations',
+          description: 'Animation controls',
+          items: filterValidItems([
+            { key: 'enableNodeAnimations', path: 'visualisation.animations.enableNodeAnimations', definition: settingsUIDefinition.visualisation.subsections.animations.settings.enableNodeAnimations },
+            { key: 'enableMotionBlur', path: 'visualisation.animations.enableMotionBlur', definition: settingsUIDefinition.visualisation.subsections.animations.settings.enableMotionBlur, isPowerUser: true },
+            { key: 'motionBlurStrength', path: 'visualisation.animations.motionBlurStrength', definition: settingsUIDefinition.visualisation.subsections.animations.settings.motionBlurStrength, isPowerUser: true },
+            { key: 'selectionWaveEnabled', path: 'visualisation.animations.selectionWaveEnabled', definition: settingsUIDefinition.visualisation.subsections.animations.settings.selectionWaveEnabled },
+            { key: 'pulseEnabled', path: 'visualisation.animations.pulseEnabled', definition: settingsUIDefinition.visualisation.subsections.animations.settings.pulseEnabled },
+            { key: 'pulseSpeed', path: 'visualisation.animations.pulseSpeed', definition: settingsUIDefinition.visualisation.subsections.animations.settings.pulseSpeed },
+            { key: 'pulseStrength', path: 'visualisation.animations.pulseStrength', definition: settingsUIDefinition.visualisation.subsections.animations.settings.pulseStrength },
+            { key: 'waveSpeed', path: 'visualisation.animations.waveSpeed', definition: settingsUIDefinition.visualisation.subsections.animations.settings.waveSpeed },
+          ])
+        }
+      ]
+    },
+    performance: {
+      label: 'Performance',
+      icon: <Settings className="h-4 w-4" />,
+      groups: [
+        {
+          title: 'Rendering Quality',
+          description: 'Balance quality and performance',
+          items: filterValidItems([
+            { key: 'nodeQuality', path: `visualisation.graphs.${selectedGraph}.nodes.quality`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Nodes`]?.settings?.quality },
+            { key: 'edgeQuality', path: `visualisation.graphs.${selectedGraph}.edges.quality`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Edges`]?.settings?.quality },
+            { key: 'enableAntialiasing', path: 'visualisation.rendering.enableAntialiasing', definition: settingsUIDefinition.visualisation.subsections.rendering.settings.enableAntialiasing },
+          ])
+        },
+        {
+          title: 'Physics Engine',
+          description: 'Node movement behavior',
+          items: filterValidItems([
+            { key: 'physicsEnabled', path: `visualisation.graphs.${selectedGraph}.physics.enabled`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.enabled },
+            { key: 'iterations', path: `visualisation.graphs.${selectedGraph}.physics.iterations`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.iterations },
+            { key: 'damping', path: `visualisation.graphs.${selectedGraph}.physics.damping`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.damping },
+            { key: 'boundsSize', path: `visualisation.graphs.${selectedGraph}.physics.boundsSize`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.boundsSize },
+            { key: 'collisionRadius', path: `visualisation.graphs.${selectedGraph}.physics.collisionRadius`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.collisionRadius },
+            { key: 'enableBounds', path: `visualisation.graphs.${selectedGraph}.physics.enableBounds`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.enableBounds },
+            { key: 'maxVelocity', path: `visualisation.graphs.${selectedGraph}.physics.maxVelocity`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.maxVelocity },
+            { key: 'repulsionDistance', path: `visualisation.graphs.${selectedGraph}.physics.repulsionDistance`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.repulsionDistance },
+            { key: 'massScale', path: `visualisation.graphs.${selectedGraph}.physics.massScale`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.massScale },
+            { key: 'boundaryDamping', path: `visualisation.graphs.${selectedGraph}.physics.boundaryDamping`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.boundaryDamping },
+          ])
+        },
+        {
+          title: 'Force Settings',
+          description: 'Control attraction, repulsion, and spring forces',
+          items: filterValidItems([
+            { key: 'attractionStrength', path: `visualisation.graphs.${selectedGraph}.physics.attractionStrength`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.attractionStrength },
+            { key: 'repulsionStrength', path: `visualisation.graphs.${selectedGraph}.physics.repulsionStrength`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.repulsionStrength },
+            { key: 'springStrength', path: `visualisation.graphs.${selectedGraph}.physics.springStrength`, definition: settingsUIDefinition.visualisation.subsections[`${selectedGraph}Physics`]?.settings?.springStrength },
+          ])
+        },
+        {
+          title: 'Network Settings',
+          description: 'Connection optimization',
+          items: filterValidItems([
+            { key: 'updateRate', path: 'system.websocket.updateRate', definition: settingsUIDefinition.system.subsections.websocket.settings.updateRate },
+            { key: 'compressionEnabled', path: 'system.websocket.compressionEnabled', definition: settingsUIDefinition.system.subsections.websocket.settings.compressionEnabled },
+          ]),
+          isPowerUser: true
+        }
+      ]
+    },
+    xr: {
+      label: 'XR/VR',
+      icon: <Smartphone className="h-4 w-4" />,
+      groups: [
+        {
+          title: 'XR Mode',
+          description: 'Virtual reality settings',
+          items: filterValidItems([
+            { key: 'clientSideEnableXR', path: settingsUIDefinition.xr.subsections.general.settings.clientSideEnableXR.path, definition: settingsUIDefinition.xr.subsections.general.settings.clientSideEnableXR },
+            { key: 'displayMode', path: settingsUIDefinition.xr.subsections.general.settings.displayMode.path, definition: settingsUIDefinition.xr.subsections.general.settings.displayMode },
+            { key: 'quality', path: settingsUIDefinition.xr.subsections.general.settings.quality.path, definition: settingsUIDefinition.xr.subsections.general.settings.quality },
+          ])
+        },
+        {
+          title: 'Interaction',
+          description: 'Hand tracking and controls',
+          items: filterValidItems([
+            { key: 'enableHandTracking', path: 'xr.handFeatures.enableHandTracking', definition: settingsUIDefinition.xr.subsections.handFeatures.settings.enableHandTracking },
+            { key: 'enableHaptics', path: 'xr.handFeatures.enableHaptics', definition: settingsUIDefinition.xr.subsections.handFeatures.settings.enableHaptics },
+            { key: 'interactionRadius', path: 'xr.handFeatures.interactionRadius', definition: settingsUIDefinition.xr.subsections.handFeatures.settings.interactionRadius },
+            { key: 'movementAxesHorizontal', path: 'xr.handFeatures.movementAxesHorizontal', definition: settingsUIDefinition.xr.subsections.handFeatures.settings.movementAxesHorizontal },
+            { key: 'movementAxesVertical', path: 'xr.handFeatures.movementAxesVertical', definition: settingsUIDefinition.xr.subsections.handFeatures.settings.movementAxesVertical },
+          ]),
+          isPowerUser: true
+        },
+        {
+          title: 'Environment Understanding',
+          description: 'Settings for AR environment features',
+          items: filterValidItems([
+            { key: 'enableLightEstimation', path: 'xr.environmentUnderstanding.enableLightEstimation', definition: settingsUIDefinition.xr.subsections.environmentUnderstanding.settings.enableLightEstimation },
+            { key: 'enablePlaneDetection', path: 'xr.environmentUnderstanding.enablePlaneDetection', definition: settingsUIDefinition.xr.subsections.environmentUnderstanding.settings.enablePlaneDetection },
+            { key: 'enableSceneUnderstanding', path: 'xr.environmentUnderstanding.enableSceneUnderstanding', definition: settingsUIDefinition.xr.subsections.environmentUnderstanding.settings.enableSceneUnderstanding },
+            { key: 'planeColor', path: 'xr.environmentUnderstanding.planeColor', definition: settingsUIDefinition.xr.subsections.environmentUnderstanding.settings.planeColor },
+            { key: 'planeOpacity', path: 'xr.environmentUnderstanding.planeOpacity', definition: settingsUIDefinition.xr.subsections.environmentUnderstanding.settings.planeOpacity },
+            { key: 'planeDetectionDistance', path: 'xr.environmentUnderstanding.planeDetectionDistance', definition: settingsUIDefinition.xr.subsections.environmentUnderstanding.settings.planeDetectionDistance },
+            { key: 'showPlaneOverlay', path: 'xr.environmentUnderstanding.showPlaneOverlay', definition: settingsUIDefinition.xr.subsections.environmentUnderstanding.settings.showPlaneOverlay },
+            { key: 'snapToFloor', path: 'xr.environmentUnderstanding.snapToFloor', definition: settingsUIDefinition.xr.subsections.environmentUnderstanding.settings.snapToFloor },
+          ]),
+          isPowerUser: true
+        },
+        {
+          title: 'Passthrough',
+          description: 'Control passthrough portal settings',
+          items: filterValidItems([
+            { key: 'enablePassthroughPortal', path: 'xr.passthrough.enablePassthroughPortal', definition: settingsUIDefinition.xr.subsections.passthrough.settings.enablePassthroughPortal },
+            { key: 'passthroughOpacity', path: 'xr.passthrough.passthroughOpacity', definition: settingsUIDefinition.xr.subsections.passthrough.settings.passthroughOpacity },
+            { key: 'passthroughBrightness', path: 'xr.passthrough.passthroughBrightness', definition: settingsUIDefinition.xr.subsections.passthrough.settings.passthroughBrightness },
+            { key: 'passthroughContrast', path: 'xr.passthrough.passthroughContrast', definition: settingsUIDefinition.xr.subsections.passthrough.settings.passthroughContrast },
+            { key: 'portalSize', path: 'xr.passthrough.portalSize', definition: settingsUIDefinition.xr.subsections.passthrough.settings.portalSize },
+            { key: 'portalEdgeColor', path: 'xr.passthrough.portalEdgeColor', definition: settingsUIDefinition.xr.subsections.passthrough.settings.portalEdgeColor },
+            { key: 'portalEdgeWidth', path: 'xr.passthrough.portalEdgeWidth', definition: settingsUIDefinition.xr.subsections.passthrough.settings.portalEdgeWidth },
+          ]),
+          isPowerUser: true
+        }
+      ]
+    },
+    auth: {
+      label: 'Authentication',
+      icon: <User className="h-4 w-4" />,
+      groups: [
+        {
+          title: 'Nostr Authentication',
+          description: 'Authenticate to access advanced features.',
+          items: [], // The NostrAuthSection component will be rendered directly.
+        },
+      ],
+    },
+    advanced: {
+      label: 'Advanced',
+      icon: <Settings className="h-4 w-4" />,
+      isPowerUser: true,
+      groups: [
+        {
+          title: 'Debug Options',
+          description: 'Developer tools',
+          items: filterValidItems([
+            { key: 'debugMode', path: settingsUIDefinition.system.subsections.debug.settings.enabled.path, definition: settingsUIDefinition.system.subsections.debug.settings.enabled },
+            { key: 'logLevel', path: 'system.debug.logLevel', definition: settingsUIDefinition.system.subsections.debug.settings.logLevel },
+          ])
+        },
+        {
+          title: 'AI Services',
+          description: 'API configuration',
+          items: filterValidItems([
+            { key: 'ragflowApiKey', path: settingsUIDefinition.ai.subsections.ragflow.settings.apiKey.path, definition: settingsUIDefinition.ai.subsections.ragflow.settings.apiKey },
+            { key: 'perplexityKey', path: settingsUIDefinition.ai.subsections.perplexity.settings.apiKey.path, definition: settingsUIDefinition.ai.subsections.perplexity.settings.apiKey },
+            { key: 'openaiKey', path: settingsUIDefinition.ai.subsections.openai.settings.apiKey.path, definition: settingsUIDefinition.ai.subsections.openai.settings.apiKey },
+          ])
+        }
+      ]
+    }
+  };
+  }, [selectedGraph]);
+
+  // Filter settings based on search query
+  const filterSettings = (groups: SettingGroup[], query: string): SettingGroup[] => {
+    if (!query.trim()) return groups;
+
+    const lowerQuery = query.toLowerCase();
+    return groups
+      .map(group => {
+        const filteredItems = group.items.filter(item => {
+          const matchesKey = item.key.toLowerCase().includes(lowerQuery);
+          const matchesLabel = item.definition?.label?.toLowerCase().includes(lowerQuery);
+          const matchesDescription = item.definition?.description?.toLowerCase().includes(lowerQuery);
+          const matchesGroup = group.title.toLowerCase().includes(lowerQuery);
+          const matchesGroupDesc = group.description?.toLowerCase().includes(lowerQuery);
+
+          return matchesKey || matchesLabel || matchesDescription || matchesGroup || matchesGroupDesc;
         });
-
-        if (Object.keys(filteredSettings).length > 0) {
-          filteredSubsections[sectionKey] = {
-            ...section,
-            settings: filteredSettings
-          };
+
+        if (filteredItems.length > 0) {
+          return { ...group, items: filteredItems };
         }
+        return null;
+      })
+      .filter(group => group !== null) as SettingGroup[];
+  };
+
+  // Auto-expand groups when searching
+  React.useEffect(() => {
+    if (searchQuery.trim()) {
+      // Expand all groups that have matching items
+      const groupsToExpand = new Set<string>();
+      Object.values(settingsStructure).forEach(section => {
+        const filtered = filterSettings(section.groups, searchQuery);
+        filtered.forEach(group => {
+          groupsToExpand.add(group.title);
+        });
       });
-
-      if (Object.keys(filteredSubsections).length > 0) {
-        filtered[category] = {
-          ...categoryDef,
-          subsections: filteredSubsections
-        };
+      setExpandedGroups(groupsToExpand);
+    }
+  }, [searchQuery, settingsStructure]);
+
+  // Register keyboard shortcuts
+  useKeyboardShortcuts({
+    'settings-search': {
+      key: '/',
+      ctrl: true,
+      description: 'Focus search in settings',
+      category: 'Settings',
+      handler: () => {
+        const searchInput = document.querySelector('input[placeholder="Search settings..."]') as HTMLInputElement;
+        searchInput?.focus();
+      }
+    },
+    'settings-shortcuts': {
+      key: '?',
+      shift: true,
+      description: 'Show keyboard shortcuts',
+      category: 'General',
+      handler: () => setShowShortcuts(true)
+    },
+    'settings-clear-search': {
+      key: 'Escape',
+      description: 'Clear search',
+      category: 'Settings',
+      handler: () => setSearchQuery(''),
+      enabled: !!searchQuery
+    },
+    'settings-toggle-poweruser': {
+      key: 'p',
+      ctrl: true,
+      shift: true,
+      description: 'Toggle power user mode',
+      category: 'Settings',
+      handler: () => {
+        toast({
+          title: 'Authentication required',
+          description: 'Please authenticate with Nostr to enable power user features',
+        });
+      }
+    }
+  });
+
+  // Simulate initial loading
+  React.useEffect(() => {
+    const timer = setTimeout(() => {
+      setIsInitializing(false);
+    }, 800);
+    return () => clearTimeout(timer);
+  }, []);
+
+  // Show loading state if store is not initialized
+  if (!initialized || isInitializing) {
+    return (
+      <div className="w-full h-full flex items-center justify-center">
+        <LoadingSpinner size="lg" />
+      </div>
+    );
+  }
+
+  const toggleGroup = (groupTitle: string) => {
+    setExpandedGroups(prev => {
+      const next = new Set(prev);
+      if (next.has(groupTitle)) {
+        next.delete(groupTitle);
+      } else {
+        next.add(groupTitle);
       }
+      return next;
     });
-
-    return filtered;
-  }, [searchQuery]);
-
-  // Handle file import
-  const handleFileImport = async (event: React.ChangeEvent<HTMLInputElement>) => {
-    const file = event.target.files?.[0];
-    if (file) {
-      await loadFromFile(file);
-      event.target.value = ''; // Reset input
+  };
+
+  const handleSettingChange = async (path: string, value: any) => {
+    // Add loading state for this setting
+    setLoadingSettings(prev => new Set(prev).add(path));
+
+    try {
+      // Save the setting - this should handle persistence automatically
+      setSetting(path, value);
+
+      // Show success toast
+      toast({
+        title: 'Setting saved',
+        description: `Successfully updated ${path.split('.').pop()}`,
+      });
+
+      // Also show inline notification
+      setSavedNotification(path);
+      setTimeout(() => setSavedNotification(null), 2000);
+    } catch (error) {
+      handleError(error, {
+        title: 'Failed to save setting',
+        actionLabel: 'Retry',
+        onAction: () => handleSettingChange(path, value)
+      });
+    } finally {
+      // Remove loading state
+      setLoadingSettings(prev => {
+        const next = new Set(prev);
+        next.delete(path);
+        return next;
+      });
     }
   };
-
-  // Tab definitions - comprehensive settings categories
-  const tabs = [
-    { id: 'dashboard', label: 'Dashboard', icon: Monitor, category: 'dashboard' },
-    { id: 'visualization', label: 'Visualization', icon: Eye, category: 'visualization' },
-    { id: 'physics', label: 'Physics', icon: Activity, category: 'physics' },
-    { id: 'analytics', label: 'Analytics', icon: BarChart3, category: 'analytics' },
-    { id: 'xr', label: 'XR/AR', icon: Smartphone, category: 'xr' },
-    { id: 'performance', label: 'Performance', icon: Activity, category: 'performance' },
-    { id: 'data', label: 'Data', icon: Database, category: 'integrations' },
-    { id: 'system', label: 'System', icon: Settings, category: 'system' },
-    { id: 'ai', label: 'AI Services', icon: Brain, category: 'ai' },
-    { id: 'developer', label: 'Developer', icon: Code, category: 'developer' },
-    { id: 'auth', label: 'Auth', icon: Shield, category: 'auth' },
-  ];
-
-  return (
-    <div className="h-full flex flex-col bg-background">
-      {/* Header */}
-      <div className="flex items-center justify-between p-4 border-b">
-        <div className="flex items-center gap-2">
-          <Settings className="w-5 h-5" />
-          <h2 className="text-lg font-semibold">Control Center</h2>
-          {hasUnsavedChanges && (
-            <span className="text-xs text-orange-500 ml-2">(Unsaved changes)</span>
-          )}
-        </div>
-
-        <div className="flex items-center gap-2">
-          {/* Search */}
-          <div className="relative">
-            <Search className="absolute left-2 top-1/2 transform -translate-y-1/2 w-4 h-4 text-muted-foreground" />
-            <Input
-              type="text"
-              placeholder="Search settings..."
-              value={searchQuery}
-              onChange={(e) => setSearchQuery(e.target.value)}
-              className="pl-8 w-64"
+
+  const renderSettingGroup = (group: SettingGroup, groupIndex: number) => {
+    if (group.isPowerUser && !isPowerUser) return null;
+
+    const isExpanded = expandedGroups.has(group.title);
+
+    return (
+      <Card key={group.title} className="mb-3 overflow-hidden">
+        <CardHeader
+          className="cursor-pointer py-3 px-4 hover:bg-muted/50 transition-colors"
+          onClick={() => toggleGroup(group.title)}
+        >
+          <div className="flex items-center justify-between">
+            <div className="flex-1">
+              <CardTitle className="text-sm font-medium flex items-center gap-2">
+                {group.title}
+                {group.isPowerUser && (
+                  <span className="text-xs px-1.5 py-0.5 bg-primary/10 text-primary rounded">
+                    Pro
+                  </span>
+                )}
+              </CardTitle>
+              {group.description && (
+                <CardDescription className="text-xs mt-1">
+                  {group.description}
+                </CardDescription>
+              )}
+            </div>
+            <ChevronDown
+              className={cn(
+                "h-4 w-4 transition-transform duration-200",
+                isExpanded ? "" : "-rotate-90"
+              )}
             />
           </div>
-
-          {/* Undo/Redo */}
-          <div className="flex gap-1">
-            <Button
-              variant="ghost"
-              size="icon"
-              onClick={undo}
-              disabled={!canUndo}
-              title="Undo"
-            >
-              <Undo className="w-4 h-4" />
-            </Button>
-            <Button
-              variant="ghost"
-              size="icon"
-              onClick={redo}
-              disabled={!canRedo}
-              title="Redo"
-            >
-              <Redo className="w-4 h-4" />
-            </Button>
-          </div>
-
-          {/* Import/Export */}
-          <div className="flex gap-1">
-            <Button
-              variant="ghost"
-              size="icon"
-              onClick={() => document.getElementById('import-settings')?.click()}
-              title="Import settings"
-            >
-              <Upload className="w-4 h-4" />
-            </Button>
-            <input
-              id="import-settings"
-              type="file"
-              accept=".json"
-              onChange={handleFileImport}
-              className="hidden"
+        </CardHeader>
+
+        {isExpanded && (
+          <CardContent className="pt-0 pb-3 px-4 space-y-3">
+            <VirtualizedSettingsGroup
+              title={group.title}
+              items={group.items}
+              isExpanded={isExpanded}
+              onToggle={() => toggleGroup(group.title)}
+              isPowerUser={isPowerUser}
+              loadingSettings={loadingSettings}
+              savedNotification={savedNotification}
+              onSettingChange={handleSettingChange}
+              groupIndex={groupIndex}
             />
-            <Button
-              variant="ghost"
-              size="icon"
-              onClick={exportToFile}
-              title="Export settings"
-            >
-              <Download className="w-4 h-4" />
-            </Button>
+          </CardContent>
+        )}
+      </Card>
+    );
+  };
+
+  const renderTabContent = (tabKey: string) => {
+    const tab = settingsStructure[tabKey];
+    if (!tab) return null;
+
+    if (tab.isPowerUser && !isPowerUser) {
+      return (
+        <div className="flex flex-col items-center justify-center h-64 text-center p-6">
+          <Settings className="h-12 w-12 text-muted-foreground mb-4" />
+          <h3 className="text-lg font-medium mb-2">Power User Features</h3>
+          <p className="text-sm text-muted-foreground max-w-sm">
+            Authenticate with Nostr to unlock advanced settings and features.
+          </p>
+        </div>
+      );
+    }
+
+    // Special handling for the Auth tab
+    if (tabKey === 'auth') {
+      return <NostrAuthSection />;
+    }
+
+    const filteredGroups = filterSettings(tab.groups, searchQuery);
+
+    if (searchQuery && filteredGroups.length === 0) {
+      return (
+        <div className="flex flex-col items-center justify-center h-64 text-center p-6">
+          <Search className="h-12 w-12 text-muted-foreground mb-4" />
+          <h3 className="text-lg font-medium mb-2">No results found</h3>
+          <p className="text-sm text-muted-foreground max-w-sm">
+            Try searching with different keywords or browse categories.
+          </p>
+        </div>
+      );
+    }
+
+    return (
+      <div className="flex-1 min-h-0 space-y-3">
+        {/* Show GraphSelector only for appearance tab */}
+        {tabKey === 'appearance' && (
+          <GraphSelector
+            selectedGraph={selectedGraph}
+            onGraphChange={setSelectedGraph}
+          />
+        )}
+        {filteredGroups.map((group, index) => (
+          <React.Fragment key={group.title}>
+            {renderSettingGroup(group, index)}
+          </React.Fragment>
+        ))}
+      </div>
+    );
+  };
+
+  return (
+    // Dynamically set background and text color from settings
+    <div className="w-full h-full flex flex-col min-h-0 bg-background text-foreground">
+      <div className="border-b border-gray-200 dark:border-gray-800">
+        <div className="px-4 py-3 flex items-center justify-between">
+          <div>
+            <h2 className="text-lg font-semibold">Settings & Controls</h2>
+            <p className="text-sm text-muted-foreground">
+              Customize your visualization and experience
+            </p>
           </div>
-
-          {/* Save/Reset */}
-          <div className="flex gap-1">
-            <Button
-              variant="ghost"
-              size="icon"
-              onClick={saveSettings}
-              disabled={saving || !hasUnsavedChanges}
-              title="Save settings"
-            >
-              <Save className="w-4 h-4" />
-            </Button>
-            <Button
-              variant="ghost"
-              size="icon"
-              onClick={resetSettings}
-              title="Reset to defaults"
+          <div className="flex items-center gap-2">
+            <UndoRedoControls showHistory />
+          </div>
+        </div>
+        <div className="px-4 pb-3">
+          <SearchInput
+            value={searchQuery}
+            onChange={setSearchQuery}
+            placeholder="Search settings..."
+            className="w-full"
+            onKeyDown={(e) => {
+              if (e.key === 'Escape') {
+                setSearchQuery('');
+              }
+            }}
+          />
+          <div className="mt-2 flex items-center justify-between text-xs text-muted-foreground">
+            <span>Press <kbd className="px-1 py-0.5 bg-muted rounded text-xs">Ctrl+/</kbd> to search</span>
+            <button
+              onClick={() => setShowShortcuts(true)}
+              className="flex items-center gap-1 hover:text-foreground transition-colors"
             >
-              <RotateCcw className="w-4 h-4" />
-            </Button>
+              <Keyboard className="h-3 w-3" />
+              <span>View shortcuts</span>
+            </button>
           </div>
-
-          {onClose && (
-            <Button variant="ghost" size="sm" onClick={onClose}>
-              Close
-            </Button>
-          )}
         </div>
       </div>
-
-      {/* Content */}
-      <div className="flex-1 overflow-hidden">
-        <Tabs value={activeTab} onValueChange={setActiveTab} className="h-full">
-          <TabsList className="px-4 w-full justify-start">
-            {tabs.map(tab => (
-              <TabsTrigger key={tab.id} value={tab.id} className="flex items-center gap-2">
-                <tab.icon className="w-4 h-4" />
-                {tab.label}
+
+      <div className="flex-1 overflow-auto">
+        <Tabs defaultValue="appearance" className="h-full">
+          <TabsList className="px-4 bg-muted/30 w-full justify-start">
+            {Object.entries(settingsStructure).map(([key, section]) => (
+              <TabsTrigger key={key} value={key} className="flex items-center gap-2">
+                {section.icon}
+                {section.label}
               </TabsTrigger>
             ))}
           </TabsList>
-
-          <ScrollArea className="flex-1 h-[calc(100%-3rem)]">
-            {tabs.map(tab => {
-              const categoryDef = filteredUIDefinition[tab.category];
-              if (!categoryDef) return null;
-
-              return (
-                <TabsContent key={tab.id} value={tab.id} className="p-4 space-y-4">
-                  {/* Tab description */}
-                  {categoryDef.description && (
-                    <div className="text-sm text-muted-foreground mb-4">
-                      {categoryDef.description}
-                    </div>
-                  )}
-
-                  {/* Settings sections */}
-                  {Object.entries(categoryDef.subsections || {}).map(([sectionKey, section]) => (
-                    <div key={sectionKey} className="space-y-2">
-                      <h3 className="text-sm font-semibold">{section.label}</h3>
-                      {section.description && (
-                        <p className="text-xs text-muted-foreground">{section.description}</p>
-                      )}
-                      <div className="space-y-2">
-                        {Object.values(section.settings || {}).map(setting => (
-                          <SettingControlComponent
-                            key={setting.path}
-                            path={setting.path}
-                            settingDef={setting}
-                            value={getSettingValue(settings, setting.path)}
-                            onChange={(value) => {
-                              updateSettings((draft) => {
-                                setSettingValue(draft, setting.path, value);
-                              });
-                            }}
-                          />
-                        ))}
-                      </div>
-                    </div>
-                  ))}
-                </TabsContent>
-              );
-            })}
-          </ScrollArea>
+          {Object.entries(settingsStructure).map(([key, section]) => (
+            <TabsContent key={key} value={key} className="px-4 py-3">
+              {renderTabContent(key)}
+            </TabsContent>
+          ))}
         </Tabs>
       </div>
-
+
       {/* Status bar */}
-      {(loading || saving) && (
-        <div className="px-4 py-2 border-t text-xs text-muted-foreground">
-          {loading && 'Loading settings...'}
-          {saving && 'Saving settings...'}
+      <div className="px-4 py-2 border-t border-gray-200 dark:border-gray-800 bg-muted/30 flex items-center justify-between text-xs">
+        <div className="flex items-center gap-2">
+          <Info className="h-3 w-3" />
+          <span>Changes save automatically</span>
         </div>
-      )}
+        {isPowerUser && (
+          <div className="flex items-center gap-1 text-primary">
+            <Settings className="h-3 w-3" />
+            <span>Power User</span>
+          </div>
+        )}
+      </div>
+
+      {/* Keyboard Shortcuts Modal */}
+      <KeyboardShortcutsModal
+        isOpen={showShortcuts}
+        onClose={() => setShowShortcuts(false)}
+      />
     </div>
   );
-};
-
-// Helper to get nested value by path
-function getSettingValue(obj: any, path: string): any {
-  return path.split('.').reduce((acc, part) => acc?.[part], obj);
-}
-
-// Helper to set nested value by path
-function setSettingValue(obj: any, path: string, value: any): void {
-  const parts = path.split('.');
-  const last = parts.pop()!;
-  const target = parts.reduce((acc, part) => {
-    if (!acc[part]) acc[part] = {};
-    return acc[part];
-  }, obj);
-  target[last] = value;
 }
\ No newline at end of file
diff --git a/client/src/features/settings/config/settings.ts b/client/src/features/settings/config/settings.ts
index e40d071a..e8e4f499 100755
--- a/client/src/features/settings/config/settings.ts
+++ b/client/src/features/settings/config/settings.ts
@@ -125,6 +125,7 @@ export interface RenderingSettings {

 // Animation settings
 export interface AnimationSettings {
+  enabled: boolean; // Global animation toggle
   enableMotionBlur: boolean;
   enableNodeAnimations: boolean;
   motionBlurStrength: number;
@@ -213,6 +214,25 @@ export interface HologramSettings {
   particleDensity: number;
 }

+// Sync settings for multi-view synchronization
+export interface SyncSettings {
+  enabled: boolean;
+  camera: boolean;
+  selection: boolean;
+}
+
+// Visual effects settings
+export interface EffectsSettings {
+  bloom: boolean;
+  glow: boolean;
+}
+
+// Export configuration settings
+export interface ExportSettings {
+  format: string;
+  includeMetadata: boolean;
+}
+
 // WebSocket settings
 export interface WebSocketSettings {
   reconnectAttempts: number;
@@ -321,9 +341,13 @@ export interface HeadTrackedParallaxSettings {
   cameraMode: 'offset' | 'asymmetricFrustum';
 }

-// Interaction settings
+// Interaction behavior settings
 export interface InteractionSettings {
   headTrackedParallax: HeadTrackedParallaxSettings;
+  enableHover: boolean;
+  enableClick: boolean;
+  enableDrag: boolean;
+  hoverDelay: number;
 }

 // Visualisation settings
@@ -355,6 +379,8 @@ export interface VisualisationSettings {
   animations: AnimationSettings;
   glow: GlowSettings;
   hologram: HologramSettings;
+  sync: SyncSettings;
+  effects: EffectsSettings;
   spacePilot?: SpacePilotConfig;
   camera?: CameraSettings;
   interaction?: InteractionSettings;
@@ -473,6 +499,9 @@ export interface PerformanceSettings {
   warmupDuration: number;
   convergenceThreshold: number;
   enableAdaptiveCooling: boolean;
+  autoOptimize: boolean;
+  simplifyEdges: boolean;
+  cullDistance: number;
 }

 // Developer GPU debug settings
@@ -520,6 +549,8 @@ export interface Settings {
   analytics?: AnalyticsSettings;
   performance?: PerformanceSettings;
   developer?: DeveloperSettings;
+  interaction: InteractionSettings;
+  export: ExportSettings;
 }

 // Partial update types for settings mutations
diff --git a/client/src/features/visualisation/components/ControlPanel/RestoredGraphTabs.tsx b/client/src/features/visualisation/components/ControlPanel/RestoredGraphTabs.tsx
index 865aceb5..7e608bbd 100644
--- a/client/src/features/visualisation/components/ControlPanel/RestoredGraphTabs.tsx
+++ b/client/src/features/visualisation/components/ControlPanel/RestoredGraphTabs.tsx
@@ -2,7 +2,7 @@
  * Restored Graph Feature Tabs - Inline Styles Version
  */

-import React, { useState, useCallback } from 'react';
+import React, { useState, useCallback, useEffect } from 'react';
 import { Eye, Zap, TrendingUp, MousePointer2, Download } from 'lucide-react';
 import { useSettingsStore } from '../../../../store/settingsStore';
 import type { GraphNode, GraphEdge } from '@/features/graph/types/graphTypes';
@@ -71,13 +71,26 @@ const SectionHeader: React.FC<{ icon: React.ComponentType<any>; title: string; c
 export const RestoredGraphVisualisationTab: React.FC<GraphTabProps> = () => {
   const settings = useSettingsStore(state => state.settings);
   const updateSettings = useSettingsStore(state => state.updateSettings);
-
-  const syncEnabled = settings?.visualization?.sync?.enabled ?? false;
-  const cameraSync = settings?.visualization?.sync?.camera ?? true;
-  const selectionSync = settings?.visualization?.sync?.selection ?? true;
-  const animationsEnabled = settings?.visualization?.animations?.enabled ?? true;
-  const bloomEffect = settings?.visualization?.effects?.bloom ?? false;
-  const glowEffect = settings?.visualization?.effects?.glow ?? true;
+  const ensureLoaded = useSettingsStore(state => state.ensureLoaded);
+
+  // Preload visualisation settings on mount
+  useEffect(() => {
+    ensureLoaded([
+      'visualisation.sync.enabled',
+      'visualisation.sync.camera',
+      'visualisation.sync.selection',
+      'visualisation.animations.enabled',
+      'visualisation.effects.bloom',
+      'visualisation.effects.glow',
+    ]);
+  }, [ensureLoaded]);
+
+  const syncEnabled = settings?.visualisation?.sync?.enabled ?? false;
+  const cameraSync = settings?.visualisation?.sync?.camera ?? true;
+  const selectionSync = settings?.visualisation?.sync?.selection ?? true;
+  const animationsEnabled = settings?.visualisation?.animations?.enabled ?? true;
+  const bloomEffect = settings?.visualisation?.effects?.bloom ?? false;
+  const glowEffect = settings?.visualisation?.effects?.glow ?? true;

   return (
     <div style={{ padding: '4px', color: 'white' }}>
@@ -85,9 +98,9 @@ export const RestoredGraphVisualisationTab: React.FC<GraphTabProps> = () => {
       <Toggle
         checked={syncEnabled}
         onChange={(val) => updateSettings((draft) => {
-          if (!draft.visualization) draft.visualization = {};
-          if (!draft.visualization.sync) draft.visualization.sync = {};
-          draft.visualization.sync.enabled = val;
+          if (!draft.visualisation) draft.visualisation = {};
+          if (!draft.visualisation.sync) draft.visualisation.sync = {};
+          draft.visualisation.sync.enabled = val;
         })}
         label="Enable Sync"
       />
@@ -96,18 +109,18 @@ export const RestoredGraphVisualisationTab: React.FC<GraphTabProps> = () => {
           <Toggle
             checked={cameraSync}
             onChange={(val) => updateSettings((draft) => {
-              if (!draft.visualization) draft.visualization = {};
-              if (!draft.visualization.sync) draft.visualization.sync = {};
-              draft.visualization.sync.camera = val;
+              if (!draft.visualisation) draft.visualisation = {};
+              if (!draft.visualisation.sync) draft.visualisation.sync = {};
+              draft.visualisation.sync.camera = val;
             })}
             label="Camera"
           />
           <Toggle
             checked={selectionSync}
             onChange={(val) => updateSettings((draft) => {
-              if (!draft.visualization) draft.visualization = {};
-              if (!draft.visualization.sync) draft.visualization.sync = {};
-              draft.visualization.sync.selection = val;
+              if (!draft.visualisation) draft.visualisation = {};
+              if (!draft.visualisation.sync) draft.visualisation.sync = {};
+              draft.visualisation.sync.selection = val;
             })}
             label="Selection"
           />
@@ -118,9 +131,9 @@ export const RestoredGraphVisualisationTab: React.FC<GraphTabProps> = () => {
       <Toggle
         checked={animationsEnabled}
         onChange={(val) => updateSettings((draft) => {
-          if (!draft.visualization) draft.visualization = {};
-          if (!draft.visualization.animations) draft.visualization.animations = {};
-          draft.visualization.animations.enabled = val;
+          if (!draft.visualisation) draft.visualisation = {};
+          if (!draft.visualisation.animations) draft.visualisation.animations = {};
+          draft.visualisation.animations.enabled = val;
         })}
         label="Enable Animations"
       />
@@ -129,18 +142,18 @@ export const RestoredGraphVisualisationTab: React.FC<GraphTabProps> = () => {
       <Toggle
         checked={bloomEffect}
         onChange={(val) => updateSettings((draft) => {
-          if (!draft.visualization) draft.visualization = {};
-          if (!draft.visualization.effects) draft.visualization.effects = {};
-          draft.visualization.effects.bloom = val;
+          if (!draft.visualisation) draft.visualisation = {};
+          if (!draft.visualisation.effects) draft.visualisation.effects = {};
+          draft.visualisation.effects.bloom = val;
         })}
         label="Bloom"
       />
       <Toggle
         checked={glowEffect}
         onChange={(val) => updateSettings((draft) => {
-          if (!draft.visualization) draft.visualization = {};
-          if (!draft.visualization.effects) draft.visualization.effects = {};
-          draft.visualization.effects.glow = val;
+          if (!draft.visualisation) draft.visualisation = {};
+          if (!draft.visualisation.effects) draft.visualisation.effects = {};
+          draft.visualisation.effects.glow = val;
         })}
         label="Glow"
       />
@@ -151,6 +164,16 @@ export const RestoredGraphVisualisationTab: React.FC<GraphTabProps> = () => {
 export const RestoredGraphOptimisationTab: React.FC<GraphTabProps> = ({ graphData }) => {
   const settings = useSettingsStore(state => state.settings);
   const updateSettings = useSettingsStore(state => state.updateSettings);
+  const ensureLoaded = useSettingsStore(state => state.ensureLoaded);
+
+  // Preload performance settings on mount
+  useEffect(() => {
+    ensureLoaded([
+      'performance.autoOptimize',
+      'performance.simplifyEdges',
+      'performance.cullDistance',
+    ]);
+  }, [ensureLoaded]);

   const autoOptimize = settings?.performance?.autoOptimize ?? false;
   const simplifyEdges = settings?.performance?.simplifyEdges ?? true;
@@ -231,6 +254,17 @@ export const RestoredGraphOptimisationTab: React.FC<GraphTabProps> = ({ graphDat
 export const RestoredGraphInteractionTab: React.FC<GraphTabProps> = () => {
   const settings = useSettingsStore(state => state.settings);
   const updateSettings = useSettingsStore(state => state.updateSettings);
+  const ensureLoaded = useSettingsStore(state => state.ensureLoaded);
+
+  // Preload interaction settings on mount
+  useEffect(() => {
+    ensureLoaded([
+      'interaction.enableHover',
+      'interaction.enableClick',
+      'interaction.enableDrag',
+      'interaction.hoverDelay',
+    ]);
+  }, [ensureLoaded]);

   const enableHover = settings?.interaction?.enableHover ?? true;
   const enableClick = settings?.interaction?.enableClick ?? true;
@@ -298,6 +332,15 @@ export const RestoredGraphInteractionTab: React.FC<GraphTabProps> = () => {
 export const RestoredGraphExportTab: React.FC<GraphTabProps> = ({ graphData }) => {
   const settings = useSettingsStore(state => state.settings);
   const updateSettings = useSettingsStore(state => state.updateSettings);
+  const ensureLoaded = useSettingsStore(state => state.ensureLoaded);
+
+  // Preload export settings on mount
+  useEffect(() => {
+    ensureLoaded([
+      'export.format',
+      'export.includeMetadata',
+    ]);
+  }, [ensureLoaded]);

   const format = settings?.export?.format ?? 'json';
   const includeMetadata = settings?.export?.includeMetadata ?? true;
diff --git a/client/src/features/visualisation/components/ControlPanel/SettingsTabContent.tsx b/client/src/features/visualisation/components/ControlPanel/SettingsTabContent.tsx
index 17ea7ef8..961d0473 100644
--- a/client/src/features/visualisation/components/ControlPanel/SettingsTabContent.tsx
+++ b/client/src/features/visualisation/components/ControlPanel/SettingsTabContent.tsx
@@ -3,7 +3,7 @@
  * Renders settings fields based on configuration
  */

-import React, { useCallback, useState } from 'react';
+import React, { useCallback, useState, useEffect } from 'react';
 import { useSettingsStore } from '../../../../store/settingsStore';
 import { SETTINGS_CONFIG } from './settingsConfig';
 import type { SettingField } from './types';
@@ -15,9 +15,19 @@ interface SettingsTabContentProps {
 export const SettingsTabContent: React.FC<SettingsTabContentProps> = ({ sectionId }) => {
   const settings = useSettingsStore(state => state.settings);
   const updateSettings = useSettingsStore(state => state.updateSettings);
+  const ensureLoaded = useSettingsStore(state => state.ensureLoaded);
   const [nostrConnected, setNostrConnected] = useState(false);
   const [nostrPublicKey, setNostrPublicKey] = useState('');

+  // Preload settings for this section on mount
+  useEffect(() => {
+    const sectionConfig = SETTINGS_CONFIG[sectionId];
+    if (sectionConfig) {
+      const paths = sectionConfig.fields.map(field => field.path);
+      ensureLoaded(paths);
+    }
+  }, [sectionId, ensureLoaded]);
+
   // Get value from settings path
   const getValueFromPath = useCallback((path: string): any => {
     const keys = path.split('.');
diff --git a/client/src/features/visualisation/components/ControlPanel/config.ts b/client/src/features/visualisation/components/ControlPanel/config.ts
index d99e2e01..00c9f4cc 100644
--- a/client/src/features/visualisation/components/ControlPanel/config.ts
+++ b/client/src/features/visualisation/components/ControlPanel/config.ts
@@ -16,6 +16,7 @@ import {
   Hand,
   Download,
   Lock,
+  GitBranch,
 } from 'lucide-react';
 import type { TabConfig } from './types';

@@ -37,6 +38,7 @@ export const TAB_CONFIGS: TabConfig[] = [
   { id: 'graph-interaction', label: 'Interaction', icon: Hand, description: 'Interactive graph features', buttonKey: 'D' },
   { id: 'graph-export', label: 'Export', icon: Download, description: 'Export and sharing options', buttonKey: 'E' },
   { id: 'auth', label: 'Auth/Nostr', icon: Lock, description: 'Authentication & Nostr', buttonKey: 'F' },
+  { id: 'ontology', label: 'Ontology', icon: GitBranch, description: 'Ontology management', buttonKey: 'G' },
 ];

 // Group tabs by row for rendering
diff --git a/client/src/features/visualisation/components/ControlPanel/settingsConfig.ts b/client/src/features/visualisation/components/ControlPanel/settingsConfig.ts
index 526600f6..74542e59 100644
--- a/client/src/features/visualisation/components/ControlPanel/settingsConfig.ts
+++ b/client/src/features/visualisation/components/ControlPanel/settingsConfig.ts
@@ -1,224 +1,170 @@
 /**
- * Settings Configuration for all tabs
- * Extracted from original IntegratedControlPanel
+ * Settings Configuration - Reorganized for Intuitive Navigation
+ * Grouped by function and typical user workflow
  */

 import type { SectionConfig } from './types';

 export const SETTINGS_CONFIG: Record<string, SectionConfig> = {
-  // dashboard section commented out - these paths don't exist in server settings
-  // dashboard: {
-  //   title: 'Dashboard',
-  //   fields: [
-  //     { key: 'graphStatus', label: 'Show Graph Status', type: 'toggle', path: 'dashboard.showStatus' },
-  //     { key: 'autoRefresh', label: 'Auto Refresh', type: 'toggle', path: 'dashboard.autoRefresh' },
-  //     { key: 'refreshInterval', label: 'Refresh Interval (s)', type: 'slider', min: 1, max: 60, path: 'dashboard.refreshInterval' },
-  //     { key: 'computeMode', label: 'Compute Mode', type: 'select', options: ['Basic Force-Directed', 'Dual Graph', 'Constraint-Enhanced', 'Visual Analytics'], path: 'dashboard.computeMode' },
-  //     { key: 'iterationCount', label: 'Iteration Count', type: 'text', path: 'dashboard.iterationCount' },
-  //     { key: 'convergenceIndicator', label: 'Show Convergence', type: 'toggle', path: 'dashboard.showConvergence' },
-  //     { key: 'activeConstraints', label: 'Active Constraints', type: 'text', path: 'dashboard.activeConstraints' },
-  //     { key: 'clusteringStatus', label: 'Clustering Active', type: 'toggle', path: 'dashboard.clusteringActive' }
-  //   ]
-  // },
-
-  visualization: {
-    title: 'Visualization Settings',
+  // 1. APPEARANCE - Visual styling of nodes, edges, and labels
+  appearance: {
+    title: 'Appearance',
     fields: [
-      // Node Settings
+      // Node Appearance
       { key: 'nodeColor', label: 'Node Color', type: 'color', path: 'visualisation.graphs.logseq.nodes.baseColor' },
       { key: 'nodeSize', label: 'Node Size', type: 'slider', min: 0.2, max: 2, path: 'visualisation.graphs.logseq.nodes.nodeSize' },
-      { key: 'nodeMetalness', label: 'Node Metalness', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.nodes.metalness' },
       { key: 'nodeOpacity', label: 'Node Opacity', type: 'slider', min: 0, max: 1, step: 0.01, path: 'visualisation.graphs.logseq.nodes.opacity' },
+      { key: 'nodeMetalness', label: 'Node Metalness', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.nodes.metalness' },
       { key: 'nodeRoughness', label: 'Node Roughness', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.nodes.roughness' },
-      { key: 'enableInstancing', label: 'Enable Instancing', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableInstancing' },
-      { key: 'enableMetadataShape', label: 'Metadata Shape', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableMetadataShape' },
-      { key: 'enableMetadataVis', label: 'Metadata Visual', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableMetadataVisualisation' },
-      { key: 'nodeImportance', label: 'Node Importance', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableImportance' },

-      // Edge Settings
+      // Edge Appearance
       { key: 'edgeColor', label: 'Edge Color', type: 'color', path: 'visualisation.graphs.logseq.edges.color' },
       { key: 'edgeWidth', label: 'Edge Width', type: 'slider', min: 0.01, max: 2, path: 'visualisation.graphs.logseq.edges.baseWidth' },
       { key: 'edgeOpacity', label: 'Edge Opacity', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.edges.opacity' },
       { key: 'enableArrows', label: 'Show Arrows', type: 'toggle', path: 'visualisation.graphs.logseq.edges.enableArrows' },
       { key: 'arrowSize', label: 'Arrow Size', type: 'slider', min: 0.01, max: 0.5, path: 'visualisation.graphs.logseq.edges.arrowSize' },
-      { key: 'glowStrength', label: 'Edge Glow', type: 'slider', min: 0, max: 5, path: 'visualisation.graphs.logseq.edges.glowStrength' },
+      { key: 'useGradient', label: 'Edge Gradient', type: 'toggle', path: 'visualisation.graphs.logseq.edges.useGradient' },

-      // Label Settings
+      // Label Appearance
       { key: 'enableLabels', label: 'Show Labels', type: 'toggle', path: 'visualisation.graphs.logseq.labels.enableLabels' },
       { key: 'labelSize', label: 'Label Size', type: 'slider', min: 0.01, max: 1.5, path: 'visualisation.graphs.logseq.labels.desktopFontSize' },
       { key: 'labelColor', label: 'Label Color', type: 'color', path: 'visualisation.graphs.logseq.labels.textColor' },
       { key: 'labelOutlineColor', label: 'Outline Color', type: 'color', path: 'visualisation.graphs.logseq.labels.textOutlineColor' },
-      { key: 'labelOutlineWidth', label: 'Outline Width', type: 'slider', min: 0, max: 0.01, path: 'visualisation.graphs.logseq.labels.textOutlineWidth' },
-
-      // GPU Visualization Features - COMMENTED OUT: These paths don't exist in server settings
-      // { key: 'temporalCoherence', label: 'Temporal Coherence', type: 'slider', min: 0, max: 1, path: 'visualisation.gpu.temporalCoherence' },
-      // { key: 'graphDifferentiation', label: 'Graph Differentiation', type: 'toggle', path: 'visualisation.gpu.enableGraphDifferentiation' },
-      // { key: 'clusterVisualization', label: 'Cluster Visualization', type: 'toggle', path: 'visualisation.gpu.enableClusterVisualization' },
-      // { key: 'stressOptimization', label: 'Stress Optimization', type: 'toggle', path: 'visualisation.gpu.enableStressOptimization' },
-
-      // Lighting
-      { key: 'ambientLight', label: 'Ambient Light', type: 'slider', min: 0, max: 2, path: 'visualisation.rendering.ambientLightIntensity' },
-      { key: 'directionalLight', label: 'Direct Light', type: 'slider', min: 0, max: 2, path: 'visualisation.rendering.directionalLightIntensity' }
-    ]
-  },
-
-  physics: {
-    title: 'Physics Settings',
-    fields: [
-      { key: 'enabled', label: 'Physics Enabled', type: 'toggle', path: 'visualisation.graphs.logseq.physics.enabled' },
-      { key: 'autoBalance', label: '⚖️ Adaptive Balancing', type: 'toggle', path: 'visualisation.graphs.logseq.physics.autoBalance' },
-      { key: 'damping', label: 'Damping', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.damping' },
-      { key: 'springK', label: 'Spring Strength (k)', type: 'slider', min: 0.0001, max: 10, path: 'visualisation.graphs.logseq.physics.springK' },
-      { key: 'repelK', label: 'Repulsion Strength (k)', type: 'slider', min: 0.1, max: 200, path: 'visualisation.graphs.logseq.physics.repelK' },
-      { key: 'attractionK', label: 'Attraction Strength (k)', type: 'slider', min: 0, max: 10, path: 'visualisation.graphs.logseq.physics.attractionK' },
-      { key: 'dt', label: 'Time Step (dt)', type: 'slider', min: 0.001, max: 0.1, path: 'visualisation.graphs.logseq.physics.dt' },
-      { key: 'maxVelocity', label: 'Max Velocity', type: 'slider', min: 0.1, max: 10, path: 'visualisation.graphs.logseq.physics.maxVelocity' },
-      { key: 'separationRadius', label: 'Separation Radius', type: 'slider', min: 0.1, max: 10, path: 'visualisation.graphs.logseq.physics.separationRadius' },
-      { key: 'enableBounds', label: 'Enable Bounds', type: 'toggle', path: 'visualisation.graphs.logseq.physics.enableBounds' },
-      { key: 'boundsSize', label: 'Bounds Size', type: 'slider', min: 1, max: 10000, path: 'visualisation.graphs.logseq.physics.boundsSize' },
-      { key: 'stressWeight', label: 'Stress Weight', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.stressWeight' },
-      { key: 'stressAlpha', label: 'Stress Alpha', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.stressAlpha' },
-      { key: 'minDistance', label: 'Min Distance', type: 'slider', min: 0.05, max: 1, path: 'visualisation.graphs.logseq.physics.minDistance' },
-      { key: 'maxRepulsionDist', label: 'Max Repulsion Dist', type: 'slider', min: 10, max: 200, path: 'visualisation.graphs.logseq.physics.maxRepulsionDist' },
-      { key: 'warmupIterations', label: 'Warmup Iterations', type: 'slider', min: 0, max: 500, path: 'visualisation.graphs.logseq.physics.warmupIterations' },
-      { key: 'coolingRate', label: 'Cooling Rate', type: 'slider', min: 0.00001, max: 0.01, path: 'visualisation.graphs.logseq.physics.coolingRate' },
-      { key: 'restLength', label: 'Rest Length', type: 'slider', min: 10, max: 200, path: 'visualisation.graphs.logseq.physics.restLength' },
-      { key: 'repulsionCutoff', label: 'Repulsion Cutoff', type: 'slider', min: 10, max: 200, path: 'visualisation.graphs.logseq.physics.repulsionCutoff' },
-      { key: 'repulsionSofteningEpsilon', label: 'Repulsion Epsilon', type: 'slider', min: 0.00001, max: 0.01, path: 'visualisation.graphs.logseq.physics.repulsionSofteningEpsilon' },
-      { key: 'centerGravityK', label: 'Centre Gravity K', type: 'slider', min: 0, max: 0.1, path: 'visualisation.graphs.logseq.physics.centerGravityK' },
-      { key: 'gridCellSize', label: 'Grid Cell Size', type: 'slider', min: 10, max: 100, path: 'visualisation.graphs.logseq.physics.gridCellSize' },
-      { key: 'boundaryExtremeMultiplier', label: 'Boundary Extreme Mult', type: 'slider', min: 1, max: 5, path: 'visualisation.graphs.logseq.physics.boundaryExtremeMultiplier' },
-      { key: 'boundaryExtremeForceMultiplier', label: 'Boundary Force Mult', type: 'slider', min: 1, max: 20, path: 'visualisation.graphs.logseq.physics.boundaryExtremeForceMultiplier' },
-      { key: 'boundaryVelocityDamping', label: 'Boundary Vel Damping', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.boundaryVelocityDamping' },
-      { key: 'iterations', label: 'Iterations', type: 'slider', min: 1, max: 1000, path: 'visualisation.graphs.logseq.physics.iterations' },
-      { key: 'massScale', label: 'Mass Scale', type: 'slider', min: 0.1, max: 10, path: 'visualisation.graphs.logseq.physics.massScale' },
-      { key: 'boundaryDamping', label: 'Boundary Damp', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.boundaryDamping' },
-      { key: 'updateThreshold', label: 'Update Threshold', type: 'slider', min: 0, max: 0.5, path: 'visualisation.graphs.logseq.physics.updateThreshold' }
+      { key: 'labelOutlineWidth', label: 'Outline Width', type: 'slider', min: 0, max: 0.01, path: 'visualisation.graphs.logseq.labels.textOutlineWidth' }
     ]
   },

-  // analytics section commented out - these paths don't exist in server settings
-  // Use physics.clusteringAlgorithm, physics.clusterCount, etc. instead
-  // analytics: {
-  //   title: 'Analytics Settings',
-  //   fields: [
-  //     { key: 'enableMetrics', label: 'Enable Metrics', type: 'toggle', path: 'analytics.enableMetrics' },
-  //     { key: 'updateInterval', label: 'Update Interval (s)', type: 'slider', min: 1, max: 60, path: 'analytics.updateInterval' },
-  //     { key: 'showDegreeDistribution', label: 'Degree Distribution', type: 'toggle', path: 'analytics.showDegreeDistribution' },
-  //     { key: 'showClustering', label: 'Clustering Coefficient', type: 'toggle', path: 'analytics.showClusteringCoefficient' },
-  //     { key: 'showCentrality', label: 'Centrality Metrics', type: 'toggle', path: 'analytics.showCentrality' },
-  //     { key: 'clusteringAlgorithm', label: 'Clustering Algorithm', type: 'select', options: ['none', 'kmeans', 'spectral', 'louvain'], path: 'analytics.clustering.algorithm' },
-  //     { key: 'clusterCount', label: 'Cluster Count', type: 'slider', min: 2, max: 20, path: 'analytics.clustering.clusterCount' },
-  //     { key: 'clusterResolution', label: 'Resolution', type: 'slider', min: 0.1, max: 2, path: 'analytics.clustering.resolution' },
-  //     { key: 'clusterIterations', label: 'Cluster Iterations', type: 'slider', min: 10, max: 100, path: 'analytics.clustering.iterations' },
-  //     { key: 'exportClusters', label: 'Export Clusters', type: 'toggle', path: 'analytics.clustering.exportEnabled' },
-  //     { key: 'importDistances', label: 'Import Distances', type: 'toggle', path: 'analytics.clustering.importEnabled' }
-  //   ]
-  // },
-
-  // performance section commented out - these paths don't exist in server settings
-  // performance: {
-  //   title: 'Performance Settings',
-  //   fields: [
-  //     { key: 'showFPS', label: 'Show FPS', type: 'toggle', path: 'performance.showFPS' },
-  //     { key: 'targetFPS', label: 'Target FPS', type: 'slider', min: 30, max: 144, path: 'performance.targetFPS' },
-  //     { key: 'gpuMemoryLimit', label: 'GPU Memory (MB)', type: 'slider', min: 256, max: 8192, step: 256, path: 'performance.gpuMemoryLimit' },
-  //     { key: 'levelOfDetail', label: 'Quality Preset', type: 'select', options: ['low', 'medium', 'high', 'ultra'], path: 'performance.levelOfDetail' },
-  //     { key: 'adaptiveQuality', label: 'Adaptive Quality', type: 'toggle', path: 'performance.enableAdaptiveQuality' },
-  //     { key: 'warmupDuration', label: 'Warmup Duration (s)', type: 'slider', min: 0, max: 10, path: 'performance.warmupDuration' },
-  //     { key: 'convergenceThreshold', label: 'Convergence Threshold', type: 'slider', min: 0.001, max: 0.1, path: 'performance.convergenceThreshold' },
-  //     { key: 'adaptiveCooling', label: 'Adaptive Cooling', type: 'toggle', path: 'performance.enableAdaptiveCooling' },
-  //     { key: 'gpuBlockSize', label: 'GPU Block Size', type: 'select', options: ['64', '128', '256', '512'], path: 'performance.gpuBlockSize' },
-  //     { key: 'memoryCoalescing', label: 'Memory Coalescing', type: 'toggle', path: 'performance.enableMemoryCoalescing' },
-  //     { key: 'iterationLimit', label: 'Iteration Limit', type: 'slider', min: 100, max: 5000, path: 'performance.iterationLimit' }
-  //   ]
-  // },
-
-  integrations: {
+  // 2. EFFECTS - Visual effects and animations
+  effects: {
     title: 'Visual Effects',
     fields: [
+      // Glow Effects
       { key: 'glow', label: 'Hologram Glow', type: 'toggle', path: 'visualisation.glow.enabled' },
       { key: 'glowIntensity', label: 'Glow Intensity', type: 'slider', min: 0, max: 5, step: 0.1, path: 'visualisation.glow.intensity' },
       { key: 'glowRadius', label: 'Glow Radius', type: 'slider', min: 0, max: 5, step: 0.05, path: 'visualisation.glow.radius' },
       { key: 'glowThreshold', label: 'Glow Threshold', type: 'slider', min: 0, max: 1, step: 0.01, path: 'visualisation.glow.threshold' },
-      // Bloom settings - COMMENTED OUT: These paths don't exist in server settings
-      // { key: 'bloom', label: 'Graph Bloom', type: 'toggle', path: 'visualisation.bloom.enabled' },
-      // { key: 'bloomStrength', label: 'Bloom Strength', type: 'slider', min: 0, max: 5, step: 0.1, path: 'visualisation.bloom.strength' },
-      // { key: 'bloomRadius', label: 'Bloom Radius', type: 'slider', min: 0, max: 1, step: 0.05, path: 'visualisation.bloom.radius' },
-      // { key: 'bloomThreshold', label: 'Bloom Threshold', type: 'slider', min: 0, max: 1, step: 0.01, path: 'visualisation.bloom.threshold' },
-      { key: 'hologram', label: 'Hologram', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableHologram' },
+      { key: 'edgeGlowStrength', label: 'Edge Glow', type: 'slider', min: 0, max: 5, path: 'visualisation.graphs.logseq.edges.glowStrength' },
+
+      // Hologram Effect
+      { key: 'hologram', label: 'Hologram Effect', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableHologram' },
       { key: 'ringCount', label: 'Ring Count', type: 'slider', min: 0, max: 10, path: 'visualisation.hologram.ringCount' },
       { key: 'ringColor', label: 'Ring Color', type: 'color', path: 'visualisation.hologram.ringColor' },
       { key: 'ringOpacity', label: 'Ring Opacity', type: 'slider', min: 0, max: 1, path: 'visualisation.hologram.ringOpacity' },
       { key: 'ringRotationSpeed', label: 'Ring Speed', type: 'slider', min: 0, max: 5, path: 'visualisation.hologram.ringRotationSpeed' },
+
+      // Flow Effect
       { key: 'flowEffect', label: 'Edge Flow', type: 'toggle', path: 'visualisation.graphs.logseq.edges.enableFlowEffect' },
       { key: 'flowSpeed', label: 'Flow Speed', type: 'slider', min: 0.1, max: 5, path: 'visualisation.graphs.logseq.edges.flowSpeed' },
       { key: 'flowIntensity', label: 'Flow Intensity', type: 'slider', min: 0, max: 10, path: 'visualisation.graphs.logseq.edges.flowIntensity' },
-      { key: 'useGradient', label: 'Edge Gradient', type: 'toggle', path: 'visualisation.graphs.logseq.edges.useGradient' },
-      { key: 'distanceIntensity', label: 'Distance Int', type: 'slider', min: 0, max: 10, path: 'visualisation.graphs.logseq.edges.distanceIntensity' },
+      { key: 'distanceIntensity', label: 'Distance Intensity', type: 'slider', min: 0, max: 10, path: 'visualisation.graphs.logseq.edges.distanceIntensity' },
+
+      // Animations
       { key: 'nodeAnimations', label: 'Node Animations', type: 'toggle', path: 'visualisation.animations.enableNodeAnimations' },
       { key: 'pulseEnabled', label: 'Pulse Effect', type: 'toggle', path: 'visualisation.animations.pulseEnabled' },
       { key: 'pulseSpeed', label: 'Pulse Speed', type: 'slider', min: 0.1, max: 2, path: 'visualisation.animations.pulseSpeed' },
       { key: 'pulseStrength', label: 'Pulse Strength', type: 'slider', min: 0.1, max: 2, path: 'visualisation.animations.pulseStrength' },
       { key: 'selectionWave', label: 'Selection Wave', type: 'toggle', path: 'visualisation.animations.selectionWaveEnabled' },
-      { key: 'waveSpeed', label: 'Wave Speed', type: 'slider', min: 0.1, max: 2, path: 'visualisation.animations.waveSpeed' },
-      { key: 'antialiasing', label: 'Antialiasing', type: 'toggle', path: 'visualisation.rendering.enableAntialiasing' },
-      { key: 'shadows', label: 'Shadows', type: 'toggle', path: 'visualisation.rendering.enableShadows' },
-      { key: 'ambientOcclusion', label: 'Ambient Occl', type: 'toggle', path: 'visualisation.rendering.enableAmbientOcclusion' }
+      { key: 'waveSpeed', label: 'Wave Speed', type: 'slider', min: 0.1, max: 2, path: 'visualisation.animations.waveSpeed' }
     ]
   },

-  developer: {
-    title: 'Developer Tools',
+  // 3. PHYSICS BASIC - Common physics parameters users typically adjust
+  physics: {
+    title: 'Physics - Basic',
     fields: [
-      // Most developer paths don't exist - only system.debug.enabled exists
-      // { key: 'consoleLogging', label: 'Console Logging', type: 'toggle', path: 'developer.consoleLogging' },
-      // { key: 'logLevel', label: 'Log Level', type: 'select', options: ['error', 'warn', 'info', 'debug'], path: 'developer.logLevel' },
-      // { key: 'showNodeIds', label: 'Show Node IDs', type: 'toggle', path: 'developer.showNodeIds' },
-      // { key: 'showEdgeWeights', label: 'Show Edge Weights', type: 'toggle', path: 'developer.showEdgeWeights' },
-      // { key: 'enableProfiler', label: 'Enable Profiler', type: 'toggle', path: 'developer.enableProfiler' },
-      // { key: 'apiDebugMode', label: 'API Debug Mode', type: 'toggle', path: 'developer.apiDebugMode' },
-      { key: 'enableDebug', label: 'Debug Mode', type: 'toggle', path: 'system.debug.enabled' },
-      // { key: 'showMemory', label: 'Show Memory', type: 'toggle', path: 'system.debug.showMemory' },
-      // { key: 'perfDebug', label: 'Performance Debug', type: 'toggle', path: 'system.debug.enablePerformanceDebug' },
-      // { key: 'telemetry', label: 'Telemetry', type: 'toggle', path: 'system.debug.enableTelemetry' },
-      // { key: 'dataDebug', label: 'Data Debug', type: 'toggle', path: 'system.debug.enableDataDebug' },
-      // { key: 'wsDebug', label: 'WebSocket Debug', type: 'toggle', path: 'system.debug.enableWebSocketDebug' },
-      // { key: 'physicsDebug', label: 'Physics Debug', type: 'toggle', path: 'system.debug.enablePhysicsDebug' },
-      // { key: 'nodeDebug', label: 'Node Debug', type: 'toggle', path: 'system.debug.enableNodeDebug' },
-      // { key: 'shaderDebug', label: 'Shader Debug', type: 'toggle', path: 'system.debug.enableShaderDebug' },
-      // { key: 'matrixDebug', label: 'Matrix Debug', type: 'toggle', path: 'system.debug.enableMatrixDebug' },
-      // { key: 'forceVectors', label: 'Show Force Vectors', type: 'toggle', path: 'developer.gpu.showForceVectors' },
-      // { key: 'constraintVisualization', label: 'Constraint Visualization', type: 'toggle', path: 'developer.gpu.showConstraints' },
-      // { key: 'boundaryForceDisplay', label: 'Boundary Forces', type: 'toggle', path: 'developer.gpu.showBoundaryForces' },
-      // { key: 'convergenceGraph', label: 'Convergence Graph', type: 'toggle', path: 'developer.gpu.showConvergenceGraph' },
-      // { key: 'gpuTimingStats', label: 'GPU Timing Stats', type: 'toggle', path: 'developer.gpu.showTimingStats' }
+      { key: 'enabled', label: 'Physics Enabled', type: 'toggle', path: 'visualisation.graphs.logseq.physics.enabled' },
+      { key: 'autoBalance', label: '⚖️ Adaptive Balancing', type: 'toggle', path: 'visualisation.graphs.logseq.physics.autoBalance' },
+      { key: 'damping', label: 'Damping', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.damping' },
+      { key: 'springK', label: 'Spring Strength', type: 'slider', min: 0.0001, max: 10, path: 'visualisation.graphs.logseq.physics.springK' },
+      { key: 'repelK', label: 'Repulsion Strength', type: 'slider', min: 0.1, max: 200, path: 'visualisation.graphs.logseq.physics.repelK' },
+      { key: 'attractionK', label: 'Attraction Strength', type: 'slider', min: 0, max: 10, path: 'visualisation.graphs.logseq.physics.attractionK' },
+      { key: 'maxVelocity', label: 'Max Velocity', type: 'slider', min: 0.1, max: 10, path: 'visualisation.graphs.logseq.physics.maxVelocity' },
+      { key: 'separationRadius', label: 'Separation Radius', type: 'slider', min: 0.1, max: 10, path: 'visualisation.graphs.logseq.physics.separationRadius' },
+      { key: 'restLength', label: 'Rest Length', type: 'slider', min: 10, max: 200, path: 'visualisation.graphs.logseq.physics.restLength' },
+      { key: 'centerGravityK', label: 'Centre Gravity', type: 'slider', min: 0, max: 0.1, path: 'visualisation.graphs.logseq.physics.centerGravityK' },
+      { key: 'enableBounds', label: 'Enable Bounds', type: 'toggle', path: 'visualisation.graphs.logseq.physics.enableBounds' },
+      { key: 'boundsSize', label: 'Bounds Size', type: 'slider', min: 1, max: 10000, path: 'visualisation.graphs.logseq.physics.boundsSize' }
     ]
   },

-  auth: {
-    title: 'Authentication Settings',
+  // 4. PHYSICS ADVANCED - Technical parameters for power users
+  physicsAdvanced: {
+    title: 'Physics - Advanced',
     fields: [
-      { key: 'nostr', label: 'Nostr Login', type: 'nostr-button', path: 'auth.nostr' },
-      { key: 'enabled', label: 'Auth Required', type: 'toggle', path: 'auth.enabled' },
-      { key: 'required', label: 'Require Auth', type: 'toggle', path: 'auth.required' },
-      { key: 'provider', label: 'Auth Provider', type: 'text', path: 'auth.provider' }
+      { key: 'dt', label: 'Time Step (dt)', type: 'slider', min: 0.001, max: 0.1, path: 'visualisation.graphs.logseq.physics.dt' },
+      { key: 'iterations', label: 'Iterations', type: 'slider', min: 1, max: 1000, path: 'visualisation.graphs.logseq.physics.iterations' },
+      { key: 'warmupIterations', label: 'Warmup Iterations', type: 'slider', min: 0, max: 500, path: 'visualisation.graphs.logseq.physics.warmupIterations' },
+      { key: 'coolingRate', label: 'Cooling Rate', type: 'slider', min: 0.00001, max: 0.01, path: 'visualisation.graphs.logseq.physics.coolingRate' },
+      { key: 'updateThreshold', label: 'Update Threshold', type: 'slider', min: 0, max: 0.5, path: 'visualisation.graphs.logseq.physics.updateThreshold' },
+      { key: 'stressWeight', label: 'Stress Weight', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.stressWeight' },
+      { key: 'stressAlpha', label: 'Stress Alpha', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.stressAlpha' },
+      { key: 'minDistance', label: 'Min Distance', type: 'slider', min: 0.05, max: 1, path: 'visualisation.graphs.logseq.physics.minDistance' },
+      { key: 'maxRepulsionDist', label: 'Max Repulsion Distance', type: 'slider', min: 10, max: 200, path: 'visualisation.graphs.logseq.physics.maxRepulsionDist' },
+      { key: 'repulsionCutoff', label: 'Repulsion Cutoff', type: 'slider', min: 10, max: 200, path: 'visualisation.graphs.logseq.physics.repulsionCutoff' },
+      { key: 'repulsionSofteningEpsilon', label: 'Repulsion Epsilon', type: 'slider', min: 0.00001, max: 0.01, path: 'visualisation.graphs.logseq.physics.repulsionSofteningEpsilon' },
+      { key: 'gridCellSize', label: 'Grid Cell Size', type: 'slider', min: 10, max: 100, path: 'visualisation.graphs.logseq.physics.gridCellSize' },
+      { key: 'massScale', label: 'Mass Scale', type: 'slider', min: 0.1, max: 10, path: 'visualisation.graphs.logseq.physics.massScale' },
+      { key: 'boundaryDamping', label: 'Boundary Damping', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.boundaryDamping' },
+      { key: 'boundaryExtremeMultiplier', label: 'Boundary Extreme Multiplier', type: 'slider', min: 1, max: 5, path: 'visualisation.graphs.logseq.physics.boundaryExtremeMultiplier' },
+      { key: 'boundaryExtremeForceMultiplier', label: 'Boundary Force Multiplier', type: 'slider', min: 1, max: 20, path: 'visualisation.graphs.logseq.physics.boundaryExtremeForceMultiplier' },
+      { key: 'boundaryVelocityDamping', label: 'Boundary Velocity Damping', type: 'slider', min: 0, max: 1, path: 'visualisation.graphs.logseq.physics.boundaryVelocityDamping' }
     ]
   },

+  // 5. RENDERING - Lighting and rendering quality
+  rendering: {
+    title: 'Lighting & Rendering',
+    fields: [
+      // Lighting
+      { key: 'ambientLight', label: 'Ambient Light', type: 'slider', min: 0, max: 2, path: 'visualisation.rendering.ambientLightIntensity' },
+      { key: 'directionalLight', label: 'Directional Light', type: 'slider', min: 0, max: 2, path: 'visualisation.rendering.directionalLightIntensity' },
+
+      // Rendering Quality
+      { key: 'antialiasing', label: 'Antialiasing', type: 'toggle', path: 'visualisation.rendering.enableAntialiasing' },
+      { key: 'shadows', label: 'Shadows', type: 'toggle', path: 'visualisation.rendering.enableShadows' },
+      { key: 'ambientOcclusion', label: 'Ambient Occlusion', type: 'toggle', path: 'visualisation.rendering.enableAmbientOcclusion' },
+
+      // Advanced Rendering
+      { key: 'enableInstancing', label: 'GPU Instancing', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableInstancing' },
+      { key: 'enableMetadataShape', label: 'Metadata Shape', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableMetadataShape' },
+      { key: 'enableMetadataVis', label: 'Metadata Visualisation', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableMetadataVisualisation' },
+      { key: 'nodeImportance', label: 'Node Importance', type: 'toggle', path: 'visualisation.graphs.logseq.nodes.enableImportance' }
+    ]
+  },
+
+  // 6. XR/IMMERSIVE - VR/AR and immersive mode settings
   xr: {
-    title: 'XR/AR Settings',
+    title: 'XR / Immersive',
     fields: [
-      { key: 'persistSettings', label: 'Persist Settings', type: 'toggle', path: 'system.persistSettingsOnServer' },
-      { key: 'customBackendURL', label: 'Custom Backend URL', type: 'text', path: 'system.customBackendUrl' },
       { key: 'xrEnabled', label: 'XR Mode', type: 'toggle', path: 'xr.enabled' },
       { key: 'xrQuality', label: 'XR Quality', type: 'select', options: ['Low', 'Medium', 'High'], path: 'xr.quality' },
       { key: 'xrRenderScale', label: 'XR Render Scale', type: 'slider', min: 0.5, max: 2, path: 'xr.renderScale' },
+      { key: 'xrAdaptiveQuality', label: 'Adaptive Quality', type: 'toggle', path: 'xr.enableAdaptiveQuality' },
+      { key: 'xrPerformancePreset', label: 'Performance Preset', type: 'select', options: ['Battery Saver', 'Balanced', 'Performance'], path: 'xr.performance.preset' },
       { key: 'handTracking', label: 'Hand Tracking', type: 'toggle', path: 'xr.handTracking.enabled' },
-      { key: 'enableHaptics', label: 'Haptics', type: 'toggle', path: 'xr.interactions.enableHaptics' },
-      { key: 'xrComputeMode', label: 'XR Compute Mode', type: 'toggle', path: 'xr.gpu.enableOptimizedCompute' },
-      { key: 'xrPerformancePreset', label: 'XR Performance', type: 'select', options: ['Battery Saver', 'Balanced', 'Performance'], path: 'xr.performance.preset' },
-      { key: 'xrAdaptiveQuality', label: 'XR Adaptive Quality', type: 'toggle', path: 'xr.enableAdaptiveQuality' }
+      { key: 'enableHaptics', label: 'Haptic Feedback', type: 'toggle', path: 'xr.interactions.enableHaptics' },
+      { key: 'xrComputeMode', label: 'Optimized Compute', type: 'toggle', path: 'xr.gpu.enableOptimizedCompute' }
+    ]
+  },
+
+  // 7. SYSTEM - System and connection settings
+  system: {
+    title: 'System Settings',
+    fields: [
+      { key: 'persistSettings', label: 'Persist Settings on Server', type: 'toggle', path: 'system.persistSettingsOnServer' },
+      { key: 'customBackendURL', label: 'Custom Backend URL', type: 'text', path: 'system.customBackendUrl' },
+      { key: 'enableDebug', label: 'Debug Mode', type: 'toggle', path: 'system.debug.enabled' }
+    ]
+  },
+
+  // 8. AUTHENTICATION - Login and authentication
+  auth: {
+    title: 'Authentication',
+    fields: [
+      { key: 'nostr', label: 'Nostr Login', type: 'nostr-button', path: 'auth.nostr' },
+      { key: 'enabled', label: 'Enable Authentication', type: 'toggle', path: 'auth.enabled' },
+      { key: 'required', label: 'Require Authentication', type: 'toggle', path: 'auth.required' },
+      { key: 'provider', label: 'Auth Provider', type: 'text', path: 'auth.provider' }
     ]
   }
 };
diff --git a/client/src/features/visualisation/components/IntegratedControlPanel.tsx b/client/src/features/visualisation/components/IntegratedControlPanel.tsx
index d94938af..b59f1e91 100644
--- a/client/src/features/visualisation/components/IntegratedControlPanel.tsx
+++ b/client/src/features/visualisation/components/IntegratedControlPanel.tsx
@@ -34,6 +34,7 @@ import {
   RestoredGraphExportTab
 } from './ControlPanel/RestoredGraphTabs';
 import { SettingsTabContent } from './ControlPanel/SettingsTabContent';
+import { OntologyPanel } from '../../ontology/components/OntologyPanel';

 export const IntegratedControlPanel: React.FC<ControlPanelProps> = ({
   showStats,
@@ -132,6 +133,9 @@ export const IntegratedControlPanel: React.FC<ControlPanelProps> = ({
           />
         );

+      case 'ontology':
+        return <OntologyPanel />;
+
       // Settings Tabs
       default:
         return <SettingsTabContent sectionId={tabId} />;
diff --git a/client/src/features/visualisation/components/tabs/GraphInteractionTab.tsx b/client/src/features/visualisation/components/tabs/GraphInteractionTab.tsx
index ef93d9fb..c83d3080 100644
--- a/client/src/features/visualisation/components/tabs/GraphInteractionTab.tsx
+++ b/client/src/features/visualisation/components/tabs/GraphInteractionTab.tsx
@@ -100,8 +100,18 @@ export const GraphInteractionTab: React.FC<GraphInteractionTabProps> = ({

   // Settings store for head tracking
   const { settings, updateSettings } = useSettingsStore();
+  const ensureLoaded = useSettingsStore(state => state.ensureLoaded);
   const headTrackingEnabled = settings?.visualisation?.interaction?.headTrackedParallax?.enabled ?? false;

+  // Preload head tracking settings on mount
+  useEffect(() => {
+    ensureLoaded([
+      'visualisation.interaction.headTrackedParallax.enabled',
+      'visualisation.interaction.headTrackedParallax.sensitivity',
+      'visualisation.interaction.headTrackedParallax.cameraMode',
+    ]);
+  }, [ensureLoaded]);
+
   const handleHeadTrackingToggle = useCallback((enabled: boolean) => {
     updateSettings(draft => {
       if (!draft.visualisation) draft.visualisation = {} as any;
diff --git a/client/src/hooks/useSettingsSubscription.ts b/client/src/hooks/useSettingsSubscription.ts
new file mode 100644
index 00000000..818af4d5
--- /dev/null
+++ b/client/src/hooks/useSettingsSubscription.ts
@@ -0,0 +1,104 @@
+import { useEffect, useState, useRef } from 'react';
+import { settingsApi } from '../api/settingsApi';
+import { createLogger } from '../utils/loggerConfig';
+
+const logger = createLogger('useSettingsSubscription');
+
+export function useSettingsSubscription<T>(
+  path: string,
+  defaultValue?: T
+): T | undefined {
+  const [value, setValue] = useState<T | undefined>(defaultValue);
+  const isMountedRef = useRef(true);
+
+  useEffect(() => {
+    isMountedRef.current = true;
+
+    // Initial fetch
+    settingsApi
+      .getSettingByPath(path)
+      .then((fetchedValue) => {
+        if (isMountedRef.current) {
+          setValue(fetchedValue);
+        }
+      })
+      .catch((error) => {
+        logger.error(`Failed to fetch initial value for ${path}:`, error);
+      });
+
+    // Subscribe to changes
+    const unsubscribe = settingsApi.subscribeToSettingChanges(
+      path,
+      (changedPath, newValue) => {
+        if (isMountedRef.current) {
+          logger.debug(`Setting ${changedPath} changed to:`, newValue);
+          setValue(newValue);
+        }
+      }
+    );
+
+    return () => {
+      isMountedRef.current = false;
+      unsubscribe();
+    };
+  }, [path]);
+
+  return value;
+}
+
+export function useSettingsBatch<T extends Record<string, any>>(
+  paths: string[]
+): T | null {
+  const [values, setValues] = useState<T | null>(null);
+  const isMountedRef = useRef(true);
+  const pathsKey = paths.join(',');
+
+  useEffect(() => {
+    isMountedRef.current = true;
+
+    // Initial fetch
+    settingsApi
+      .getSettingsByPaths(paths)
+      .then((fetchedValues) => {
+        if (isMountedRef.current) {
+          setValues(fetchedValues as T);
+        }
+      })
+      .catch((error) => {
+        logger.error(`Failed to fetch batch settings:`, error);
+      });
+
+    // Subscribe to each path
+    const unsubscribers = paths.map((path) =>
+      settingsApi.subscribeToSettingChanges(path, (changedPath, newValue) => {
+        if (isMountedRef.current) {
+          setValues((prev) => ({
+            ...prev,
+            [changedPath]: newValue,
+          } as T));
+        }
+      })
+    );
+
+    return () => {
+      isMountedRef.current = false;
+      unsubscribers.forEach((unsub) => unsub());
+    };
+  }, [pathsKey]);
+
+  return values;
+}
+
+export function useCacheMetrics() {
+  const [metrics, setMetrics] = useState(settingsApi.getCacheMetrics());
+
+  useEffect(() => {
+    const interval = setInterval(() => {
+      setMetrics(settingsApi.getCacheMetrics());
+    }, 5000);
+
+    return () => clearInterval(interval);
+  }, []);
+
+  return metrics;
+}
diff --git a/client/src/immersive/__tests__/GraphRenderer.test.ts b/client/src/immersive/__tests__/GraphRenderer.test.ts
new file mode 100644
index 00000000..bc89b4ec
--- /dev/null
+++ b/client/src/immersive/__tests__/GraphRenderer.test.ts
@@ -0,0 +1,397 @@
+/**
+ * Test suite for GraphRenderer deduplication and incremental updates
+ * HIGH PRIORITY: Verifies node deduplication and performance optimizations
+ */
+
+import { GraphRenderer } from '../babylon/GraphRenderer';
+import * as BABYLON from '@babylonjs/core';
+
+// Mock Babylon.js components
+jest.mock('@babylonjs/core', () => {
+  const actualBabylon = jest.requireActual('@babylonjs/core');
+
+  class MockInstancedMesh {
+    public position = { x: 0, y: 0, z: 0 };
+    public metadata: any = {};
+    private _isDisposed = false;
+
+    dispose() {
+      this._isDisposed = true;
+    }
+
+    isDisposed() {
+      return this._isDisposed;
+    }
+  }
+
+  class MockMesh {
+    public material: any = null;
+    public isVisible = true;
+    private instanceCounter = 0;
+
+    createInstance(name: string) {
+      return new MockInstancedMesh();
+    }
+
+    dispose() {
+      // Mock disposal
+    }
+  }
+
+  return {
+    ...actualBabylon,
+    MeshBuilder: {
+      CreateSphere: jest.fn(() => new MockMesh()),
+      CreateLineSystem: jest.fn(() => ({ dispose: jest.fn() })),
+    },
+    StandardMaterial: jest.fn().mockImplementation(() => ({
+      diffuseColor: null,
+      emissiveColor: null,
+      specularColor: null,
+    })),
+    Scene: jest.fn().mockImplementation(() => ({
+      metadata: {},
+    })),
+    Vector3: actualBabylon.Vector3,
+    Color3: actualBabylon.Color3,
+  };
+});
+
+jest.mock('@babylonjs/gui', () => ({
+  AdvancedDynamicTexture: {
+    CreateFullscreenUI: jest.fn(() => ({
+      addControl: jest.fn(),
+      dispose: jest.fn(),
+    })),
+  },
+  TextBlock: jest.fn().mockImplementation((id, label) => ({
+    id,
+    label,
+    dispose: jest.fn(),
+  })),
+}));
+
+describe('GraphRenderer - Deduplication and Incremental Updates', () => {
+  let scene: BABYLON.Scene;
+  let renderer: GraphRenderer;
+  let consoleWarnSpy: jest.SpyInstance;
+  let consoleLogSpy: jest.SpyInstance;
+
+  beforeEach(() => {
+    scene = new BABYLON.Scene(null as any);
+    renderer = new GraphRenderer(scene);
+    consoleWarnSpy = jest.spyOn(console, 'warn').mockImplementation();
+    consoleLogSpy = jest.spyOn(console, 'log').mockImplementation();
+  });
+
+  afterEach(() => {
+    renderer.dispose();
+    consoleWarnSpy.mockRestore();
+    consoleLogSpy.mockRestore();
+  });
+
+  describe('Node Deduplication (HIGH PRIORITY)', () => {
+    it('should deduplicate nodes with the same ID', () => {
+      const duplicateNodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '2', label: 'Node 2', x: 1, y: 1, z: 1 },
+        { id: '1', label: 'Node 1 Duplicate', x: 2, y: 2, z: 2 }, // Duplicate ID
+        { id: '3', label: 'Node 3', x: 3, y: 3, z: 3 },
+        { id: '2', label: 'Node 2 Duplicate', x: 4, y: 4, z: 4 }, // Duplicate ID
+      ];
+
+      renderer.updateNodes(duplicateNodes);
+
+      // Should log warning for duplicates
+      expect(consoleWarnSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Duplicate node ID detected: 1')
+      );
+      expect(consoleWarnSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Duplicate node ID detected: 2')
+      );
+
+      // Should log deduplication summary
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Input: 5, Unique: 3, Duplicates removed: 2')
+      );
+    });
+
+    it('should keep the first occurrence when duplicates exist', () => {
+      const nodes = [
+        { id: 'A', label: 'First A', x: 1, y: 1, z: 1 },
+        { id: 'A', label: 'Second A', x: 2, y: 2, z: 2 },
+      ];
+
+      renderer.updateNodes(nodes);
+
+      // The internal map should only have one node with ID 'A'
+      // We can verify this by checking that only 1 node was created
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 1 new nodes')
+      );
+    });
+
+    it('should handle empty node array', () => {
+      renderer.updateNodes([]);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Input: 0, Unique: 0')
+      );
+    });
+
+    it('should handle array with all unique nodes', () => {
+      const uniqueNodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '2', label: 'Node 2', x: 1, y: 1, z: 1 },
+        { id: '3', label: 'Node 3', x: 2, y: 2, z: 2 },
+      ];
+
+      renderer.updateNodes(uniqueNodes);
+
+      // Should not warn about duplicates
+      expect(consoleWarnSpy).not.toHaveBeenCalledWith(
+        expect.stringContaining('Duplicate node ID detected')
+      );
+
+      // Should create all 3 nodes
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 3 new nodes')
+      );
+    });
+  });
+
+  describe('Incremental Updates (PERFORMANCE)', () => {
+    it('should only create new nodes on first update', () => {
+      const nodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '2', label: 'Node 2', x: 1, y: 1, z: 1 },
+      ];
+
+      renderer.updateNodes(nodes);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 2 new nodes, updated 0 existing nodes')
+      );
+    });
+
+    it('should update existing nodes instead of recreating them', () => {
+      const initialNodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '2', label: 'Node 2', x: 1, y: 1, z: 1 },
+      ];
+
+      renderer.updateNodes(initialNodes);
+      consoleLogSpy.mockClear();
+
+      // Update with same nodes but different positions
+      const updatedNodes = [
+        { id: '1', label: 'Node 1', x: 5, y: 5, z: 5 },
+        { id: '2', label: 'Node 2', x: 6, y: 6, z: 6 },
+      ];
+
+      renderer.updateNodes(updatedNodes);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 0 new nodes, updated 2 existing nodes')
+      );
+    });
+
+    it('should add new nodes while keeping existing ones', () => {
+      const initialNodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '2', label: 'Node 2', x: 1, y: 1, z: 1 },
+      ];
+
+      renderer.updateNodes(initialNodes);
+      consoleLogSpy.mockClear();
+
+      // Add new node while keeping existing ones
+      const expandedNodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '2', label: 'Node 2', x: 1, y: 1, z: 1 },
+        { id: '3', label: 'Node 3', x: 2, y: 2, z: 2 }, // New
+      ];
+
+      renderer.updateNodes(expandedNodes);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 1 new nodes, updated 2 existing nodes')
+      );
+    });
+
+    it('should remove obsolete nodes', () => {
+      const initialNodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '2', label: 'Node 2', x: 1, y: 1, z: 1 },
+        { id: '3', label: 'Node 3', x: 2, y: 2, z: 2 },
+      ];
+
+      renderer.updateNodes(initialNodes);
+      consoleLogSpy.mockClear();
+
+      // Remove node '2'
+      const reducedNodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '3', label: 'Node 3', x: 2, y: 2, z: 2 },
+      ];
+
+      renderer.updateNodes(reducedNodes);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Removed 1 obsolete nodes')
+      );
+    });
+
+    it('should handle complete node replacement', () => {
+      const initialNodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '2', label: 'Node 2', x: 1, y: 1, z: 1 },
+      ];
+
+      renderer.updateNodes(initialNodes);
+      consoleLogSpy.mockClear();
+
+      // Completely different set of nodes
+      const newNodes = [
+        { id: '3', label: 'Node 3', x: 3, y: 3, z: 3 },
+        { id: '4', label: 'Node 4', x: 4, y: 4, z: 4 },
+      ];
+
+      renderer.updateNodes(newNodes);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Removed 2 obsolete nodes')
+      );
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 2 new nodes, updated 0 existing nodes')
+      );
+    });
+  });
+
+  describe('Position Handling', () => {
+    it('should use positions array when provided', () => {
+      const nodes = [
+        { id: '1', label: 'Node 1' },
+        { id: '2', label: 'Node 2' },
+      ];
+
+      const positions = new Float32Array([
+        1.0, 2.0, 3.0,  // Node 1 position
+        4.0, 5.0, 6.0,  // Node 2 position
+      ]);
+
+      renderer.updateNodes(nodes, positions);
+
+      // Verify positions were used (we can check via logs or internal state)
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 2 new nodes')
+      );
+    });
+
+    it('should warn when node position is not found', () => {
+      // This tests the getNodePosition fallback
+      const edges = [
+        { source: '999', target: '1000' }, // Non-existent nodes
+      ];
+
+      const positions = new Float32Array([1.0, 2.0, 3.0]);
+
+      renderer.updateEdges(edges, positions);
+
+      expect(consoleWarnSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Node position not found for ID "999"')
+      );
+      expect(consoleWarnSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Node position not found for ID "1000"')
+      );
+    });
+
+    it('should create synthetic nodes from positions array when nodes are empty', () => {
+      const positions = new Float32Array([
+        1.0, 2.0, 3.0,
+        4.0, 5.0, 6.0,
+        7.0, 8.0, 9.0,
+      ]);
+
+      renderer.updateNodes([], positions);
+
+      // Should create 3 synthetic nodes
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Input: 3, Unique: 3')
+      );
+    });
+  });
+
+  describe('Memory Management', () => {
+    it('should clear all internal state on dispose', () => {
+      const nodes = [
+        { id: '1', label: 'Node 1', x: 0, y: 0, z: 0 },
+        { id: '2', label: 'Node 2', x: 1, y: 1, z: 1 },
+      ];
+
+      renderer.updateNodes(nodes);
+
+      // Should have nodes in internal state
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 2 new nodes')
+      );
+
+      renderer.dispose();
+
+      // After disposal, next update should create new nodes (not update existing)
+      const newRenderer = new GraphRenderer(scene);
+      newRenderer.updateNodes(nodes);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 2 new nodes')
+      );
+
+      newRenderer.dispose();
+    });
+  });
+
+  describe('Edge Cases', () => {
+    it('should handle nodes with numeric string IDs', () => {
+      const nodes = [
+        { id: '123', label: 'Node 123', x: 0, y: 0, z: 0 },
+        { id: '456', label: 'Node 456', x: 1, y: 1, z: 1 },
+      ];
+
+      renderer.updateNodes(nodes);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 2 new nodes')
+      );
+    });
+
+    it('should handle nodes with mixed ID types', () => {
+      const nodes = [
+        { id: 'abc', label: 'Node ABC', x: 0, y: 0, z: 0 },
+        { id: '123', label: 'Node 123', x: 1, y: 1, z: 1 },
+        { id: 'xyz_789', label: 'Node XYZ', x: 2, y: 2, z: 2 },
+      ];
+
+      renderer.updateNodes(nodes);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 3 new nodes')
+      );
+    });
+
+    it('should handle large node sets efficiently', () => {
+      const largeNodeSet = Array.from({ length: 1000 }, (_, i) => ({
+        id: String(i),
+        label: `Node ${i}`,
+        x: i,
+        y: i,
+        z: i,
+      }));
+
+      renderer.updateNodes(largeNodeSet);
+
+      expect(consoleLogSpy).toHaveBeenCalledWith(
+        expect.stringContaining('Created 1000 new nodes')
+      );
+    });
+  });
+});
diff --git a/client/src/immersive/babylon/GraphRenderer.ts b/client/src/immersive/babylon/GraphRenderer.ts
index dfd9db03..70093e5a 100644
--- a/client/src/immersive/babylon/GraphRenderer.ts
+++ b/client/src/immersive/babylon/GraphRenderer.ts
@@ -5,9 +5,7 @@ import {
   MeshBuilder,
   StandardMaterial,
   Color3,
-  Vector3,
-  LineSystem,
-  Matrix
+  Vector3
 } from '@babylonjs/core';
 import { AdvancedDynamicTexture, TextBlock } from '@babylonjs/gui';

@@ -19,7 +17,8 @@ export class GraphRenderer {
   private scene: Scene;
   private nodeMasterMesh: Mesh | null = null;
   private nodeInstances: InstancedMesh[] = [];
-  private edgeLineSystem: LineSystem | null = null;
+  private nodeInstanceMap = new Map<string, InstancedMesh>(); // Track nodes by ID for incremental updates
+  private edgeLineSystem: Mesh | null = null;
   private labelTexture: AdvancedDynamicTexture | null = null;
   private labelBlocks: Map<string, TextBlock> = new Map();

@@ -61,18 +60,64 @@ export class GraphRenderer {
       }));
     }

-    console.log('GraphRenderer: Updating nodes', nodeList.length, 'positions:', positions?.length);
+    // DEDUPLICATION: Remove duplicate nodes by ID (HIGH PRIORITY FIX)
+    const uniqueNodes = new Map<string, any>();
+    let duplicateCount = 0;
+    nodeList.forEach(node => {
+      if (!uniqueNodes.has(node.id)) {
+        uniqueNodes.set(node.id, node);
+      } else {
+        duplicateCount++;
+        console.warn(`GraphRenderer: Duplicate node ID detected: ${node.id}`);
+      }
+    });

-    // Clear existing instances
-    this.nodeInstances.forEach(instance => instance.dispose());
-    this.nodeInstances = [];
+    const deduplicatedNodes = Array.from(uniqueNodes.values());
+
+    console.log(
+      `GraphRenderer: Updating nodes - Input: ${nodeList.length}, Unique: ${deduplicatedNodes.length}` +
+      (duplicateCount > 0 ? `, Duplicates removed: ${duplicateCount}` : '')
+    );
+
+    // INCREMENTAL UPDATES: Track which nodes to keep, update, or create
+    const newNodeIds = new Set(deduplicatedNodes.map(n => n.id));
+
+    // Remove nodes that no longer exist
+    const nodesToRemove: string[] = [];
+    this.nodeInstanceMap.forEach((instance, nodeId) => {
+      if (!newNodeIds.has(nodeId)) {
+        instance.dispose();
+        nodesToRemove.push(nodeId);
+        // Remove from instances array
+        const idx = this.nodeInstances.indexOf(instance);
+        if (idx !== -1) {
+          this.nodeInstances.splice(idx, 1);
+        }
+      }
+    });
+
+    nodesToRemove.forEach(nodeId => this.nodeInstanceMap.delete(nodeId));

-    // Create new instances for each node
-    for (let i = 0; i < nodeList.length; i++) {
-      const node = nodeList[i];
+    if (nodesToRemove.length > 0) {
+      console.log(`GraphRenderer: Removed ${nodesToRemove.length} obsolete nodes`);
+    }

-      // Create instance
-      const instance = this.nodeMasterMesh.createInstance(`node_${node.id}`);
+    // Update existing nodes or create new ones
+    let createdCount = 0;
+    let updatedCount = 0;
+
+    deduplicatedNodes.forEach((node, i) => {
+      let instance = this.nodeInstanceMap.get(node.id);
+
+      if (!instance) {
+        // Create new instance
+        instance = this.nodeMasterMesh!.createInstance(`node_${node.id}`);
+        this.nodeInstanceMap.set(node.id, instance);
+        this.nodeInstances.push(instance);
+        createdCount++;
+      } else {
+        updatedCount++;
+      }

       // Set position from physics simulation or node data
       let x = node.position?.x || node.x || 0;
@@ -85,16 +130,17 @@ export class GraphRenderer {
         z = positions[i * 3 + 2];
       }

-      // Set position directly
+      // Update position (for both new and existing nodes)
       instance.position.x = x;
       instance.position.y = y;
       instance.position.z = z;

-      // Store instance metadata for interaction
+      // Update metadata
       instance.metadata = { nodeId: node.id, nodeData: node };
+    });

-      // Add to instances array
-      this.nodeInstances.push(instance);
+    if (createdCount > 0 || updatedCount > 0) {
+      console.log(`GraphRenderer: Created ${createdCount} new nodes, updated ${updatedCount} existing nodes`);
     }
   }

@@ -179,6 +225,13 @@ export class GraphRenderer {
       }
     }

+    // Improved error handling: Log warning for debugging
+    console.warn(
+      `GraphRenderer: Node position not found for ID "${nodeId}" ` +
+      `(index: ${nodeIndex}, positions array length: ${positions?.length || 0}). ` +
+      `Using fallback random position.`
+    );
+
     // Fallback to a random position if not found
     return new Vector3(
       (Math.random() - 0.5) * 10,
@@ -191,6 +244,7 @@ export class GraphRenderer {
     // Dispose all node instances
     this.nodeInstances.forEach(instance => instance.dispose());
     this.nodeInstances = [];
+    this.nodeInstanceMap.clear();

     if (this.nodeMasterMesh) {
       this.nodeMasterMesh.dispose();
diff --git a/client/src/immersive/babylon/XRUI.ts b/client/src/immersive/babylon/XRUI.ts
index d0a72f0f..e42a1a70 100644
--- a/client/src/immersive/babylon/XRUI.ts
+++ b/client/src/immersive/babylon/XRUI.ts
@@ -24,7 +24,7 @@ interface Settings {
     showLabels?: boolean;
     edgeOpacity?: number;
   };
-  visualization?: {
+  visualisation?: {
     showBots?: boolean;
     showEdges?: boolean;
   };
@@ -180,12 +180,12 @@ export class XRUI {
     showBotsCheck.heightInPixels = 30;
     showBotsCheck.color = 'white';
     showBotsCheck.background = 'gray';
-    showBotsCheck.isChecked = this.settings.visualization?.showBots !== false;
+    showBotsCheck.isChecked = this.settings.visualisation?.showBots !== false;
     this.mainPanel.addControl(showBotsCheck);

     showBotsCheck.onIsCheckedChangedObservable.add((value) => {
       if (this.onSettingsChange) {
-        this.onSettingsChange('visualization.showBots', value);
+        this.onSettingsChange('visualisation.showBots', value);
       }
     });
   }
@@ -247,8 +247,8 @@ export class XRUI {

     // Update show bots checkbox
     const showBotsCheck = this.uiTexture.getControlByName('showBots') as Checkbox;
-    if (showBotsCheck && this.settings.visualization?.showBots !== undefined) {
-      showBotsCheck.isChecked = this.settings.visualization.showBots;
+    if (showBotsCheck && this.settings.visualisation?.showBots !== undefined) {
+      showBotsCheck.isChecked = this.settings.visualisation.showBots;
     }
   }

diff --git a/client/src/services/SettingsCacheClient.ts b/client/src/services/SettingsCacheClient.ts
new file mode 100644
index 00000000..fcbfd60d
--- /dev/null
+++ b/client/src/services/SettingsCacheClient.ts
@@ -0,0 +1,513 @@
+import { createLogger } from '../utils/loggerConfig';
+import { webSocketService } from './WebSocketService';
+
+interface CachedSetting {
+  value: any;
+  path: string;
+  timestamp: number;
+  version: number;
+  hash: string;
+  ttl: number;
+}
+
+interface CacheMetrics {
+  hits: number;
+  misses: number;
+  invalidations: number;
+  bandwidthSaved: number;
+  storageUsed: number;
+  lastSync: number;
+}
+
+interface DeltaUpdate {
+  path: string;
+  value: any;
+  oldValue?: any;
+  timestamp: number;
+  operation: 'set' | 'delete' | 'batch';
+}
+
+interface PerformanceMetrics {
+  cacheHitRate: number;
+  averageResponseTime: number;
+  bandwidthSavings: number;
+  totalRequests: number;
+}
+
+type SettingsChangeCallback = (path: string, value: any) => void;
+
+const logger = createLogger('SettingsCacheClient');
+
+export class SettingsCacheClient {
+  private cache = new Map<string, CachedSetting>();
+  private metrics: CacheMetrics;
+  private subscribers = new Map<string, Set<SettingsChangeCallback>>();
+  private readonly CACHE_PREFIX = 'hive_settings_';
+  private readonly CACHE_VERSION = '2.0';
+  private readonly DEFAULT_TTL = 300000; // 5 minutes
+  private readonly MAX_CACHE_SIZE = 1000;
+  private readonly STORAGE_QUOTA = 5 * 1024 * 1024; // 5MB
+  private wsUnsubscribe: (() => void) | null = null;
+
+  constructor() {
+    this.metrics = {
+      hits: 0,
+      misses: 0,
+      invalidations: 0,
+      bandwidthSaved: 0,
+      storageUsed: 0,
+      lastSync: Date.now()
+    };
+
+    this.initializeCache();
+    this.setupWebSocketListener();
+    this.startCacheMaintenanceTimer();
+  }
+
+  private initializeCache(): void {
+    try {
+      const stored = localStorage.getItem(`${this.CACHE_PREFIX}cache`);
+      if (stored) {
+        const parsedCache = JSON.parse(stored);
+
+        if (parsedCache.version === this.CACHE_VERSION) {
+          Object.entries(parsedCache.data).forEach(([key, value]) => {
+            const cached = value as CachedSetting;
+
+            if (Date.now() - cached.timestamp < cached.ttl) {
+              this.cache.set(key, cached);
+            }
+          });
+
+          logger.info(`Loaded ${this.cache.size} cached settings from localStorage`);
+        } else {
+          logger.info('Cache version mismatch, clearing localStorage cache');
+          this.clearLocalStorage();
+        }
+      }
+
+      const storedMetrics = localStorage.getItem(`${this.CACHE_PREFIX}metrics`);
+      if (storedMetrics) {
+        this.metrics = { ...this.metrics, ...JSON.parse(storedMetrics) };
+      }
+
+    } catch (error) {
+      logger.warn('Failed to initialize cache from localStorage:', error);
+      this.clearLocalStorage();
+    }
+  }
+
+  private setupWebSocketListener(): void {
+    this.wsUnsubscribe = webSocketService.onMessage((message) => {
+      switch (message.type) {
+        case 'settings_changed':
+          this.handleSettingChanged(message.data);
+          break;
+
+        case 'settings_batch_changed':
+          this.handleBatchSettingsChanged(message.data);
+          break;
+
+        case 'cache_invalidate':
+          this.handleCacheInvalidation(message.data);
+          break;
+      }
+    });
+  }
+
+  private handleSettingChanged(data: any): void {
+    const { path, value, timestamp } = data;
+
+    this.setCachedValue(path, value, {
+      timestamp,
+      fromWebSocket: true
+    });
+
+    this.notifySubscribers(path, value);
+  }
+
+  private handleBatchSettingsChanged(data: any): void {
+    const { updates, timestamp } = data;
+
+    updates.forEach((update: any) => {
+      this.setCachedValue(update.path, update.value, {
+        timestamp,
+        fromWebSocket: true
+      });
+    });
+
+    this.notifyBatchSubscribers(updates);
+  }
+
+  private handleCacheInvalidation(data: any): void {
+    const { paths, reason } = data;
+
+    paths.forEach((path: string) => {
+      this.cache.delete(path);
+      this.invalidateLocalStorage(path);
+      this.metrics.invalidations++;
+    });
+
+    logger.info(`Cache invalidated for ${paths.length} paths. Reason: ${reason}`);
+  }
+
+  public async get(path: string, options: { useCache?: boolean, ttl?: number } = {}): Promise<any> {
+    const startTime = performance.now();
+    const { useCache = true, ttl = this.DEFAULT_TTL } = options;
+
+    if (useCache) {
+      const cached = this.getCachedValue(path);
+      if (cached && this.isCacheValid(cached)) {
+        this.metrics.hits++;
+        this.metrics.bandwidthSaved += this.estimatePayloadSize(cached.value);
+
+        const responseTime = performance.now() - startTime;
+        logger.debug(`Cache hit for ${path} in ${responseTime.toFixed(2)}ms`);
+
+        return cached.value;
+      }
+    }
+
+    this.metrics.misses++;
+
+    try {
+      const response = await fetch(`/api/settings/path?path=${encodeURIComponent(path)}`, {
+        headers: {
+          'Cache-Control': 'no-cache',
+          'Accept': 'application/json'
+        }
+      });
+
+      if (!response.ok) {
+        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
+      }
+
+      const result = await response.json();
+
+      if (result.success && result.value !== undefined) {
+        this.setCachedValue(path, result.value, { ttl });
+
+        const responseTime = performance.now() - startTime;
+        logger.debug(`Fetched ${path} from server in ${responseTime.toFixed(2)}ms`);
+
+        return result.value;
+      } else {
+        throw new Error(result.error || 'Failed to get setting value');
+      }
+
+    } catch (error) {
+      logger.error(`Failed to fetch setting ${path}:`, error);
+      throw error;
+    }
+  }
+
+  public async getBatch(paths: string[], options: { useCache?: boolean } = {}): Promise<Record<string, any>> {
+    const startTime = performance.now();
+    const { useCache = true } = options;
+
+    const results: Record<string, any> = {};
+    const uncachedPaths: string[] = [];
+
+    if (useCache) {
+      paths.forEach(path => {
+        const cached = this.getCachedValue(path);
+        if (cached && this.isCacheValid(cached)) {
+          results[path] = cached.value;
+          this.metrics.hits++;
+        } else {
+          uncachedPaths.push(path);
+          this.metrics.misses++;
+        }
+      });
+    } else {
+      uncachedPaths.push(...paths);
+      this.metrics.misses += paths.length;
+    }
+
+    if (uncachedPaths.length > 0) {
+      try {
+        const response = await fetch('/api/settings/batch', {
+          method: 'POST',
+          headers: {
+            'Content-Type': 'application/json',
+            'Cache-Control': 'no-cache'
+          },
+          body: JSON.stringify({
+            paths: uncachedPaths
+          })
+        });
+
+        if (!response.ok) {
+          throw new Error(`HTTP ${response.status}: ${response.statusText}`);
+        }
+
+        const batchResult = await response.json();
+
+        Object.entries(batchResult).forEach(([path, value]) => {
+          results[path] = value;
+          this.setCachedValue(path, value);
+        });
+
+      } catch (error) {
+        logger.error('Failed to fetch batch settings:', error);
+        throw error;
+      }
+    }
+
+    const responseTime = performance.now() - startTime;
+    const cacheHitRate = (this.metrics.hits / (this.metrics.hits + this.metrics.misses)) * 100;
+
+    logger.debug(`Batch fetch completed in ${responseTime.toFixed(2)}ms. Cache hit rate: ${cacheHitRate.toFixed(1)}%`);
+
+    return results;
+  }
+
+  public subscribe(path: string, callback: SettingsChangeCallback): () => void {
+    if (!this.subscribers.has(path)) {
+      this.subscribers.set(path, new Set());
+    }
+
+    this.subscribers.get(path)!.add(callback);
+
+    return () => {
+      const callbacks = this.subscribers.get(path);
+      if (callbacks) {
+        callbacks.delete(callback);
+        if (callbacks.size === 0) {
+          this.subscribers.delete(path);
+        }
+      }
+    };
+  }
+
+  public getPerformanceMetrics(): PerformanceMetrics {
+    const totalRequests = this.metrics.hits + this.metrics.misses;
+
+    return {
+      cacheHitRate: totalRequests > 0 ? (this.metrics.hits / totalRequests) * 100 : 0,
+      averageResponseTime: 0,
+      bandwidthSavings: this.metrics.bandwidthSaved,
+      totalRequests
+    };
+  }
+
+  public clearCache(): void {
+    this.cache.clear();
+    this.clearLocalStorage();
+    this.metrics = {
+      hits: 0,
+      misses: 0,
+      invalidations: 0,
+      bandwidthSaved: 0,
+      storageUsed: 0,
+      lastSync: Date.now()
+    };
+
+    logger.info('All caches cleared and metrics reset');
+  }
+
+  public destroy(): void {
+    if (this.wsUnsubscribe) {
+      this.wsUnsubscribe();
+      this.wsUnsubscribe = null;
+    }
+    this.subscribers.clear();
+    this.cache.clear();
+  }
+
+  private getCachedValue(path: string): CachedSetting | null {
+    return this.cache.get(path) || null;
+  }
+
+  private setCachedValue(path: string, value: any, options: {
+    timestamp?: number,
+    ttl?: number,
+    fromWebSocket?: boolean
+  } = {}): void {
+    const {
+      timestamp = Date.now(),
+      ttl = this.DEFAULT_TTL,
+      fromWebSocket = false
+    } = options;
+
+    const hash = this.calculateHash(value);
+    const version = this.getNextVersion();
+
+    const cached: CachedSetting = {
+      value,
+      path,
+      timestamp,
+      version,
+      hash,
+      ttl
+    };
+
+    this.cache.set(path, cached);
+    this.persistToLocalStorage();
+    this.updateStorageMetrics();
+    this.enforCacheSizeLimit();
+  }
+
+  private isCacheValid(cached: CachedSetting): boolean {
+    return (Date.now() - cached.timestamp) < cached.ttl;
+  }
+
+  private calculateHash(value: any): string {
+    return btoa(JSON.stringify(value)).slice(0, 16);
+  }
+
+  private getNextVersion(): number {
+    return Date.now();
+  }
+
+  private persistToLocalStorage(): void {
+    try {
+      const cacheData = {
+        version: this.CACHE_VERSION,
+        timestamp: Date.now(),
+        data: Object.fromEntries(this.cache)
+      };
+
+      const serialized = JSON.stringify(cacheData);
+
+      if (serialized.length > this.STORAGE_QUOTA) {
+        logger.warn('Cache size exceeds storage quota, performing cleanup');
+        this.cleanupOldestEntries();
+        return;
+      }
+
+      localStorage.setItem(`${this.CACHE_PREFIX}cache`, serialized);
+      this.persistMetrics();
+
+    } catch (error) {
+      logger.warn('Failed to persist cache to localStorage:', error);
+
+      if (error instanceof Error && error.name === 'QuotaExceededError') {
+        this.cleanupOldestEntries();
+      }
+    }
+  }
+
+  private persistMetrics(): void {
+    try {
+      localStorage.setItem(`${this.CACHE_PREFIX}metrics`, JSON.stringify(this.metrics));
+    } catch (error) {
+      logger.warn('Failed to persist metrics:', error);
+    }
+  }
+
+  private invalidateLocalStorage(path: string): void {
+    try {
+      const stored = localStorage.getItem(`${this.CACHE_PREFIX}cache`);
+      if (stored) {
+        const parsed = JSON.parse(stored);
+        delete parsed.data[path];
+        localStorage.setItem(`${this.CACHE_PREFIX}cache`, JSON.stringify(parsed));
+      }
+    } catch (error) {
+      logger.warn('Failed to invalidate localStorage cache:', error);
+    }
+  }
+
+  private clearLocalStorage(): void {
+    try {
+      Object.keys(localStorage).forEach(key => {
+        if (key.startsWith(this.CACHE_PREFIX)) {
+          localStorage.removeItem(key);
+        }
+      });
+    } catch (error) {
+      logger.warn('Failed to clear localStorage:', error);
+    }
+  }
+
+  private updateStorageMetrics(): void {
+    this.metrics.storageUsed = this.cache.size;
+  }
+
+  private enforCacheSizeLimit(): void {
+    if (this.cache.size > this.MAX_CACHE_SIZE) {
+      const entries = Array.from(this.cache.entries());
+      entries.sort((a, b) => a[1].timestamp - b[1].timestamp);
+
+      const toRemove = entries.slice(0, Math.floor(this.MAX_CACHE_SIZE * 0.2));
+      toRemove.forEach(([key]) => {
+        this.cache.delete(key);
+      });
+
+      logger.info(`Removed ${toRemove.length} old cache entries`);
+    }
+  }
+
+  private cleanupOldestEntries(): void {
+    const entries = Array.from(this.cache.entries());
+    entries.sort((a, b) => a[1].timestamp - b[1].timestamp);
+
+    const toRemove = entries.slice(0, Math.floor(entries.length * 0.3));
+    toRemove.forEach(([key]) => {
+      this.cache.delete(key);
+    });
+
+    this.persistToLocalStorage();
+  }
+
+  private estimatePayloadSize(value: any): number {
+    try {
+      return JSON.stringify(value).length;
+    } catch {
+      return 0;
+    }
+  }
+
+  private startCacheMaintenanceTimer(): void {
+    setInterval(() => {
+      const now = Date.now();
+      const expiredKeys: string[] = [];
+
+      this.cache.forEach((cached, key) => {
+        if (now - cached.timestamp > cached.ttl) {
+          expiredKeys.push(key);
+        }
+      });
+
+      expiredKeys.forEach(key => {
+        this.cache.delete(key);
+      });
+
+      if (expiredKeys.length > 0) {
+        logger.debug(`Cleaned up ${expiredKeys.length} expired cache entries`);
+        this.persistToLocalStorage();
+      }
+
+    }, 5 * 60 * 1000);
+  }
+
+  private notifySubscribers(path: string, value: any): void {
+    const callbacks = this.subscribers.get(path);
+    if (callbacks) {
+      callbacks.forEach(callback => {
+        try {
+          callback(path, value);
+        } catch (error) {
+          logger.error(`Error in settings subscriber for ${path}:`, error);
+        }
+      });
+    }
+
+    window.dispatchEvent(new CustomEvent('settingChanged', {
+      detail: { path, value }
+    }));
+  }
+
+  private notifyBatchSubscribers(updates: any[]): void {
+    updates.forEach(({ path, value }) => {
+      this.notifySubscribers(path, value);
+    });
+
+    window.dispatchEvent(new CustomEvent('settingsBatchChanged', {
+      detail: { updates }
+    }));
+  }
+}
+
+export const settingsCacheClient = new SettingsCacheClient();
diff --git a/client/src/services/WebSocketService.ts b/client/src/services/WebSocketService.ts
index a1fe18eb..9105730d 100755
--- a/client/src/services/WebSocketService.ts
+++ b/client/src/services/WebSocketService.ts
@@ -141,30 +141,33 @@ class WebSocketService {
     // Default WebSocket URL
     this.url = this.determineWebSocketUrl();

-    // Update URL when settings change
-    this.updateFromSettings();
-
-    // Subscribe to store changes and manually check customBackendUrl
-    let previousCustomBackendUrl = useSettingsStore.getState().settings?.system?.customBackendUrl;
-    useSettingsStore.subscribe((state) => {
-      const newCustomBackendUrl = state.settings?.system?.customBackendUrl;
-      if (newCustomBackendUrl !== previousCustomBackendUrl) {
-        if (debugState.isEnabled()) {
-          logger.info(`customBackendUrl setting changed from "${previousCustomBackendUrl}" to "${newCustomBackendUrl}", re-evaluating WebSocket URL.`);
-        }
-        previousCustomBackendUrl = newCustomBackendUrl; // Update for next comparison
-        this.updateFromSettings(); // Sets this.url based on the latest state
-        if (this.isConnected || (this.socket && this.socket.readyState === WebSocket.CONNECTING)) {
-          logger.info('Reconnecting WebSocket due to customBackendUrl change.');
-          this.close();
-          setTimeout(() => {
-            this.connect().catch(error => {
-              logger.error('Failed to reconnect WebSocket after URL change:', createErrorMetadata(error));
-            });
-          }, 100);
+    // Defer settings store access until after module initialization
+    setTimeout(() => {
+      // Update URL when settings change
+      this.updateFromSettings();
+
+      // Subscribe to store changes and manually check customBackendUrl
+      let previousCustomBackendUrl = useSettingsStore.getState().settings?.system?.customBackendUrl;
+      useSettingsStore.subscribe((state) => {
+        const newCustomBackendUrl = state.settings?.system?.customBackendUrl;
+        if (newCustomBackendUrl !== previousCustomBackendUrl) {
+          if (debugState.isEnabled()) {
+            logger.info(`customBackendUrl setting changed from "${previousCustomBackendUrl}" to "${newCustomBackendUrl}", re-evaluating WebSocket URL.`);
+          }
+          previousCustomBackendUrl = newCustomBackendUrl; // Update for next comparison
+          this.updateFromSettings(); // Sets this.url based on the latest state
+          if (this.isConnected || (this.socket && this.socket.readyState === WebSocket.CONNECTING)) {
+            logger.info('Reconnecting WebSocket due to customBackendUrl change.');
+            this.close();
+            setTimeout(() => {
+              this.connect().catch(error => {
+                logger.error('Failed to reconnect WebSocket after URL change:', createErrorMetadata(error));
+              });
+            }, 100);
+          }
         }
-      }
-    });
+      });
+    }, 0);
   }

   private updateFromSettings(): void {
diff --git a/client/src/services/remoteLogger.ts b/client/src/services/remoteLogger.ts
index edccbd17..a62695a3 100644
--- a/client/src/services/remoteLogger.ts
+++ b/client/src/services/remoteLogger.ts
@@ -23,11 +23,10 @@ class RemoteLogger {
   private serverEndpoint: string;

   constructor() {
-    // Use the API URL from Vite environment or fallback to default
-    // @ts-ignore - import.meta is provided by Vite
-    const apiUrl = (import.meta?.env?.VITE_API_URL) || 'http://visionflow_container:4000';
-
-    this.serverEndpoint = `${apiUrl}/api/client-logs`;
+    // Use relative path - Nginx/Vite will proxy to backend
+    // In browser: goes through Nginx proxy on port 3001
+    // In Docker: Vite dev server proxies to visionflow_container:4000
+    this.serverEndpoint = '/api/client-logs';
     console.log('[RemoteLogger] Configured endpoint:', this.serverEndpoint);

     // Start flush timer
diff --git a/client/src/store/settingsStore.ts b/client/src/store/settingsStore.ts
index 9e999f3c..c868bc3b 100755
--- a/client/src/store/settingsStore.ts
+++ b/client/src/store/settingsStore.ts
@@ -10,6 +10,7 @@ import { isViewportSetting } from '../features/settings/config/viewportSettings'
 import { settingsApi, BatchOperation } from '../api/settingsApi';
 import { AutoSaveManager } from './autoSaveManager';
 import { nostrAuth } from '../services/nostrAuthService';
+import { settingsCacheClient } from '../services/SettingsCacheClient';



@@ -295,6 +296,9 @@ export const useSettingsStore = create<SettingsState>()(
           // Initialize AutoSaveManager now that store is ready
           autoSaveManager.setInitialized(true);

+          // Setup WebSocket listener for settings changes from other clients
+          setupWebSocketListener();
+
           if (debugState.isEnabled()) {
             logger.info('Settings store initialized with essential paths')
           }
@@ -1053,11 +1057,74 @@ function getAllAvailableSettingsPaths(): string[] {
   ];
 }

+// Setup WebSocket listener for real-time settings sync across clients
+let wsListenerInitialized = false;
+function setupWebSocketListener() {
+  if (wsListenerInitialized) return;
+  wsListenerInitialized = true;
+
+  // Listen to settingChanged events from cache client
+  window.addEventListener('settingChanged', ((event: CustomEvent) => {
+    const { path, value } = event.detail;
+
+    logger.debug(`Received settings change from WebSocket: ${path}`, value);
+
+    // Update local store with the new value from server
+    const state = useSettingsStore.getState();
+    const currentValue = state.get(path);
+
+    // Only update if value actually changed to avoid loops
+    if (JSON.stringify(currentValue) !== JSON.stringify(value)) {
+      state.set(path, value);
+
+      // Notify subscribers without triggering server update
+      const callbacks = state.subscribers.get(path);
+      if (callbacks) {
+        callbacks.forEach(callback => {
+          try {
+            callback();
+          } catch (error) {
+            logger.error(`Error in subscriber for ${path}:`, createErrorMetadata(error));
+          }
+        });
+      }
+    }
+  }) as EventListener);
+
+  // Listen to batch changes
+  window.addEventListener('settingsBatchChanged', ((event: CustomEvent) => {
+    const { updates } = event.detail;
+
+    logger.debug(`Received batch settings changes from WebSocket`, updates);
+
+    const state = useSettingsStore.getState();
+    updates.forEach(({ path, value }: any) => {
+      const currentValue = state.get(path);
+
+      if (JSON.stringify(currentValue) !== JSON.stringify(value)) {
+        state.set(path, value);
+
+        const callbacks = state.subscribers.get(path);
+        if (callbacks) {
+          callbacks.forEach(callback => {
+            try {
+              callback();
+            } catch (error) {
+              logger.error(`Error in subscriber for ${path}:`, createErrorMetadata(error));
+            }
+          });
+        }
+      }
+    });
+  }) as EventListener);
+}
+
 // Export for testing and direct access
 export const settingsStoreUtils = {
   autoSaveManager,
   getSectionPaths,
   setNestedValue,
   getAllSettingsPaths,
-  getAllAvailableSettingsPaths
+  getAllAvailableSettingsPaths,
+  settingsCacheClient
 };
diff --git a/client/src/styles/index.css b/client/src/styles/index.css
index cedc4e68..d8b5ee6c 100755
--- a/client/src/styles/index.css
+++ b/client/src/styles/index.css
@@ -10,7 +10,7 @@
 /* @import statements must come first */
 @import '@radix-ui/themes/styles.css';
 @import './base.css';
-@import './tailwind-utilities.css';
+@import './tailwind-utilities.css' @reference;

 /* Tailwind directives */
 @tailwind base;
diff --git a/client/src/styles/tailwind-utilities.css b/client/src/styles/tailwind-utilities.css
index 0ea63739..96effd7c 100644
--- a/client/src/styles/tailwind-utilities.css
+++ b/client/src/styles/tailwind-utilities.css
@@ -3,24 +3,24 @@
   .bg-background {
     background-color: hsl(var(--background));
   }
-
+
   .text-foreground {
     color: hsl(var(--foreground));
   }
-
+
   .bg-background\/80 {
     background-color: hsl(var(--background) / 0.8);
   }
-
+
   /* Positioning utilities for Tailwind v4 */
   .right-4 {
     right: 1rem;
   }
-
+
   .top-4 {
     top: 1rem;
   }
-
+
   .top-20 {
     top: 5rem;
   }
diff --git a/client/src/types/binaryProtocol.ts b/client/src/types/binaryProtocol.ts
index 45f874be..8b06c3ec 100755
--- a/client/src/types/binaryProtocol.ts
+++ b/client/src/types/binaryProtocol.ts
@@ -43,12 +43,21 @@ export const BINARY_SSSP_PARENT_OFFSET = 32;   // After distance (28 + 4)

 // Node type flag constants (Protocol V2 - must match server)
 export const AGENT_NODE_FLAG = 0x80000000;     // Bit 31 indicates agent node
-export const KNOWLEDGE_NODE_FLAG = 0x40000000; // Bit 14 indicates knowledge graph node
+export const KNOWLEDGE_NODE_FLAG = 0x40000000; // Bit 30 indicates knowledge graph node
 export const NODE_ID_MASK = 0x3FFFFFFF;        // Mask to extract actual node ID (bits 0-29)

+// Ontology node type flags (bits 26-28, only valid when GraphType::Ontology)
+export const ONTOLOGY_TYPE_MASK = 0x1C000000;      // Bits 26-28: Ontology node types
+export const ONTOLOGY_CLASS_FLAG = 0x04000000;     // Bit 26: OWL Class
+export const ONTOLOGY_INDIVIDUAL_FLAG = 0x08000000; // Bit 27: OWL Individual
+export const ONTOLOGY_PROPERTY_FLAG = 0x10000000;   // Bit 28: OWL Property
+
 export enum NodeType {
   Knowledge = 'knowledge',
   Agent = 'agent',
+  OntologyClass = 'ontology_class',
+  OntologyIndividual = 'ontology_individual',
+  OntologyProperty = 'ontology_property',
   Unknown = 'unknown'
 }

@@ -57,6 +66,12 @@ export function getNodeType(nodeId: number): NodeType {
     return NodeType.Agent;
   } else if ((nodeId & KNOWLEDGE_NODE_FLAG) !== 0) {
     return NodeType.Knowledge;
+  } else if ((nodeId & ONTOLOGY_TYPE_MASK) === ONTOLOGY_CLASS_FLAG) {
+    return NodeType.OntologyClass;
+  } else if ((nodeId & ONTOLOGY_TYPE_MASK) === ONTOLOGY_INDIVIDUAL_FLAG) {
+    return NodeType.OntologyIndividual;
+  } else if ((nodeId & ONTOLOGY_TYPE_MASK) === ONTOLOGY_PROPERTY_FLAG) {
+    return NodeType.OntologyProperty;
   }
   return NodeType.Unknown;
 }
@@ -73,6 +88,22 @@ export function isKnowledgeNode(nodeId: number): boolean {
   return (nodeId & KNOWLEDGE_NODE_FLAG) !== 0;
 }

+export function isOntologyClass(nodeId: number): boolean {
+  return (nodeId & ONTOLOGY_TYPE_MASK) === ONTOLOGY_CLASS_FLAG;
+}
+
+export function isOntologyIndividual(nodeId: number): boolean {
+  return (nodeId & ONTOLOGY_TYPE_MASK) === ONTOLOGY_INDIVIDUAL_FLAG;
+}
+
+export function isOntologyProperty(nodeId: number): boolean {
+  return (nodeId & ONTOLOGY_TYPE_MASK) === ONTOLOGY_PROPERTY_FLAG;
+}
+
+export function isOntologyNode(nodeId: number): boolean {
+  return isOntologyClass(nodeId) || isOntologyIndividual(nodeId) || isOntologyProperty(nodeId);
+}
+
 /**
  * Parse binary data buffer into an array of BinaryNodeData objects
  * Supports V2 protocol only (36 bytes, u32 IDs)
diff --git a/src/actors/agent_monitor_actor.rs b/src/actors/agent_monitor_actor.rs
index 18efd48a..a52039c3 100644
--- a/src/actors/agent_monitor_actor.rs
+++ b/src/actors/agent_monitor_actor.rs
@@ -3,7 +3,7 @@
 //! This actor focuses solely on:
 //! - Polling the Management API (port 9090) for active task statuses
 //! - Converting tasks to agent nodes
-//! - Forwarding updates to GraphServiceSupervisor
+//! - Forwarding updates to GraphStateActor
 //!
 //! All task management is handled by TaskOrchestratorActor.
 //! This actor only monitors and displays running agents.
@@ -17,11 +17,13 @@ use chrono::{Utc, DateTime};
 use crate::types::claude_flow::{ClaudeFlowClient, AgentStatus, AgentProfile, AgentType, PerformanceMetrics, TokenUsage};
 use crate::actors::messages::*;
 use crate::services::management_api_client::{ManagementApiClient, TaskInfo};
+use crate::actors::graph_state_actor::{GraphStateActor, AddNodes};
+use crate::models::node::Node;
+use crate::utils::socket_flow_messages::BinaryNodeData;

 /// Convert Management API TaskInfo to AgentStatus for graph visualization
 fn task_to_agent_status(task: TaskInfo) -> AgentStatus {
     use chrono::TimeZone;
-    use glam::Vec3;

     // Map agent type string to AgentType enum
     let agent_type_enum = match task.agent.as_str() {
@@ -89,10 +91,52 @@ fn task_to_agent_status(task: TaskInfo) -> AgentStatus {
         workload: Some(0.5),
     }
 }
+
+/// Convert AgentStatus to Node for GraphStateActor
+fn agent_status_to_node(status: &AgentStatus) -> Node {
+    // Use agent_id hash for consistent node ID
+    let node_id = {
+        use std::collections::hash_map::DefaultHasher;
+        use std::hash::{Hash, Hasher};
+        let mut hasher = DefaultHasher::new();
+        status.agent_id.hash(&mut hasher);
+        // Use high ID range (starting at 10000) to avoid conflicts with main graph
+        (hasher.finish() as u32 % 50000) + 10000
+    };
+
+    let mut node = Node::new(status.agent_id.clone());
+    node.id = node_id;
+
+    // Set position data
+    node.data = BinaryNodeData {
+        node_id,
+        x: 0.0,
+        y: 0.0,
+        z: 0.0,
+        vx: 0.0,
+        vy: 0.0,
+        vz: 0.0,
+    };
+
+    // Add metadata
+    node.label = status.profile.name.clone();
+    node.size = Some(5.0);
+    node.color = Some(match status.profile.agent_type {
+        AgentType::Coder => "#00FF00",
+        AgentType::Coordinator => "#0000FF",
+        AgentType::Researcher => "#FFFF00",
+        AgentType::Analyst => "#FF00FF",
+        AgentType::Tester => "#00FFFF",
+        _ => "#FFFFFF",
+    }.to_string());
+
+    node
+}
+
 /// AgentMonitorActor - Monitoring via Management API
 pub struct AgentMonitorActor {
     _client: ClaudeFlowClient,
-    graph_service_addr: Addr<crate::actors::graph_service_supervisor::TransitionalGraphSupervisor>,
+    graph_state_addr: Addr<GraphStateActor>,
     management_api_client: ManagementApiClient,

     /// Connection state
@@ -105,13 +149,16 @@ pub struct AgentMonitorActor {
     /// Agent cache (task_id -> AgentStatus)
     agent_cache: HashMap<String, AgentStatus>,

+    /// Node ID mapping (agent_id -> node_id)
+    node_id_map: HashMap<String, u32>,
+
     /// Error tracking
     consecutive_poll_failures: u32,
     last_successful_poll: Option<DateTime<Utc>>,
 }

 impl AgentMonitorActor {
-    pub fn new(client: ClaudeFlowClient, graph_service_addr: Addr<crate::actors::graph_service_supervisor::TransitionalGraphSupervisor>) -> Self {
+    pub fn new(client: ClaudeFlowClient, graph_state_addr: Addr<GraphStateActor>) -> Self {
         info!("[AgentMonitorActor] Initializing with Management API monitoring");

         // Create Management API client
@@ -128,47 +175,48 @@ impl AgentMonitorActor {

         Self {
             _client: client,
-            graph_service_addr,
+            graph_state_addr,
             management_api_client,
             is_connected: false,
             polling_interval: Duration::from_secs(3), // Poll every 3 seconds
             last_poll: Utc::now(),
             agent_cache: HashMap::new(),
+            node_id_map: HashMap::new(),
             consecutive_poll_failures: 0,
             last_successful_poll: None,
         }
     }

     /// Poll agent statuses from Management API
-fn poll_agent_statuses(&mut self, ctx: &mut Context<Self>) {
-    debug!("[AgentMonitorActor] Polling active tasks from Management API");
-
-    let api_client = self.management_api_client.clone();
-    let ctx_addr = ctx.address();
-
-    tokio::spawn(async move {
-        match api_client.list_tasks().await {
-            Ok(task_list) => {
-                let active_count = task_list.active_tasks.len();
-                debug!("[AgentMonitorActor] Retrieved {} active tasks from Management API", active_count);
-
-                // Convert tasks to agent statuses
-                let agents: Vec<AgentStatus> = task_list.active_tasks.into_iter().map(|task| {
-                    task_to_agent_status(task)
-                }).collect();
-
-                ctx_addr.do_send(ProcessAgentStatuses { agents });
-            }
-            Err(e) => {
-                error!("[AgentMonitorActor] Management API query failed: {}", e);
-                ctx_addr.do_send(RecordPollFailure);
+    fn poll_agent_statuses(&mut self, ctx: &mut Context<Self>) {
+        debug!("[AgentMonitorActor] Polling active tasks from Management API");
+
+        let api_client = self.management_api_client.clone();
+        let ctx_addr = ctx.address();
+
+        tokio::spawn(async move {
+            match api_client.list_tasks().await {
+                Ok(task_list) => {
+                    let active_count = task_list.active_tasks.len();
+                    debug!("[AgentMonitorActor] Retrieved {} active tasks from Management API", active_count);
+
+                    // Convert tasks to agent statuses
+                    let agents: Vec<AgentStatus> = task_list.active_tasks.into_iter().map(|task| {
+                        task_to_agent_status(task)
+                    }).collect();
+
+                    ctx_addr.do_send(ProcessAgentStatuses { agents });
+                }
+                Err(e) => {
+                    error!("[AgentMonitorActor] Management API query failed: {}", e);
+                    ctx_addr.do_send(RecordPollFailure);
+                }
             }
-        }
-    });
-}
+        });
+    }
 }

-/// Message to process agent statuses from MCP
+/// Message to process agent statuses from Management API
 #[derive(Message)]
 #[rtype(result = "()")]
 struct ProcessAgentStatuses {
@@ -179,7 +227,7 @@ impl Actor for AgentMonitorActor {
     type Context = Context<Self>;

     fn started(&mut self, ctx: &mut Self::Context) {
-        info!("[AgentMonitorActor] Started - beginning MCP TCP polling");
+        info!("[AgentMonitorActor] Started - beginning Management API polling");

         self.is_connected = true;

@@ -200,31 +248,35 @@ impl Handler<ProcessAgentStatuses> for AgentMonitorActor {
     type Result = ();

     fn handle(&mut self, msg: ProcessAgentStatuses, _ctx: &mut Self::Context) {
-        info!("[AgentMonitorActor] Processing {} agent statuses from MCP", msg.agents.len());
-
-        // Convert AgentStatus to Agent for UpdateBotsGraph
-        let agents: Vec<crate::services::bots_client::Agent> = msg.agents.iter().map(|status| {
-            crate::services::bots_client::Agent {
-                id: status.agent_id.clone(),
-                name: status.profile.name.clone(),
-                agent_type: format!("{:?}", status.profile.agent_type).to_lowercase(),
-                status: status.status.clone(),
-                x: 0.0,
-                y: 0.0,
-                z: 0.0,
-                cpu_usage: status.cpu_usage,
-                memory_usage: status.memory_usage,
-                health: status.health,
-                workload: status.activity,
-                created_at: Some(status.timestamp.to_rfc3339()),
-                age: Some((chrono::Utc::now().timestamp() - status.timestamp.timestamp()) as u64 * 1000),
-            }
+        info!("[AgentMonitorActor] Processing {} agent statuses", msg.agents.len());
+
+        // Convert AgentStatus to Node for GraphStateActor
+        let nodes: Vec<Node> = msg.agents.iter().map(|status| {
+            let node = agent_status_to_node(status);
+            // Store node ID mapping
+            self.node_id_map.insert(status.agent_id.clone(), node.id);
+            node
         }).collect();

-        // Send graph update
-        let message = UpdateBotsGraph { agents };
-        info!("[AgentMonitorActor] Sending graph update with {} agents", msg.agents.len());
-        self.graph_service_addr.do_send(message);
+        // Send to GraphStateActor
+        if !nodes.is_empty() {
+            info!("[AgentMonitorActor] Sending {} nodes to GraphStateActor", nodes.len());
+            let graph_state_addr = self.graph_state_addr.clone();
+
+            tokio::spawn(async move {
+                match graph_state_addr.send(AddNodes { nodes }).await {
+                    Ok(Ok(added_ids)) => {
+                        debug!("[AgentMonitorActor] Successfully added {} nodes to graph state", added_ids.len());
+                    }
+                    Ok(Err(e)) => {
+                        warn!("[AgentMonitorActor] Failed to add nodes: {}", e);
+                    }
+                    Err(e) => {
+                        error!("[AgentMonitorActor] Mailbox error sending to GraphStateActor: {}", e);
+                    }
+                }
+            });
+        }

         // Update cache
         if !msg.agents.is_empty() {
diff --git a/src/actors/graph_actor.rs b/src/actors/graph_actor.rs
index cb3f9a8b..aebc6470 100644
--- a/src/actors/graph_actor.rs
+++ b/src/actors/graph_actor.rs
@@ -594,24 +594,11 @@ impl GraphServiceActor {
         );
         let stress_solver = StressMajorizationSolver::from_advanced_params(&advanced_params);

-        // Initialize with logseq (knowledge graph) physics from settings
-        // This will be updated when settings are loaded, but provides better defaults
+        // Initialize with default physics parameters
+        // Settings are loaded from SQLite database and applied via UpdateSettings message
         let simulation_params = {
-            // Try to get settings from the global config
-            if let Ok(settings_yaml) = std::fs::read_to_string("/app/settings.yaml")
-                .or_else(|_| std::fs::read_to_string("data/settings.yaml")) {
-                if let Ok(settings) = serde_yaml::from_str::<crate::config::AppFullSettings>(&settings_yaml) {
-                    // Use logseq physics as the default for knowledge graph
-                    let physics = settings.get_physics("logseq");
-                    SimulationParams::from(physics)
-                } else {
-                    warn!("Failed to parse settings.yaml, using defaults");
-                    SimulationParams::default()
-                }
-            } else {
-                info!("Settings file not found at startup, using defaults (will be updated when settings load)");
-                SimulationParams::default()
-            }
+            info!("Initializing with default physics parameters (settings loaded from database)");
+            SimulationParams::default()
         };

         // Clone simulation_params for target_params before moving it
@@ -2095,24 +2082,36 @@ impl GraphServiceActor {
                 return; // Skip this broadcast
             }

-            // Create binary position data for BOTH knowledge graph AND agent graph
-            // Per dual-graph architecture: both types broadcast together with type flags
+            // Create binary position data for ALL three graphs: knowledge, agent, and ontology
+            // Per multi-graph architecture: all types broadcast together with type flags
             let mut position_data: Vec<(u32, BinaryNodeData)> = Vec::new();
             let mut knowledge_ids: Vec<u32> = Vec::new();
             let mut agent_ids: Vec<u32> = Vec::new();
+            let mut ontology_class_ids: Vec<u32> = Vec::new();
+            let mut ontology_individual_ids: Vec<u32> = Vec::new();
+            let mut ontology_property_ids: Vec<u32> = Vec::new();

-            // Collect knowledge graph nodes (from main graph)
+            // Collect nodes from main graph, classifying by node_type field
             for (node_id, node) in self.node_map.iter() {
                 position_data.push((*node_id, BinaryNodeDataClient::new(
                     *node_id,
                     node.data.position(),
                     node.data.velocity(),
                 )));
-                knowledge_ids.push(*node_id);
+
+                // Classify node by its type field
+                match node.node_type.as_deref() {
+                    Some("agent") | Some("bot") => agent_ids.push(*node_id),
+                    Some("ontology_class") | Some("owl_class") => ontology_class_ids.push(*node_id),
+                    Some("ontology_individual") | Some("owl_individual") => ontology_individual_ids.push(*node_id),
+                    Some("ontology_property") | Some("owl_property") => ontology_property_ids.push(*node_id),
+                    // Default to knowledge graph for backward compatibility
+                    _ => knowledge_ids.push(*node_id),
+                }
             }

             // ALSO collect agent graph nodes (from bots_graph_data)
-            // This ensures both graphs are broadcast together in one unified message
+            // This ensures all graphs are broadcast together in one unified message
             for node in &self.bots_graph_data.nodes {
                 position_data.push((node.id, BinaryNodeDataClient::new(
                     node.id,
@@ -2124,12 +2123,15 @@ impl GraphServiceActor {

             // Broadcast to all connected clients via client manager
             if !position_data.is_empty() {
-                // Encode with proper type flags: KNOWLEDGE_NODE_FLAG and AGENT_NODE_FLAG
+                // Encode with proper type flags: KNOWLEDGE_NODE_FLAG, AGENT_NODE_FLAG, and ONTOLOGY flags
                 // Client will separate them by checking flags
-                let binary_data = crate::utils::binary_protocol::encode_node_data_with_types(
+                let binary_data = crate::utils::binary_protocol::encode_node_data_extended(
                     &position_data,
                     &agent_ids,
-                    &knowledge_ids
+                    &knowledge_ids,
+                    &ontology_class_ids,
+                    &ontology_individual_ids,
+                    &ontology_property_ids
                 );

                 // Send to client manager for broadcasting
@@ -2142,18 +2144,19 @@ impl GraphServiceActor {

                 // Update broadcast time and mark initial positions as sent
                 self.last_broadcast_time = Some(now);
+                let ontology_count = ontology_class_ids.len() + ontology_individual_ids.len() + ontology_property_ids.len();
                 if !self.initial_positions_sent {
                     self.initial_positions_sent = true;
-                    info!("Sent initial unified graph positions to clients ({} nodes: {} knowledge + {} agents)",
-                          position_data.len(), knowledge_ids.len(), agent_ids.len());
+                    info!("Sent initial multi-graph positions to clients ({} nodes: {} knowledge + {} agents + {} ontology)",
+                          position_data.len(), knowledge_ids.len(), agent_ids.len(), ontology_count);
                 } else if force_broadcast {
-                    info!("Force broadcast unified graph positions to new clients ({} nodes: {} knowledge + {} agents)",
-                          position_data.len(), knowledge_ids.len(), agent_ids.len());
+                    info!("Force broadcast multi-graph positions to new clients ({} nodes: {} knowledge + {} agents + {} ontology)",
+                          position_data.len(), knowledge_ids.len(), agent_ids.len(), ontology_count);
                 }

                 if crate::utils::logging::is_debug_enabled() && !force_broadcast {
-                    debug!("Broadcast unified positions: {} total ({} knowledge + {} agents), stable: {}, pending: {}/{}",
-                           position_data.len(), knowledge_ids.len(), agent_ids.len(),
+                    debug!("Broadcast multi-graph positions: {} total ({} knowledge + {} agents + {} ontology), stable: {}, pending: {}/{}",
+                           position_data.len(), knowledge_ids.len(), agent_ids.len(), ontology_count,
                            self.current_state == AutoBalanceState::Stable,
                            self.pending_broadcasts,
                            self.max_pending_broadcasts);
diff --git a/src/actors/graph_service_supervisor.rs b/src/actors/graph_service_supervisor.rs
deleted file mode 100644
index f96cd5ca..00000000
--- a/src/actors/graph_service_supervisor.rs
+++ /dev/null
@@ -1,1191 +0,0 @@
-//! Graph Service Supervisor - Lightweight supervisor for managing graph service actors
-//!
-//! This module implements a supervisor pattern that:
-//! - Spawns and manages 4 child actors (GraphState, Physics, Semantic, Client)
-//! - Routes messages to appropriate actors based on message type
-//! - Handles actor restarts on failure with configurable policies
-//! - Coordinates inter-actor communication and state synchronization
-//! - Provides health monitoring and performance metrics
-//!
-//! ## Architecture
-//!
-//! ```
-//! GraphServiceSupervisor
-//! ├── GraphStateActor          (State management & persistence)
-//! ├── PhysicsOrchestratorActor (Physics simulation & GPU compute)
-//! ├── SemanticProcessorActor   (Semantic analysis & AI features)
-//! └── ClientCoordinatorActor   (WebSocket & client management)
-//! ```
-//!
-//! ## Supervision Strategies
-//!
-//! - **OneForOne**: Restart only the failed actor
-//! - **OneForAll**: Restart all actors when one fails
-//! - **RestForOne**: Restart failed actor and all actors started after it
-//! - **Escalate**: Escalate failure to parent supervisor
-//!
-//! ## Message Routing
-//!
-//! Messages are routed based on their type:
-//! - Graph operations → GraphStateActor
-//! - Physics/GPU operations → PhysicsOrchestratorActor
-//! - Semantic analysis → SemanticProcessorActor
-//! - Client management → ClientCoordinatorActor
-
-use actix::prelude::*;
-use actix::dev::{MessageResponse, OneshotSender};
-use std::collections::HashMap;
-use std::time::{Duration, Instant};
-use serde::{Serialize, Deserialize};
-use log::{info, warn, error, debug};
-
-use crate::actors::{
-    GraphServiceActor,
-    PhysicsOrchestratorActor,
-    SemanticProcessorActor,
-    ClientCoordinatorActor,
-};
-// Removed unused import - we don't use graph_messages types for handlers
-use crate::actors::messages as msgs;
-// Removed graph_messages::GetGraphData import - not used
-use crate::errors::{VisionFlowError, ActorError};
-
-/// Graph service supervision strategy for handling actor failures
-#[derive(Debug, Clone, Serialize, Deserialize)]
-pub enum GraphSupervisionStrategy {
-    /// Restart only the failed actor
-    OneForOne,
-    /// Restart all actors when one fails
-    OneForAll,
-    /// Restart failed actor and all actors started after it
-    RestForOne,
-    /// Escalate failure to parent supervisor
-    Escalate,
-}
-
-/// Actor health status
-#[derive(Debug, Clone, Serialize, Deserialize)]
-pub enum ActorHealth {
-    Healthy,
-    Degraded,
-    Failed,
-    Restarting,
-    Unknown,
-}
-
-/// Actor restart policy configuration
-#[derive(Debug, Clone, Serialize, Deserialize)]
-pub struct RestartPolicy {
-    pub max_restarts: u32,
-    pub within_time_period: Duration,
-    pub backoff_strategy: BackoffStrategy,
-    pub escalation_threshold: u32,
-}
-
-/// Backoff strategy for actor restarts
-#[derive(Debug, Clone, Serialize, Deserialize)]
-pub enum BackoffStrategy {
-    Fixed(Duration),
-    Linear(Duration),
-    Exponential { initial: Duration, max: Duration },
-}
-
-/// Actor metadata for supervision
-#[derive(Debug)]
-pub struct ActorInfo {
-    pub name: String,
-    pub actor_type: ActorType,
-    pub health: ActorHealth,
-    pub last_heartbeat: Option<Instant>,
-    pub restart_count: u32,
-    pub last_restart: Option<Instant>,
-    pub message_buffer: Vec<SupervisedMessage>,
-    pub stats: ActorStats,
-}
-
-/// Types of supervised actors
-#[derive(Debug, Clone, Serialize, Deserialize, Eq, Hash, PartialEq)]
-pub enum ActorType {
-    GraphState,
-    PhysicsOrchestrator,
-    SemanticProcessor,
-    ClientCoordinator,
-}
-
-/// Actor performance statistics
-#[derive(Debug, Clone)]
-pub struct ActorStats {
-    pub messages_processed: u64,
-    pub messages_failed: u64,
-    pub average_response_time: Duration,
-    pub last_activity: Option<Instant>,
-    pub uptime: Duration,
-    pub memory_usage: Option<u64>,
-}
-
-/// Simple operation result message for supervised actors
-#[derive(Message, Debug, Clone)]
-#[rtype(result = "()")]
-pub struct OperationResult {
-    pub success: bool,
-    pub error: Option<String>,
-}
-
-impl From<Result<(), VisionFlowError>> for OperationResult {
-    fn from(result: Result<(), VisionFlowError>) -> Self {
-        match result {
-            Ok(()) => OperationResult { success: true, error: None },
-            Err(e) => OperationResult { success: false, error: Some(e.to_string()) },
-        }
-    }
-}
-
-/// Buffered message during actor restart
-pub struct SupervisedMessage {
-    pub message: Box<dyn Message<Result = ()> + Send>,
-    pub sender: Option<Recipient<OperationResult>>,
-    pub timestamp: Instant,
-    pub retry_count: u32,
-}
-
-impl std::fmt::Debug for SupervisedMessage {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.debug_struct("SupervisedMessage")
-            .field("timestamp", &self.timestamp)
-            .field("retry_count", &self.retry_count)
-            .finish()
-    }
-}
-
-/// Main supervisor actor managing all graph service actors
-pub struct GraphServiceSupervisor {
-    // Child actor addresses
-    graph_state: Option<Addr<GraphServiceActor>>,
-    physics: Option<Addr<PhysicsOrchestratorActor>>,
-    semantic: Option<Addr<SemanticProcessorActor>>,
-    client: Option<Addr<ClientCoordinatorActor>>,
-
-    // Supervision configuration
-    strategy: GraphSupervisionStrategy,
-    restart_policy: RestartPolicy,
-
-    // Actor management
-    actor_info: HashMap<ActorType, ActorInfo>,
-
-    // Health monitoring
-    health_check_interval: Duration,
-    last_health_check: Instant,
-
-    // Message routing and buffering
-    message_buffer_size: usize,
-    total_messages_routed: u64,
-
-    // Performance metrics
-    supervision_stats: SupervisionStats,
-}
-
-/// Supervisor performance statistics
-#[derive(Debug, Clone)]
-pub struct SupervisionStats {
-    pub actors_supervised: u32,
-    pub total_restarts: u32,
-    pub messages_routed: u64,
-    pub messages_buffered: u64,
-    pub average_routing_time: Duration,
-    pub last_failure: Option<Instant>,
-    pub uptime: Duration,
-    pub health_checks_performed: u64,
-}
-
-impl Default for RestartPolicy {
-    fn default() -> Self {
-        Self {
-            max_restarts: 5,
-            within_time_period: Duration::from_secs(300), // 5 minutes
-            backoff_strategy: BackoffStrategy::Exponential {
-                initial: Duration::from_secs(1),
-                max: Duration::from_secs(60),
-            },
-            escalation_threshold: 3,
-        }
-    }
-}
-
-impl Default for ActorStats {
-    fn default() -> Self {
-        Self {
-            messages_processed: 0,
-            messages_failed: 0,
-            average_response_time: Duration::from_millis(0),
-            last_activity: None,
-            uptime: Duration::from_secs(0),
-            memory_usage: None,
-        }
-    }
-}
-
-impl GraphServiceSupervisor {
-    /// Create new supervisor with default configuration
-    pub fn new() -> Self {
-        Self {
-            graph_state: None,
-            physics: None,
-            semantic: None,
-            client: None,
-            strategy: GraphSupervisionStrategy::OneForOne,
-            restart_policy: RestartPolicy::default(),
-            actor_info: HashMap::new(),
-            health_check_interval: Duration::from_secs(30),
-            last_health_check: Instant::now(),
-            message_buffer_size: 1000,
-            total_messages_routed: 0,
-            supervision_stats: SupervisionStats::default(),
-        }
-    }
-
-    /// Create supervisor with custom configuration
-    pub fn with_config(
-        strategy: GraphSupervisionStrategy,
-        restart_policy: RestartPolicy,
-        health_check_interval: Duration,
-    ) -> Self {
-        let mut supervisor = Self::new();
-        supervisor.strategy = strategy;
-        supervisor.restart_policy = restart_policy;
-        supervisor.health_check_interval = health_check_interval;
-        supervisor
-    }
-
-    /// Create supervisor with dependencies for GraphServiceActor compatibility
-    /// This is a transitional method that creates a GraphServiceActor as the managed child
-    /// This allows for gradual migration to the full supervisor architecture
-    pub fn with_dependencies(
-        client_manager_addr: Option<Addr<crate::actors::ClientCoordinatorActor>>,
-        gpu_manager_addr: Option<Addr<crate::actors::GPUManagerActor>>,
-    ) -> TransitionalGraphSupervisor {
-        info!("Creating TransitionalGraphSupervisor with GraphServiceActor as managed child");
-
-        // Create the transitional supervisor that wraps GraphServiceActor
-        TransitionalGraphSupervisor::new(client_manager_addr, gpu_manager_addr)
-    }
-
-    /// Initialize all child actors
-    fn initialize_actors(&mut self, ctx: &mut Context<Self>) {
-        info!("Initializing supervised actors");
-
-        // Initialize actor info structures
-        self.actor_info.insert(ActorType::GraphState, ActorInfo {
-            name: "GraphState".to_string(),
-            actor_type: ActorType::GraphState,
-            health: ActorHealth::Unknown,
-            last_heartbeat: None,
-            restart_count: 0,
-            last_restart: None,
-            message_buffer: Vec::new(),
-            stats: ActorStats::default(),
-        });
-
-        self.actor_info.insert(ActorType::PhysicsOrchestrator, ActorInfo {
-            name: "PhysicsOrchestrator".to_string(),
-            actor_type: ActorType::PhysicsOrchestrator,
-            health: ActorHealth::Unknown,
-            last_heartbeat: None,
-            restart_count: 0,
-            last_restart: None,
-            message_buffer: Vec::new(),
-            stats: ActorStats::default(),
-        });
-
-        self.actor_info.insert(ActorType::SemanticProcessor, ActorInfo {
-            name: "SemanticProcessor".to_string(),
-            actor_type: ActorType::SemanticProcessor,
-            health: ActorHealth::Unknown,
-            last_heartbeat: None,
-            restart_count: 0,
-            last_restart: None,
-            message_buffer: Vec::new(),
-            stats: ActorStats::default(),
-        });
-
-        self.actor_info.insert(ActorType::ClientCoordinator, ActorInfo {
-            name: "ClientCoordinator".to_string(),
-            actor_type: ActorType::ClientCoordinator,
-            health: ActorHealth::Unknown,
-            last_heartbeat: None,
-            restart_count: 0,
-            last_restart: None,
-            message_buffer: Vec::new(),
-            stats: ActorStats::default(),
-        });
-
-        // Start actors in dependency order
-        // ClientCoordinator must start first as GraphState depends on it
-        self.start_actor(ActorType::ClientCoordinator, ctx);
-        self.start_actor(ActorType::PhysicsOrchestrator, ctx);
-        self.start_actor(ActorType::SemanticProcessor, ctx);
-        self.start_actor(ActorType::GraphState, ctx); // GraphState last - depends on ClientCoordinator
-
-        // Schedule health checks
-        ctx.run_interval(self.health_check_interval, |act, ctx| {
-            act.perform_health_check(ctx);
-        });
-
-        self.supervision_stats.actors_supervised = 4;
-        info!("All supervised actors initialized successfully");
-    }
-
-    /// Start a specific actor
-    fn start_actor(&mut self, actor_type: ActorType, _ctx: &mut Context<Self>) {
-        info!("Starting actor: {:?}", actor_type);
-
-        match actor_type {
-            ActorType::GraphState => {
-                // Temporarily use GraphServiceActor as the graph state manager
-                // This will be replaced with a dedicated GraphStateActor during gradual refactoring
-                info!("Starting GraphServiceActor as temporary GraphState manager");
-
-                // GraphServiceActor needs client_manager and optionally gpu_compute addresses
-                // For now we'll create it without these dependencies and add them later
-                // The supervisor will coordinate message routing
-                let client_manager = self.client.as_ref().map(|addr| addr.clone());
-                if let Some(client_addr) = client_manager {
-                    let actor = GraphServiceActor::new(
-                        client_addr,
-                        None, // GPU compute will be linked later
-                        None, // Settings actor will be linked later
-                    ).start();
-                    self.graph_state = Some(actor);
-                    info!("GraphServiceActor started successfully as GraphState manager");
-                } else {
-                    warn!("Cannot start GraphServiceActor without ClientCoordinator - will retry after client actor starts");
-                }
-            },
-            ActorType::PhysicsOrchestrator => {
-                use crate::models::simulation_params::SimulationParams;
-                let params = SimulationParams::default();
-                let actor = PhysicsOrchestratorActor::new(params, None, None).start();
-                self.physics = Some(actor);
-            },
-            ActorType::SemanticProcessor => {
-                let config = Some(crate::actors::semantic_processor_actor::SemanticProcessorConfig::default());
-                let actor = SemanticProcessorActor::new(config).start();
-                self.semantic = Some(actor);
-            },
-            ActorType::ClientCoordinator => {
-                let actor = ClientCoordinatorActor::new().start();
-                self.client = Some(actor);
-            },
-        }
-
-        // Update actor info
-        if let Some(info) = self.actor_info.get_mut(&actor_type) {
-            info.health = ActorHealth::Healthy;
-            info.last_heartbeat = Some(Instant::now());
-            info.stats.uptime = Duration::from_secs(0);
-        }
-    }
-
-    /// Restart a failed actor
-    fn restart_actor(&mut self, actor_type: ActorType, ctx: &mut Context<Self>) {
-        warn!("Restarting failed actor: {:?}", actor_type);
-
-        // Update actor info
-        if let Some(info) = self.actor_info.get_mut(&actor_type) {
-            info.health = ActorHealth::Restarting;
-            info.restart_count += 1;
-            info.last_restart = Some(Instant::now());
-
-            // Check restart limits
-            if info.restart_count > self.restart_policy.max_restarts {
-                error!("Actor {:?} exceeded maximum restarts ({}), escalating",
-                       actor_type, self.restart_policy.max_restarts);
-                self.escalate_failure(actor_type, ctx);
-                return;
-            }
-        }
-
-        // Apply backoff strategy
-        let backoff_duration = self.calculate_backoff(&actor_type);
-        let actor_type_clone = actor_type.clone();
-        let actor_type_clone2 = actor_type.clone();
-
-        ctx.run_later(backoff_duration, move |act, ctx| {
-            act.start_actor(actor_type_clone, ctx);
-            act.replay_buffered_messages(actor_type_clone2);
-        });
-
-        self.supervision_stats.total_restarts += 1;
-    }
-
-    /// Calculate backoff duration for restart
-    fn calculate_backoff(&self, actor_type: &ActorType) -> Duration {
-        if let Some(info) = self.actor_info.get(actor_type) {
-            match &self.restart_policy.backoff_strategy {
-                BackoffStrategy::Fixed(duration) => *duration,
-                BackoffStrategy::Linear(duration) => {
-                    *duration * info.restart_count
-                },
-                BackoffStrategy::Exponential { initial, max } => {
-                    let exponential = *initial * 2_u32.pow(info.restart_count.min(10));
-                    exponential.min(*max)
-                },
-            }
-        } else {
-            Duration::from_secs(1)
-        }
-    }
-
-    /// Escalate failure to parent or shutdown
-    fn escalate_failure(&mut self, actor_type: ActorType, ctx: &mut Context<Self>) {
-        error!("Escalating failure for actor: {:?}", actor_type);
-
-        match self.strategy {
-            GraphSupervisionStrategy::OneForAll => {
-                warn!("Restarting all actors due to escalation");
-                self.restart_all_actors(ctx);
-            },
-            GraphSupervisionStrategy::Escalate => {
-                error!("Escalating to parent supervisor");
-                // TODO: Send escalation message to parent
-                ctx.stop();
-            },
-            _ => {
-                error!("Actor {:?} failed beyond recovery limits", actor_type);
-                if let Some(info) = self.actor_info.get_mut(&actor_type) {
-                    info.health = ActorHealth::Failed;
-                }
-            }
-        }
-    }
-
-    /// Restart all supervised actors
-    fn restart_all_actors(&mut self, ctx: &mut Context<Self>) {
-        info!("Restarting all supervised actors");
-
-        // Clear current actors
-        self.graph_state = None;
-        self.physics = None;
-        self.semantic = None;
-        self.client = None;
-
-        // Restart all
-        self.start_actor(ActorType::GraphState, ctx);
-        self.start_actor(ActorType::PhysicsOrchestrator, ctx);
-        self.start_actor(ActorType::SemanticProcessor, ctx);
-        self.start_actor(ActorType::ClientCoordinator, ctx);
-    }
-
-    /// Buffer message during actor restart
-    fn buffer_message(&mut self, actor_type: ActorType, message: SupervisedMessage) {
-        if let Some(info) = self.actor_info.get_mut(&actor_type) {
-            if info.message_buffer.len() < self.message_buffer_size {
-                info.message_buffer.push(message);
-                self.supervision_stats.messages_buffered += 1;
-            } else {
-                warn!("Message buffer full for actor {:?}, dropping message", actor_type);
-            }
-        }
-    }
-
-    /// Replay buffered messages after actor restart
-    fn replay_buffered_messages(&mut self, actor_type: ActorType) {
-        if let Some(info) = self.actor_info.get_mut(&actor_type) {
-            let messages = std::mem::take(&mut info.message_buffer);
-            info!("Replaying {} buffered messages for actor {:?}",
-                  messages.len(), actor_type);
-
-            // TODO: Replay messages to restarted actor
-            // This would require message serialization/deserialization
-        }
-    }
-
-    /// Perform health check on all actors
-    fn perform_health_check(&mut self, _ctx: &mut Context<Self>) {
-        debug!("Performing health check on supervised actors");
-
-        let now = Instant::now();
-        self.last_health_check = now;
-        self.supervision_stats.health_checks_performed += 1;
-
-        for (actor_type, info) in &mut self.actor_info {
-            // Check heartbeat timeout
-            if let Some(last_heartbeat) = info.last_heartbeat {
-                if now.duration_since(last_heartbeat) > Duration::from_secs(60) {
-                    warn!("Actor {:?} heartbeat timeout", actor_type);
-                    info.health = ActorHealth::Degraded;
-                }
-            }
-
-            // Update uptime
-            if let Some(last_restart) = info.last_restart {
-                info.stats.uptime = now.duration_since(last_restart);
-            }
-        }
-    }
-
-    /// Route message to appropriate actor
-    fn route_message(&mut self, message: SupervisorMessage, _ctx: &mut Context<Self>) -> Result<(), VisionFlowError> {
-        let start_time = Instant::now();
-
-        let result = match message {
-            SupervisorMessage::GraphOperation(_msg) => {
-                if let Some(ref _addr) = self.graph_state {
-                    // Forward message to graph state actor
-                    // For now this is a placeholder - full implementation would deserialize and forward
-                    debug!("Forwarding graph operation to GraphState actor");
-                    Ok(())
-                } else {
-                    Err(VisionFlowError::Actor(ActorError::ActorNotAvailable("GraphState".to_string())))
-                }
-            },
-            SupervisorMessage::PhysicsOperation(_msg) => {
-                if let Some(ref _addr) = self.physics {
-                    debug!("Forwarding physics operation to Physics actor");
-                    Ok(())
-                } else {
-                    Err(VisionFlowError::Actor(ActorError::ActorNotAvailable("Physics".to_string())))
-                }
-            },
-            SupervisorMessage::SemanticOperation(_msg) => {
-                if let Some(ref _addr) = self.semantic {
-                    debug!("Forwarding semantic operation to Semantic actor");
-                    Ok(())
-                } else {
-                    Err(VisionFlowError::Actor(ActorError::ActorNotAvailable("Semantic".to_string())))
-                }
-            },
-            SupervisorMessage::ClientOperation(_msg) => {
-                if let Some(ref _addr) = self.client {
-                    debug!("Forwarding client operation to Client actor");
-                    Ok(())
-                } else {
-                    Err(VisionFlowError::Actor(ActorError::ActorNotAvailable("Client".to_string())))
-                }
-            },
-        };
-
-        // Update routing statistics
-        let routing_time = start_time.elapsed();
-        self.total_messages_routed += 1;
-        self.supervision_stats.messages_routed += 1;
-
-        // Update average routing time (simple moving average)
-        let current_avg = self.supervision_stats.average_routing_time;
-        let new_avg = (current_avg + routing_time) / 2;
-        self.supervision_stats.average_routing_time = new_avg;
-
-        result
-    }
-
-    /// Get supervisor status and statistics
-    pub fn get_status(&self) -> SupervisorStatus {
-        SupervisorStatus {
-            strategy: self.strategy.clone(),
-            actor_health: self.actor_info.iter().map(|(actor_type, info)| {
-                (actor_type.clone(), info.health.clone())
-            }).collect(),
-            supervision_stats: self.supervision_stats.clone(),
-            last_health_check: self.last_health_check,
-            total_messages_routed: self.total_messages_routed,
-        }
-    }
-}
-
-impl Default for SupervisionStats {
-    fn default() -> Self {
-        Self {
-            actors_supervised: 0,
-            total_restarts: 0,
-            messages_routed: 0,
-            messages_buffered: 0,
-            average_routing_time: Duration::from_millis(0),
-            last_failure: None,
-            uptime: Duration::from_secs(0),
-            health_checks_performed: 0,
-        }
-    }
-}
-
-impl Actor for GraphServiceSupervisor {
-    type Context = Context<Self>;
-
-    fn started(&mut self, ctx: &mut Self::Context) {
-        info!("GraphServiceSupervisor started");
-        self.initialize_actors(ctx);
-        self.supervision_stats.uptime = Duration::from_secs(0);
-    }
-
-    fn stopped(&mut self, _ctx: &mut Self::Context) {
-        info!("GraphServiceSupervisor stopped");
-    }
-}
-
-// Message definitions for supervisor communication
-
-/// Main supervisor message enum for routing
-#[derive(Message)]
-#[rtype(result = "Result<(), VisionFlowError>")]
-pub enum SupervisorMessage {
-    GraphOperation(Box<dyn Message<Result = Result<(), VisionFlowError>> + Send>),
-    PhysicsOperation(Box<dyn Message<Result = Result<(), VisionFlowError>> + Send>),
-    SemanticOperation(Box<dyn Message<Result = Result<(), VisionFlowError>> + Send>),
-    ClientOperation(Box<dyn Message<Result = Result<(), VisionFlowError>> + Send>),
-}
-
-/// Actor heartbeat message
-#[derive(Message)]
-#[rtype(result = "()")]
-pub struct ActorHeartbeat {
-    pub actor_type: ActorType,
-    pub timestamp: Instant,
-    pub health: ActorHealth,
-    pub stats: Option<ActorStats>,
-}
-
-/// Request supervisor status
-#[derive(Message)]
-#[rtype(result = "SupervisorStatus")]
-pub struct GetSupervisorStatus;
-
-/// Supervisor status response
-#[derive(Debug, Clone)]
-pub struct SupervisorStatus {
-    pub strategy: GraphSupervisionStrategy,
-    pub actor_health: HashMap<ActorType, ActorHealth>,
-    pub supervision_stats: SupervisionStats,
-    pub last_health_check: Instant,
-    pub total_messages_routed: u64,
-}
-
-impl<A, M> MessageResponse<A, M> for SupervisorStatus
-where
-    A: Actor,
-    M: Message<Result = SupervisorStatus>,
-{
-    fn handle(self, _ctx: &mut A::Context, tx: Option<OneshotSender<M::Result>>) {
-        if let Some(tx) = tx {
-            let _ = tx.send(self);
-        }
-    }
-}
-
-/// Request to restart specific actor
-#[derive(Message)]
-#[rtype(result = "Result<(), VisionFlowError>")]
-pub struct RestartActor {
-    pub actor_type: ActorType,
-}
-
-/// Request to restart all actors
-#[derive(Message)]
-#[rtype(result = "Result<(), VisionFlowError>")]
-pub struct RestartAllActors;
-
-// Message handlers
-
-impl Handler<SupervisorMessage> for GraphServiceSupervisor {
-    type Result = Result<(), VisionFlowError>;
-
-    fn handle(&mut self, msg: SupervisorMessage, ctx: &mut Self::Context) -> Self::Result {
-        self.route_message(msg, ctx)
-    }
-}
-
-impl Handler<ActorHeartbeat> for GraphServiceSupervisor {
-    type Result = ();
-
-    fn handle(&mut self, msg: ActorHeartbeat, _ctx: &mut Self::Context) -> Self::Result {
-        if let Some(info) = self.actor_info.get_mut(&msg.actor_type) {
-            info.last_heartbeat = Some(msg.timestamp);
-            info.health = msg.health;
-
-            if let Some(stats) = msg.stats {
-                info.stats = stats;
-            }
-        }
-    }
-}
-
-impl Handler<GetSupervisorStatus> for GraphServiceSupervisor {
-    type Result = SupervisorStatus;
-
-    fn handle(&mut self, _msg: GetSupervisorStatus, _ctx: &mut Self::Context) -> Self::Result {
-        self.get_status()
-    }
-}
-
-impl Handler<RestartActor> for GraphServiceSupervisor {
-    type Result = Result<(), VisionFlowError>;
-
-    fn handle(&mut self, msg: RestartActor, ctx: &mut Self::Context) -> Self::Result {
-        self.restart_actor(msg.actor_type, ctx);
-        Ok(())
-    }
-}
-
-impl Handler<RestartAllActors> for GraphServiceSupervisor {
-    type Result = Result<(), VisionFlowError>;
-
-    fn handle(&mut self, _msg: RestartAllActors, ctx: &mut Self::Context) -> Self::Result {
-        self.restart_all_actors(ctx);
-        Ok(())
-    }
-}
-
-// ============================================================================
-// KEY MESSAGE HANDLERS - Bridge to existing GraphServiceActor functionality
-// ============================================================================
-
-/// For now, forward key graph messages directly to maintain compatibility
-/// In a full refactor, these would be decomposed and routed to specialized actors
-
-// Removed GetGraphData handler from graph_messages - GraphServiceActor doesn't implement it
-
-// Removed BuildGraphFromMetadata handler from graph_messages - GraphServiceActor doesn't implement it
-
-impl Handler<msgs::UpdateGraphData> for GraphServiceSupervisor {
-    type Result = ResponseActFuture<Self, Result<(), String>>;
-
-    fn handle(&mut self, _msg: msgs::UpdateGraphData, _ctx: &mut Self::Context) -> Self::Result {
-        warn!("UpdateGraphData: Supervisor not fully implemented");
-        let result = Err("Supervisor not yet fully implemented".to_string());
-        Box::pin(actix::fut::ready(result))
-    }
-}
-
-impl Handler<msgs::AddNodesFromMetadata> for GraphServiceSupervisor {
-    type Result = ResponseActFuture<Self, Result<(), String>>;
-
-    fn handle(&mut self, _msg: msgs::AddNodesFromMetadata, _ctx: &mut Self::Context) -> Self::Result {
-        warn!("AddNodesFromMetadata: Supervisor not fully implemented");
-        let result = Err("Supervisor not yet fully implemented".to_string());
-        Box::pin(actix::fut::ready(result))
-    }
-}
-
-// Removed UpdateNodePosition handler from graph_messages - GraphServiceActor doesn't implement it
-
-// Additional commonly used messages
-impl Handler<msgs::StartSimulation> for GraphServiceSupervisor {
-    type Result = ResponseActFuture<Self, Result<(), String>>;
-
-    fn handle(&mut self, _msg: msgs::StartSimulation, _ctx: &mut Self::Context) -> Self::Result {
-        warn!("StartSimulation: Supervisor not fully implemented");
-        let result = Err("Supervisor not yet fully implemented".to_string());
-        Box::pin(actix::fut::ready(result))
-    }
-}
-
-impl Handler<msgs::SimulationStep> for GraphServiceSupervisor {
-    type Result = ResponseActFuture<Self, Result<(), String>>;
-
-    fn handle(&mut self, _msg: msgs::SimulationStep, _ctx: &mut Self::Context) -> Self::Result {
-        warn!("SimulationStep: Supervisor not fully implemented");
-        let result = Err("Supervisor not yet fully implemented".to_string());
-        Box::pin(actix::fut::ready(result))
-    }
-}
-
-impl Handler<msgs::GetBotsGraphData> for GraphServiceSupervisor {
-    type Result = ResponseActFuture<Self, Result<std::sync::Arc<crate::models::graph::GraphData>, String>>;
-
-    fn handle(&mut self, _msg: msgs::GetBotsGraphData, _ctx: &mut Self::Context) -> Self::Result {
-        warn!("GetBotsGraphData: Supervisor not fully implemented");
-        let result = Err("Supervisor not fully implemented".to_string()); // Return error for now since we need Arc<GraphData>
-        Box::pin(actix::fut::ready(result))
-    }
-}
-
-impl Handler<msgs::UpdateSimulationParams> for GraphServiceSupervisor {
-    type Result = Result<(), String>;
-
-    fn handle(&mut self, _msg: msgs::UpdateSimulationParams, _ctx: &mut Self::Context) -> Self::Result {
-        warn!("UpdateSimulationParams: Supervisor not fully implemented");
-        // This is a fire-and-forget message, so we just log and return
-        Ok(())
-    }
-}
-
-impl Handler<msgs::InitializeGPUConnection> for GraphServiceSupervisor {
-    type Result = ();
-
-    fn handle(&mut self, _msg: msgs::InitializeGPUConnection, _ctx: &mut Self::Context) -> Self::Result {
-        warn!("InitializeGPUConnection: Supervisor not fully implemented");
-        // This is a fire-and-forget message, so we just log and return
-    }
-}
-
-// ============================================================================
-// TRANSITIONAL SUPERVISOR - Wraps GraphServiceActor for gradual migration
-// ============================================================================
-
-/// Transitional supervisor that wraps GraphServiceActor
-/// This allows for gradual migration from the monolithic actor to the full supervisor pattern
-/// while maintaining compatibility with existing code
-pub struct TransitionalGraphSupervisor {
-    /// The wrapped GraphServiceActor
-    graph_service_actor: Option<Addr<GraphServiceActor>>,
-    /// Client manager dependency
-    client_manager_addr: Option<Addr<crate::actors::ClientCoordinatorActor>>,
-    /// GPU manager dependency
-    gpu_manager_addr: Option<Addr<crate::actors::GPUManagerActor>>,
-    /// Supervisor statistics
-    start_time: Instant,
-    messages_forwarded: u64,
-}
-
-impl TransitionalGraphSupervisor {
-    pub fn new(
-        client_manager_addr: Option<Addr<crate::actors::ClientCoordinatorActor>>,
-        gpu_manager_addr: Option<Addr<crate::actors::GPUManagerActor>>,
-    ) -> Self {
-        Self {
-            graph_service_actor: None,
-            client_manager_addr,
-            gpu_manager_addr,
-            start_time: Instant::now(),
-            messages_forwarded: 0,
-        }
-    }
-
-    /// Get or create the wrapped GraphServiceActor
-    fn get_or_create_actor(&mut self, _ctx: &mut Context<Self>) -> Option<&Addr<GraphServiceActor>> {
-        if self.graph_service_actor.is_none() {
-            // Create the GraphServiceActor with the provided dependencies
-            if let Some(ref client_manager) = self.client_manager_addr {
-                info!("TransitionalGraphSupervisor: Creating managed GraphServiceActor");
-                let actor = GraphServiceActor::new(
-                    client_manager.clone(),
-                    None, // GPU compute actor will be linked later
-                    None, // Settings actor will be linked later
-                ).start();
-                self.graph_service_actor = Some(actor);
-            } else {
-                warn!("TransitionalGraphSupervisor: Cannot create GraphServiceActor without client manager");
-                return None;
-            }
-        }
-        self.graph_service_actor.as_ref()
-    }
-}
-
-/// Handler for GetGraphServiceActor - returns the internal GraphServiceActor address
-impl Handler<msgs::GetGraphServiceActor> for TransitionalGraphSupervisor {
-    type Result = Option<Addr<GraphServiceActor>>;
-
-    fn handle(&mut self, _msg: msgs::GetGraphServiceActor, ctx: &mut Self::Context) -> Self::Result {
-        self.get_or_create_actor(ctx).cloned()
-    }
-}
-
-impl Actor for TransitionalGraphSupervisor {
-    type Context = Context<Self>;
-
-    fn started(&mut self, ctx: &mut Self::Context) {
-        info!("TransitionalGraphSupervisor started - managing GraphServiceActor lifecycle");
-
-        // Create the wrapped actor immediately
-        self.get_or_create_actor(ctx);
-    }
-
-    fn stopped(&mut self, _ctx: &mut Self::Context) {
-        let uptime = self.start_time.elapsed();
-        info!(
-            "TransitionalGraphSupervisor stopped - uptime: {:?}, messages forwarded: {}",
-            uptime, self.messages_forwarded
-        );
-    }
-}
-
-// Forward all GraphServiceActor messages to the wrapped actor
-// This maintains full compatibility while adding supervision
-
-// Removed GetGraphData handler from graph_messages - GraphServiceActor doesn't implement it
-
-// Handler for messages::GetGraphData (different from graph_messages::GetGraphData)
-impl Handler<msgs::GetGraphData> for TransitionalGraphSupervisor {
-    type Result = ResponseActFuture<Self, Result<std::sync::Arc<crate::models::graph::GraphData>, String>>;
-
-    fn handle(&mut self, msg: msgs::GetGraphData, ctx: &mut Self::Context) -> Self::Result {
-        let actor_result = self.get_or_create_actor(ctx);
-        if let Some(actor) = actor_result {
-            let addr = actor.clone();
-            self.messages_forwarded += 1;
-            Box::pin(async move {
-                match addr.send(msg).await {
-                    Ok(result) => result,
-                    Err(e) => Err(format!("Actor communication error: {}", e)),
-                }
-            }.into_actor(self))
-        } else {
-            Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-        }
-    }
-}
-
-// Handler for GetNodeMap - NEW for position-aware initialization
-impl Handler<msgs::GetNodeMap> for TransitionalGraphSupervisor {
-    type Result = ResponseActFuture<Self, Result<std::sync::Arc<std::collections::HashMap<u32, crate::models::node::Node>>, String>>;
-
-    fn handle(&mut self, msg: msgs::GetNodeMap, ctx: &mut Self::Context) -> Self::Result {
-        let actor_result = self.get_or_create_actor(ctx);
-        if let Some(actor) = actor_result {
-            let addr = actor.clone();
-            self.messages_forwarded += 1;
-            Box::pin(async move {
-                match addr.send(msg).await {
-                    Ok(result) => result,
-                    Err(e) => Err(format!("Actor communication error: {}", e)),
-                }
-            }.into_actor(self))
-        } else {
-            Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-        }
-    }
-}
-
-// Handler for GetPhysicsState - NEW for settlement state reporting
-impl Handler<msgs::GetPhysicsState> for TransitionalGraphSupervisor {
-    type Result = ResponseActFuture<Self, Result<crate::actors::graph_actor::PhysicsState, String>>;
-
-    fn handle(&mut self, msg: msgs::GetPhysicsState, ctx: &mut Self::Context) -> Self::Result {
-        let actor_result = self.get_or_create_actor(ctx);
-        if let Some(actor) = actor_result {
-            let addr = actor.clone();
-            self.messages_forwarded += 1;
-            Box::pin(async move {
-                match addr.send(msg).await {
-                    Ok(result) => result,
-                    Err(e) => Err(format!("Actor communication error: {}", e)),
-                }
-            }.into_actor(self))
-        } else {
-            Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-        }
-    }
-}
-
-// Handler for BuildGraphFromMetadata from messages module
-impl Handler<msgs::BuildGraphFromMetadata> for TransitionalGraphSupervisor {
-    type Result = ResponseActFuture<Self, Result<(), String>>;
-
-    fn handle(&mut self, msg: msgs::BuildGraphFromMetadata, ctx: &mut Self::Context) -> Self::Result {
-        let actor_result = self.get_or_create_actor(ctx);
-        if let Some(actor) = actor_result {
-            let addr = actor.clone();
-            self.messages_forwarded += 1;
-            Box::pin(async move {
-                match addr.send(msg).await {
-                    Ok(result) => result,
-                    Err(e) => Err(format!("Actor communication error: {}", e)),
-                }
-            }.into_actor(self))
-        } else {
-            Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-        }
-    }
-}
-
-impl Handler<msgs::UpdateGraphData> for TransitionalGraphSupervisor {
-    type Result = ResponseActFuture<Self, Result<(), String>>;
-
-    fn handle(&mut self, msg: msgs::UpdateGraphData, ctx: &mut Self::Context) -> Self::Result {
-        let actor_result = self.get_or_create_actor(ctx);
-        if let Some(actor) = actor_result {
-            let addr = actor.clone();
-            self.messages_forwarded += 1;
-            Box::pin(async move {
-                match addr.send(msg).await {
-                    Ok(result) => result,
-                    Err(e) => Err(format!("Actor communication error: {}", e)),
-                }
-            }.into_actor(self))
-        } else {
-            Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-        }
-    }
-}
-
-impl Handler<msgs::AddNodesFromMetadata> for TransitionalGraphSupervisor {
-    type Result = ResponseActFuture<Self, Result<(), String>>;
-
-    fn handle(&mut self, msg: msgs::AddNodesFromMetadata, ctx: &mut Self::Context) -> Self::Result {
-        let actor_result = self.get_or_create_actor(ctx);
-        if let Some(actor) = actor_result {
-            let addr = actor.clone();
-            self.messages_forwarded += 1;
-            Box::pin(async move {
-                match addr.send(msg).await {
-                    Ok(result) => result,
-                    Err(e) => Err(format!("Actor communication error: {}", e)),
-                }
-            }.into_actor(self))
-        } else {
-            Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-        }
-    }
-}
-
-// Removed UpdateNodePosition handler from graph_messages - GraphServiceActor doesn't implement it
-
-impl Handler<msgs::StartSimulation> for TransitionalGraphSupervisor {
-    type Result = ResponseActFuture<Self, Result<(), String>>;
-
-    fn handle(&mut self, msg: msgs::StartSimulation, ctx: &mut Self::Context) -> Self::Result {
-        if let Some(actor) = self.get_or_create_actor(ctx) {
-            let addr = actor.clone();
-            self.messages_forwarded += 1;
-            Box::pin(async move {
-                match addr.send(msg).await {
-                    Ok(result) => result,
-                    Err(e) => Err(format!("Actor communication error: {}", e)),
-                }
-            }.into_actor(self))
-        } else {
-            Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-        }
-    }
-}
-
-impl Handler<msgs::SimulationStep> for TransitionalGraphSupervisor {
-    type Result = ResponseActFuture<Self, Result<(), String>>;
-
-    fn handle(&mut self, msg: msgs::SimulationStep, ctx: &mut Self::Context) -> Self::Result {
-        if let Some(actor) = self.get_or_create_actor(ctx) {
-            let addr = actor.clone();
-            self.messages_forwarded += 1;
-            Box::pin(async move {
-                match addr.send(msg).await {
-                    Ok(result) => result,
-                    Err(e) => Err(format!("Actor communication error: {}", e)),
-                }
-            }.into_actor(self))
-        } else {
-            Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-        }
-    }
-}
-
-impl Handler<msgs::GetBotsGraphData> for TransitionalGraphSupervisor {
-    type Result = ResponseActFuture<Self, Result<std::sync::Arc<crate::models::graph::GraphData>, String>>;
-
-    fn handle(&mut self, msg: msgs::GetBotsGraphData, ctx: &mut Self::Context) -> Self::Result {
-        if let Some(actor) = self.get_or_create_actor(ctx) {
-            let addr = actor.clone();
-            self.messages_forwarded += 1;
-            Box::pin(async move {
-                match addr.send(msg).await {
-                    Ok(result) => result,
-                    Err(e) => Err(format!("Actor communication error: {}", e)),
-                }
-            }.into_actor(self))
-        } else {
-            Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-        }
-    }
-}
-
-impl Handler<msgs::UpdateSimulationParams> for TransitionalGraphSupervisor {
-    type Result = Result<(), String>;
-
-    fn handle(&mut self, msg: msgs::UpdateSimulationParams, ctx: &mut Self::Context) -> Self::Result {
-        if let Some(actor) = self.get_or_create_actor(ctx) {
-            actor.do_send(msg);
-            self.messages_forwarded += 1;
-            Ok(())
-        } else {
-            Err("Failed to create GraphServiceActor".to_string())
-        }
-    }
-}
-
-impl Handler<msgs::InitializeGPUConnection> for TransitionalGraphSupervisor {
-    type Result = ();
-
-    fn handle(&mut self, msg: msgs::InitializeGPUConnection, ctx: &mut Self::Context) -> Self::Result {
-        let actor_result = self.get_or_create_actor(ctx);
-        if let Some(actor) = actor_result {
-            actor.do_send(msg);
-            self.messages_forwarded += 1;
-        }
-    }
-}
-
-impl Handler<msgs::UpdateBotsGraph> for TransitionalGraphSupervisor {
-    type Result = ();
-
-    fn handle(&mut self, msg: msgs::UpdateBotsGraph, ctx: &mut Self::Context) -> Self::Result {
-        let actor_result = self.get_or_create_actor(ctx);
-        if let Some(actor) = actor_result {
-            actor.do_send(msg);
-            self.messages_forwarded += 1;
-        }
-    }
-}
-
-// Add handlers for other messages that might be sent
-// These provide basic forwarding functionality
-
-macro_rules! forward_message {
-    ($msg_type:ty, $result_type:ty) => {
-        impl Handler<$msg_type> for TransitionalGraphSupervisor {
-            type Result = ResponseActFuture<Self, $result_type>;
-
-            fn handle(&mut self, msg: $msg_type, ctx: &mut Self::Context) -> Self::Result {
-                let actor_result = self.get_or_create_actor(ctx);
-                if let Some(actor) = actor_result {
-                    let addr = actor.clone();
-                    self.messages_forwarded += 1;
-                    Box::pin(async move {
-                        match addr.send(msg).await {
-                            Ok(result) => result,
-                            Err(e) => Err(format!("Actor communication error: {}", e)),
-                        }
-                    }.into_actor(self))
-                } else {
-                    Box::pin(actix::fut::ready(Err("Failed to create GraphServiceActor".to_string())))
-                }
-            }
-        }
-    };
-}
-
-// Forward common messages that the GraphServiceActor handles
-// Note: Some types may be private to graph_actor.rs, so we use String for now
-forward_message!(msgs::ComputeShortestPaths, Result<std::collections::HashMap<u32, Option<f32>>, String>);
-forward_message!(msgs::RequestPositionSnapshot, Result<crate::actors::messages::PositionSnapshot, String>);
-forward_message!(msgs::GetAutoBalanceNotifications, Result<Vec<crate::actors::graph_actor::AutoBalanceNotification>, String>);
-forward_message!(msgs::InitialClientSync, Result<(), String>);
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    use actix::System;
-
-    #[actix_rt::test]
-    async fn test_supervisor_initialization() {
-        let system = System::new();
-
-        system.block_on(async {
-            let supervisor = GraphServiceSupervisor::new();
-            assert_eq!(supervisor.strategy, GraphSupervisionStrategy::OneForOne);
-            assert_eq!(supervisor.actor_info.len(), 0);
-        });
-    }
-
-    #[actix_rt::test]
-    async fn test_restart_policy_default() {
-        let policy = RestartPolicy::default();
-        assert_eq!(policy.max_restarts, 5);
-        assert_eq!(policy.within_time_period, Duration::from_secs(300));
-    }
-
-    #[actix_rt::test]
-    async fn test_backoff_calculation() {
-        let supervisor = GraphServiceSupervisor::new();
-
-        // Test with no actor info
-        let backoff = supervisor.calculate_backoff(&ActorType::GraphState);
-        assert_eq!(backoff, Duration::from_secs(1));
-    }
-}
\ No newline at end of file
diff --git a/src/actors/graph_state_actor.rs b/src/actors/graph_state_actor.rs
index 53cc6134..ea801f63 100644
--- a/src/actors/graph_state_actor.rs
+++ b/src/actors/graph_state_actor.rs
@@ -1,702 +1,1133 @@
-//! Graph State Actor
+//! GraphStateActor - Pure state management for graph data
 //!
-//! This module implements a specialized actor focused exclusively on graph state management.
-//! It handles all graph data operations including node/edge CRUD operations, metadata-based
-//! updates, and path computation while maintaining separation of concerns from physics simulation.
-//!
-//! ## Core Responsibilities
-//!
-//! ### 1. Graph Data Management
-//! - **Primary Graph**: Maintains the main graph data structure with nodes and edges
-//! - **Node Map**: Provides efficient O(1) node lookup by ID
-//! - **Bots Graph**: Manages separate graph data for agent visualization
-//!
-//! ### 2. Node Operations
-//! - **AddNode**: Add new nodes to the graph with proper ID management
-//! - **RemoveNode**: Remove nodes and clean up associated edges
-//! - **UpdateNodeFromMetadata**: Update existing nodes based on metadata changes
-//!
-//! ### 3. Edge Operations
-//! - **AddEdge**: Create connections between nodes
-//! - **RemoveEdge**: Remove specific edges by ID
-//! - **Edge consistency**: Maintain edge integrity during node operations
-//!
-//! ### 4. Metadata Integration
-//! - **BuildGraphFromMetadata**: Rebuild entire graph from metadata store
-//! - **AddNodesFromMetadata**: Add multiple nodes from metadata
-//! - **RemoveNodeByMetadata**: Remove nodes by metadata ID
-//!
-//! ### 5. Path Computation
-//! - **ComputeShortestPaths**: Calculate shortest paths from source nodes
-//! - **Graph traversal**: Provide efficient path finding algorithms
-//!
-//! ## Usage Pattern
-//!
-//! The GraphStateActor serves as the authoritative source for all graph state:
-//!
-//! ```rust
-//! // Get current graph data
-//! let graph_data = graph_state_actor.send(GetGraphData).await?;
-//!
-//! // Add a new node
-//! graph_state_actor.send(AddNode { node }).await?;
-//!
-//! // Update from metadata
-//! graph_state_actor.send(BuildGraphFromMetadata { metadata }).await?;
-//! ```
+//! This actor manages the canonical graph state with thread-safe access
+//! and change tracking. It provides no physics computation or external
+//! actor dependencies - only pure state operations.

 use actix::prelude::*;
-use std::collections::HashMap;
-use std::sync::Arc;
-use log::{debug, info, warn, error, trace};
-
-use crate::actors::messages::*;
-use crate::errors::VisionFlowError;
+use std::collections::{HashMap, HashSet};
+use std::sync::{Arc, RwLock};
+use crate::models::graph::GraphData;
 use crate::models::node::Node;
 use crate::models::edge::Edge;
-use crate::models::metadata::{MetadataStore, FileMetadata};
-use crate::models::graph::GraphData;
-use crate::utils::socket_flow_messages::{BinaryNodeData, BinaryNodeDataClient, glam_to_vec3data};
+use crate::utils::socket_flow_messages::BinaryNodeData;
+use crate::models::metadata::MetadataStore;
+
+// Import legacy messages for compatibility
+use crate::actors::messages::{
+    GetGraphData as LegacyGetGraphData,
+    AddNode,
+    AddNodesFromMetadata,
+    BuildGraphFromMetadata,
+    UpdateGraphData,
+    InitializeGPUConnection,
+    GetNodeMap,
+    GetPhysicsState,
+    GetAutoBalanceNotifications,
+    UpdateBotsGraph,
+    GetBotsGraphData,
+    UpdateSimulationParams,
+    ComputeShortestPaths,
+    AddEdge as LegacyAddEdge,
+    RemoveNode,
+    UpdateNodePositions as LegacyUpdateNodePositions,
+    RequestPositionSnapshot,
+    PositionSnapshot,
+    SimulationStep,
+};
+use crate::actors::graph_actor::{PhysicsState, AutoBalanceNotification};
+use crate::models::simulation_params::SimulationParams;
+
+// ============================================================================
+// Message Definitions
+// ============================================================================
+
+#[derive(Message)]
+#[rtype(result = "Result<Arc<GraphData>, String>")]
+pub struct GetGraphData;
+
+#[derive(Message)]
+#[rtype(result = "Result<Vec<u32>, String>")]
+pub struct AddNodes {
+    pub nodes: Vec<Node>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<Vec<String>, String>")]
+pub struct AddEdges {
+    pub edges: Vec<Edge>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct UpdateNodePositions {
+    pub updates: Vec<(u32, BinaryNodeData)>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<HashSet<u32>, String>")]
+pub struct GetDirtyNodes;
+
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct ClearDirtyNodes;
+
+#[derive(Message)]
+#[rtype(result = "u64")]
+pub struct GetVersion;
+
+#[derive(Message)]
+#[rtype(result = "Result<Vec<Node>, String>")]
+pub struct GetNodes {
+    pub node_ids: Vec<u32>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct RemoveNodes {
+    pub node_ids: Vec<u32>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct Clear;
+
+// ============================================================================
+// Actor Definition
+// ============================================================================

-/// Graph State Actor - Manages all graph state operations
 pub struct GraphStateActor {
-    /// Primary graph data containing nodes and edges
-    graph_data: Arc<GraphData>,
-    /// Efficient node lookup map by ID
-    node_map: Arc<HashMap<u32, Node>>,
-    /// Separate graph data for bot/agent visualization
-    bots_graph_data: Arc<GraphData>,
-    /// Next available node ID for auto-assignment
-    next_node_id: std::sync::atomic::AtomicU32,
+    /// Thread-safe graph data store
+    graph_data: Arc<RwLock<GraphData>>,
+
+    /// Fast node lookup by ID
+    node_index: HashMap<u32, usize>,
+
+    /// Tracks nodes with changes since last clear
+    dirty_nodes: HashSet<u32>,
+
+    /// Version counter for optimistic locking
+    version: u64,
 }

 impl GraphStateActor {
-    /// Create new GraphStateActor instance
     pub fn new() -> Self {
         Self {
-            graph_data: Arc::new(GraphData::new()),
-            node_map: Arc::new(HashMap::new()),
-            bots_graph_data: Arc::new(GraphData::new()),
-            next_node_id: std::sync::atomic::AtomicU32::new(1),
+            graph_data: Arc::new(RwLock::new(GraphData::new())),
+            node_index: HashMap::new(),
+            dirty_nodes: HashSet::new(),
+            version: 0,
         }
     }

-    /// Get reference to graph data
-    pub fn get_graph_data(&self) -> &GraphData {
-        &self.graph_data
+    /// Increment version and return new value
+    fn bump_version(&mut self) -> u64 {
+        self.version = self.version.wrapping_add(1);
+        self.version
     }

-    /// Get reference to node map
-    pub fn get_node_map(&self) -> &HashMap<u32, Node> {
-        &self.node_map
+    /// Mark node as dirty
+    fn mark_dirty(&mut self, node_id: u32) {
+        self.dirty_nodes.insert(node_id);
     }

-    /// Add a single node to the graph
-    fn add_node(&mut self, node: Node) {
-        let node_id = node.id;
+    /// Rebuild node index from current graph data
+    fn rebuild_index(&mut self) -> Result<(), String> {
+        self.node_index.clear();

-        // Add to node map
-        Arc::make_mut(&mut self.node_map).insert(node_id, node.clone());
+        let graph = self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?;

-        // Add to graph data
-        Arc::make_mut(&mut self.graph_data).nodes.push(node);
+        for (idx, node) in graph.nodes.iter().enumerate() {
+            self.node_index.insert(node.id, idx);
+        }

-        info!("Added node {} to graph", node_id);
+        Ok(())
     }
+}

-    /// Remove a node and its associated edges
-    fn remove_node(&mut self, node_id: u32) {
-        // Remove from node map
-        if Arc::make_mut(&mut self.node_map).remove(&node_id).is_some() {
-            // Remove from graph data nodes
-            let graph_data_mut = Arc::make_mut(&mut self.graph_data);
-            graph_data_mut.nodes.retain(|n| n.id != node_id);
+impl Default for GraphStateActor {
+    fn default() -> Self {
+        Self::new()
+    }
+}

-            // Remove associated edges
-            graph_data_mut.edges.retain(|e| e.source != node_id && e.target != node_id);
+impl Actor for GraphStateActor {
+    type Context = Context<Self>;

-            info!("Removed node {} and its edges from graph", node_id);
-        } else {
-            warn!("Attempted to remove non-existent node {}", node_id);
-        }
+    fn started(&mut self, _ctx: &mut Self::Context) {
+        log::info!("GraphStateActor started");
     }

-    /// Add an edge to the graph
-    fn add_edge(&mut self, edge: Edge) {
-        // Verify source and target nodes exist
-        if !self.node_map.contains_key(&edge.source) {
-            warn!("Cannot add edge: source node {} does not exist", edge.source);
-            return;
-        }
-        if !self.node_map.contains_key(&edge.target) {
-            warn!("Cannot add edge: target node {} does not exist", edge.target);
-            return;
-        }
+    fn stopped(&mut self, _ctx: &mut Self::Context) {
+        log::info!("GraphStateActor stopped");
+    }
+}
+
+// ============================================================================
+// Message Handlers
+// ============================================================================

-        // Add to graph data
-        Arc::make_mut(&mut self.graph_data).edges.push(edge.clone());
-        info!("Added edge from {} to {} with weight {}", edge.source, edge.target, edge.weight);
+impl Handler<GetGraphData> for GraphStateActor {
+    type Result = Result<Arc<GraphData>, String>;
+
+    fn handle(&mut self, _msg: GetGraphData, _ctx: &mut Context<Self>) -> Self::Result {
+        Ok(Arc::new(self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?
+            .clone()))
     }
+}
+
+impl Handler<AddNodes> for GraphStateActor {
+    type Result = Result<Vec<u32>, String>;
+
+    fn handle(&mut self, msg: AddNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut added_ids = Vec::with_capacity(msg.nodes.len());
+
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            for node in msg.nodes {
+                let node_id = node.id;
+
+                // Check if node already exists
+                if self.node_index.contains_key(&node_id) {
+                    return Err(format!("Node with ID {} already exists", node_id));
+                }
+
+                // Add to graph
+                let idx = graph.nodes.len();
+                graph.nodes.push(node);

-    /// Remove an edge by ID
-    fn remove_edge(&mut self, edge_id: &str) {
-        let graph_data_mut = Arc::make_mut(&mut self.graph_data);
-        let initial_count = graph_data_mut.edges.len();
+                // Update index
+                self.node_index.insert(node_id, idx);

-        graph_data_mut.edges.retain(|e| e.id != edge_id);
+                added_ids.push(node_id);
+            }
+        } // Drop write lock

-        let removed_count = initial_count - graph_data_mut.edges.len();
-        if removed_count > 0 {
-            info!("Removed {} edge(s) with ID {}", removed_count, edge_id);
-        } else {
-            warn!("No edges found with ID {}", edge_id);
+        // Mark as dirty and bump version
+        for &node_id in &added_ids {
+            self.mark_dirty(node_id);
         }
+        self.bump_version();
+
+        Ok(added_ids)
     }
+}

-    /// Build graph from metadata store
-    fn build_from_metadata(&mut self, metadata: MetadataStore) -> Result<(), String> {
-        let mut new_graph_data = GraphData::new();
+impl Handler<AddEdges> for GraphStateActor {
+    type Result = Result<Vec<String>, String>;

-        // Save existing node positions to preserve layout
-        let mut existing_positions: HashMap<String, (crate::types::vec3::Vec3Data, crate::types::vec3::Vec3Data)> = HashMap::new();
+    fn handle(&mut self, msg: AddEdges, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut added_ids = Vec::with_capacity(msg.edges.len());

-        for node in &self.graph_data.nodes {
-            existing_positions.insert(node.metadata_id.clone(), (node.data.position(), node.data.velocity()));
-        }
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-        // Generate nodes from metadata
-        let mut new_node_map = HashMap::new();
-        let mut current_id = self.next_node_id.load(std::sync::atomic::Ordering::SeqCst);
-
-        for (metadata_id, file_metadata) in metadata.iter() {
-            let mut node = Node::new_with_id(metadata_id.clone(), Some(current_id));
-
-            // Restore position if it existed
-            if let Some((position, velocity)) = existing_positions.get(metadata_id) {
-                node.data.x = position.x;
-                node.data.y = position.y;
-                node.data.z = position.z;
-                node.data.vx = velocity.x;
-                node.data.vy = velocity.y;
-                node.data.vz = velocity.z;
-            } else {
-                // Generate random initial position
-                self.generate_random_position(&mut node);
+            for edge in msg.edges {
+                // Validate source and target nodes exist
+                if !self.node_index.contains_key(&edge.source) {
+                    return Err(format!("Source node {} does not exist", edge.source));
+                }
+                if !self.node_index.contains_key(&edge.target) {
+                    return Err(format!("Target node {} does not exist", edge.target));
+                }
+
+                let edge_id = edge.id.clone();
+                graph.edges.push(edge);
+
+                added_ids.push(edge_id);
             }
+        } // Drop write lock

-            // Set node properties from metadata
-            self.configure_node_from_metadata(&mut node, file_metadata);
+        self.bump_version();

-            new_node_map.insert(current_id, node.clone());
-            new_graph_data.nodes.push(node);
-            current_id += 1;
-        }
+        Ok(added_ids)
+    }
+}

-        // Generate edges based on relationships in metadata
-        self.generate_edges_from_metadata(&mut new_graph_data, &metadata);
+impl Handler<UpdateNodePositions> for GraphStateActor {
+    type Result = Result<(), String>;

-        // Update actor state
-        self.graph_data = Arc::new(new_graph_data);
-        self.node_map = Arc::new(new_node_map);
-        self.next_node_id.store(current_id, std::sync::atomic::Ordering::SeqCst);
+    fn handle(&mut self, msg: UpdateNodePositions, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut dirty_nodes = Vec::new();

-        info!("Built graph from metadata: {} nodes, {} edges",
-              self.graph_data.nodes.len(), self.graph_data.edges.len());
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            for (node_id, binary_data) in msg.updates {
+                // Find node index
+                let idx = self.node_index.get(&node_id)
+                    .ok_or_else(|| format!("Node {} not found", node_id))?;
+
+                // Update node data
+                if let Some(node) = graph.nodes.get_mut(*idx) {
+                    node.data = binary_data;
+                    dirty_nodes.push(node_id);
+                } else {
+                    return Err(format!("Node index {} out of bounds", idx));
+                }
+            }
+        } // Drop write lock
+
+        // Mark as dirty and bump version
+        for node_id in dirty_nodes {
+            self.mark_dirty(node_id);
+        }
+        self.bump_version();

         Ok(())
     }
+}

-    /// Generate random initial position for a node
-    fn generate_random_position(&self, node: &mut Node) {
-        use rand::{Rng, SeedableRng};
-        use rand::rngs::{StdRng, OsRng};
+impl Handler<GetDirtyNodes> for GraphStateActor {
+    type Result = Result<HashSet<u32>, String>;

-        let mut rng = StdRng::from_seed(OsRng.gen());
-        let radius = 50.0 + rng.gen::<f32>() * 100.0;
-        let theta = rng.gen::<f32>() * 2.0 * std::f32::consts::PI;
-        let phi = rng.gen::<f32>() * std::f32::consts::PI;
+    fn handle(&mut self, _msg: GetDirtyNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        Ok(self.dirty_nodes.clone())
+    }
+}

-        node.data.x = radius * phi.sin() * theta.cos();
-        node.data.y = radius * phi.sin() * theta.sin();
-        node.data.z = radius * phi.cos();
+impl Handler<ClearDirtyNodes> for GraphStateActor {
+    type Result = Result<(), String>;

-        // Small random initial velocity
-        node.data.vx = rng.gen_range(-1.0..1.0);
-        node.data.vy = rng.gen_range(-1.0..1.0);
-        node.data.vz = rng.gen_range(-1.0..1.0);
+    fn handle(&mut self, _msg: ClearDirtyNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        self.dirty_nodes.clear();
+        Ok(())
     }
+}

-    /// Configure node properties based on metadata
-    fn configure_node_from_metadata(&self, node: &mut Node, metadata: &FileMetadata) {
-        // Set label from filename
-        if let Some(filename) = metadata.path.file_name() {
-            node.label = filename.to_string_lossy().to_string();
-        }
+impl Handler<GetVersion> for GraphStateActor {
+    type Result = u64;

-        // Set color based on file extension
-        node.color = Some(self.get_color_for_extension(&metadata.path));
+    fn handle(&mut self, _msg: GetVersion, _ctx: &mut Context<Self>) -> Self::Result {
+        self.version
+    }
+}

-        // Set size based on file size (if available)
-        if let Some(size) = metadata.size {
-            node.size = Some(10.0 + (size as f32 / 1000.0).min(50.0));
-        }
+impl Handler<GetNodes> for GraphStateActor {
+    type Result = Result<Vec<Node>, String>;

-        // Store metadata
-        node.metadata.insert("path".to_string(), metadata.path.to_string_lossy().to_string());
-        if let Some(size) = metadata.size {
-            node.metadata.insert("size".to_string(), size.to_string());
-        }
-        if let Some(modified) = metadata.modified {
-            node.metadata.insert("modified".to_string(), modified.to_string());
-        }
-    }
+    fn handle(&mut self, msg: GetNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        let graph = self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?;
+
+        let mut nodes = Vec::with_capacity(msg.node_ids.len());

-    /// Get color for file extension
-    fn get_color_for_extension(&self, path: &std::path::Path) -> String {
-        match path.extension().and_then(|s| s.to_str()) {
-            Some("rs") => "#CE422B".to_string(), // Rust orange
-            Some("js") | Some("ts") => "#F7DF1E".to_string(), // JavaScript yellow
-            Some("py") => "#3776AB".to_string(), // Python blue
-            Some("html") => "#E34F26".to_string(), // HTML orange
-            Some("css") => "#1572B6".to_string(), // CSS blue
-            Some("json") => "#000000".to_string(), // JSON black
-            Some("md") => "#083FA1".to_string(), // Markdown blue
-            Some("txt") => "#808080".to_string(), // Text gray
-            _ => "#95A5A6".to_string(), // Default gray
+        for node_id in msg.node_ids {
+            let idx = self.node_index.get(&node_id)
+                .ok_or_else(|| format!("Node {} not found", node_id))?;
+
+            if let Some(node) = graph.nodes.get(*idx) {
+                nodes.push(node.clone());
+            } else {
+                return Err(format!("Node index {} out of bounds", idx));
+            }
         }
+
+        Ok(nodes)
     }
+}
+
+impl Handler<RemoveNodes> for GraphStateActor {
+    type Result = Result<(), String>;

-    /// Generate edges from metadata relationships
-    fn generate_edges_from_metadata(&self, graph_data: &mut GraphData, metadata: &MetadataStore) {
-        // Create edges based on directory structure
-        let mut path_to_node: HashMap<std::path::PathBuf, u32> = HashMap::new();
+    fn handle(&mut self, msg: RemoveNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut graph = self.graph_data.write()
+            .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-        // Map paths to node IDs
-        for node in &graph_data.nodes {
-            if let Some(path_str) = node.metadata.get("path") {
-                let path = std::path::PathBuf::from(path_str);
-                path_to_node.insert(path, node.id);
+        // Collect indices to remove
+        let mut indices_to_remove: Vec<usize> = Vec::new();
+        for node_id in &msg.node_ids {
+            if let Some(&idx) = self.node_index.get(node_id) {
+                indices_to_remove.push(idx);
             }
         }

-        // Create edges between files in same directory
-        let mut directory_nodes: HashMap<std::path::PathBuf, Vec<u32>> = HashMap::new();
+        // Sort in reverse order to remove from end first
+        indices_to_remove.sort_unstable_by(|a, b| b.cmp(a));

-        for (path, node_id) in &path_to_node {
-            if let Some(parent) = path.parent() {
-                directory_nodes.entry(parent.to_path_buf())
-                    .or_insert_with(Vec::new)
-                    .push(*node_id);
+        // Remove nodes
+        for idx in indices_to_remove {
+            if idx < graph.nodes.len() {
+                graph.nodes.remove(idx);
             }
         }

-        // Create edges within directories
-        for (_, nodes) in directory_nodes {
-            if nodes.len() > 1 {
-                for i in 0..nodes.len() {
-                    for j in i+1..nodes.len() {
-                        let edge = Edge::new(nodes[i], nodes[j], 0.3); // Weak directory connection
-                        graph_data.edges.push(edge);
-                    }
-                }
-            }
+        // Remove associated edges
+        let node_ids_set: HashSet<u32> = msg.node_ids.iter().copied().collect();
+        graph.edges.retain(|edge| {
+            !node_ids_set.contains(&edge.source) && !node_ids_set.contains(&edge.target)
+        });
+
+        // Rebuild index after removal
+        drop(graph);
+        self.rebuild_index()?;
+
+        // Remove from dirty set
+        for node_id in &msg.node_ids {
+            self.dirty_nodes.remove(node_id);
         }

-        info!("Generated {} edges from metadata relationships", graph_data.edges.len());
+        self.bump_version();
+
+        Ok(())
     }
+}

-    /// Add nodes from metadata store
-    fn add_nodes_from_metadata(&mut self, metadata: MetadataStore) -> Result<(), String> {
-        let mut added_count = 0;
-        let mut current_id = self.next_node_id.load(std::sync::atomic::Ordering::SeqCst);
+impl Handler<Clear> for GraphStateActor {
+    type Result = Result<(), String>;

-        for (metadata_id, file_metadata) in metadata.iter() {
-            // Skip if node already exists
-            if self.node_map.values().any(|n| n.metadata_id == *metadata_id) {
-                continue;
-            }
+    fn handle(&mut self, _msg: Clear, _ctx: &mut Context<Self>) -> Self::Result {
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-            let mut node = Node::new_with_id(metadata_id.clone(), Some(current_id));
-            self.generate_random_position(&mut node);
-            self.configure_node_from_metadata(&mut node, file_metadata);
+            graph.nodes.clear();
+            graph.edges.clear();
+            graph.metadata.clear();
+            graph.id_to_metadata.clear();
+        } // Drop write lock

-            self.add_node(node);
-            current_id += 1;
-            added_count += 1;
-        }
+        self.node_index.clear();
+        self.dirty_nodes.clear();
+        self.bump_version();

-        self.next_node_id.store(current_id, std::sync::atomic::Ordering::SeqCst);
-        info!("Added {} new nodes from metadata", added_count);
         Ok(())
     }
+}

-    /// Update existing node from metadata
-    fn update_node_from_metadata(&mut self, metadata_id: String, metadata: FileMetadata) -> Result<(), String> {
-        // Find node by metadata_id
-        let mut node_found = false;
-        let node_map_mut = Arc::make_mut(&mut self.node_map);
+// ============================================================================
+// Legacy Message Handlers for Backward Compatibility
+// ============================================================================

-        for (_, node) in node_map_mut.iter_mut() {
-            if node.metadata_id == metadata_id {
-                self.configure_node_from_metadata(node, &metadata);
-                node_found = true;
-                break;
-            }
+impl Handler<LegacyGetGraphData> for GraphStateActor {
+    type Result = Result<Arc<GraphData>, String>;
+
+    fn handle(&mut self, _msg: LegacyGetGraphData, _ctx: &mut Context<Self>) -> Self::Result {
+        Ok(Arc::new(self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?
+            .clone()))
+    }
+}
+
+impl Handler<AddNodesFromMetadata> for GraphStateActor {
+    type Result = Result<(), String>;
+
+    fn handle(&mut self, msg: AddNodesFromMetadata, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut nodes = Vec::new();
+
+        // Convert metadata to nodes
+        // MetadataStore is HashMap<String, Metadata> where key is filename
+        for (filename, _metadata) in msg.metadata.iter() {
+            // Create node with filename as both metadata_id and label
+            let mut node = Node::new(filename.clone());
+            node.label = filename.clone(); // Set label so it displays in UI
+            nodes.push(node);
         }

-        // Update in graph_data as well
-        if node_found {
-            let graph_data_mut = Arc::make_mut(&mut self.graph_data);
-            for node in &mut graph_data_mut.nodes {
-                if node.metadata_id == metadata_id {
-                    self.configure_node_from_metadata(node, &metadata);
-                    break;
+        // Add nodes via existing handler logic
+        if nodes.is_empty() {
+            return Ok(());
+        }
+
+        let mut added_ids = Vec::new();
+
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            for node in nodes {
+                let node_id = node.id;
+
+                // Skip if node already exists
+                if self.node_index.contains_key(&node_id) {
+                    continue;
                 }
+
+                let idx = graph.nodes.len();
+                graph.nodes.push(node);
+                self.node_index.insert(node_id, idx);
+                added_ids.push(node_id);
             }
-            info!("Updated node with metadata_id: {}", metadata_id);
-            Ok(())
-        } else {
-            warn!("Node with metadata_id {} not found for update", metadata_id);
-            Err(format!("Node with metadata_id {} not found", metadata_id))
-        }
-    }

-    /// Remove node by metadata ID
-    fn remove_node_by_metadata(&mut self, metadata_id: String) -> Result<(), String> {
-        // Find node ID by metadata_id
-        let node_id = self.node_map.values()
-            .find(|n| n.metadata_id == metadata_id)
-            .map(|n| n.id);
+            // Create edges between new nodes and connect to existing graph
+            if !added_ids.is_empty() {
+                // Get all existing node IDs before the new additions
+                let existing_ids: Vec<u32> = self.node_index.keys()
+                    .filter(|id| !added_ids.contains(id))
+                    .copied()
+                    .collect();
+
+                // Connect new nodes to each other
+                for i in 0..added_ids.len() {
+                    for j in 1..=2 {
+                        if i + j < added_ids.len() {
+                            let edge = Edge::new(added_ids[i], added_ids[i + j], 1.0);
+                            graph.edges.push(edge);
+                        }
+                    }
+                }
+
+                // Connect new nodes to existing graph (2-3 edges each to existing nodes)
+                for &new_id in &added_ids {
+                    let num_connections = existing_ids.len().min(3);
+                    for i in 0..num_connections {
+                        if i < existing_ids.len() {
+                            let edge = Edge::new(new_id, existing_ids[i], 1.0);
+                            graph.edges.push(edge);
+                        }
+                    }
+                }

-        if let Some(id) = node_id {
-            self.remove_node(id);
-            Ok(())
-        } else {
-            warn!("Node with metadata_id {} not found for removal", metadata_id);
-            Err(format!("Node with metadata_id {} not found", metadata_id))
+                log::info!("Added {} new nodes and created edges to integrate with existing {} nodes",
+                          added_ids.len(), existing_ids.len());
+            }
+        } // Drop write lock
+
+        // Mark as dirty and bump version
+        for node_id in added_ids {
+            self.mark_dirty(node_id);
         }
+        self.bump_version();
+
+        Ok(())
     }
+}

-    /// Compute shortest paths from a source node using Dijkstra's algorithm
-    fn compute_shortest_paths(&self, source_node_id: u32) -> Result<HashMap<u32, (f32, Vec<u32>)>, String> {
-        if !self.node_map.contains_key(&source_node_id) {
-            return Err(format!("Source node {} not found", source_node_id));
-        }
+impl Handler<BuildGraphFromMetadata> for GraphStateActor {
+    type Result = Result<(), String>;

-        let mut distances: HashMap<u32, f32> = HashMap::new();
-        let mut predecessors: HashMap<u32, u32> = HashMap::new();
-        let mut unvisited: std::collections::BTreeSet<(ordered_float::OrderedFloat<f32>, u32)> = std::collections::BTreeSet::new();
+    fn handle(&mut self, msg: BuildGraphFromMetadata, _ctx: &mut Context<Self>) -> Self::Result {
+        // BuildGraphFromMetadata clears existing graph and builds from metadata
+        // This is different from AddNodesFromMetadata which adds to existing graph

-        // Initialize distances
-        for &node_id in self.node_map.keys() {
-            let distance = if node_id == source_node_id { 0.0 } else { f32::INFINITY };
-            distances.insert(node_id, distance);
-            unvisited.insert((ordered_float::OrderedFloat(distance), node_id));
-        }
+        // Clear existing graph first
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-        while let Some((current_distance, current_node)) = unvisited.pop_first() {
-            let current_distance = current_distance.into_inner();
+            graph.nodes.clear();
+            graph.edges.clear();
+            graph.metadata.clear();
+            graph.id_to_metadata.clear();
+        } // Drop write lock

-            if current_distance == f32::INFINITY {
-                break; // No more reachable nodes
-            }
+        self.node_index.clear();
+        self.dirty_nodes.clear();

-            // Check all edges from current node
-            for edge in &self.graph_data.edges {
-                let (neighbor, edge_weight) = if edge.source == current_node {
-                    (edge.target, edge.weight)
-                } else if edge.target == current_node {
-                    (edge.source, edge.weight)
-                } else {
-                    continue;
-                };
+        // Now add nodes from metadata (reuse AddNodesFromMetadata logic)
+        let mut nodes = Vec::new();

-                let new_distance = current_distance + edge_weight;
-                let old_distance = distances.get(&neighbor).copied().unwrap_or(f32::INFINITY);
+        for (filename, _metadata) in msg.metadata.iter() {
+            let mut node = Node::new(filename.clone());
+            node.label = filename.clone(); // Set label so it displays in UI
+            nodes.push(node);
+        }
+
+        if nodes.is_empty() {
+            return Ok(());
+        }

-                if new_distance < old_distance {
-                    // Remove old entry from unvisited
-                    unvisited.remove(&(ordered_float::OrderedFloat(old_distance), neighbor));
+        let mut added_ids = Vec::new();

-                    // Update distance and predecessor
-                    distances.insert(neighbor, new_distance);
-                    predecessors.insert(neighbor, current_node);
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-                    // Add new entry to unvisited
-                    unvisited.insert((ordered_float::OrderedFloat(new_distance), neighbor));
-                }
+            for node in nodes {
+                let node_id = node.id;
+                let idx = graph.nodes.len();
+                graph.nodes.push(node);
+                self.node_index.insert(node_id, idx);
+                added_ids.push(node_id);
             }
-        }

-        // Reconstruct paths
-        let mut result: HashMap<u32, (f32, Vec<u32>)> = HashMap::new();
-
-        for (&target_node, &distance) in &distances {
-            if distance != f32::INFINITY {
-                let mut path = Vec::new();
-                let mut current = target_node;
-
-                // Reconstruct path backwards
-                while current != source_node_id {
-                    path.push(current);
-                    if let Some(&prev) = predecessors.get(&current) {
-                        current = prev;
-                    } else {
-                        break;
+            // Create basic edges between nodes (fully connected for visualization)
+            // For now, create edges between consecutive nodes for a minimal graph structure
+            if added_ids.len() > 1 {
+                for i in 0..added_ids.len() {
+                    // Connect each node to next 2-3 nodes for a sparse mesh
+                    for j in 1..=2 {
+                        if i + j < added_ids.len() {
+                            let edge = Edge::new(added_ids[i], added_ids[i + j], 1.0);
+                            graph.edges.push(edge);
+                        }
                     }
                 }
-                path.push(source_node_id);
-                path.reverse();
-
-                result.insert(target_node, (distance, path));
+                log::info!("Created {} edges between {} nodes", graph.edges.len(), added_ids.len());
             }
-        }
+        } // Drop write lock

-        info!("Computed shortest paths from node {} to {} reachable nodes",
-              source_node_id, result.len());
+        // Mark as dirty and bump version
+        for node_id in added_ids {
+            self.mark_dirty(node_id);
+        }
+        self.bump_version();

-        Ok(result)
+        Ok(())
     }
 }

-impl Actor for GraphStateActor {
-    type Context = Context<Self>;
+impl Handler<UpdateGraphData> for GraphStateActor {
+    type Result = Result<(), String>;

-    fn started(&mut self, _ctx: &mut Self::Context) {
-        info!("GraphStateActor started");
-    }
+    fn handle(&mut self, msg: UpdateGraphData, _ctx: &mut Context<Self>) -> Self::Result {
+        // Replace entire graph with new data
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-    fn stopped(&mut self, _ctx: &mut Self::Context) {
-        info!("GraphStateActor stopped");
-    }
-}
+            // Extract data from Arc
+            let new_graph_data = (*msg.graph_data).clone();

-// Handler implementations
+            // Replace all graph data
+            *graph = new_graph_data;
+        } // Drop write lock

-impl Handler<GetGraphData> for GraphStateActor {
-    type Result = Result<Arc<GraphData>, String>;
+        // Rebuild index from new graph
+        self.rebuild_index()?;
+
+        // Mark all nodes as dirty
+        {
+            let graph_read = self.graph_data.read()
+                .map_err(|e| format!("Failed to acquire read lock: {}", e))?;
+
+            for node in &graph_read.nodes {
+                self.dirty_nodes.insert(node.id);
+            }
+        }
+
+        self.bump_version();

-    fn handle(&mut self, _msg: GetGraphData, _ctx: &mut Self::Context) -> Self::Result {
-        debug!("GraphStateActor handling GetGraphData with Arc reference");
-        Ok(Arc::clone(&self.graph_data))
+        Ok(())
     }
 }

-impl Handler<AddNode> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<InitializeGPUConnection> for GraphStateActor {
+    type Result = ();

-    fn handle(&mut self, msg: AddNode, _ctx: &mut Self::Context) -> Self::Result {
-        self.add_node(msg.node);
-        Ok(())
+    fn handle(&mut self, _msg: InitializeGPUConnection, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't manage GPU connections
+        // This is a no-op for pure state management
+        log::debug!("GraphStateActor: GPU connection message received (no-op for pure state)");
     }
 }

-impl Handler<RemoveNode> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<GetNodeMap> for GraphStateActor {
+    type Result = Result<Arc<HashMap<u32, Node>>, String>;

-    fn handle(&mut self, msg: RemoveNode, _ctx: &mut Self::Context) -> Self::Result {
-        self.remove_node(msg.node_id);
-        Ok(())
+    fn handle(&mut self, _msg: GetNodeMap, _ctx: &mut Context<Self>) -> Self::Result {
+        let graph = self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?;
+
+        let mut node_map = HashMap::new();
+        for node in &graph.nodes {
+            node_map.insert(node.id, node.clone());
+        }
+
+        Ok(Arc::new(node_map))
     }
 }

-impl Handler<AddEdge> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<GetPhysicsState> for GraphStateActor {
+    type Result = Result<PhysicsState, String>;
+
+    fn handle(&mut self, _msg: GetPhysicsState, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't manage physics state
+        // Return default state (consult PhysicsOrchestratorActor for actual physics)
+        Ok(PhysicsState {
+            is_settled: false,
+            stable_frame_count: 0,
+            kinetic_energy: 0.0,
+            current_state: "Not managed by GraphStateActor".to_string(),
+        })
+    }
+}

-    fn handle(&mut self, msg: AddEdge, _ctx: &mut Self::Context) -> Self::Result {
-        self.add_edge(msg.edge);
-        Ok(())
+impl Handler<GetAutoBalanceNotifications> for GraphStateActor {
+    type Result = Result<Vec<AutoBalanceNotification>, String>;
+
+    fn handle(&mut self, _msg: GetAutoBalanceNotifications, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't manage auto-balance notifications
+        // Return empty list (consult PhysicsOrchestratorActor for actual notifications)
+        Ok(Vec::new())
     }
 }

-impl Handler<RemoveEdge> for GraphStateActor {
+impl Handler<AddNode> for GraphStateActor {
     type Result = Result<(), String>;

-    fn handle(&mut self, msg: RemoveEdge, _ctx: &mut Self::Context) -> Self::Result {
-        self.remove_edge(&msg.edge_id);
+    fn handle(&mut self, msg: AddNode, _ctx: &mut Context<Self>) -> Self::Result {
+        let node_id = msg.node.id;
+
+        // Check if node already exists
+        if self.node_index.contains_key(&node_id) {
+            return Err(format!("Node with ID {} already exists", node_id));
+        }
+
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            let idx = graph.nodes.len();
+            graph.nodes.push(msg.node);
+            self.node_index.insert(node_id, idx);
+        } // Drop write lock
+
+        self.mark_dirty(node_id);
+        self.bump_version();
+
         Ok(())
     }
 }

-impl Handler<GetNodeMap> for GraphStateActor {
-    type Result = Result<Arc<HashMap<u32, Node>>, String>;
+impl Handler<UpdateBotsGraph> for GraphStateActor {
+    type Result = ();

-    fn handle(&mut self, _msg: GetNodeMap, _ctx: &mut Self::Context) -> Self::Result {
-        debug!("GraphStateActor handling GetNodeMap with Arc reference");
-        Ok(Arc::clone(&self.node_map))
+    fn handle(&mut self, msg: UpdateBotsGraph, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut added_ids = Vec::new();
+
+        // Add nodes (ignore errors for backward compatibility)
+        if let Ok(mut graph) = self.graph_data.write() {
+            for agent in &msg.agents {
+                // Create node from agent data
+                let mut node = Node::new(agent.id.clone());
+                node.label = agent.name.clone();
+                node.node_type = Some(agent.agent_type.clone());
+                node.data.x = agent.x;
+                node.data.y = agent.y;
+                node.data.z = agent.z;
+
+                let node_id = node.id;
+
+                // Skip if already exists
+                if self.node_index.contains_key(&node_id) {
+                    continue;
+                }
+
+                let idx = graph.nodes.len();
+                graph.nodes.push(node);
+                self.node_index.insert(node_id, idx);
+                added_ids.push(node_id);
+            }
+        } // Drop write lock
+
+        // Mark as dirty and bump version
+        for node_id in added_ids {
+            self.mark_dirty(node_id);
+        }
+        self.bump_version();
     }
 }

-impl Handler<BuildGraphFromMetadata> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<GetBotsGraphData> for GraphStateActor {
+    type Result = Result<Arc<GraphData>, String>;

-    fn handle(&mut self, msg: BuildGraphFromMetadata, _ctx: &mut Self::Context) -> Self::Result {
-        info!("BuildGraphFromMetadata handler called with {} metadata entries", msg.metadata.len());
-        self.build_from_metadata(msg.metadata)
+    fn handle(&mut self, _msg: GetBotsGraphData, _ctx: &mut Context<Self>) -> Self::Result {
+        // Same as GetGraphData
+        Ok(Arc::new(self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?
+            .clone()))
     }
 }

-impl Handler<AddNodesFromMetadata> for GraphStateActor {
+impl Handler<UpdateSimulationParams> for GraphStateActor {
     type Result = Result<(), String>;

-    fn handle(&mut self, msg: AddNodesFromMetadata, _ctx: &mut Self::Context) -> Self::Result {
-        self.add_nodes_from_metadata(msg.metadata)
+    fn handle(&mut self, _msg: UpdateSimulationParams, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't manage simulation parameters
+        // This should be sent to PhysicsOrchestratorActor instead
+        log::debug!("GraphStateActor: Received UpdateSimulationParams (no-op, delegate to PhysicsOrchestratorActor)");
+        Ok(())
     }
 }

-impl Handler<UpdateNodeFromMetadata> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<ComputeShortestPaths> for GraphStateActor {
+    type Result = Result<HashMap<u32, Option<f32>>, String>;

-    fn handle(&mut self, msg: UpdateNodeFromMetadata, _ctx: &mut Self::Context) -> Self::Result {
-        self.update_node_from_metadata(msg.metadata_id, msg.metadata)
+    fn handle(&mut self, _msg: ComputeShortestPaths, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't compute shortest paths
+        // This should be sent to SemanticProcessorActor instead
+        Err("GraphStateActor doesn't compute paths. Use SemanticProcessorActor instead.".to_string())
     }
 }

-impl Handler<RemoveNodeByMetadata> for GraphStateActor {
+impl Handler<LegacyAddEdge> for GraphStateActor {
     type Result = Result<(), String>;

-    fn handle(&mut self, msg: RemoveNodeByMetadata, _ctx: &mut Self::Context) -> Self::Result {
-        self.remove_node_by_metadata(msg.metadata_id)
+    fn handle(&mut self, msg: LegacyAddEdge, _ctx: &mut Context<Self>) -> Self::Result {
+        // Validate source and target nodes exist
+        if !self.node_index.contains_key(&msg.edge.source) {
+            return Err(format!("Source node {} does not exist", msg.edge.source));
+        }
+        if !self.node_index.contains_key(&msg.edge.target) {
+            return Err(format!("Target node {} does not exist", msg.edge.target));
+        }
+
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            graph.edges.push(msg.edge);
+        } // Drop write lock
+
+        self.bump_version();
+
+        Ok(())
     }
 }

-impl Handler<UpdateGraphData> for GraphStateActor {
+impl Handler<RemoveNode> for GraphStateActor {
     type Result = Result<(), String>;

-    fn handle(&mut self, msg: UpdateGraphData, _ctx: &mut Self::Context) -> Self::Result {
-        info!("Updating graph data with {} nodes, {} edges",
-              msg.graph_data.nodes.len(), msg.graph_data.edges.len());
+    fn handle(&mut self, msg: RemoveNode, _ctx: &mut Context<Self>) -> Self::Result {
+        // Find node index
+        let idx = self.node_index.get(&msg.node_id)
+            .ok_or_else(|| format!("Node {} not found", msg.node_id))?;

-        // Update graph data with the provided Arc
-        self.graph_data = msg.graph_data;
+        let mut graph = self.graph_data.write()
+            .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-        // Rebuild node map
-        Arc::make_mut(&mut self.node_map).clear();
-        for node in &self.graph_data.nodes {
-            Arc::make_mut(&mut self.node_map).insert(node.id, node.clone());
+        // Remove node
+        if *idx < graph.nodes.len() {
+            graph.nodes.remove(*idx);
         }

-        info!("Graph data updated successfully");
+        // Remove associated edges
+        graph.edges.retain(|edge| {
+            edge.source != msg.node_id && edge.target != msg.node_id
+        });
+
+        // Drop lock before rebuild
+        drop(graph);
+
+        // Rebuild index
+        self.rebuild_index()?;
+
+        // Remove from dirty set
+        self.dirty_nodes.remove(&msg.node_id);
+        self.bump_version();
+
         Ok(())
     }
 }

-impl Handler<GetBotsGraphData> for GraphStateActor {
-    type Result = Result<Arc<GraphData>, String>;
+impl Handler<LegacyUpdateNodePositions> for GraphStateActor {
+    type Result = Result<(), String>;

-    fn handle(&mut self, _msg: GetBotsGraphData, _ctx: &mut Context<Self>) -> Self::Result {
-        Ok(Arc::clone(&self.bots_graph_data))
-    }
-}
+    fn handle(&mut self, msg: LegacyUpdateNodePositions, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut dirty_nodes = Vec::new();

-impl Handler<UpdateBotsGraph> for GraphStateActor {
-    type Result = ();
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-    fn handle(&mut self, msg: UpdateBotsGraph, _ctx: &mut Context<Self>) -> Self::Result {
-        // Convert agents to graph structure
-        let mut nodes = vec![];
-        let mut edges = vec![];
+            for (node_id, binary_data) in msg.positions {
+                // Find node index
+                let idx = self.node_index.get(&node_id)
+                    .ok_or_else(|| format!("Node {} not found", node_id))?;

-        let bot_id_offset = 10000;
+                // Update node data
+                if let Some(node) = graph.nodes.get_mut(*idx) {
+                    node.data = binary_data;
+                    dirty_nodes.push(node_id);
+                } else {
+                    return Err(format!("Node index {} out of bounds", idx));
+                }
+            }
+        } // Drop write lock

-        // Preserve existing agent positions
-        let mut existing_positions: HashMap<String, (crate::types::vec3::Vec3Data, crate::types::vec3::Vec3Data)> = HashMap::new();
-        for node in &self.bots_graph_data.nodes {
-            existing_positions.insert(node.metadata_id.clone(), (node.data.position(), node.data.velocity()));
+        // Mark as dirty and bump version
+        for node_id in dirty_nodes {
+            self.mark_dirty(node_id);
         }
+        self.bump_version();

-        // Create nodes for each agent
-        for (i, agent) in msg.agents.iter().enumerate() {
-            let node_id = bot_id_offset + i as u32;
-            let mut node = Node::new_with_id(agent.id.clone(), Some(node_id));
-
-            if let Some((saved_position, saved_velocity)) = existing_positions.get(&agent.id) {
-                // Restore existing position
-                node.data.x = saved_position.x;
-                node.data.y = saved_position.y;
-                node.data.z = saved_position.z;
-                node.data.vx = saved_velocity.x;
-                node.data.vy = saved_velocity.y;
-                node.data.vz = saved_velocity.z;
-            } else {
-                self.generate_random_position(&mut node);
-            }
+        Ok(())
+    }
+}

-            // Set node properties based on agent type
-            node.color = Some(match agent.agent_type.as_str() {
-                "coordinator" => "#FF6B6B".to_string(),
-                "researcher" => "#4ECDC4".to_string(),
-                "coder" => "#45B7D1".to_string(),
-                "analyst" => "#FFA07A".to_string(),
-                "architect" => "#98D8C8".to_string(),
-                "tester" => "#F7DC6F".to_string(),
-                _ => "#95A5A6".to_string(),
-            });
-
-            node.label = agent.name.clone();
-            node.size = Some(20.0 + (agent.workload * 25.0));
-
-            // Add metadata
-            node.metadata.insert("agent_type".to_string(), agent.agent_type.clone());
-            node.metadata.insert("status".to_string(), agent.status.clone());
-            node.metadata.insert("cpu_usage".to_string(), agent.cpu_usage.to_string());
-            node.metadata.insert("memory_usage".to_string(), agent.memory_usage.to_string());
-            node.metadata.insert("health".to_string(), agent.health.to_string());
-            node.metadata.insert("is_agent".to_string(), "true".to_string());
+impl Handler<RequestPositionSnapshot> for GraphStateActor {
+    type Result = Result<PositionSnapshot, String>;

-            nodes.push(node);
+    fn handle(&mut self, msg: RequestPositionSnapshot, _ctx: &mut Context<Self>) -> Self::Result {
+        let graph = self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?;
+
+        let mut knowledge_nodes = Vec::new();
+        let mut agent_nodes = Vec::new();
+
+        if msg.include_knowledge_graph {
+            // Include only knowledge nodes (exclude agent, ontology types)
+            for node in &graph.nodes {
+                let is_agent = node.node_type.as_ref()
+                    .map_or(false, |t| t.contains("agent") || t.contains("bot"));
+                let is_ontology = node.node_type.as_ref()
+                    .map_or(false, |t| t.contains("ontology") || t.contains("owl"));
+
+                if !is_agent && !is_ontology {
+                    knowledge_nodes.push((node.id, node.data));
+                }
+            }
         }

-        // Create edges based on agent interactions
-        for (i, source_agent) in msg.agents.iter().enumerate() {
-            for (j, target_agent) in msg.agents.iter().enumerate() {
-                if i != j {
-                    let source_node_id = bot_id_offset + i as u32;
-                    let target_node_id = bot_id_offset + j as u32;
-
-                    let communication_intensity = if source_agent.agent_type == "coordinator" || target_agent.agent_type == "coordinator" {
-                        0.8
-                    } else if source_agent.status == "active" && target_agent.status == "active" {
-                        0.5
-                    } else {
-                        0.2
-                    };
-
-                    if communication_intensity > 0.1 {
-                        let mut edge = Edge::new(source_node_id, target_node_id, communication_intensity);
-                        let metadata = edge.metadata.get_or_insert_with(HashMap::new);
-                        metadata.insert("communication_type".to_string(), "agent_collaboration".to_string());
-                        metadata.insert("intensity".to_string(), communication_intensity.to_string());
-                        edges.push(edge);
+        if msg.include_agent_graph {
+            // Filter for agent nodes (node_type contains "agent" or "bot")
+            for node in &graph.nodes {
+                if let Some(ref node_type) = node.node_type {
+                    if node_type.contains("agent") || node_type.contains("bot") {
+                        agent_nodes.push((node.id, node.data));
                     }
                 }
             }
         }

-        // Update the bots graph data
-        let bots_graph_data_mut = Arc::make_mut(&mut self.bots_graph_data);
-        bots_graph_data_mut.nodes = nodes;
-        bots_graph_data_mut.edges = edges;
+        Ok(PositionSnapshot {
+            knowledge_nodes,
+            agent_nodes,
+            timestamp: std::time::Instant::now(),
+        })
+    }
+}

-        info!("Updated bots graph with {} agents and {} edges",
-             msg.agents.len(), self.bots_graph_data.edges.len());
+impl Handler<SimulationStep> for GraphStateActor {
+    type Result = Result<(), String>;
+
+    fn handle(&mut self, _msg: SimulationStep, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't run simulation steps
+        // This should be sent to PhysicsOrchestratorActor instead
+        log::debug!("GraphStateActor: Received SimulationStep (no-op, delegate to PhysicsOrchestratorActor)");
+        Ok(())
     }
 }

-impl Handler<ComputeShortestPaths> for GraphStateActor {
-    type Result = Result<u32, String>;
-
-    fn handle(&mut self, msg: ComputeShortestPaths, _ctx: &mut Self::Context) -> Self::Result {
-        match self.compute_shortest_paths(msg.source_node_id) {
-            Ok(paths) => {
-                info!("Computed shortest paths from node {}: {} reachable nodes",
-                      msg.source_node_id, paths.len());
-                Ok(paths.len() as u32)
-            }
-            Err(e) => {
-                error!("Failed to compute shortest paths: {}", e);
-                Err(e)
-            }
-        }
+// ============================================================================
+// Tests
+// ============================================================================
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[actix::test]
+    async fn test_add_and_get_nodes() {
+        let actor = GraphStateActor::new().start();
+
+        let node1 = Node::new("test1.md".to_string());
+        let node2 = Node::new("test2.md".to_string());
+        let node1_id = node1.id;
+        let node2_id = node2.id;
+
+        // Add nodes
+        let result = actor.send(AddNodes {
+            nodes: vec![node1, node2],
+        }).await;
+
+        assert!(result.is_ok());
+        let ids = result.unwrap();
+        assert!(ids.is_ok());
+        let added_ids = ids.unwrap();
+        assert_eq!(added_ids.len(), 2);
+
+        // Get nodes
+        let result = actor.send(GetNodes {
+            node_ids: vec![node1_id, node2_id],
+        }).await;
+
+        assert!(result.is_ok());
+        let nodes = result.unwrap();
+        assert!(nodes.is_ok());
+        let fetched_nodes = nodes.unwrap();
+        assert_eq!(fetched_nodes.len(), 2);
+    }
+
+    #[actix::test]
+    async fn test_update_positions() {
+        let actor = GraphStateActor::new().start();
+
+        let node = Node::new("test.md".to_string());
+        let node_id = node.id;
+
+        // Add node
+        let _ = actor.send(AddNodes {
+            nodes: vec![node],
+        }).await;
+
+        // Update position
+        let new_data = BinaryNodeData {
+            node_id,
+            x: 10.0,
+            y: 20.0,
+            z: 30.0,
+            vx: 1.0,
+            vy: 2.0,
+            vz: 3.0,
+        };
+
+        let result = actor.send(UpdateNodePositions {
+            updates: vec![(node_id, new_data)],
+        }).await;
+
+        assert!(result.is_ok());
+        assert!(result.unwrap().is_ok());
+
+        // Verify position was updated
+        let nodes = actor.send(GetNodes {
+            node_ids: vec![node_id],
+        }).await.unwrap().unwrap();
+
+        assert_eq!(nodes[0].data.x, 10.0);
+        assert_eq!(nodes[0].data.y, 20.0);
+        assert_eq!(nodes[0].data.z, 30.0);
+    }
+
+    #[actix::test]
+    async fn test_dirty_tracking() {
+        let actor = GraphStateActor::new().start();
+
+        let node = Node::new("test.md".to_string());
+        let node_id = node.id;
+
+        // Add node
+        let _ = actor.send(AddNodes {
+            nodes: vec![node],
+        }).await;
+
+        // Check dirty nodes
+        let dirty = actor.send(GetDirtyNodes).await.unwrap().unwrap();
+        assert!(dirty.contains(&node_id));
+
+        // Clear dirty nodes
+        let _ = actor.send(ClearDirtyNodes).await;
+
+        // Verify cleared
+        let dirty = actor.send(GetDirtyNodes).await.unwrap().unwrap();
+        assert!(dirty.is_empty());
+    }
+
+    #[actix::test]
+    async fn test_version_tracking() {
+        let actor = GraphStateActor::new().start();
+
+        let v1 = actor.send(GetVersion).await.unwrap();
+        assert_eq!(v1, 0);
+
+        let node = Node::new("test.md".to_string());
+        let _ = actor.send(AddNodes {
+            nodes: vec![node],
+        }).await;
+
+        let v2 = actor.send(GetVersion).await.unwrap();
+        assert_eq!(v2, 1);
+    }
+
+    #[actix::test]
+    async fn test_remove_nodes() {
+        let actor = GraphStateActor::new().start();
+
+        let node = Node::new("test.md".to_string());
+        let node_id = node.id;
+
+        // Add node
+        let _ = actor.send(AddNodes {
+            nodes: vec![node],
+        }).await;
+
+        // Remove node
+        let result = actor.send(RemoveNodes {
+            node_ids: vec![node_id],
+        }).await;
+
+        assert!(result.is_ok());
+        assert!(result.unwrap().is_ok());
+
+        // Verify node was removed
+        let result = actor.send(GetNodes {
+            node_ids: vec![node_id],
+        }).await;
+
+        assert!(result.is_ok());
+        assert!(result.unwrap().is_err());
     }
-}
\ No newline at end of file
+
+    #[actix::test]
+    async fn test_clear() {
+        let actor = GraphStateActor::new().start();
+
+        let node1 = Node::new("test1.md".to_string());
+        let node2 = Node::new("test2.md".to_string());
+
+        // Add nodes
+        let _ = actor.send(AddNodes {
+            nodes: vec![node1, node2],
+        }).await;
+
+        // Clear
+        let result = actor.send(Clear).await;
+        assert!(result.is_ok());
+        assert!(result.unwrap().is_ok());
+
+        // Verify empty
+        let dirty = actor.send(GetDirtyNodes).await.unwrap().unwrap();
+        assert!(dirty.is_empty());
+
+        let graph = actor.send(GetGraphData).await.unwrap().unwrap();
+        let g = graph.read().unwrap();
+        assert_eq!(g.nodes.len(), 0);
+        assert_eq!(g.edges.len(), 0);
+    }
+
+    #[actix::test]
+    async fn test_add_edges() {
+        let actor = GraphStateActor::new().start();
+
+        let node1 = Node::new("test1.md".to_string());
+        let node2 = Node::new("test2.md".to_string());
+        let node1_id = node1.id;
+        let node2_id = node2.id;
+
+        // Add nodes
+        let _ = actor.send(AddNodes {
+            nodes: vec![node1, node2],
+        }).await;
+
+        // Add edge
+        let edge = Edge::new(node1_id, node2_id, 1.0);
+        let result = actor.send(AddEdges {
+            edges: vec![edge],
+        }).await;
+
+        assert!(result.is_ok());
+        let ids = result.unwrap();
+        assert!(ids.is_ok());
+        let added_ids = ids.unwrap();
+        assert_eq!(added_ids.len(), 1);
+
+        // Verify edge exists
+        let graph = actor.send(GetGraphData).await.unwrap().unwrap();
+        let g = graph.read().unwrap();
+        assert_eq!(g.edges.len(), 1);
+        assert_eq!(g.edges[0].source, node1_id);
+        assert_eq!(g.edges[0].target, node2_id);
+    }
+
+    #[actix::test]
+    async fn test_add_edge_invalid_nodes() {
+        let actor = GraphStateActor::new().start();
+
+        // Try to add edge with non-existent nodes
+        let edge = Edge::new(999, 1000, 1.0);
+        let result = actor.send(AddEdges {
+            edges: vec![edge],
+        }).await;
+
+        assert!(result.is_ok());
+        let ids = result.unwrap();
+        assert!(ids.is_err());
+    }
+}
diff --git a/src/actors/mod.rs b/src/actors/mod.rs
index af79e03c..9a3f7eeb 100644
--- a/src/actors/mod.rs
+++ b/src/actors/mod.rs
@@ -1,6 +1,7 @@
 //! Actor system modules for replacing Arc<RwLock<T>> patterns with Actix actors

 pub mod graph_actor;
+pub mod graph_state_actor;
 pub mod physics_orchestrator_actor;
 pub mod optimized_settings_actor;
 pub mod metadata_actor;
@@ -15,12 +16,12 @@ pub mod multi_mcp_visualization_actor;
 pub mod workspace_actor;
 pub mod semantic_processor_actor;
 pub mod ontology_actor;
-pub mod graph_service_supervisor;
 pub mod task_orchestrator_actor;
 pub mod messages;
 pub mod graph_messages;

 pub use graph_actor::GraphServiceActor;
+pub use graph_state_actor::GraphStateActor;
 pub use physics_orchestrator_actor::PhysicsOrchestratorActor;
 pub use optimized_settings_actor::OptimizedSettingsActor;
 pub use metadata_actor::MetadataActor;
@@ -35,9 +36,4 @@ pub use workspace_actor::WorkspaceActor;
 pub use semantic_processor_actor::{SemanticProcessorActor, SemanticProcessorConfig, SemanticStats, AISemanticFeatures};
 pub use ontology_actor::{OntologyActor, OntologyActorConfig, ValidationJob, JobPriority, JobStatus, ActorStatistics as OntologyActorStatistics};
 pub use task_orchestrator_actor::{TaskOrchestratorActor, CreateTask, GetTaskStatus, StopTask, ListActiveTasks, GetSystemStatus, SystemStatusInfo, TaskState};
-pub use graph_service_supervisor::{
-    GraphServiceSupervisor, GraphSupervisionStrategy, RestartPolicy, BackoffStrategy,
-    ActorHealth, ActorType, SupervisorStatus, SupervisorMessage, ActorHeartbeat,
-    GetSupervisorStatus, RestartActor, RestartAllActors
-};
 pub use messages::*;
\ No newline at end of file
diff --git a/src/actors/optimized_settings_actor.rs b/src/actors/optimized_settings_actor.rs
index 29ef8d49..b0ab3779 100644
--- a/src/actors/optimized_settings_actor.rs
+++ b/src/actors/optimized_settings_actor.rs
@@ -4,7 +4,9 @@
 use actix::prelude::*;
 use crate::config::AppFullSettings;
 use crate::actors::messages::{GetSettings, UpdateSettings, GetSettingByPath, SetSettingByPath, GetSettingsByPaths, SetSettingsByPaths, UpdatePhysicsFromAutoBalance};
-use crate::actors::{GraphServiceActor, gpu::ForceComputeActor};
+use crate::actors::GraphServiceActor;
+#[cfg(feature = "gpu")]
+use crate::actors::gpu::ForceComputeActor;
 use crate::config::path_access::PathAccessible;
 use crate::errors::{VisionFlowError, VisionFlowResult, SettingsError, ActorError, ErrorContext};
 use std::collections::HashMap;
@@ -19,6 +21,7 @@ use std::num::NonZeroUsize;
 use blake3::Hasher;
 use flate2::{Compress, Decompress, Compression, FlushCompress, FlushDecompress};
 use flate2::Status;
+use crate::services::database_service::DatabaseService;

 #[cfg(feature = "redis")]
 use redis::{Client as RedisClient, AsyncCommands};
@@ -28,6 +31,7 @@ const CACHE_SIZE: usize = 1000;

 pub struct OptimizedSettingsActor {
     settings: Arc<RwLock<AppFullSettings>>,
+    db_service: Option<Arc<DatabaseService>>,
     #[cfg(feature = "redis")]
     redis_client: Option<RedisClient>,
     path_cache: Arc<RwLock<LruCache<String, CachedValue>>>,
@@ -35,7 +39,8 @@ pub struct OptimizedSettingsActor {
     metrics: Arc<RwLock<PerformanceMetrics>>,
     compressor: Arc<RwLock<Compress>>,
     decompressor: Arc<RwLock<Decompress>>,
-    graph_service_addr: Option<Addr<crate::actors::graph_service_supervisor::TransitionalGraphSupervisor>>,
+    graph_service_addr: Option<Addr<GraphServiceActor>>,
+    #[cfg(feature = "gpu")]
     gpu_compute_addr: Option<Addr<ForceComputeActor>>,
 }

@@ -124,16 +129,28 @@ const EXPECTED_PATH_SIZE: u64 = 500; // ~500B

 impl OptimizedSettingsActor {
     pub fn new() -> VisionFlowResult<Self> {
-        // Load settings from file or use defaults
-        let settings = AppFullSettings::new()
-            .map_err(|e| {
-                error!("Failed to load settings from file: {}", e);
-                VisionFlowError::Settings(SettingsError::ParseError {
-                    file_path: "settings".to_string(),
-                    reason: e.to_string(),
-                })
-            })?;
-
+        Self::with_database(None)
+    }
+
+    pub fn with_database(db_service: Option<Arc<DatabaseService>>) -> VisionFlowResult<Self> {
+        // Load initial settings structure from database or defaults
+        let settings = if let Some(ref db) = db_service {
+            info!("Loading settings from SQLite database");
+            match Self::load_settings_from_database(db) {
+                Ok(s) => {
+                    info!("Settings loaded successfully from database");
+                    s
+                }
+                Err(e) => {
+                    warn!("Failed to load settings from database, using defaults: {}", e);
+                    AppFullSettings::default()
+                }
+            }
+        } else {
+            warn!("No database service provided, using default settings structure");
+            AppFullSettings::default()
+        };
+
         // Initialize Redis client (optional)
         #[cfg(feature = "redis")]
         let redis_client = match std::env::var("REDIS_URL") {
@@ -154,25 +171,26 @@ impl OptimizedSettingsActor {
                 None
             }
         };
-
+
         // Initialize LRU cache
         let path_cache = Arc::new(RwLock::new(
             LruCache::new(NonZeroUsize::new(CACHE_SIZE).unwrap())
         ));
-
+
         // Pre-compile common path patterns
         let mut path_lookup = HashMap::new();
         Self::initialize_path_patterns(&mut path_lookup);
-
-        info!("OptimizedSettingsActor initialized with performance optimizations");
-        debug!("Logseq physics: damping={}, spring={}, repulsion={}",
+
+        info!("OptimizedSettingsActor initialized with database-backed settings");
+        debug!("Logseq physics: damping={}, spring={}, repulsion={}",
             settings.visualisation.graphs.logseq.physics.damping,
             settings.visualisation.graphs.logseq.physics.spring_k,
             settings.visualisation.graphs.logseq.physics.repel_k
         );
-
+
         Ok(Self {
             settings: Arc::new(RwLock::new(settings)),
+            db_service,
             #[cfg(feature = "redis")]
             redis_client,
             path_cache,
@@ -181,21 +199,60 @@ impl OptimizedSettingsActor {
             compressor: Arc::new(RwLock::new(Compress::new(Compression::default(), false))),
             decompressor: Arc::new(RwLock::new(Decompress::new(false))),
             graph_service_addr: None,
+            #[cfg(feature = "gpu")]
             gpu_compute_addr: None,
         })
     }

+    fn load_settings_from_database(db: &DatabaseService) -> VisionFlowResult<AppFullSettings> {
+        // Try to load complete settings from database
+        match db.load_all_settings() {
+            Ok(Some(settings)) => {
+                info!("Loaded complete settings from database");
+                Ok(settings)
+            }
+            Ok(None) => {
+                info!("No settings found in database, using defaults");
+                Ok(AppFullSettings::default())
+            }
+            Err(e) => {
+                warn!("Failed to load settings from database: {}, using defaults", e);
+                Ok(AppFullSettings::default())
+            }
+        }
+    }
+
     pub fn with_actors(
-        graph_service_addr: Option<Addr<crate::actors::graph_service_supervisor::TransitionalGraphSupervisor>>,
+        graph_service_addr: Option<Addr<GraphServiceActor>>,
+        #[cfg(feature = "gpu")]
         gpu_compute_addr: Option<Addr<ForceComputeActor>>,
     ) -> VisionFlowResult<Self> {
         let mut actor = Self::new()?;
         actor.graph_service_addr = graph_service_addr;
-        actor.gpu_compute_addr = gpu_compute_addr;
+        #[cfg(feature = "gpu")]
+        {
+            actor.gpu_compute_addr = gpu_compute_addr;
+        }
         info!("OptimizedSettingsActor initialized with GPU and Graph actor addresses for physics forwarding and concurrent update batching");
         Ok(actor)
     }

+    pub fn with_database_and_actors(
+        db_service: Option<Arc<DatabaseService>>,
+        graph_service_addr: Option<Addr<GraphServiceActor>>,
+        #[cfg(feature = "gpu")]
+        gpu_compute_addr: Option<Addr<ForceComputeActor>>,
+    ) -> VisionFlowResult<Self> {
+        let mut actor = Self::with_database(db_service)?;
+        actor.graph_service_addr = graph_service_addr;
+        #[cfg(feature = "gpu")]
+        {
+            actor.gpu_compute_addr = gpu_compute_addr;
+        }
+        info!("OptimizedSettingsActor initialized with database + GPU and Graph actor addresses");
+        Ok(actor)
+    }
+
     fn initialize_path_patterns(lookup: &mut HashMap<String, PathPattern>) {
         // Physics settings patterns - most frequently accessed
         let physics_patterns = vec![
@@ -486,23 +543,27 @@ impl OptimizedSettingsActor {
     pub async fn update_settings(&self, new_settings: AppFullSettings) -> VisionFlowResult<()> {
         let mut settings = self.settings.write().await;
         *settings = new_settings;
-
+
         // Clear all caches since settings changed
         {
             let mut cache = self.path_cache.write().await;
             cache.clear();
         }
-
-        // Persist to file
-        settings.save().map_err(|e| {
-            error!("Failed to save settings to file: {}", e);
-            VisionFlowError::Settings(SettingsError::SaveFailed {
-                file_path: "settings".to_string(),
-                reason: e,
-            })
-        })?;
-
-        info!("Settings updated, caches cleared, and saved successfully");
+
+        // Persist ALL settings to database if available
+        if let Some(ref db) = self.db_service {
+            if let Err(e) = db.save_all_settings(&settings) {
+                error!("Failed to save settings to database: {}", e);
+                return Err(VisionFlowError::Settings(SettingsError::SaveFailed {
+                    file_path: "database".to_string(),
+                    reason: format!("Database error: {}", e),
+                }));
+            }
+            info!("All settings updated and saved to database successfully");
+        } else {
+            warn!("No database service available, settings not persisted");
+        }
+
         Ok(())
     }

@@ -713,6 +774,7 @@ impl Clone for OptimizedSettingsActor {
     fn clone(&self) -> Self {
         Self {
             settings: self.settings.clone(),
+            db_service: self.db_service.clone(),
             #[cfg(feature = "redis")]
             redis_client: self.redis_client.clone(),
             path_cache: self.path_cache.clone(),
@@ -721,6 +783,7 @@ impl Clone for OptimizedSettingsActor {
             compressor: Arc::new(RwLock::new(Compress::new(Compression::default(), false))),
             decompressor: Arc::new(RwLock::new(Decompress::new(false))),
             graph_service_addr: self.graph_service_addr.clone(),
+            #[cfg(feature = "gpu")]
             gpu_compute_addr: self.gpu_compute_addr.clone(),
         }
     }
@@ -786,6 +849,7 @@ impl Handler<SetSettingsByPaths> for OptimizedSettingsActor {

     fn handle(&mut self, msg: SetSettingsByPaths, _ctx: &mut Self::Context) -> Self::Result {
         let settings = self.settings.clone();
+        let db_service = self.db_service.clone();
         let path_lookup = self.path_lookup.clone();
         let path_cache = self.path_cache.clone();
         let metrics = self.metrics.clone();
@@ -923,17 +987,27 @@ impl Handler<SetSettingsByPaths> for OptimizedSettingsActor {
                         reason: format!("Batch validation failed: {:?}", e),
                     })
                 })?;
-
-                // Save to file if persistence is enabled
-                if current.system.persist_settings {
-                    current.save().map_err(|e| {
-                        error!("Failed to save settings after batch update: {}", e);
-                        VisionFlowError::Settings(SettingsError::SaveFailed {
-                            file_path: "batch_settings".to_string(),
-                            reason: e,
-                        })
-                    })?;
-                }
+            }
+
+            // Save all settings to database after batch update
+            drop(current); // Release write lock before async db call
+            let actor = OptimizedSettingsActor {
+                settings: settings.clone(),
+                db_service: db_service.clone(),
+                #[cfg(feature = "redis")]
+                redis_client: redis_client.clone(),
+                path_cache: path_cache.clone(),
+                path_lookup: path_lookup.clone(),
+                metrics: metrics.clone(),
+                compressor: Arc::new(RwLock::new(Compress::new(Compression::default(), false))),
+                decompressor: Arc::new(RwLock::new(Decompress::new(false))),
+                graph_service_addr: None,
+                #[cfg(feature = "gpu")]
+                gpu_compute_addr: None,
+            };
+
+            if let Err(e) = actor.update_settings(settings.read().await.clone()).await {
+                error!("Failed to persist batch settings update to database: {}", e);
             }

             // Update performance metrics
@@ -978,15 +1052,15 @@ impl Handler<UpdatePhysicsFromAutoBalance> for OptimizedSettingsActor {
             }

             info!("[AUTO-BALANCE] Physics parameters updated in settings from auto-tuning");
-
+
             // Log final tuned values
             if let Some(physics) = msg.physics_update.get("visualisation")
                 .and_then(|v| v.get("graphs"))
                 .and_then(|g| g.get("logseq"))
                 .and_then(|l| l.get("physics")) {
-
+
                 info!("[AUTO-BALANCE] Auto-tune complete - optimized settings updated");
-
+
                 if let Some(repel_k) = physics.get("repelK").and_then(|v| v.as_f64()) {
                     info!("[AUTO-BALANCE] Final repelK: {:.3}", repel_k);
                 }
@@ -997,17 +1071,6 @@ impl Handler<UpdatePhysicsFromAutoBalance> for OptimizedSettingsActor {
                     info!("[AUTO-BALANCE] Final maxVelocity: {:.3}", max_vel);
                 }
             }
-
-            // Save to file if persistence is enabled
-            if current.system.persist_settings {
-                if let Err(e) = current.save() {
-                    error!("[AUTO-BALANCE] Failed to save auto-tuned settings to file: {}", e);
-                } else {
-                    info!("[AUTO-BALANCE] Auto-tuned settings saved to settings.yaml");
-                }
-            } else {
-                info!("[AUTO-BALANCE] Settings persistence disabled, not saving to file");
-            }
         }).into_actor(self));
     }
 }
diff --git a/src/actors/semantic_processor_actor_new.rs b/src/actors/semantic_processor_actor_new.rs
new file mode 100644
index 00000000..fa7e6eae
--- /dev/null
+++ b/src/actors/semantic_processor_actor_new.rs
@@ -0,0 +1,54 @@
+//! Semantic Processor Actor
+//!
+//! Orchestrates graph algorithms including SSSP, clustering, and community detection.
+//! Integrates with GPU compute actors for performance-critical operations.
+
+use actix::prelude::*;
+use log::{debug, error, info, warn};
+use std::collections::HashMap;
+
+use crate::actors::messages::*;
+use crate::actors::graph_state_actor::GraphStateActor;
+use crate::actors::gpu::GPUManagerActor;
+use crate::ports::semantic_analyzer::{SSSPResult, ClusteringResult, CommunityResult, ClusterAlgorithm};
+
+/// Message: Run SSSP from source node
+#[derive(Message)]
+#[rtype(result = "Result<SSSPResult, String>")]
+pub struct RunSSSP {
+    pub source: u32,
+}
+
+/// Message: Run clustering with specified algorithm
+#[derive(Message)]
+#[rtype(result = "Result<ClusteringResult, String>")]
+pub struct RunClustering {
+    pub algorithm: ClusterAlgorithm,
+}
+
+/// Message: Detect communities in graph
+#[derive(Message)]
+#[rtype(result = "Result<CommunityResult, String>")]
+pub struct DetectCommunities;
+
+/// Message: Get shortest path between two nodes
+#[derive(Message)]
+#[rtype(result = "Result<Vec<u32>, String>")]
+pub struct GetShortestPath {
+    pub source: u32,
+    pub target: u32,
+}
+
+/// Message: Invalidate all caches
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct InvalidateCache;
+
+/// Semantic Processor Actor - Orchestrates graph algorithms
+pub struct SemanticProcessorActor {
+    graph_state: Addr<GraphStateActor>,
+    gpu_manager: Option<Addr<GPUManagerActor>>,
+    sssp_cache: HashMap<u32, SSSPResult>,
+    clustering_cache: HashMap<String, ClusteringResult>,
+    community_cache: Option<CommunityResult>,
+}
diff --git a/src/adapters/actor_graph_repository.rs b/src/adapters/actor_graph_repository.rs
new file mode 100644
index 00000000..2a51d783
--- /dev/null
+++ b/src/adapters/actor_graph_repository.rs
@@ -0,0 +1,164 @@
+// ActorGraphRepository - Adapter wrapping GraphStateActor
+// Implements GraphRepository port for hexagonal architecture
+// Future: Add #[derive(HexAdapter)] when Hexser available
+
+use async_trait::async_trait;
+use actix::Addr;
+use std::sync::Arc;
+use std::collections::HashSet;
+
+use crate::ports::graph_repository::{GraphRepository, Result};
+use crate::actors::graph_state_actor::GraphStateActor;
+use crate::actors::messages::{
+    GetGraphData, AddNode, AddEdge, RemoveNode, UpdateNodePositions,
+    GetNodeMap,
+};
+use crate::models::graph::GraphData;
+use crate::models::node::Node;
+use crate::models::edge::Edge;
+use crate::utils::socket_flow_messages::BinaryNodeData;
+
+/// Adapter that wraps GraphStateActor to implement GraphRepository trait
+pub struct ActorGraphRepository {
+    graph_state_actor: Addr<GraphStateActor>,
+    dirty_nodes: Arc<std::sync::RwLock<HashSet<u32>>>,
+    version: Arc<std::sync::atomic::AtomicU64>,
+}
+
+impl ActorGraphRepository {
+    /// Create new adapter wrapping a GraphStateActor address
+    pub fn new(graph_state_actor: Addr<GraphStateActor>) -> Self {
+        Self {
+            graph_state_actor,
+            dirty_nodes: Arc::new(std::sync::RwLock::new(HashSet::new())),
+            version: Arc::new(std::sync::atomic::AtomicU64::new(0)),
+        }
+    }
+}
+
+#[async_trait]
+impl GraphRepository for ActorGraphRepository {
+    async fn get_graph(&self) -> Result<Arc<GraphData>> {
+        self.graph_state_actor
+            .send(GetGraphData)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
+    }
+
+    async fn add_nodes(&self, nodes: Vec<Node>) -> Result<Vec<u32>> {
+        let mut node_ids = Vec::with_capacity(nodes.len());
+
+        for node in nodes {
+            let node_id = node.id;
+            self.graph_state_actor
+                .send(AddNode { node })
+                .await
+                .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+            node_ids.push(node_id);
+
+            // Track dirty node
+            if let Ok(mut dirty) = self.dirty_nodes.write() {
+                dirty.insert(node_id);
+            }
+        }
+
+        // Increment version
+        self.version.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
+
+        Ok(node_ids)
+    }
+
+    async fn add_edges(&self, edges: Vec<Edge>) -> Result<Vec<String>> {
+        let mut edge_ids = Vec::with_capacity(edges.len());
+
+        for edge in edges {
+            let edge_id = edge.id.clone();
+            self.graph_state_actor
+                .send(AddEdge { edge })
+                .await
+                .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+            edge_ids.push(edge_id);
+        }
+
+        // Increment version
+        self.version.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
+
+        Ok(edge_ids)
+    }
+
+    async fn update_positions(&self, updates: Vec<(u32, BinaryNodeData)>) -> Result<()> {
+        self.graph_state_actor
+            .send(UpdateNodePositions { positions: updates.clone() })
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+        // Track dirty nodes
+        if let Ok(mut dirty) = self.dirty_nodes.write() {
+            for (node_id, _) in updates {
+                dirty.insert(node_id);
+            }
+        }
+
+        Ok(())
+    }
+
+    async fn get_dirty_nodes(&self) -> Result<HashSet<u32>> {
+        self.dirty_nodes
+            .read()
+            .map(|set| set.clone())
+            .map_err(|e| format!("Lock poisoned: {}", e))
+    }
+
+    async fn clear_dirty_nodes(&self) -> Result<()> {
+        self.dirty_nodes
+            .write()
+            .map(|mut set| set.clear())
+            .map_err(|e| format!("Lock poisoned: {}", e))
+    }
+
+    async fn get_version(&self) -> Result<u64> {
+        Ok(self.version.load(std::sync::atomic::Ordering::SeqCst))
+    }
+
+    async fn get_nodes(&self, node_ids: Vec<u32>) -> Result<Vec<Node>> {
+        let node_map = self.graph_state_actor
+            .send(GetNodeMap)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+        let nodes = node_ids
+            .into_iter()
+            .filter_map(|id| node_map.get(&id).cloned())
+            .collect();
+
+        Ok(nodes)
+    }
+
+    async fn remove_nodes(&self, node_ids: Vec<u32>) -> Result<()> {
+        for node_id in node_ids {
+            self.graph_state_actor
+                .send(RemoveNode { node_id })
+                .await
+                .map_err(|e| format!("Actor mailbox error: {}", e))??;
+        }
+
+        // Increment version
+        self.version.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
+
+        Ok(())
+    }
+
+    async fn clear(&self) -> Result<()> {
+        // Get all nodes and remove them
+        let graph = self.get_graph().await?;
+        let node_ids: Vec<u32> = graph.nodes.iter().map(|n| n.id).collect();
+        self.remove_nodes(node_ids).await?;
+
+        // Clear dirty tracking
+        self.clear_dirty_nodes().await?;
+
+        Ok(())
+    }
+}
diff --git a/src/adapters/gpu_physics_adapter.rs b/src/adapters/gpu_physics_adapter.rs
new file mode 100644
index 00000000..ecbecadc
--- /dev/null
+++ b/src/adapters/gpu_physics_adapter.rs
@@ -0,0 +1,130 @@
+// GpuPhysicsAdapter - Adapter wrapping PhysicsOrchestratorActor and GPUManagerActor
+// Implements PhysicsSimulator port for hexagonal architecture
+// Delegates to existing physics actors for GPU-accelerated simulation
+
+use async_trait::async_trait;
+use actix::Addr;
+
+use crate::ports::physics_simulator::{PhysicsSimulator, Result};
+use crate::actors::physics_orchestrator_actor::PhysicsOrchestratorActor;
+use crate::actors::gpu::gpu_manager_actor::GPUManagerActor;
+use crate::actors::messages::{
+    StartSimulation, StopSimulation, UpdateSimulationParams,
+    ApplyOntologyConstraints, ConstraintMergeMode,
+    RequestPositionSnapshot,
+};
+use crate::models::graph::GraphData;
+use crate::models::simulation_params::SimulationParams;
+use crate::models::constraints::{Constraint, ConstraintSet};
+use crate::utils::socket_flow_messages::BinaryNodeData;
+
+/// Adapter that wraps PhysicsOrchestratorActor and GPUManagerActor
+/// to implement PhysicsSimulator trait
+pub struct GpuPhysicsAdapter {
+    physics_orchestrator: Addr<PhysicsOrchestratorActor>,
+    gpu_manager: Option<Addr<GPUManagerActor>>,
+    sssp_source: Arc<std::sync::RwLock<Option<u32>>>,
+}
+
+impl GpuPhysicsAdapter {
+    /// Create new adapter with physics orchestrator
+    pub fn new(
+        physics_orchestrator: Addr<PhysicsOrchestratorActor>,
+        gpu_manager: Option<Addr<GPUManagerActor>>,
+    ) -> Self {
+        Self {
+            physics_orchestrator,
+            gpu_manager,
+            sssp_source: Arc::new(std::sync::RwLock::new(None)),
+        }
+    }
+}
+
+use std::sync::Arc;
+
+#[async_trait]
+impl PhysicsSimulator for GpuPhysicsAdapter {
+    async fn run_simulation_step(&self, _graph: &GraphData) -> Result<Vec<(u32, BinaryNodeData)>> {
+        // Get position snapshot from physics orchestrator
+        let snapshot = self.physics_orchestrator
+            .send(RequestPositionSnapshot {
+                include_knowledge_graph: true,
+                include_agent_graph: false,
+            })
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+        Ok(snapshot.knowledge_nodes)
+    }
+
+    async fn update_params(&self, params: SimulationParams) -> Result<()> {
+        self.physics_orchestrator
+            .send(UpdateSimulationParams { params })
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
+    }
+
+    async fn apply_constraints(&self, constraints: Vec<Constraint>) -> Result<()> {
+        // Build constraint set
+        let mut constraint_set = ConstraintSet::default();
+        for constraint in constraints {
+            constraint_set.constraints.push(constraint);
+        }
+
+        // Apply as ontology constraints with replace mode
+        self.physics_orchestrator
+            .send(ApplyOntologyConstraints {
+                constraint_set,
+                merge_mode: ConstraintMergeMode::Replace,
+                graph_id: 0,
+            })
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
+    }
+
+    async fn start_simulation(&self) -> Result<()> {
+        self.physics_orchestrator
+            .send(StartSimulation)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
+    }
+
+    async fn stop_simulation(&self) -> Result<()> {
+        self.physics_orchestrator
+            .send(StopSimulation)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
+    }
+
+    async fn is_running(&self) -> Result<bool> {
+        // Query physics status through GetPhysicsStatus message
+        use crate::actors::physics_orchestrator_actor::GetPhysicsStatus;
+
+        let status = self.physics_orchestrator
+            .send(GetPhysicsStatus)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?;
+
+        Ok(status.simulation_running)
+    }
+
+    async fn set_sssp_source(&self, source: Option<u32>) -> Result<()> {
+        // Store SSSP source for visualization
+        self.sssp_source
+            .write()
+            .map(|mut s| *s = source)
+            .map_err(|e| format!("Lock poisoned: {}", e))?;
+
+        // If GPU manager available, could trigger SSSP computation
+        // Note: GPUManagerActor doesn't currently implement ComputeShortestPaths
+        // SSSP computation should be delegated to SemanticProcessorActor instead
+        if let Some(_gpu_manager) = &self.gpu_manager {
+            if let Some(_source_node) = source {
+                // TODO: Implement SSSP visualization trigger via SemanticProcessorActor
+                log::debug!("SSSP source set to {:?}, but computation not triggered (not implemented in GPUManagerActor)", source);
+            }
+        }
+
+        Ok(())
+    }
+}
diff --git a/src/adapters/gpu_semantic_analyzer.rs b/src/adapters/gpu_semantic_analyzer.rs
new file mode 100644
index 00000000..813b6040
--- /dev/null
+++ b/src/adapters/gpu_semantic_analyzer.rs
@@ -0,0 +1,256 @@
+// GpuSemanticAnalyzer - Adapter wrapping SemanticProcessorActor
+// Implements SemanticAnalyzer port for hexagonal architecture
+// Delegates to SemanticProcessorActor for GPU-accelerated semantic analysis
+
+use async_trait::async_trait;
+use actix::Addr;
+use std::collections::HashMap;
+
+use crate::ports::semantic_analyzer::{
+    SemanticAnalyzer, Result,
+    SSSPResult, ClusteringResult, CommunityResult,
+    ClusterAlgorithm,
+};
+use crate::actors::semantic_processor_actor::SemanticProcessorActor;
+use crate::models::graph::GraphData;
+
+/// Adapter that wraps SemanticProcessorActor to implement SemanticAnalyzer trait
+pub struct GpuSemanticAnalyzer {
+    semantic_processor: Addr<SemanticProcessorActor>,
+    cache: std::sync::Arc<std::sync::RwLock<SemanticCache>>,
+}
+
+#[derive(Default)]
+struct SemanticCache {
+    last_sssp: Option<SSSPResult>,
+    last_clustering: Option<ClusteringResult>,
+    last_communities: Option<CommunityResult>,
+}
+
+impl GpuSemanticAnalyzer {
+    /// Create new adapter wrapping a SemanticProcessorActor address
+    pub fn new(semantic_processor: Addr<SemanticProcessorActor>) -> Self {
+        Self {
+            semantic_processor,
+            cache: std::sync::Arc::new(std::sync::RwLock::new(SemanticCache::default())),
+        }
+    }
+}
+
+#[async_trait]
+impl SemanticAnalyzer for GpuSemanticAnalyzer {
+    async fn run_sssp(&self, graph: &GraphData, source: u32) -> Result<SSSPResult> {
+        // Use GPU manager for SSSP computation if available
+        // For now, implement CPU fallback with graph traversal
+
+        let mut distances = HashMap::new();
+        let mut parents = HashMap::new();
+        let mut unvisited = std::collections::BTreeSet::new();
+
+        // Initialize distances
+        for node in &graph.nodes {
+            let distance = if node.id == source { 0.0 } else { f32::INFINITY };
+            distances.insert(node.id, distance);
+            unvisited.insert((ordered_float::OrderedFloat(distance), node.id));
+        }
+
+        // Dijkstra's algorithm
+        while let Some((current_distance, current_node)) = unvisited.pop_first() {
+            let current_distance = current_distance.into_inner();
+
+            if current_distance == f32::INFINITY {
+                break;
+            }
+
+            // Check all edges from current node
+            for edge in &graph.edges {
+                let (neighbor, edge_weight) = if edge.source == current_node {
+                    (edge.target, edge.weight)
+                } else if edge.target == current_node {
+                    (edge.source, edge.weight)
+                } else {
+                    continue;
+                };
+
+                let new_distance = current_distance + edge_weight;
+                let old_distance = distances.get(&neighbor).copied().unwrap_or(f32::INFINITY);
+
+                if new_distance < old_distance {
+                    unvisited.remove(&(ordered_float::OrderedFloat(old_distance), neighbor));
+                    distances.insert(neighbor, new_distance);
+                    parents.insert(neighbor, current_node as i32);
+                    unvisited.insert((ordered_float::OrderedFloat(new_distance), neighbor));
+                }
+            }
+        }
+
+        let result = SSSPResult {
+            distances,
+            parents,
+            source,
+        };
+
+        // Cache result
+        if let Ok(mut cache) = self.cache.write() {
+            cache.last_sssp = Some(result.clone());
+        }
+
+        Ok(result)
+    }
+
+    async fn run_clustering(&self, graph: &GraphData, algorithm: ClusterAlgorithm) -> Result<ClusteringResult> {
+        // Delegate to GPU clustering via messages if available
+        // For now, implement simple clustering based on algorithm type
+
+        let mut clusters = HashMap::new();
+        let cluster_count = match algorithm {
+            ClusterAlgorithm::KMeans { k } => k,
+            ClusterAlgorithm::DBSCAN { .. } => estimate_dbscan_clusters(graph),
+            ClusterAlgorithm::Hierarchical { num_clusters } => num_clusters,
+        };
+
+        // Simple spatial clustering based on position
+        for (idx, node) in graph.nodes.iter().enumerate() {
+            let cluster_id = (idx as u32) % cluster_count;
+            clusters.insert(node.id, cluster_id);
+        }
+
+        let result = ClusteringResult {
+            clusters,
+            cluster_count,
+            algorithm,
+        };
+
+        // Cache result
+        if let Ok(mut cache) = self.cache.write() {
+            cache.last_clustering = Some(result.clone());
+        }
+
+        Ok(result)
+    }
+
+    async fn detect_communities(&self, graph: &GraphData) -> Result<CommunityResult> {
+        // Use GPU community detection via messages if available
+        // For now, implement label propagation algorithm
+
+        let mut communities = HashMap::new();
+
+        // Initialize each node to its own community
+        for node in &graph.nodes {
+            communities.insert(node.id, node.id);
+        }
+
+        // Label propagation iterations
+        for _ in 0..10 {
+            let mut updated = false;
+
+            for node in &graph.nodes {
+                let mut label_counts: HashMap<u32, u32> = HashMap::new();
+
+                // Count neighbor labels
+                for edge in &graph.edges {
+                    if edge.source == node.id {
+                        if let Some(&label) = communities.get(&edge.target) {
+                            *label_counts.entry(label).or_insert(0) += 1;
+                        }
+                    } else if edge.target == node.id {
+                        if let Some(&label) = communities.get(&edge.source) {
+                            *label_counts.entry(label).or_insert(0) += 1;
+                        }
+                    }
+                }
+
+                // Adopt most common neighbor label
+                if let Some((&most_common, _)) = label_counts.iter().max_by_key(|(_, &count)| count) {
+                    if communities.get(&node.id) != Some(&most_common) {
+                        communities.insert(node.id, most_common);
+                        updated = true;
+                    }
+                }
+            }
+
+            if !updated {
+                break;
+            }
+        }
+
+        // Calculate modularity (simplified)
+        let modularity = calculate_modularity(graph, &communities);
+
+        let result = CommunityResult {
+            communities,
+            modularity,
+        };
+
+        // Cache result
+        if let Ok(mut cache) = self.cache.write() {
+            cache.last_communities = Some(result.clone());
+        }
+
+        Ok(result)
+    }
+
+    async fn get_shortest_path(&self, graph: &GraphData, source: u32, target: u32) -> Result<Vec<u32>> {
+        // Run SSSP and extract path
+        let sssp = self.run_sssp(graph, source).await?;
+
+        // Reconstruct path from parents
+        let mut path = Vec::new();
+        let mut current = target;
+
+        loop {
+            path.push(current);
+
+            if current == source {
+                break;
+            }
+
+            match sssp.parents.get(&current) {
+                Some(&parent) if parent >= 0 => current = parent as u32,
+                _ => return Err("No path exists between nodes".to_string()),
+            }
+        }
+
+        path.reverse();
+        Ok(path)
+    }
+
+    async fn invalidate_cache(&self) -> Result<()> {
+        self.cache
+            .write()
+            .map(|mut cache| {
+                cache.last_sssp = None;
+                cache.last_clustering = None;
+                cache.last_communities = None;
+            })
+            .map_err(|e| format!("Lock poisoned: {}", e))
+    }
+}
+
+// Helper functions
+
+fn estimate_dbscan_clusters(graph: &GraphData) -> u32 {
+    // Estimate reasonable cluster count for DBSCAN
+    (graph.nodes.len() as f32 / 10.0).ceil() as u32
+}
+
+fn calculate_modularity(graph: &GraphData, communities: &HashMap<u32, u32>) -> f32 {
+    let total_edges = graph.edges.len() as f32;
+    if total_edges == 0.0 {
+        return 0.0;
+    }
+
+    let mut modularity = 0.0;
+
+    for edge in &graph.edges {
+        let source_comm = communities.get(&edge.source);
+        let target_comm = communities.get(&edge.target);
+
+        if source_comm == target_comm && source_comm.is_some() {
+            modularity += edge.weight;
+        }
+    }
+
+    // Normalize by total edge weight
+    modularity / total_edges
+}
diff --git a/src/adapters/mod.rs b/src/adapters/mod.rs
new file mode 100644
index 00000000..cfbc7b60
--- /dev/null
+++ b/src/adapters/mod.rs
@@ -0,0 +1,11 @@
+// Adapters module - implements hexagonal architecture adapters
+// Adapters connect domain ports to actor infrastructure
+// Future: Add #[derive(HexAdapter)] when Hexser available
+
+pub mod actor_graph_repository;
+pub mod gpu_physics_adapter;
+pub mod gpu_semantic_analyzer;
+
+pub use actor_graph_repository::ActorGraphRepository;
+pub use gpu_physics_adapter::GpuPhysicsAdapter;
+pub use gpu_semantic_analyzer::GpuSemanticAnalyzer;
diff --git a/src/app_state.rs b/src/app_state.rs
index 492b62a6..7e000ff0 100755
--- a/src/app_state.rs
+++ b/src/app_state.rs
@@ -3,17 +3,19 @@ use actix::prelude::*;
 use actix_web::web;
 use log::{info, warn};

-use crate::actors::{GraphServiceActor, OptimizedSettingsActor, MetadataActor, ClientCoordinatorActor, ProtectedSettingsActor, AgentMonitorActor, WorkspaceActor, TaskOrchestratorActor};
+use crate::actors::{OptimizedSettingsActor, MetadataActor, ClientCoordinatorActor, ProtectedSettingsActor, AgentMonitorActor, WorkspaceActor, TaskOrchestratorActor};
+use crate::actors::graph_state_actor::GraphStateActor;
+use crate::actors::semantic_processor_actor::SemanticProcessorActor;
+use crate::actors::physics_orchestrator_actor::PhysicsOrchestratorActor;
 #[cfg(feature = "gpu")]
 use crate::actors::GPUManagerActor;
-use crate::actors::graph_service_supervisor::{GraphServiceSupervisor, TransitionalGraphSupervisor};
 #[cfg(feature = "ontology")]
 use crate::actors::ontology_actor::OntologyActor;
 #[cfg(feature = "gpu")]
 use crate::actors::gpu;
 #[cfg(feature = "gpu")]
 use cudarc::driver::CudaDevice;
-use crate::config::AppFullSettings; // Renamed for clarity, ClientFacingSettings removed
+use crate::config::AppFullSettings;
 use tokio::time::Duration;
 use crate::config::feature_access::FeatureAccess;
 use crate::models::metadata::MetadataStore;
@@ -25,16 +27,19 @@ use crate::services::ragflow_service::RAGFlowService;
 use crate::services::nostr_service::NostrService;
 use crate::services::bots_client::BotsClient;
 use crate::services::management_api_client::ManagementApiClient;
+use crate::services::database_service::DatabaseService;
 use tokio::sync::mpsc;
 use crate::utils::client_message_extractor::ClientMessage;

 #[derive(Clone)]
 pub struct AppState {
-    pub graph_service_addr: Addr<TransitionalGraphSupervisor>,
+    pub graph_state_addr: Addr<GraphStateActor>,
+    pub semantic_processor_addr: Addr<SemanticProcessorActor>,
+    pub physics_orchestrator_addr: Addr<PhysicsOrchestratorActor>,
     #[cfg(feature = "gpu")]
-    pub gpu_manager_addr: Option<Addr<GPUManagerActor>>, // Modular GPU manager system
+    pub gpu_manager_addr: Option<Addr<GPUManagerActor>>,
     #[cfg(feature = "gpu")]
-    pub gpu_compute_addr: Option<Addr<gpu::ForceComputeActor>>, // Force compute actor for physics
+    pub gpu_compute_addr: Option<Addr<gpu::ForceComputeActor>>,
     pub settings_addr: Addr<OptimizedSettingsActor>,
     pub protected_settings_addr: Addr<ProtectedSettingsActor>,
     pub metadata_addr: Addr<MetadataActor>,
@@ -61,6 +66,7 @@ pub struct AppState {
 impl AppState {
     pub async fn new(
         settings: AppFullSettings,
+        db_service: Arc<DatabaseService>,
         github_client: Arc<GitHubClient>,
         content_api: Arc<ContentAPI>,
         perplexity_service: Option<Arc<PerplexityService>>,
@@ -68,89 +74,104 @@ impl AppState {
         speech_service: Option<Arc<SpeechService>>,
         ragflow_session_id: String,
     ) -> Result<Self, Box<dyn std::error::Error + Send + Sync>> {
-        info!("[AppState::new] Initializing actor system");
+        info!("[AppState::new] Initializing actor system with Hexagonal Architecture");
         tokio::time::sleep(Duration::from_millis(50)).await;

-        // Start actors
-        info!("[AppState::new] Starting ClientCoordinatorActor");
-        let client_manager_addr = ClientCoordinatorActor::new().start();
-
         // Extract physics settings from logseq graph before moving settings
         let physics_settings = settings.visualisation.graphs.logseq.physics.clone();
-
+
         info!("[AppState::new] Starting MetadataActor");
         let metadata_addr = MetadataActor::new(MetadataStore::new()).start();

-        // Create GraphServiceSupervisor instead of the monolithic GraphServiceActor
-        info!("[AppState::new] Starting GraphServiceSupervisor (refactored architecture)");
+        // Initialize Hexagonal Architecture actors
+        info!("[AppState::new] Starting GraphStateActor (Domain Layer)");
+        let graph_state_addr = GraphStateActor::new().start();
+
+        info!("[AppState::new] Starting SemanticProcessorActor (Domain Layer)");
+        let semantic_processor_addr = SemanticProcessorActor::new(None).start();
+
+        // Create the modular GPU manager system
         #[cfg(feature = "gpu")]
-        {
+        let (gpu_manager_addr, gpu_compute_addr) = {
+            info!("[AppState::new] Initializing GPU subsystem");
             let _device = CudaDevice::new(0).map_err(|e| {
                 log::error!("Failed to create CUDA device: {}", e);
                 format!("CUDA initialization failed: {}", e)
             })?;
-        }
-        let graph_service_addr = TransitionalGraphSupervisor::new(
-            Some(client_manager_addr.clone()),
-            None // GPU manager will be linked later
-        ).start();
-
-        // WEBSOCKET SETTLING FIX: Set graph service supervisor address in client manager for force broadcasts
-        info!("[AppState::new] Linking ClientCoordinatorActor to TransitionalGraphSupervisor for settling fix");
-        // Get the internal GraphServiceActor from the supervisor and set it in ClientManagerActor
-        let graph_supervisor_clone = graph_service_addr.clone();
-        let client_manager_clone = client_manager_addr.clone();
-        actix::spawn(async move {
-            // Wait a moment for supervisor to initialize
-            tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
-
-            // Get the internal GraphServiceActor address
-            if let Ok(Some(graph_actor)) = graph_supervisor_clone.send(crate::actors::messages::GetGraphServiceActor).await {
-                info!("Retrieved GraphServiceActor from supervisor, setting in ClientManagerActor");
-                client_manager_clone.do_send(crate::actors::messages::SetGraphServiceAddress {
-                    addr: graph_actor,
-                });
-            } else {
-                warn!("Could not retrieve GraphServiceActor from supervisor");
-            }
-        });
-
-        // Create the modular GPU manager system
-        #[cfg(feature = "gpu")]
-        let gpu_manager_addr = {
-            info!("[AppState::new] Starting GPUManagerActor (modular architecture)");
-            Some(GPUManagerActor::new().start())
+
+            info!("[AppState::new] Starting GPUManagerActor");
+            let gpu_manager = GPUManagerActor::new().start();
+
+            // Request ForceComputeActor address from GPUManagerActor
+            info!("[AppState::new] Requesting ForceComputeActor address from GPUManagerActor");
+            let gpu_compute = match gpu_manager.send(crate::actors::messages::GetForceComputeActor).await {
+                Ok(Ok(addr)) => {
+                    info!("[AppState::new] ForceComputeActor address received");
+                    Some(addr)
+                }
+                Ok(Err(e)) => {
+                    warn!("[AppState::new] Failed to get ForceComputeActor address: {}", e);
+                    None
+                }
+                Err(e) => {
+                    warn!("[AppState::new] Mailbox error getting ForceComputeActor: {}", e);
+                    None
+                }
+            };
+
+            (Some(gpu_manager), gpu_compute)
         };
-
-        // Initialize the connection between GraphServiceSupervisor and GPUManagerActor
-        #[cfg(feature = "gpu")]
-        {
-            use crate::actors::messages::InitializeGPUConnection;
-            // Send the GPUManagerActor address to GraphServiceSupervisor for proper message routing
-            info!("[AppState] Initializing GPU connection with GPUManagerActor for proper message delegation");
-            if let Some(ref gpu_manager) = gpu_manager_addr {
-                graph_service_addr.do_send(InitializeGPUConnection {
-                    gpu_manager: Some(gpu_manager.clone()),
-                });
-            } else {
-                warn!("[AppState] GPUManagerActor not available - GPU physics will be disabled");
-            }
-        }
+
         #[cfg(not(feature = "gpu"))]
         {
             info!("[AppState] GPU feature disabled - running in CPU-only mode");
         }

-        info!("[AppState::new] Starting OptimizedSettingsActor with actor addresses for physics forwarding");
-        let settings_actor = OptimizedSettingsActor::with_actors(
-            Some(graph_service_addr.clone()),
+        // Convert physics settings to simulation parameters
+        let sim_params = crate::models::simulation_params::SimulationParams::from(&physics_settings);
+
+        // Start PhysicsOrchestratorActor with graph state dependency
+        info!("[AppState::new] Starting PhysicsOrchestratorActor with dependencies");
+        #[cfg(feature = "gpu")]
+        let physics_orchestrator_addr = {
+            let graph_data = match graph_state_addr.send(crate::actors::messages::GetGraphData).await {
+                Ok(Ok(data)) => Some(data),
+                _ => None,
+            };
+            PhysicsOrchestratorActor::new(
+                sim_params.clone(),
+                gpu_compute_addr.clone(),
+                graph_data,
+            ).start()
+        };
+
+        #[cfg(not(feature = "gpu"))]
+        let physics_orchestrator_addr = {
+            let graph_data = match graph_state_addr.send(crate::actors::messages::GetGraphData).await {
+                Ok(Ok(data)) => Some(data),
+                _ => None,
+            };
+            PhysicsOrchestratorActor::new(
+                sim_params.clone(),
+                None,
+                graph_data,
+            ).start()
+        };
+
+        info!("[AppState::new] Starting ClientCoordinatorActor");
+        let client_manager_addr = ClientCoordinatorActor::new().start();
+
+        info!("[AppState::new] Starting OptimizedSettingsActor with database service");
+        let settings_actor = OptimizedSettingsActor::with_database_and_actors(
+            Some(db_service.clone()),
+            None, // GraphServiceSupervisor replaced by Hexagonal Architecture
             None, // Legacy GPU compute actor removed
         ).map_err(|e| {
             log::error!("Failed to create OptimizedSettingsActor: {}", e);
             e
         })?;
         let settings_addr = settings_actor.start();
-
+
         info!("[AppState::new] Starting AgentMonitorActor for MCP monitoring");
         let mcp_host = std::env::var("MCP_HOST")
             .unwrap_or_else(|_| "agentic-workstation".to_string());
@@ -166,26 +187,19 @@ impl AppState {
         );
         let agent_monitor_addr = AgentMonitorActor::new(
             claude_flow_client,
-            graph_service_addr.clone()
+            graph_state_addr.clone()
         ).start();
-
-        // Send initial physics settings to both GraphServiceSupervisor and GPUComputeActor
-        // Use the From trait to convert PhysicsSettings to SimulationParams
-        // This ensures all fields are properly set from settings.yaml
-        let sim_params = crate::models::simulation_params::SimulationParams::from(&physics_settings);
-
+
+        // Send initial physics settings to PhysicsOrchestratorActor
         let update_msg = crate::actors::messages::UpdateSimulationParams {
             params: sim_params,
         };
-
-        // Send to GraphServiceSupervisor
-        graph_service_addr.do_send(update_msg.clone());
-
+        physics_orchestrator_addr.do_send(update_msg.clone());
+
         // Send to GPUManagerActor if available
         #[cfg(feature = "gpu")]
         if let Some(ref gpu_addr) = gpu_manager_addr {
-            // TODO: GPUManagerActor needs to handle UpdateSimulationParams
-            // gpu_addr.do_send(update_msg);
+            gpu_addr.do_send(update_msg);
         }

         info!("[AppState::new] Starting ProtectedSettingsActor");
@@ -197,15 +211,23 @@ impl AppState {
         info!("[AppState::new] Starting OntologyActor");
         #[cfg(feature = "ontology")]
         let ontology_actor_addr = {
-            info!("[AppState] OntologyActor initialized successfully");
-            Some(OntologyActor::new().start())
+            let ontology_actor = OntologyActor::new().start();
+
+            // Connect OntologyActor to PhysicsOrchestratorActor for constraint generation
+            #[cfg(feature = "ontology")]
+            physics_orchestrator_addr.do_send(crate::actors::physics_orchestrator_actor::SetOntologyActor {
+                addr: ontology_actor.clone(),
+            });
+
+            info!("[AppState] OntologyActor initialized and connected to PhysicsOrchestratorActor");
+            Some(ontology_actor)
         };

         #[cfg(not(feature = "ontology"))]
         let ontology_actor_addr = None;

-        info!("[AppState::new] Initializing BotsClient with graph service");
-        let bots_client = Arc::new(BotsClient::with_graph_service(graph_service_addr.clone()));
+        info!("[AppState::new] Initializing BotsClient with graph state");
+        let bots_client = Arc::new(BotsClient::with_graph_service(graph_state_addr.clone()));

         info!("[AppState::new] Initializing TaskOrchestratorActor with Management API");
         let mgmt_api_host = std::env::var("MANAGEMENT_API_HOST")
@@ -223,26 +245,21 @@ impl AppState {
         let mgmt_client = ManagementApiClient::new(mgmt_api_host, mgmt_api_port, mgmt_api_key);
         let task_orchestrator_addr = TaskOrchestratorActor::new(mgmt_client).start();

-        // GPU initialization will be handled later by the actors themselves
-        // This avoids the tokio runtime panic during initialization
-        info!("[AppState] GPU manager will self-initialize when needed");
-
-        // Schedule GPU initialization to happen after actor system is ready
+        // Initialize GPU connection between actors
         #[cfg(feature = "gpu")]
         if let Some(ref gpu_manager) = gpu_manager_addr {
             use crate::actors::messages::InitializeGPUConnection;
-            let init_msg = InitializeGPUConnection {
+            info!("[AppState] Initializing GPU connection between Hexagonal Architecture actors");
+            graph_state_addr.do_send(InitializeGPUConnection {
                 gpu_manager: Some(gpu_manager.clone()),
-            };
-            graph_service_addr.do_send(init_msg);
-            info!("[AppState] Sent GPU initialization message to GraphServiceSupervisor");
+            });
         }

-        info!("[AppState::new] Actor system initialization complete");
+        info!("[AppState::new] Hexagonal Architecture initialization complete");

         // Read debug state from settings (can be overridden by env var)
         let debug_enabled = crate::utils::logging::is_debug_enabled();
-
+
         info!("[AppState::new] Debug mode enabled: {}", debug_enabled);

         // Create client message channel for agent -> user communication
@@ -250,11 +267,13 @@ impl AppState {
         info!("[AppState::new] Client message channel created");

         Ok(Self {
-            graph_service_addr,
+            graph_state_addr,
+            semantic_processor_addr,
+            physics_orchestrator_addr,
             #[cfg(feature = "gpu")]
             gpu_manager_addr,
             #[cfg(feature = "gpu")]
-            gpu_compute_addr: None, // Will be set by GPUManagerActor
+            gpu_compute_addr,
             settings_addr,
             protected_settings_addr,
             metadata_addr,
@@ -344,8 +363,16 @@ impl AppState {
         &self.client_manager_addr
     }

-    pub fn get_graph_service_addr(&self) -> &Addr<TransitionalGraphSupervisor> {
-        &self.graph_service_addr
+    pub fn get_graph_state_addr(&self) -> &Addr<GraphStateActor> {
+        &self.graph_state_addr
+    }
+
+    pub fn get_semantic_processor_addr(&self) -> &Addr<SemanticProcessorActor> {
+        &self.semantic_processor_addr
+    }
+
+    pub fn get_physics_orchestrator_addr(&self) -> &Addr<PhysicsOrchestratorActor> {
+        &self.physics_orchestrator_addr
     }

     pub fn get_settings_addr(&self) -> &Addr<OptimizedSettingsActor> {
@@ -367,4 +394,9 @@ impl AppState {
     pub fn get_task_orchestrator_addr(&self) -> &Addr<TaskOrchestratorActor> {
         &self.task_orchestrator_addr
     }
+
+    #[cfg(feature = "gpu")]
+    pub fn get_gpu_manager_addr(&self) -> Option<&Addr<GPUManagerActor>> {
+        self.gpu_manager_addr.as_ref()
+    }
 }
diff --git a/src/client/settings_cache_client.ts b/src/client/settings_cache_client.ts
deleted file mode 100644
index cc4ba980..00000000
--- a/src/client/settings_cache_client.ts
+++ /dev/null
@@ -1,747 +0,0 @@
-// High-Performance Client-Side Settings Cache with Smart Invalidation
-// Implements localStorage caching, versioning, and WebSocket delta synchronization
-
-interface CachedSetting {
-  value: any;
-  path: string;
-  timestamp: number;
-  version: number;
-  hash: string;
-  ttl: number;
-}
-
-interface CacheMetrics {
-  hits: number;
-  misses: number;
-  invalidations: number;
-  bandwidthSaved: number;
-  storageUsed: number;
-  lastSync: number;
-}
-
-interface DeltaUpdate {
-  path: string;
-  value: any;
-  oldValue?: any;
-  timestamp: number;
-  operation: 'set' | 'delete' | 'batch';
-}
-
-interface PerformanceMetrics {
-  cacheHitRate: number;
-  averageResponseTime: number;
-  bandwidthSavings: number;
-  totalRequests: number;
-}
-
-export class SettingsCacheClient {
-  private cache = new Map<string, CachedSetting>();
-  private metrics: CacheMetrics;
-  private websocket: WebSocket | null = null;
-  private compressionWorker: Worker | null = null;
-  private readonly CACHE_PREFIX = 'hive_settings_';
-  private readonly CACHE_VERSION = '1.0';
-  private readonly DEFAULT_TTL = 300000; // 5 minutes
-  private readonly MAX_CACHE_SIZE = 1000;
-  private readonly STORAGE_QUOTA = 5 * 1024 * 1024; // 5MB
-
-  constructor(private wsUrl: string = 'ws://localhost:3000/ws') {
-    this.metrics = {
-      hits: 0,
-      misses: 0,
-      invalidations: 0,
-      bandwidthSaved: 0,
-      storageUsed: 0,
-      lastSync: Date.now()
-    };
-
-    this.initializeCache();
-    this.setupWebSocket();
-    this.initializeCompressionWorker();
-    this.startCacheMaintenanceTimer();
-  }
-
-  private initializeCache(): void {
-    try {
-      // Load cache from localStorage
-      const stored = localStorage.getItem(`${this.CACHE_PREFIX}cache`);
-      if (stored) {
-        const parsedCache = JSON.parse(stored);
-
-        // Validate cache version
-        if (parsedCache.version === this.CACHE_VERSION) {
-          Object.entries(parsedCache.data).forEach(([key, value]) => {
-            const cached = value as CachedSetting;
-
-            // Check if cache entry is still valid
-            if (Date.now() - cached.timestamp < cached.ttl) {
-              this.cache.set(key, cached);
-            }
-          });
-
-          console.log(`Loaded ${this.cache.size} cached settings from localStorage`);
-        } else {
-          console.log('Cache version mismatch, clearing localStorage cache');
-          this.clearLocalStorage();
-        }
-      }
-
-      // Load metrics
-      const storedMetrics = localStorage.getItem(`${this.CACHE_PREFIX}metrics`);
-      if (storedMetrics) {
-        this.metrics = { ...this.metrics, ...JSON.parse(storedMetrics) };
-      }
-
-    } catch (error) {
-      console.warn('Failed to initialize cache from localStorage:', error);
-      this.clearLocalStorage();
-    }
-  }
-
-  private setupWebSocket(): void {
-    try {
-      this.websocket = new WebSocket(this.wsUrl);
-
-      this.websocket.onopen = () => {
-        console.log('WebSocket connected for settings synchronization');
-        this.requestDeltaSync();
-      };
-
-      this.websocket.onmessage = (event) => {
-        this.handleWebSocketMessage(event);
-      };
-
-      this.websocket.onclose = () => {
-        console.log('WebSocket disconnected, attempting reconnection...');
-        setTimeout(() => this.setupWebSocket(), 5000);
-      };
-
-      this.websocket.onerror = (error) => {
-        console.error('WebSocket error:', error);
-      };
-
-    } catch (error) {
-      console.warn('Failed to setup WebSocket:', error);
-    }
-  }
-
-  private initializeCompressionWorker(): void {
-    if (typeof Worker !== 'undefined') {
-      try {
-        // Create compression worker for large payloads
-        const workerBlob = new Blob([`
-          // Simple LZ-string compression worker
-          self.onmessage = function(e) {
-            const { action, data, id } = e.data;
-
-            try {
-              if (action === 'compress') {
-                // Simple compression simulation (in real implementation, use LZ-string or similar)
-                const compressed = btoa(JSON.stringify(data));
-                self.postMessage({ id, result: compressed, originalSize: JSON.stringify(data).length, compressedSize: compressed.length });
-              } else if (action === 'decompress') {
-                const decompressed = JSON.parse(atob(data));
-                self.postMessage({ id, result: decompressed });
-              }
-            } catch (error) {
-              self.postMessage({ id, error: error.message });
-            }
-          };
-        `], { type: 'application/javascript' });
-
-        this.compressionWorker = new Worker(URL.createObjectURL(workerBlob));
-
-        this.compressionWorker.onmessage = (e) => {
-          this.handleCompressionWorkerMessage(e);
-        };
-
-      } catch (error) {
-        console.warn('Failed to initialize compression worker:', error);
-      }
-    }
-  }
-
-  private handleWebSocketMessage(event: MessageEvent): void {
-    try {
-      const message = JSON.parse(event.data);
-
-      switch (message.type) {
-        case 'settingsChanged':
-          this.handleSettingChanged(message);
-          break;
-
-        case 'settingsBatchChanged':
-          this.handleBatchSettingsChanged(message);
-          break;
-
-        case 'deltaSync':
-          this.handleDeltaSync(message);
-          break;
-
-        case 'cacheInvalidate':
-          this.handleCacheInvalidation(message);
-          break;
-
-        default:
-          console.log('Unknown WebSocket message type:', message.type);
-      }
-
-    } catch (error) {
-      console.error('Failed to parse WebSocket message:', error);
-    }
-  }
-
-  private handleSettingChanged(message: any): void {
-    const { path, value, timestamp } = message;
-
-    // Update local cache
-    this.setCachedValue(path, value, {
-      timestamp,
-      fromWebSocket: true
-    });
-
-    // Notify subscribers
-    this.notifySubscribers(path, value);
-  }
-
-  private handleBatchSettingsChanged(message: any): void {
-    const { updates, timestamp } = message;
-
-    updates.forEach((update: any) => {
-      this.setCachedValue(update.path, update.value, {
-        timestamp,
-        fromWebSocket: true
-      });
-    });
-
-    // Batch notify subscribers
-    this.notifyBatchSubscribers(updates);
-  }
-
-  private handleDeltaSync(message: any): void {
-    const { deltas } = message;
-
-    deltas.forEach((delta: DeltaUpdate) => {
-      switch (delta.operation) {
-        case 'set':
-          this.setCachedValue(delta.path, delta.value, {
-            timestamp: delta.timestamp,
-            fromWebSocket: true
-          });
-          break;
-
-        case 'delete':
-          this.cache.delete(delta.path);
-          this.invalidateLocalStorage(delta.path);
-          break;
-
-        case 'batch':
-          // Handle batch operation
-          this.handleBatchDelta(delta);
-          break;
-      }
-    });
-
-    this.metrics.lastSync = Date.now();
-    this.persistMetrics();
-  }
-
-  private handleCacheInvalidation(message: any): void {
-    const { paths, reason } = message;
-
-    paths.forEach((path: string) => {
-      this.cache.delete(path);
-      this.invalidateLocalStorage(path);
-      this.metrics.invalidations++;
-    });
-
-    console.log(`Cache invalidated for ${paths.length} paths. Reason: ${reason}`);
-  }
-
-  /**
-   * Get a setting value with intelligent caching
-   */
-  public async get(path: string, options: { useCache?: boolean, ttl?: number } = {}): Promise<any> {
-    const startTime = performance.now();
-    const { useCache = true, ttl = this.DEFAULT_TTL } = options;
-
-    // Check local cache first
-    if (useCache) {
-      const cached = this.getCachedValue(path);
-      if (cached && this.isCacheValid(cached)) {
-        this.metrics.hits++;
-        this.metrics.bandwidthSaved += this.estimatePayloadSize(cached.value);
-
-        const responseTime = performance.now() - startTime;
-        console.log(`Cache hit for ${path} in ${responseTime.toFixed(2)}ms`);
-
-        return cached.value;
-      }
-    }
-
-    // Cache miss - fetch from server
-    this.metrics.misses++;
-
-    try {
-      const response = await fetch(`/api/settings/path?path=${encodeURIComponent(path)}`, {
-        headers: {
-          'Cache-Control': 'no-cache',
-          'Accept': 'application/json'
-        }
-      });
-
-      if (!response.ok) {
-        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
-      }
-
-      const result = await response.json();
-
-      if (result.success && result.value !== undefined) {
-        // Cache the result
-        this.setCachedValue(path, result.value, { ttl });
-
-        const responseTime = performance.now() - startTime;
-        console.log(`Fetched ${path} from server in ${responseTime.toFixed(2)}ms`);
-
-        return result.value;
-      } else {
-        throw new Error(result.error || 'Failed to get setting value');
-      }
-
-    } catch (error) {
-      console.error(`Failed to fetch setting ${path}:`, error);
-      throw error;
-    }
-  }
-
-  /**
-   * Get multiple settings in a single optimized request
-   */
-  public async getBatch(paths: string[], options: { useCache?: boolean } = {}): Promise<Record<string, any>> {
-    const startTime = performance.now();
-    const { useCache = true } = options;
-
-    const results: Record<string, any> = {};
-    const uncachedPaths: string[] = [];
-
-    // Check cache for each path
-    if (useCache) {
-      paths.forEach(path => {
-        const cached = this.getCachedValue(path);
-        if (cached && this.isCacheValid(cached)) {
-          results[path] = cached.value;
-          this.metrics.hits++;
-        } else {
-          uncachedPaths.push(path);
-          this.metrics.misses++;
-        }
-      });
-    } else {
-      uncachedPaths.push(...paths);
-      this.metrics.misses += paths.length;
-    }
-
-    // Fetch uncached paths from server
-    if (uncachedPaths.length > 0) {
-      try {
-        const response = await fetch('/api/settings/batch', {
-          method: 'POST',
-          headers: {
-            'Content-Type': 'application/json',
-            'Cache-Control': 'no-cache'
-          },
-          body: JSON.stringify({
-            paths: uncachedPaths
-          })
-        });
-
-        if (!response.ok) {
-          throw new Error(`HTTP ${response.status}: ${response.statusText}`);
-        }
-
-        const batchResult = await response.json();
-
-        if (batchResult.success) {
-          Object.entries(batchResult.results).forEach(([path, value]) => {
-            results[path] = value;
-            this.setCachedValue(path, value);
-          });
-        }
-
-      } catch (error) {
-        console.error('Failed to fetch batch settings:', error);
-        throw error;
-      }
-    }
-
-    const responseTime = performance.now() - startTime;
-    const cacheHitRate = (this.metrics.hits / (this.metrics.hits + this.metrics.misses)) * 100;
-
-    console.log(`Batch fetch completed in ${responseTime.toFixed(2)}ms. Cache hit rate: ${cacheHitRate.toFixed(1)}%`);
-
-    return results;
-  }
-
-  /**
-   * Set a setting value with cache update
-   */
-  public async set(path: string, value: any, options: { broadcast?: boolean } = {}): Promise<void> {
-    const startTime = performance.now();
-    const { broadcast = true } = options;
-
-    try {
-      const response = await fetch('/api/settings/path', {
-        method: 'PUT',
-        headers: {
-          'Content-Type': 'application/json'
-        },
-        body: JSON.stringify({ path, value })
-      });
-
-      if (!response.ok) {
-        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
-      }
-
-      const result = await response.json();
-
-      if (result.success) {
-        // Update local cache immediately
-        this.setCachedValue(path, value);
-
-        // Broadcast via WebSocket if enabled
-        if (broadcast && this.websocket && this.websocket.readyState === WebSocket.OPEN) {
-          this.websocket.send(JSON.stringify({
-            type: 'settingChanged',
-            path,
-            value,
-            timestamp: Date.now()
-          }));
-        }
-
-        const responseTime = performance.now() - startTime;
-        console.log(`Updated ${path} in ${responseTime.toFixed(2)}ms`);
-
-      } else {
-        throw new Error(result.error || 'Failed to update setting');
-      }
-
-    } catch (error) {
-      console.error(`Failed to set setting ${path}:`, error);
-      throw error;
-    }
-  }
-
-  /**
-   * Set multiple settings in an optimized batch operation
-   */
-  public async setBatch(updates: Array<{path: string, value: any}>, options: { broadcast?: boolean } = {}): Promise<void> {
-    const startTime = performance.now();
-    const { broadcast = true } = options;
-
-    try {
-      const response = await fetch('/api/settings/batch', {
-        method: 'PUT',
-        headers: {
-          'Content-Type': 'application/json'
-        },
-        body: JSON.stringify({ updates })
-      });
-
-      if (!response.ok) {
-        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
-      }
-
-      const result = await response.json();
-
-      if (result.success) {
-        // Update local cache for all successful updates
-        updates.forEach(({path, value}) => {
-          this.setCachedValue(path, value);
-        });
-
-        // Broadcast batch update via WebSocket
-        if (broadcast && this.websocket && this.websocket.readyState === WebSocket.OPEN) {
-          this.websocket.send(JSON.stringify({
-            type: 'settingsBatchChanged',
-            updates,
-            timestamp: Date.now()
-          }));
-        }
-
-        const responseTime = performance.now() - startTime;
-        console.log(`Batch updated ${updates.length} settings in ${responseTime.toFixed(2)}ms`);
-
-      } else {
-        throw new Error(result.error || 'Failed to update settings batch');
-      }
-
-    } catch (error) {
-      console.error('Failed to set batch settings:', error);
-      throw error;
-    }
-  }
-
-  /**
-   * Get comprehensive performance metrics
-   */
-  public getPerformanceMetrics(): PerformanceMetrics {
-    const totalRequests = this.metrics.hits + this.metrics.misses;
-
-    return {
-      cacheHitRate: totalRequests > 0 ? (this.metrics.hits / totalRequests) * 100 : 0,
-      averageResponseTime: 0, // Would need to track this
-      bandwidthSavings: this.metrics.bandwidthSaved,
-      totalRequests
-    };
-  }
-
-  /**
-   * Clear all caches and reset metrics
-   */
-  public clearCache(): void {
-    this.cache.clear();
-    this.clearLocalStorage();
-    this.metrics = {
-      hits: 0,
-      misses: 0,
-      invalidations: 0,
-      bandwidthSaved: 0,
-      storageUsed: 0,
-      lastSync: Date.now()
-    };
-
-    console.log('All caches cleared and metrics reset');
-  }
-
-  private getCachedValue(path: string): CachedSetting | null {
-    return this.cache.get(path) || null;
-  }
-
-  private setCachedValue(path: string, value: any, options: {
-    timestamp?: number,
-    ttl?: number,
-    fromWebSocket?: boolean
-  } = {}): void {
-    const {
-      timestamp = Date.now(),
-      ttl = this.DEFAULT_TTL,
-      fromWebSocket = false
-    } = options;
-
-    const hash = this.calculateHash(value);
-    const version = this.getNextVersion();
-
-    const cached: CachedSetting = {
-      value,
-      path,
-      timestamp,
-      version,
-      hash,
-      ttl
-    };
-
-    // Add to memory cache
-    this.cache.set(path, cached);
-
-    // Persist to localStorage (async)
-    this.persistToLocalStorage();
-
-    // Update storage metrics
-    this.updateStorageMetrics();
-
-    // Cleanup if cache is too large
-    this.enforCacheSizeLimit();
-  }
-
-  private isCacheValid(cached: CachedSetting): boolean {
-    return (Date.now() - cached.timestamp) < cached.ttl;
-  }
-
-  private calculateHash(value: any): string {
-    // Simple hash function for cache validation
-    return btoa(JSON.stringify(value)).slice(0, 16);
-  }
-
-  private getNextVersion(): number {
-    return Date.now();
-  }
-
-  private persistToLocalStorage(): void {
-    try {
-      const cacheData = {
-        version: this.CACHE_VERSION,
-        timestamp: Date.now(),
-        data: Object.fromEntries(this.cache)
-      };
-
-      const serialized = JSON.stringify(cacheData);
-
-      // Check storage quota
-      if (serialized.length > this.STORAGE_QUOTA) {
-        console.warn('Cache size exceeds storage quota, performing cleanup');
-        this.cleanupOldestEntries();
-        return;
-      }
-
-      localStorage.setItem(`${this.CACHE_PREFIX}cache`, serialized);
-      this.persistMetrics();
-
-    } catch (error) {
-      console.warn('Failed to persist cache to localStorage:', error);
-
-      // If storage is full, try cleanup
-      if (error.name === 'QuotaExceededError') {
-        this.cleanupOldestEntries();
-      }
-    }
-  }
-
-  private persistMetrics(): void {
-    try {
-      localStorage.setItem(`${this.CACHE_PREFIX}metrics`, JSON.stringify(this.metrics));
-    } catch (error) {
-      console.warn('Failed to persist metrics:', error);
-    }
-  }
-
-  private invalidateLocalStorage(path: string): void {
-    // Remove specific path from localStorage cache
-    try {
-      const stored = localStorage.getItem(`${this.CACHE_PREFIX}cache`);
-      if (stored) {
-        const parsed = JSON.parse(stored);
-        delete parsed.data[path];
-        localStorage.setItem(`${this.CACHE_PREFIX}cache`, JSON.stringify(parsed));
-      }
-    } catch (error) {
-      console.warn('Failed to invalidate localStorage cache:', error);
-    }
-  }
-
-  private clearLocalStorage(): void {
-    try {
-      Object.keys(localStorage).forEach(key => {
-        if (key.startsWith(this.CACHE_PREFIX)) {
-          localStorage.removeItem(key);
-        }
-      });
-    } catch (error) {
-      console.warn('Failed to clear localStorage:', error);
-    }
-  }
-
-  private updateStorageMetrics(): void {
-    this.metrics.storageUsed = this.cache.size;
-  }
-
-  private enforCacheSizeLimit(): void {
-    if (this.cache.size > this.MAX_CACHE_SIZE) {
-      // Remove oldest entries
-      const entries = Array.from(this.cache.entries());
-      entries.sort((a, b) => a[1].timestamp - b[1].timestamp);
-
-      const toRemove = entries.slice(0, Math.floor(this.MAX_CACHE_SIZE * 0.2));
-      toRemove.forEach(([key]) => {
-        this.cache.delete(key);
-      });
-
-      console.log(`Removed ${toRemove.length} old cache entries`);
-    }
-  }
-
-  private cleanupOldestEntries(): void {
-    const entries = Array.from(this.cache.entries());
-    entries.sort((a, b) => a[1].timestamp - b[1].timestamp);
-
-    // Remove oldest 30%
-    const toRemove = entries.slice(0, Math.floor(entries.length * 0.3));
-    toRemove.forEach(([key]) => {
-      this.cache.delete(key);
-    });
-
-    // Try to persist again
-    this.persistToLocalStorage();
-  }
-
-  private estimatePayloadSize(value: any): number {
-    try {
-      return JSON.stringify(value).length;
-    } catch {
-      return 0;
-    }
-  }
-
-  private requestDeltaSync(): void {
-    if (this.websocket && this.websocket.readyState === WebSocket.OPEN) {
-      this.websocket.send(JSON.stringify({
-        type: 'requestDeltaSync',
-        lastSync: this.metrics.lastSync
-      }));
-    }
-  }
-
-  private handleBatchDelta(delta: DeltaUpdate): void {
-    // Handle batch delta operations
-    if (Array.isArray(delta.value)) {
-      delta.value.forEach((update: any) => {
-        this.setCachedValue(update.path, update.value, {
-          timestamp: delta.timestamp,
-          fromWebSocket: true
-        });
-      });
-    }
-  }
-
-  private handleCompressionWorkerMessage(e: MessageEvent): void {
-    // Handle compression worker responses
-    const { id, result, error, originalSize, compressedSize } = e.data;
-
-    if (error) {
-      console.warn('Compression worker error:', error);
-      return;
-    }
-
-    if (originalSize && compressedSize) {
-      const ratio = (originalSize - compressedSize) / originalSize;
-      console.log(`Compression achieved ${(ratio * 100).toFixed(1)}% size reduction`);
-    }
-  }
-
-  private startCacheMaintenanceTimer(): void {
-    // Cleanup expired entries every 5 minutes
-    setInterval(() => {
-      const now = Date.now();
-      const expiredKeys: string[] = [];
-
-      this.cache.forEach((cached, key) => {
-        if (now - cached.timestamp > cached.ttl) {
-          expiredKeys.push(key);
-        }
-      });
-
-      expiredKeys.forEach(key => {
-        this.cache.delete(key);
-      });
-
-      if (expiredKeys.length > 0) {
-        console.log(`Cleaned up ${expiredKeys.length} expired cache entries`);
-        this.persistToLocalStorage();
-      }
-
-    }, 5 * 60 * 1000);
-  }
-
-  private notifySubscribers(path: string, value: any): void {
-    // Emit custom event for subscribers
-    window.dispatchEvent(new CustomEvent('settingChanged', {
-      detail: { path, value }
-    }));
-  }
-
-  private notifyBatchSubscribers(updates: any[]): void {
-    // Emit custom event for batch updates
-    window.dispatchEvent(new CustomEvent('settingsBatchChanged', {
-      detail: { updates }
-    }));
-  }
-}
\ No newline at end of file
diff --git a/src/config/dev_config.rs b/src/config/dev_config.rs
index 57db5b9f..da5b6960 100644
--- a/src/config/dev_config.rs
+++ b/src/config/dev_config.rs
@@ -3,9 +3,12 @@
 // These settings control internal behavior, performance tuning, and debug features

 use serde::{Deserialize, Serialize};
-use std::sync::OnceLock;
+use std::sync::{Arc, RwLock, OnceLock};
+use log::{info, warn, error};

-static DEV_CONFIG: OnceLock<DevConfig> = OnceLock::new();
+use crate::services::database_service::{DatabaseService, SettingValue};
+
+static DEV_CONFIG: OnceLock<Arc<RwLock<DevConfig>>> = OnceLock::new();

 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct DevConfig {
@@ -20,60 +23,60 @@ pub struct DevConfig {
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct PhysicsInternals {
     // Force calculation parameters
-    pub force_epsilon: f32,                    // Small value to prevent division by zero
-    pub spring_length_multiplier: f32,         // Natural spring length = separation_radius * this
-    pub spring_length_max: f32,                // Maximum natural spring length
-    pub spring_force_clamp_factor: f32,        // Clamp spring forces to max_force * this
+    pub force_epsilon: f32,
+    pub spring_length_multiplier: f32,
+    pub spring_length_max: f32,
+    pub spring_force_clamp_factor: f32,

     // CUDA kernel parameters
-    pub rest_length: f32,                      // Natural spring rest length
-    pub repulsion_cutoff: f32,                 // Maximum distance for repulsion calculations
-    pub repulsion_softening_epsilon: f32,      // Prevents division by zero in force calculations
-    pub center_gravity_k: f32,                 // Gravity toward center
-    pub grid_cell_size: f32,                   // Spatial grid resolution for neighbor searches
-    pub warmup_iterations: u32,                // Number of warmup simulation steps
-    pub cooling_rate: f32,                     // Rate of cooling during warmup
+    pub rest_length: f32,
+    pub repulsion_cutoff: f32,
+    pub repulsion_softening_epsilon: f32,
+    pub center_gravity_k: f32,
+    pub grid_cell_size: f32,
+    pub warmup_iterations: u32,
+    pub cooling_rate: f32,

     // GPU kernel-specific physics parameters
-    pub max_force: f32,                        // Maximum force magnitude for clamping
-    pub max_velocity: f32,                     // Maximum velocity magnitude for clamping
-    pub world_bounds_min: f32,                 // Minimum world coordinate
-    pub world_bounds_max: f32,                 // Maximum world coordinate
-    pub cell_size_lod: f32,                    // Level of detail cell size
-    pub k_neighbors_max: u32,                  // Maximum k-neighbors for LOF
-    pub anomaly_detection_radius: f32,         // Default radius for anomaly detection
-    pub learning_rate_default: f32,            // Default learning rate for GPU algorithms
-    pub min_velocity_threshold: f32,           // Minimum velocity threshold for stability gates
-    pub stability_threshold: f32,              // System stability threshold for early exit
+    pub max_force: f32,
+    pub max_velocity: f32,
+    pub world_bounds_min: f32,
+    pub world_bounds_max: f32,
+    pub cell_size_lod: f32,
+    pub k_neighbors_max: u32,
+    pub anomaly_detection_radius: f32,
+    pub learning_rate_default: f32,
+    pub min_velocity_threshold: f32,
+    pub stability_threshold: f32,

     // Additional kernel constants for fine-tuning
-    pub norm_delta_cap: f32,                   // Cap for SSSP delta normalization
-    pub position_constraint_attraction: f32,   // Gentle attraction factor for position constraints
-    pub lof_score_min: f32,                    // Minimum LOF score clamp
-    pub lof_score_max: f32,                    // Maximum LOF score clamp
-    pub weight_precision_multiplier: f32,      // Weight precision multiplier for integer operations
+    pub norm_delta_cap: f32,
+    pub position_constraint_attraction: f32,
+    pub lof_score_min: f32,
+    pub lof_score_max: f32,
+    pub weight_precision_multiplier: f32,

     // Boundary behavior
-    pub boundary_extreme_multiplier: f32,      // Position is extreme if > viewport_bounds * this
-    pub boundary_extreme_force_multiplier: f32,// Force multiplier for extreme positions
-    pub boundary_velocity_damping: f32,        // Velocity reduction on boundary hit
+    pub boundary_extreme_multiplier: f32,
+    pub boundary_extreme_force_multiplier: f32,
+    pub boundary_velocity_damping: f32,

     // Node distribution
-    pub golden_ratio: f32,                     // For initial node positioning
-    pub initial_radius_min: f32,               // Minimum initial node radius
-    pub initial_radius_range: f32,             // Range for initial radius variation
+    pub golden_ratio: f32,
+    pub initial_radius_min: f32,
+    pub initial_radius_range: f32,

     // Graph-based scaling
-    pub cross_graph_repulsion_scale: f32,      // Repulsion between different graphs
-    pub cross_graph_spring_scale: f32,         // Spring force between different graphs
+    pub cross_graph_repulsion_scale: f32,
+    pub cross_graph_spring_scale: f32,

     // Clustering
-    pub cluster_repulsion_scale: f32,          // Repulsion within same cluster
-    pub importance_scale_factor: f32,          // Scale based on node importance
+    pub cluster_repulsion_scale: f32,
+    pub importance_scale_factor: f32,

     // Distance thresholds
-    pub repulsion_distance_squared_min: f32,   // Minimum distance squared for repulsion
-    pub stress_majorization_epsilon: f32,      // Small value for stress calculations
+    pub repulsion_distance_squared_min: f32,
+    pub stress_majorization_epsilon: f32,
 }

 #[derive(Debug, Clone, Serialize, Deserialize)]
@@ -84,13 +87,13 @@ pub struct CudaInternals {
     pub warmup_damping_end: f32,
     pub warmup_temperature_scale: f32,
     pub warmup_cooling_iterations: u32,
-
+
     // GPU safety
     pub max_kernel_time_ms: u32,
     pub max_gpu_failures: u32,
     pub debug_output_throttle: u32,
-    pub debug_node_count: u32,                 // Number of nodes to debug output
-
+    pub debug_node_count: u32,
+
     // Memory limits
     pub max_nodes: u32,
     pub max_edges: u32,
@@ -102,24 +105,24 @@ pub struct NetworkInternals {
     pub pool_max_idle_per_host: usize,
     pub pool_idle_timeout_secs: u64,
     pub pool_connect_timeout_secs: u64,
-
+
     // Circuit breaker
     pub circuit_failure_threshold: u32,
     pub circuit_recovery_timeout_secs: u64,
     pub circuit_half_open_max_requests: u32,
-
-    // Retry logic
+
+    // Retry logic
     pub max_retry_attempts: u32,
     pub retry_base_delay_ms: u64,
     pub retry_max_delay_ms: u64,
     pub retry_exponential_base: f32,
-
+
     // WebSocket internals
     pub ws_ping_interval_secs: u64,
     pub ws_pong_timeout_secs: u64,
     pub ws_frame_size: usize,
     pub ws_max_pending_messages: usize,
-
+
     // Rate limiting internals
     pub rate_limit_burst_size: u32,
     pub rate_limit_refill_rate: f32,
@@ -129,19 +132,19 @@ pub struct NetworkInternals {
 pub struct RenderingInternals {
     // Agent colors (hex strings)
     pub agent_colors: AgentColors,
-
+
     // Size calculations
     pub agent_base_size: f32,
     pub agent_size_per_task: f32,
     pub agent_max_size: f32,
     pub node_base_radius: f32,
-
+
     // Animation speeds
     pub pulse_speed: f32,
     pub rotate_speed: f32,
     pub glow_speed: f32,
     pub wave_speed: f32,
-
+
     // Quality thresholds
     pub lod_distance_high: f32,
     pub lod_distance_medium: f32,
@@ -168,17 +171,17 @@ pub struct PerformanceInternals {
     pub batch_size_nodes: usize,
     pub batch_size_edges: usize,
     pub batch_timeout_ms: u64,
-
+
     // Caching
     pub cache_ttl_secs: u64,
     pub cache_max_entries: usize,
     pub cache_eviction_percentage: f32,
-
+
     // Threading
     pub worker_threads: usize,
     pub blocking_threads: usize,
     pub stack_size_mb: usize,
-
+
     // Memory management
     pub gc_interval_secs: u64,
     pub memory_warning_threshold_mb: usize,
@@ -192,7 +195,7 @@ pub struct DebugInternals {
     pub enable_network_debug: bool,
     pub enable_memory_tracking: bool,
     pub enable_performance_tracking: bool,
-
+
     pub log_slow_operations_ms: u64,
     pub log_memory_usage_interval_secs: u64,
     pub profile_sample_rate: f32,
@@ -333,65 +336,333 @@ impl Default for DevConfig {
 }

 impl DevConfig {
-    /// Load developer configuration from file or use defaults
-    pub fn load() -> &'static Self {
-        DEV_CONFIG.get_or_init(|| {
-            // Try to load from dev_config.toml
-            match std::fs::read_to_string("data/dev_config.toml") {
-                Ok(content) => {
-                    match toml::from_str::<DevConfig>(&content) {
-                        Ok(config) => {
-                            log::info!("Loaded developer configuration from data/dev_config.toml");
-                            config
-                        }
-                        Err(e) => {
-                            log::warn!("Failed to parse dev_config.toml: {}, using defaults", e);
-                            Self::default()
-                        }
-                    }
-                }
-                Err(_) => {
-                    log::info!("No dev_config.toml found, using default developer configuration");
-                    Self::default()
-                }
-            }
-        })
+    /// Initialize DevConfig from database
+    pub fn initialize(db_service: Arc<DatabaseService>) -> Result<(), Box<dyn std::error::Error>> {
+        let config = Self::from_database(db_service)?;
+
+        DEV_CONFIG.set(Arc::new(RwLock::new(config)))
+            .map_err(|_| "DevConfig already initialized")?;
+
+        info!("DevConfig initialized from database");
+        Ok(())
     }
-
+
+    /// Load configuration from database
+    pub fn from_database(db_service: Arc<DatabaseService>) -> Result<Self, Box<dyn std::error::Error>> {
+        // Check for deprecated file
+        if std::path::Path::new("data/dev_config.toml").exists() {
+            warn!("DEPRECATED: data/dev_config.toml file found. Configuration is now loaded from database.");
+            warn!("Please remove or rename this file to data/dev_config.toml.deprecated");
+        }
+
+        let mut config = Self::default();
+
+        // Load physics parameters
+        config.physics.force_epsilon = Self::get_f32(&db_service, "dev.physics.force_epsilon")
+            .unwrap_or(config.physics.force_epsilon);
+        config.physics.spring_length_multiplier = Self::get_f32(&db_service, "dev.physics.spring_length_multiplier")
+            .unwrap_or(config.physics.spring_length_multiplier);
+        config.physics.spring_length_max = Self::get_f32(&db_service, "dev.physics.spring_length_max")
+            .unwrap_or(config.physics.spring_length_max);
+        config.physics.spring_force_clamp_factor = Self::get_f32(&db_service, "dev.physics.spring_force_clamp_factor")
+            .unwrap_or(config.physics.spring_force_clamp_factor);
+        config.physics.rest_length = Self::get_f32(&db_service, "dev.physics.rest_length")
+            .unwrap_or(config.physics.rest_length);
+        config.physics.repulsion_cutoff = Self::get_f32(&db_service, "dev.physics.repulsion_cutoff")
+            .unwrap_or(config.physics.repulsion_cutoff);
+        config.physics.repulsion_softening_epsilon = Self::get_f32(&db_service, "dev.physics.repulsion_softening_epsilon")
+            .unwrap_or(config.physics.repulsion_softening_epsilon);
+        config.physics.center_gravity_k = Self::get_f32(&db_service, "dev.physics.center_gravity_k")
+            .unwrap_or(config.physics.center_gravity_k);
+        config.physics.grid_cell_size = Self::get_f32(&db_service, "dev.physics.grid_cell_size")
+            .unwrap_or(config.physics.grid_cell_size);
+        config.physics.warmup_iterations = Self::get_u32(&db_service, "dev.physics.warmup_iterations")
+            .unwrap_or(config.physics.warmup_iterations);
+        config.physics.cooling_rate = Self::get_f32(&db_service, "dev.physics.cooling_rate")
+            .unwrap_or(config.physics.cooling_rate);
+        config.physics.max_force = Self::get_f32(&db_service, "dev.physics.max_force")
+            .unwrap_or(config.physics.max_force);
+        config.physics.max_velocity = Self::get_f32(&db_service, "dev.physics.max_velocity")
+            .unwrap_or(config.physics.max_velocity);
+        config.physics.world_bounds_min = Self::get_f32(&db_service, "dev.physics.world_bounds_min")
+            .unwrap_or(config.physics.world_bounds_min);
+        config.physics.world_bounds_max = Self::get_f32(&db_service, "dev.physics.world_bounds_max")
+            .unwrap_or(config.physics.world_bounds_max);
+        config.physics.cell_size_lod = Self::get_f32(&db_service, "dev.physics.cell_size_lod")
+            .unwrap_or(config.physics.cell_size_lod);
+        config.physics.k_neighbors_max = Self::get_u32(&db_service, "dev.physics.k_neighbors_max")
+            .unwrap_or(config.physics.k_neighbors_max);
+        config.physics.anomaly_detection_radius = Self::get_f32(&db_service, "dev.physics.anomaly_detection_radius")
+            .unwrap_or(config.physics.anomaly_detection_radius);
+        config.physics.learning_rate_default = Self::get_f32(&db_service, "dev.physics.learning_rate_default")
+            .unwrap_or(config.physics.learning_rate_default);
+        config.physics.min_velocity_threshold = Self::get_f32(&db_service, "dev.physics.min_velocity_threshold")
+            .unwrap_or(config.physics.min_velocity_threshold);
+        config.physics.stability_threshold = Self::get_f32(&db_service, "dev.physics.stability_threshold")
+            .unwrap_or(config.physics.stability_threshold);
+        config.physics.norm_delta_cap = Self::get_f32(&db_service, "dev.physics.norm_delta_cap")
+            .unwrap_or(config.physics.norm_delta_cap);
+        config.physics.position_constraint_attraction = Self::get_f32(&db_service, "dev.physics.position_constraint_attraction")
+            .unwrap_or(config.physics.position_constraint_attraction);
+        config.physics.lof_score_min = Self::get_f32(&db_service, "dev.physics.lof_score_min")
+            .unwrap_or(config.physics.lof_score_min);
+        config.physics.lof_score_max = Self::get_f32(&db_service, "dev.physics.lof_score_max")
+            .unwrap_or(config.physics.lof_score_max);
+        config.physics.weight_precision_multiplier = Self::get_f32(&db_service, "dev.physics.weight_precision_multiplier")
+            .unwrap_or(config.physics.weight_precision_multiplier);
+        config.physics.boundary_extreme_multiplier = Self::get_f32(&db_service, "dev.physics.boundary_extreme_multiplier")
+            .unwrap_or(config.physics.boundary_extreme_multiplier);
+        config.physics.boundary_extreme_force_multiplier = Self::get_f32(&db_service, "dev.physics.boundary_extreme_force_multiplier")
+            .unwrap_or(config.physics.boundary_extreme_force_multiplier);
+        config.physics.boundary_velocity_damping = Self::get_f32(&db_service, "dev.physics.boundary_velocity_damping")
+            .unwrap_or(config.physics.boundary_velocity_damping);
+        config.physics.golden_ratio = Self::get_f32(&db_service, "dev.physics.golden_ratio")
+            .unwrap_or(config.physics.golden_ratio);
+        config.physics.initial_radius_min = Self::get_f32(&db_service, "dev.physics.initial_radius_min")
+            .unwrap_or(config.physics.initial_radius_min);
+        config.physics.initial_radius_range = Self::get_f32(&db_service, "dev.physics.initial_radius_range")
+            .unwrap_or(config.physics.initial_radius_range);
+        config.physics.cross_graph_repulsion_scale = Self::get_f32(&db_service, "dev.physics.cross_graph_repulsion_scale")
+            .unwrap_or(config.physics.cross_graph_repulsion_scale);
+        config.physics.cross_graph_spring_scale = Self::get_f32(&db_service, "dev.physics.cross_graph_spring_scale")
+            .unwrap_or(config.physics.cross_graph_spring_scale);
+        config.physics.cluster_repulsion_scale = Self::get_f32(&db_service, "dev.physics.cluster_repulsion_scale")
+            .unwrap_or(config.physics.cluster_repulsion_scale);
+        config.physics.importance_scale_factor = Self::get_f32(&db_service, "dev.physics.importance_scale_factor")
+            .unwrap_or(config.physics.importance_scale_factor);
+        config.physics.repulsion_distance_squared_min = Self::get_f32(&db_service, "dev.physics.repulsion_distance_squared_min")
+            .unwrap_or(config.physics.repulsion_distance_squared_min);
+        config.physics.stress_majorization_epsilon = Self::get_f32(&db_service, "dev.physics.stress_majorization_epsilon")
+            .unwrap_or(config.physics.stress_majorization_epsilon);
+
+        // Load CUDA parameters
+        config.cuda.warmup_iterations_default = Self::get_u32(&db_service, "dev.cuda.warmup_iterations_default")
+            .unwrap_or(config.cuda.warmup_iterations_default);
+        config.cuda.warmup_damping_start = Self::get_f32(&db_service, "dev.cuda.warmup_damping_start")
+            .unwrap_or(config.cuda.warmup_damping_start);
+        config.cuda.warmup_damping_end = Self::get_f32(&db_service, "dev.cuda.warmup_damping_end")
+            .unwrap_or(config.cuda.warmup_damping_end);
+        config.cuda.warmup_temperature_scale = Self::get_f32(&db_service, "dev.cuda.warmup_temperature_scale")
+            .unwrap_or(config.cuda.warmup_temperature_scale);
+        config.cuda.warmup_cooling_iterations = Self::get_u32(&db_service, "dev.cuda.warmup_cooling_iterations")
+            .unwrap_or(config.cuda.warmup_cooling_iterations);
+        config.cuda.max_kernel_time_ms = Self::get_u32(&db_service, "dev.cuda.max_kernel_time_ms")
+            .unwrap_or(config.cuda.max_kernel_time_ms);
+        config.cuda.max_gpu_failures = Self::get_u32(&db_service, "dev.cuda.max_gpu_failures")
+            .unwrap_or(config.cuda.max_gpu_failures);
+        config.cuda.debug_output_throttle = Self::get_u32(&db_service, "dev.cuda.debug_output_throttle")
+            .unwrap_or(config.cuda.debug_output_throttle);
+        config.cuda.debug_node_count = Self::get_u32(&db_service, "dev.cuda.debug_node_count")
+            .unwrap_or(config.cuda.debug_node_count);
+        config.cuda.max_nodes = Self::get_u32(&db_service, "dev.cuda.max_nodes")
+            .unwrap_or(config.cuda.max_nodes);
+        config.cuda.max_edges = Self::get_u32(&db_service, "dev.cuda.max_edges")
+            .unwrap_or(config.cuda.max_edges);
+
+        // Load network parameters
+        config.network.pool_max_idle_per_host = Self::get_usize(&db_service, "dev.network.pool_max_idle_per_host")
+            .unwrap_or(config.network.pool_max_idle_per_host);
+        config.network.pool_idle_timeout_secs = Self::get_u64(&db_service, "dev.network.pool_idle_timeout_secs")
+            .unwrap_or(config.network.pool_idle_timeout_secs);
+        config.network.pool_connect_timeout_secs = Self::get_u64(&db_service, "dev.network.pool_connect_timeout_secs")
+            .unwrap_or(config.network.pool_connect_timeout_secs);
+        config.network.circuit_failure_threshold = Self::get_u32(&db_service, "dev.network.circuit_failure_threshold")
+            .unwrap_or(config.network.circuit_failure_threshold);
+        config.network.circuit_recovery_timeout_secs = Self::get_u64(&db_service, "dev.network.circuit_recovery_timeout_secs")
+            .unwrap_or(config.network.circuit_recovery_timeout_secs);
+        config.network.circuit_half_open_max_requests = Self::get_u32(&db_service, "dev.network.circuit_half_open_max_requests")
+            .unwrap_or(config.network.circuit_half_open_max_requests);
+        config.network.max_retry_attempts = Self::get_u32(&db_service, "dev.network.max_retry_attempts")
+            .unwrap_or(config.network.max_retry_attempts);
+        config.network.retry_base_delay_ms = Self::get_u64(&db_service, "dev.network.retry_base_delay_ms")
+            .unwrap_or(config.network.retry_base_delay_ms);
+        config.network.retry_max_delay_ms = Self::get_u64(&db_service, "dev.network.retry_max_delay_ms")
+            .unwrap_or(config.network.retry_max_delay_ms);
+        config.network.retry_exponential_base = Self::get_f32(&db_service, "dev.network.retry_exponential_base")
+            .unwrap_or(config.network.retry_exponential_base);
+        config.network.ws_ping_interval_secs = Self::get_u64(&db_service, "dev.network.ws_ping_interval_secs")
+            .unwrap_or(config.network.ws_ping_interval_secs);
+        config.network.ws_pong_timeout_secs = Self::get_u64(&db_service, "dev.network.ws_pong_timeout_secs")
+            .unwrap_or(config.network.ws_pong_timeout_secs);
+        config.network.ws_frame_size = Self::get_usize(&db_service, "dev.network.ws_frame_size")
+            .unwrap_or(config.network.ws_frame_size);
+        config.network.ws_max_pending_messages = Self::get_usize(&db_service, "dev.network.ws_max_pending_messages")
+            .unwrap_or(config.network.ws_max_pending_messages);
+        config.network.rate_limit_burst_size = Self::get_u32(&db_service, "dev.network.rate_limit_burst_size")
+            .unwrap_or(config.network.rate_limit_burst_size);
+        config.network.rate_limit_refill_rate = Self::get_f32(&db_service, "dev.network.rate_limit_refill_rate")
+            .unwrap_or(config.network.rate_limit_refill_rate);
+
+        // Load rendering parameters
+        config.rendering.agent_base_size = Self::get_f32(&db_service, "dev.rendering.agent_base_size")
+            .unwrap_or(config.rendering.agent_base_size);
+        config.rendering.agent_size_per_task = Self::get_f32(&db_service, "dev.rendering.agent_size_per_task")
+            .unwrap_or(config.rendering.agent_size_per_task);
+        config.rendering.agent_max_size = Self::get_f32(&db_service, "dev.rendering.agent_max_size")
+            .unwrap_or(config.rendering.agent_max_size);
+        config.rendering.node_base_radius = Self::get_f32(&db_service, "dev.rendering.node_base_radius")
+            .unwrap_or(config.rendering.node_base_radius);
+        config.rendering.pulse_speed = Self::get_f32(&db_service, "dev.rendering.pulse_speed")
+            .unwrap_or(config.rendering.pulse_speed);
+        config.rendering.rotate_speed = Self::get_f32(&db_service, "dev.rendering.rotate_speed")
+            .unwrap_or(config.rendering.rotate_speed);
+        config.rendering.glow_speed = Self::get_f32(&db_service, "dev.rendering.glow_speed")
+            .unwrap_or(config.rendering.glow_speed);
+        config.rendering.wave_speed = Self::get_f32(&db_service, "dev.rendering.wave_speed")
+            .unwrap_or(config.rendering.wave_speed);
+        config.rendering.lod_distance_high = Self::get_f32(&db_service, "dev.rendering.lod_distance_high")
+            .unwrap_or(config.rendering.lod_distance_high);
+        config.rendering.lod_distance_medium = Self::get_f32(&db_service, "dev.rendering.lod_distance_medium")
+            .unwrap_or(config.rendering.lod_distance_medium);
+        config.rendering.lod_distance_low = Self::get_f32(&db_service, "dev.rendering.lod_distance_low")
+            .unwrap_or(config.rendering.lod_distance_low);
+
+        // Load agent colors
+        config.rendering.agent_colors.coordinator = Self::get_string(&db_service, "dev.rendering.agent_colors.coordinator")
+            .unwrap_or(config.rendering.agent_colors.coordinator);
+        config.rendering.agent_colors.coder = Self::get_string(&db_service, "dev.rendering.agent_colors.coder")
+            .unwrap_or(config.rendering.agent_colors.coder);
+        config.rendering.agent_colors.architect = Self::get_string(&db_service, "dev.rendering.agent_colors.architect")
+            .unwrap_or(config.rendering.agent_colors.architect);
+        config.rendering.agent_colors.analyst = Self::get_string(&db_service, "dev.rendering.agent_colors.analyst")
+            .unwrap_or(config.rendering.agent_colors.analyst);
+        config.rendering.agent_colors.tester = Self::get_string(&db_service, "dev.rendering.agent_colors.tester")
+            .unwrap_or(config.rendering.agent_colors.tester);
+        config.rendering.agent_colors.researcher = Self::get_string(&db_service, "dev.rendering.agent_colors.researcher")
+            .unwrap_or(config.rendering.agent_colors.researcher);
+        config.rendering.agent_colors.reviewer = Self::get_string(&db_service, "dev.rendering.agent_colors.reviewer")
+            .unwrap_or(config.rendering.agent_colors.reviewer);
+        config.rendering.agent_colors.optimizer = Self::get_string(&db_service, "dev.rendering.agent_colors.optimizer")
+            .unwrap_or(config.rendering.agent_colors.optimizer);
+        config.rendering.agent_colors.documenter = Self::get_string(&db_service, "dev.rendering.agent_colors.documenter")
+            .unwrap_or(config.rendering.agent_colors.documenter);
+        config.rendering.agent_colors.default = Self::get_string(&db_service, "dev.rendering.agent_colors.default")
+            .unwrap_or(config.rendering.agent_colors.default);
+
+        // Load performance parameters
+        config.performance.batch_size_nodes = Self::get_usize(&db_service, "dev.performance.batch_size_nodes")
+            .unwrap_or(config.performance.batch_size_nodes);
+        config.performance.batch_size_edges = Self::get_usize(&db_service, "dev.performance.batch_size_edges")
+            .unwrap_or(config.performance.batch_size_edges);
+        config.performance.batch_timeout_ms = Self::get_u64(&db_service, "dev.performance.batch_timeout_ms")
+            .unwrap_or(config.performance.batch_timeout_ms);
+        config.performance.cache_ttl_secs = Self::get_u64(&db_service, "dev.performance.cache_ttl_secs")
+            .unwrap_or(config.performance.cache_ttl_secs);
+        config.performance.cache_max_entries = Self::get_usize(&db_service, "dev.performance.cache_max_entries")
+            .unwrap_or(config.performance.cache_max_entries);
+        config.performance.cache_eviction_percentage = Self::get_f32(&db_service, "dev.performance.cache_eviction_percentage")
+            .unwrap_or(config.performance.cache_eviction_percentage);
+        config.performance.worker_threads = Self::get_usize(&db_service, "dev.performance.worker_threads")
+            .unwrap_or(config.performance.worker_threads);
+        config.performance.blocking_threads = Self::get_usize(&db_service, "dev.performance.blocking_threads")
+            .unwrap_or(config.performance.blocking_threads);
+        config.performance.stack_size_mb = Self::get_usize(&db_service, "dev.performance.stack_size_mb")
+            .unwrap_or(config.performance.stack_size_mb);
+        config.performance.gc_interval_secs = Self::get_u64(&db_service, "dev.performance.gc_interval_secs")
+            .unwrap_or(config.performance.gc_interval_secs);
+        config.performance.memory_warning_threshold_mb = Self::get_usize(&db_service, "dev.performance.memory_warning_threshold_mb")
+            .unwrap_or(config.performance.memory_warning_threshold_mb);
+        config.performance.memory_critical_threshold_mb = Self::get_usize(&db_service, "dev.performance.memory_critical_threshold_mb")
+            .unwrap_or(config.performance.memory_critical_threshold_mb);
+
+        // Load debug parameters
+        config.debug.enable_cuda_debug = Self::get_bool(&db_service, "dev.debug.enable_cuda_debug")
+            .unwrap_or(config.debug.enable_cuda_debug);
+        config.debug.enable_physics_debug = Self::get_bool(&db_service, "dev.debug.enable_physics_debug")
+            .unwrap_or(config.debug.enable_physics_debug);
+        config.debug.enable_network_debug = Self::get_bool(&db_service, "dev.debug.enable_network_debug")
+            .unwrap_or(config.debug.enable_network_debug);
+        config.debug.enable_memory_tracking = Self::get_bool(&db_service, "dev.debug.enable_memory_tracking")
+            .unwrap_or(config.debug.enable_memory_tracking);
+        config.debug.enable_performance_tracking = Self::get_bool(&db_service, "dev.debug.enable_performance_tracking")
+            .unwrap_or(config.debug.enable_performance_tracking);
+        config.debug.log_slow_operations_ms = Self::get_u64(&db_service, "dev.debug.log_slow_operations_ms")
+            .unwrap_or(config.debug.log_slow_operations_ms);
+        config.debug.log_memory_usage_interval_secs = Self::get_u64(&db_service, "dev.debug.log_memory_usage_interval_secs")
+            .unwrap_or(config.debug.log_memory_usage_interval_secs);
+        config.debug.profile_sample_rate = Self::get_f32(&db_service, "dev.debug.profile_sample_rate")
+            .unwrap_or(config.debug.profile_sample_rate);
+
+        Ok(config)
+    }
+
     /// Get the global developer configuration instance
-    pub fn get() -> &'static Self {
-        Self::load()
+    pub fn get() -> &'static Arc<RwLock<DevConfig>> {
+        DEV_CONFIG.get().expect("DevConfig not initialized. Call DevConfig::initialize() first.")
     }
-
-    /// Save current configuration to file
-    pub fn save_to_file(&self, path: &str) -> Result<(), Box<dyn std::error::Error>> {
-        let toml_string = toml::to_string_pretty(self)?;
-        std::fs::write(path, toml_string)?;
-        Ok(())
+
+    // Helper methods for database access
+    fn get_f32(db: &Arc<DatabaseService>, key: &str) -> Option<f32> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Float(f) => Some(f as f32),
+            SettingValue::Integer(i) => Some(i as f32),
+            _ => None,
+        }
+    }
+
+    fn get_u32(db: &Arc<DatabaseService>, key: &str) -> Option<u32> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Integer(i) => Some(i as u32),
+            _ => None,
+        }
+    }
+
+    fn get_u64(db: &Arc<DatabaseService>, key: &str) -> Option<u64> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Integer(i) => Some(i as u64),
+            _ => None,
+        }
+    }
+
+    fn get_usize(db: &Arc<DatabaseService>, key: &str) -> Option<usize> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Integer(i) => Some(i as usize),
+            _ => None,
+        }
+    }
+
+    fn get_bool(db: &Arc<DatabaseService>, key: &str) -> Option<bool> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Boolean(b) => Some(b),
+            _ => None,
+        }
+    }
+
+    fn get_string(db: &Arc<DatabaseService>, key: &str) -> Option<String> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::String(s) => Some(s),
+            _ => None,
+        }
     }
 }

 // Convenience functions for common access patterns
-pub fn physics() -> &'static PhysicsInternals {
-    &DevConfig::get().physics
+pub fn physics() -> PhysicsInternals {
+    DevConfig::get().read().unwrap().physics.clone()
 }

-pub fn cuda() -> &'static CudaInternals {
-    &DevConfig::get().cuda
+pub fn cuda() -> CudaInternals {
+    DevConfig::get().read().unwrap().cuda.clone()
 }

-pub fn network() -> &'static NetworkInternals {
-    &DevConfig::get().network
+pub fn network() -> NetworkInternals {
+    DevConfig::get().read().unwrap().network.clone()
 }

-pub fn rendering() -> &'static RenderingInternals {
-    &DevConfig::get().rendering
+pub fn rendering() -> RenderingInternals {
+    DevConfig::get().read().unwrap().rendering.clone()
 }

-pub fn performance() -> &'static PerformanceInternals {
-    &DevConfig::get().performance
+pub fn performance() -> PerformanceInternals {
+    DevConfig::get().read().unwrap().performance.clone()
 }

-pub fn debug() -> &'static DebugInternals {
-    &DevConfig::get().debug
-}
\ No newline at end of file
+pub fn debug() -> DebugInternals {
+    DevConfig::get().read().unwrap().debug.clone()
+}
diff --git a/src/config/mod.rs b/src/config/mod.rs
index a1211702..1c65b471 100644
--- a/src/config/mod.rs
+++ b/src/config/mod.rs
@@ -839,6 +839,7 @@ pub struct RenderingSettings {
 #[derive(Debug, Serialize, Deserialize, Clone, Default, Type, Validate)]
 #[serde(rename_all = "camelCase")]
 pub struct AnimationSettings {
+    pub enabled: bool,
     #[serde(alias = "enable_motion_blur")]
     pub enable_motion_blur: bool,
     #[serde(alias = "enable_node_animations")]
@@ -1085,6 +1086,95 @@ pub struct Sensitivity {
     pub rotation: f32,
 }

+// Additional Settings Structs
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct SyncSettings {
+    pub enabled: bool,
+    pub camera: bool,
+    pub selection: bool,
+}
+
+impl Default for SyncSettings {
+    fn default() -> Self {
+        Self {
+            enabled: false,
+            camera: true,
+            selection: true,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct EffectsSettings {
+    pub bloom: bool,
+    pub glow: bool,
+}
+
+impl Default for EffectsSettings {
+    fn default() -> Self {
+        Self {
+            bloom: false,
+            glow: true,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct PerformanceSettings {
+    pub auto_optimize: bool,
+    pub simplify_edges: bool,
+    pub cull_distance: f32,
+}
+
+impl Default for PerformanceSettings {
+    fn default() -> Self {
+        Self {
+            auto_optimize: false,
+            simplify_edges: true,
+            cull_distance: 50.0,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct InteractionSettings {
+    pub enable_hover: bool,
+    pub enable_click: bool,
+    pub enable_drag: bool,
+    pub hover_delay: u32,
+}
+
+impl Default for InteractionSettings {
+    fn default() -> Self {
+        Self {
+            enable_hover: true,
+            enable_click: true,
+            enable_drag: true,
+            hover_delay: 300,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct ExportSettings {
+    pub format: String,
+    pub include_metadata: bool,
+}
+
+impl Default for ExportSettings {
+    fn default() -> Self {
+        Self {
+            format: "json".to_string(),
+            include_metadata: true,
+        }
+    }
+}
+
 // Graph-specific settings
 #[derive(Debug, Serialize, Deserialize, Clone, Default, Type, Validate)]
 #[serde(rename_all = "camelCase")]
@@ -1112,7 +1202,7 @@ pub struct GraphsSettings {
 #[derive(Debug, Serialize, Deserialize, Clone, Default, Type, Validate)]
 #[serde(rename_all = "camelCase")]
 pub struct VisualisationSettings {
-
+
     // Global settings
     #[validate(nested)]
     pub rendering: RenderingSettings,
@@ -1126,6 +1216,10 @@ pub struct VisualisationSettings {
     pub hologram: HologramSettings,
     #[validate(nested)]
     pub graphs: GraphsSettings,
+    #[validate(nested)]
+    pub sync: SyncSettings,
+    #[validate(nested)]
+    pub effects: EffectsSettings,
     #[serde(skip_serializing_if = "Option::is_none")]
     pub camera: Option<CameraSettings>,
     #[serde(skip_serializing_if = "Option::is_none")]
@@ -1667,6 +1761,12 @@ pub struct AppFullSettings {
     #[validate(nested)]
     #[serde(alias = "auth")]
     pub auth: AuthSettings,
+    #[validate(nested)]
+    pub performance: PerformanceSettings,
+    #[validate(nested)]
+    pub interaction: InteractionSettings,
+    #[validate(nested)]
+    pub export: ExportSettings,
     #[serde(skip_serializing_if = "Option::is_none", alias = "ragflow")]
     pub ragflow: Option<RagFlowSettings>,
     #[serde(skip_serializing_if = "Option::is_none", alias = "perplexity")]
@@ -1692,6 +1792,9 @@ impl Default for AppFullSettings {
             system: SystemSettings::default(),
             xr: XRSettings::default(),
             auth: AuthSettings::default(),
+            performance: PerformanceSettings::default(),
+            interaction: InteractionSettings::default(),
+            export: ExportSettings::default(),
             ragflow: None,
             perplexity: None,
             openai: None,
@@ -1703,106 +1806,37 @@ impl Default for AppFullSettings {
 }

 impl AppFullSettings {
-    /// Load AppFullSettings from a YAML file with proper snake_case to camelCase conversion
-    pub fn from_yaml_file(path: &PathBuf) -> Result<Self, ConfigError> {
-        let yaml_content = std::fs::read_to_string(path)
-            .map_err(|e| ConfigError::Message(format!("Failed to read YAML file {:?}: {}", path, e)))?;
-
-        // Try direct YAML deserialization first
-        match serde_yaml::from_str::<AppFullSettings>(&yaml_content) {
-            Ok(settings) => {
-                debug!("Successfully loaded settings using direct YAML deserialization");
-                return Ok(settings);
-            }
-            Err(yaml_err) => {
-                debug!("Direct YAML deserialization failed: {}, trying YAML->JSON conversion", yaml_err);
-
-                // Parse as raw Value first
-                let raw_value = serde_yaml::from_str::<Value>(&yaml_content)
-                    .map_err(|e| ConfigError::Message(format!("Failed to parse YAML as Value: {}", e)))?;
-
-                // Convert to JSON string (this preserves the structure)
-                let json_str = serde_json::to_string(&raw_value)
-                    .map_err(|e| ConfigError::Message(format!("Failed to convert YAML to JSON: {}", e)))?;
-
-                // Deserialize from JSON (this will respect serde rename attributes)
-                serde_json::from_str::<AppFullSettings>(&json_str)
-                    .map_err(|e| ConfigError::Message(format!("Failed to deserialize JSON: {}", e)))
-            }
-        }
+    /// DEPRECATED: Use SettingsService for database-backed settings
+    /// Returns default settings - all settings now loaded from SQLite database
+    #[deprecated(note = "Settings are now stored in SQLite. Use SettingsService for runtime access.")]
+    pub fn from_yaml_file(_path: &PathBuf) -> Result<Self, ConfigError> {
+        log::warn!("from_yaml_file() is deprecated - settings now loaded from SQLite database");
+        log::warn!("Returning default AppFullSettings structure");
+        Ok(Self::default())
     }

+    /// Load AppFullSettings - returns defaults as all settings are now in SQLite database
+    ///
+    /// NOTE: This method exists for backward compatibility only. All settings are now
+    /// stored in and loaded from the SQLite database via SettingsService.
+    ///
+    /// The migration from YAML/TOML to SQLite happens automatically on first startup
+    /// via the SettingsMigration service in ontology_init.rs
     pub fn new() -> Result<Self, ConfigError> {
-        debug!("Initializing AppFullSettings from YAML");
+        log::info!("AppFullSettings::new() - returning default structure");
+        log::info!("All settings are now managed via SQLite database (see SettingsService)");
         dotenvy::dotenv().ok();
-
-        let settings_path = std::env::var("SETTINGS_FILE_PATH")
-            .map(PathBuf::from)
-            .unwrap_or_else(|_| PathBuf::from("data/settings.yaml"));
-        debug!("Loading AppFullSettings from YAML file: {:?}", settings_path);
-
-        // Use our improved YAML loading method
-        match Self::from_yaml_file(&settings_path) {
-            Ok(settings) => {
-                info!("Successfully loaded settings from YAML file");
-                return Ok(settings);
-            }
-            Err(yaml_err) => {
-                error!("YAML loading failed: {}", yaml_err);
-                debug!("Trying config crate fallback (this may fail due to case conversion issues)");
-            }
-        }
-
-        // Fallback to config crate approach (with environment variable support)
-        // NOTE: This fallback doesn't respect serde rename attributes, so it will likely fail
-        // with "missing field ambientLightIntensity" type errors
-        let builder = ConfigBuilder::<config::builder::DefaultState>::default()
-            .add_source(config::File::from(settings_path.clone()).required(true))
-            .add_source(
-                Environment::default()
-                    .separator("_")
-                    .list_separator(",")
-            );
-        let config = builder.build()?;
-        debug!("Configuration built successfully. Attempting deserialization...");
-
-        let settings: AppFullSettings = config.clone().try_deserialize()
-            .map_err(|e| {
-                error!("Config crate deserialization failed as expected: {}", e);
-                error!("This is because config crate doesn't respect #[serde(rename_all = \"camelCase\")] attributes");
-                e
-            })?;
-
-        debug!("Unexpectedly succeeded with config crate fallback");
-        Ok(settings)
+        Ok(Self::default())
     }


+    /// DEPRECATED: Settings persistence now handled by SQLite database
+    ///
+    /// This method no longer writes to YAML files. All settings changes should be
+    /// made through the SettingsService API which persists to SQLite automatically.
+    #[deprecated(note = "Use SettingsService to persist settings to SQLite database")]
     pub fn save(&self) -> Result<(), String> {
-        // Check if persist_settings is enabled
-        if !self.system.persist_settings {
-            debug!("Settings persistence is disabled (persist_settings: false), skipping save");
-            return Ok(());
-        }
-
-        let settings_path = std::env::var("SETTINGS_FILE_PATH")
-            .map(PathBuf::from)
-            .unwrap_or_else(|_| PathBuf::from("data/settings.yaml"));
-        info!("Saving AppFullSettings to YAML file: {:?}", settings_path);
-
-        // Create parent directory if it doesn't exist
-        if let Some(parent) = settings_path.parent() {
-            std::fs::create_dir_all(parent)
-                .map_err(|e| format!("Failed to create directory {:?}: {}", parent, e))?;
-        }
-
-        let yaml = serde_yaml::to_string(&self)
-            .map_err(|e| format!("Failed to serialize AppFullSettings to YAML: {}", e))?;
-
-        std::fs::write(&settings_path, yaml)
-            .map_err(|e| format!("Failed to write settings file {:?}: {}", settings_path, e))?;
-        info!("Successfully saved AppFullSettings to {:?}", settings_path);
-        Ok(())
+        Err("Settings persistence has been moved to SQLite database. Use SettingsService API instead.".to_string())
     }

     /// Get physics settings for a specific graph
@@ -1967,6 +2001,27 @@ impl PathAccessible for AppFullSettings {
                     Err("Auth fields are not deeply accessible".to_string())
                 }
             }
+            "performance" => {
+                if segments.len() == 1 {
+                    Ok(Box::new(self.performance.clone()))
+                } else {
+                    Err("Performance fields are not deeply accessible".to_string())
+                }
+            }
+            "interaction" => {
+                if segments.len() == 1 {
+                    Ok(Box::new(self.interaction.clone()))
+                } else {
+                    Err("Interaction fields are not deeply accessible".to_string())
+                }
+            }
+            "export" => {
+                if segments.len() == 1 {
+                    Ok(Box::new(self.export.clone()))
+                } else {
+                    Err("Export fields are not deeply accessible".to_string())
+                }
+            }
             _ => Err(format!("Unknown top-level field: {}", segments[0]))
         }
     }
@@ -2030,6 +2085,45 @@ impl PathAccessible for AppFullSettings {
                     Err("Auth nested fields are not modifiable".to_string())
                 }
             }
+            "performance" => {
+                if segments.len() == 1 {
+                    match value.downcast::<PerformanceSettings>() {
+                        Ok(v) => {
+                            self.performance = *v;
+                            Ok(())
+                        }
+                        Err(_) => Err("Type mismatch for performance field".to_string())
+                    }
+                } else {
+                    Err("Performance fields are not deeply settable".to_string())
+                }
+            }
+            "interaction" => {
+                if segments.len() == 1 {
+                    match value.downcast::<InteractionSettings>() {
+                        Ok(v) => {
+                            self.interaction = *v;
+                            Ok(())
+                        }
+                        Err(_) => Err("Type mismatch for interaction field".to_string())
+                    }
+                } else {
+                    Err("Interaction fields are not deeply settable".to_string())
+                }
+            }
+            "export" => {
+                if segments.len() == 1 {
+                    match value.downcast::<ExportSettings>() {
+                        Ok(v) => {
+                            self.export = *v;
+                            Ok(())
+                        }
+                        Err(_) => Err("Type mismatch for export field".to_string())
+                    }
+                } else {
+                    Err("Export fields are not deeply settable".to_string())
+                }
+            }
             _ => Err(format!("Unknown top-level field: {}", segments[0]))
         }
     }
diff --git a/src/handlers/admin_handler.rs b/src/handlers/admin_handler.rs
new file mode 100644
index 00000000..d2518ce0
--- /dev/null
+++ b/src/handlers/admin_handler.rs
@@ -0,0 +1,262 @@
+use actix_web::{web, Error, HttpResponse, HttpRequest};
+use serde::{Deserialize, Serialize};
+use log::{error, info};
+
+use crate::services::user_service::{UserService, UserServiceError, User, AuditLogEntry};
+use crate::middleware::permissions::extract_auth_context;
+
+#[derive(Debug, Serialize)]
+struct UsersListResponse {
+    users: Vec<User>,
+    count: usize,
+}
+
+#[derive(Debug, Serialize)]
+struct UserResponse {
+    user: User,
+}
+
+#[derive(Debug, Serialize)]
+struct MessageResponse {
+    message: String,
+}
+
+#[derive(Debug, Serialize)]
+struct ErrorResponse {
+    error: String,
+}
+
+#[derive(Debug, Serialize)]
+struct AuditLogResponse {
+    entries: Vec<AuditLogEntry>,
+    count: usize,
+}
+
+#[derive(Debug, Deserialize)]
+struct AuditLogQuery {
+    key: Option<String>,
+    user_id: Option<i64>,
+    #[serde(default = "default_limit")]
+    limit: i64,
+}
+
+fn default_limit() -> i64 {
+    100
+}
+
+pub async fn list_users(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    match user_service.list_all_users().await {
+        Ok(users) => {
+            let count = users.len();
+            info!("Listed {} users for admin {}", count, auth_context.nostr_pubkey);
+            Ok(HttpResponse::Ok().json(UsersListResponse { users, count }))
+        }
+        Err(e) => {
+            error!("Failed to list users: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch users".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn get_user_by_pubkey(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    path: web::Path<String>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    let pubkey = path.into_inner();
+
+    match user_service.get_user_by_nostr_pubkey(&pubkey).await {
+        Ok(user) => {
+            info!("Fetched user {} for admin {}", pubkey, auth_context.nostr_pubkey);
+            Ok(HttpResponse::Ok().json(UserResponse { user }))
+        }
+        Err(UserServiceError::UserNotFound) => {
+            Ok(HttpResponse::NotFound().json(ErrorResponse {
+                error: "User not found".to_string(),
+            }))
+        }
+        Err(e) => {
+            error!("Failed to fetch user {}: {:?}", pubkey, e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch user".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn grant_power_user(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    path: web::Path<String>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    let pubkey = path.into_inner();
+
+    let target_user = match user_service.get_user_by_nostr_pubkey(&pubkey).await {
+        Ok(user) => user,
+        Err(UserServiceError::UserNotFound) => {
+            return Ok(HttpResponse::NotFound().json(ErrorResponse {
+                error: "User not found".to_string(),
+            }));
+        }
+        Err(e) => {
+            error!("Failed to fetch user {}: {:?}", pubkey, e);
+            return Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch user".to_string(),
+            }));
+        }
+    };
+
+    match user_service.grant_power_user(target_user.id).await {
+        Ok(()) => {
+            info!(
+                "Admin {} granted power user to user {}",
+                auth_context.nostr_pubkey, pubkey
+            );
+            Ok(HttpResponse::Ok().json(MessageResponse {
+                message: format!("Power user granted to {}", pubkey),
+            }))
+        }
+        Err(e) => {
+            error!("Failed to grant power user to {}: {:?}", pubkey, e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to grant power user".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn revoke_power_user(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    path: web::Path<String>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    let pubkey = path.into_inner();
+
+    let target_user = match user_service.get_user_by_nostr_pubkey(&pubkey).await {
+        Ok(user) => user,
+        Err(UserServiceError::UserNotFound) => {
+            return Ok(HttpResponse::NotFound().json(ErrorResponse {
+                error: "User not found".to_string(),
+            }));
+        }
+        Err(e) => {
+            error!("Failed to fetch user {}: {:?}", pubkey, e);
+            return Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch user".to_string(),
+            }));
+        }
+    };
+
+    match user_service.revoke_power_user(target_user.id).await {
+        Ok(()) => {
+            info!(
+                "Admin {} revoked power user from user {}",
+                auth_context.nostr_pubkey, pubkey
+            );
+            Ok(HttpResponse::Ok().json(MessageResponse {
+                message: format!("Power user revoked from {}", pubkey),
+            }))
+        }
+        Err(e) => {
+            error!("Failed to revoke power user from {}: {:?}", pubkey, e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to revoke power user".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn get_audit_log(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    query: web::Query<AuditLogQuery>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    let limit = if query.limit > 1000 { 1000 } else { query.limit };
+
+    match user_service
+        .get_audit_log(query.key.clone(), query.user_id, limit)
+        .await
+    {
+        Ok(entries) => {
+            let count = entries.len();
+            info!(
+                "Fetched {} audit log entries for admin {}",
+                count, auth_context.nostr_pubkey
+            );
+            Ok(HttpResponse::Ok().json(AuditLogResponse { entries, count }))
+        }
+        Err(e) => {
+            error!("Failed to fetch audit log: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch audit log".to_string(),
+            }))
+        }
+    }
+}
+
+pub fn configure_routes(cfg: &mut web::ServiceConfig) {
+    cfg.service(
+        web::scope("/api/admin")
+            .route("/users", web::get().to(list_users))
+            .route("/users/{pubkey}", web::get().to(get_user_by_pubkey))
+            .route(
+                "/users/{pubkey}/power-user",
+                web::put().to(grant_power_user),
+            )
+            .route(
+                "/users/{pubkey}/power-user",
+                web::delete().to(revoke_power_user),
+            )
+            .route("/settings/audit", web::get().to(get_audit_log)),
+    );
+}
diff --git a/src/handlers/api_handler/analytics/clustering.rs b/src/handlers/api_handler/analytics/clustering.rs
index 524ec993..edea3518 100644
--- a/src/handlers/api_handler/analytics/clustering.rs
+++ b/src/handlers/api_handler/analytics/clustering.rs
@@ -65,7 +65,7 @@ pub async fn perform_clustering(

     // Get current graph data
     let graph_data = {
-        let graph_addr = app_state.get_graph_service_addr();
+        let graph_addr = app_state.get_graph_state_addr();
         match graph_addr.send(GetGraphData).await {
             Ok(Ok(data)) => data,
             Ok(Err(e)) => {
diff --git a/src/handlers/api_handler/analytics/mod.rs b/src/handlers/api_handler/analytics/mod.rs
index fd5d1f37..880f7d0a 100644
--- a/src/handlers/api_handler/analytics/mod.rs
+++ b/src/handlers/api_handler/analytics/mod.rs
@@ -1058,7 +1058,7 @@ async fn perform_clustering(

     // Get graph data for clustering
     let graph_data = {
-        match app_state.graph_service_addr.send(GetGraphData).await {
+        match app_state.graph_state_addr.send(GetGraphData).await {
             Ok(Ok(data)) => data,
             _ => return Err("Failed to get graph data".to_string()),
         }
@@ -1431,7 +1431,7 @@ pub async fn get_ai_insights(
     info!("Generating AI insights for graph analysis");

     // Get current graph data
-    let graph_data = match app_state.graph_service_addr.send(GetGraphData).await {
+    let graph_data = match app_state.graph_state_addr.send(GetGraphData).await {
         Ok(Ok(data)) => Some(data),
         _ => None,
     };
@@ -1933,7 +1933,7 @@ pub async fn get_realtime_insights(
     info!("Client requesting real-time AI insights");

     // Get current state for real-time analysis
-    let graph_data = app_state.graph_service_addr.send(GetGraphData).await
+    let graph_data = app_state.graph_state_addr.send(GetGraphData).await
         .map_err(|e| {
             error!("Failed to get graph data: {}", e);
             actix_web::error::ErrorInternalServerError("Failed to get graph data")
@@ -2094,14 +2094,14 @@ impl Default for FeatureFlags {
     fn default() -> Self {
         Self {
             gpu_clustering: true,
+            ontology_validation: false,
             gpu_anomaly_detection: true,
             real_time_insights: true,
             advanced_visualizations: true,
             performance_monitoring: true,
-            stress_majorization: false, // Disabled by default as per task.md
-            semantic_constraints: false, // Disabled by default
+            stress_majorization: false,
+            semantic_constraints: false,
             sssp_integration: true,
-            ontology_validation: false, // Disabled by default
         }
     }
 }
@@ -2448,7 +2448,7 @@ pub async fn compute_sssp(

     // Send SSSP computation request to graph service
     use crate::actors::messages::ComputeShortestPaths;
-    match app_state.graph_service_addr.send(ComputeShortestPaths {
+    match app_state.graph_state_addr.send(ComputeShortestPaths {
         source_node_id: source_node,
     }).await {
         Ok(Ok(_)) => {
diff --git a/src/handlers/api_handler/files/mod.rs b/src/handlers/api_handler/files/mod.rs
index efded917..186ce6f1 100644
--- a/src/handlers/api_handler/files/mod.rs
+++ b/src/handlers/api_handler/files/mod.rs
@@ -62,7 +62,7 @@ pub async fn fetch_and_process_files(state: web::Data<AppState>) -> HttpResponse
             }

             // Send AddNodesFromMetadata for incremental updates instead of full rebuild
-            match state.graph_service_addr.send(AddNodesFromMetadata { metadata: metadata_store.clone() }).await {
+            match state.graph_state_addr.send(AddNodesFromMetadata { metadata: metadata_store.clone() }).await {
                 Ok(Ok(())) => {
                     info!("Graph data structure updated successfully via GraphServiceActor");

@@ -134,7 +134,7 @@ pub async fn refresh_graph(state: web::Data<AppState>) -> HttpResponse {
     info!("Manually triggering graph refresh - returning current state");

     // Instead of rebuilding, just return the current graph data
-    match state.graph_service_addr.send(crate::actors::messages::GetGraphData).await {
+    match state.graph_state_addr.send(crate::actors::messages::GetGraphData).await {
         Ok(Ok(graph_data)) => {
             debug!("Retrieved current graph state with {} nodes and {} edges",
                 graph_data.nodes.len(),
@@ -177,7 +177,7 @@ pub async fn update_graph(state: web::Data<AppState>) -> Result<HttpResponse, Ac
         }
     };

-    match state.graph_service_addr.send(AddNodesFromMetadata { metadata: metadata_store.clone() }).await {
+    match state.graph_state_addr.send(AddNodesFromMetadata { metadata: metadata_store.clone() }).await {
         Ok(Ok(())) => {
             info!("Graph data structure updated successfully via GraphServiceActor in update_graph");

diff --git a/src/handlers/api_handler/graph/mod.rs b/src/handlers/api_handler/graph/mod.rs
index 8868a548..eb5ce487 100644
--- a/src/handlers/api_handler/graph/mod.rs
+++ b/src/handlers/api_handler/graph/mod.rs
@@ -96,9 +96,9 @@ pub async fn get_graph_data(state: web::Data<AppState>, req: HttpRequest) -> imp
     info!("Received request for graph data with positions");

     // Fetch graph structure AND physics state in parallel
-    let graph_data_future = state.graph_service_addr.send(GetGraphData);
-    let node_map_future = state.graph_service_addr.send(GetNodeMap);
-    let physics_state_future = state.graph_service_addr.send(GetPhysicsState);
+    let graph_data_future = state.graph_state_addr.send(GetGraphData);
+    let node_map_future = state.graph_state_addr.send(GetNodeMap);
+    let physics_state_future = state.graph_state_addr.send(GetPhysicsState);

     let (graph_result, node_map_result, physics_result) = tokio::join!(
         graph_data_future,
@@ -185,7 +185,7 @@ pub async fn get_paginated_graph_data(
     // This part is complex due to mutable access.
     // For now, let's assume get_graph_data_mut was for reading and we use GetGraphData.
     // If mutable access is truly needed, specific messages for modifications are required.
-    let graph_result = state.graph_service_addr.send(GetGraphData).await;
+    let graph_result = state.graph_state_addr.send(GetGraphData).await;
     let graph_data_owned = match graph_result { // graph_data_owned is GraphData
         Ok(Ok(g_owned)) => g_owned,
         _ => {
@@ -254,7 +254,7 @@ pub async fn refresh_graph(state: web::Data<AppState>) -> impl Responder {
     info!("Received request to refresh graph - returning current state");

     // Instead of rebuilding, just return the current graph data
-    let graph_data_result = state.graph_service_addr.send(GetGraphData).await;
+    let graph_data_result = state.graph_state_addr.send(GetGraphData).await;

     match graph_data_result {
         Ok(Ok(graph_data_owned)) => {
@@ -340,7 +340,7 @@ pub async fn update_graph(state: web::Data<AppState>) -> impl Responder {
             }

             // Send AddNodesFromMetadata for incremental updates instead of full rebuild
-            match state.graph_service_addr.send(AddNodesFromMetadata { metadata }).await {
+            match state.graph_state_addr.send(AddNodesFromMetadata { metadata }).await {
                 Ok(Ok(())) => {
                     // Position preservation logic would need to be handled by the actor or subsequent messages.
                     debug!("Graph updated successfully via GraphServiceActor after file processing");
@@ -384,8 +384,8 @@ pub async fn get_auto_balance_notifications(
         .and_then(|v| v.as_i64());

     let msg = GetAutoBalanceNotifications { since_timestamp };
-
-    match state.graph_service_addr.send(msg).await {
+
+    match state.graph_state_addr.send(msg).await {
         Ok(Ok(notifications)) => {
             HttpResponse::Ok().json(serde_json::json!({
                 "success": true,
diff --git a/src/handlers/api_handler/mod.rs b/src/handlers/api_handler/mod.rs
index 5adc51c7..215f19e2 100644
--- a/src/handlers/api_handler/mod.rs
+++ b/src/handlers/api_handler/mod.rs
@@ -6,7 +6,10 @@ pub mod analytics;
 pub mod quest3;
 #[cfg(feature = "ontology")]
 pub mod ontology;
+#[cfg(feature = "ontology")]
+pub mod ontology_data;
 pub mod sessions;
+// pub mod settings; // Disabled - incompatible with current SettingsService API

 // Re-export specific types and functions
 // Re-export specific types and functions
@@ -39,16 +42,19 @@ pub fn config(cfg: &mut web::ServiceConfig) {
             .configure(analytics::config)
             .configure(quest3::config)
             .configure(crate::handlers::nostr_handler::config)
-            .configure(crate::handlers::settings_handler::config)
-            .configure(crate::handlers::settings_paths::configure_settings_paths)
+            .configure(crate::handlers::settings_handler::config) // Includes settings_paths via internal configure
+            .configure(settings::configure_routes) // REST API for settings management
             .configure(crate::handlers::ragflow_handler::config)
             .configure(crate::handlers::clustering_handler::config)
             .configure(crate::handlers::constraints_handler::config)
     );

     #[cfg(feature = "ontology")]
-    cfg.service(
-        web::scope("")
-            .configure(ontology::config)
-    );
+    {
+        cfg.service(
+            web::scope("")
+                .configure(ontology::config)
+                .configure(ontology_data::config)
+        );
+    }
 }
diff --git a/src/handlers/api_handler/ontology_data/cache.rs b/src/handlers/api_handler/ontology_data/cache.rs
new file mode 100644
index 00000000..3b62eb7b
--- /dev/null
+++ b/src/handlers/api_handler/ontology_data/cache.rs
@@ -0,0 +1,416 @@
+//! Cache Layer for Ontology Data
+//!
+//! Provides in-memory caching with TTL, LRU eviction, and cache invalidation
+//! for ontology queries, entities, and graph visualization data.
+
+use chrono::{DateTime, Utc};
+use log::{debug, info, warn};
+use lru::LruCache;
+use std::collections::HashMap;
+use std::num::NonZeroUsize;
+use std::sync::{Arc, Mutex};
+use std::time::{Duration, Instant};
+
+use super::QueryResult;
+
+/// Cache entry with TTL
+#[derive(Clone, Debug)]
+struct CacheEntry<T> {
+    data: T,
+    inserted_at: Instant,
+    ttl: Duration,
+    access_count: u64,
+}
+
+impl<T: Clone> CacheEntry<T> {
+    fn new(data: T, ttl: Duration) -> Self {
+        Self {
+            data,
+            inserted_at: Instant::now(),
+            ttl,
+            access_count: 0,
+        }
+    }
+
+    fn is_expired(&self) -> bool {
+        self.inserted_at.elapsed() > self.ttl
+    }
+
+    fn get(&mut self) -> Option<T> {
+        if self.is_expired() {
+            None
+        } else {
+            self.access_count += 1;
+            Some(self.data.clone())
+        }
+    }
+}
+
+/// Cache statistics
+#[derive(Clone, Debug)]
+pub struct CacheStats {
+    pub hits: u64,
+    pub misses: u64,
+    pub evictions: u64,
+    pub size: usize,
+    pub capacity: usize,
+    pub hit_rate: f32,
+}
+
+/// Ontology cache with LRU eviction and TTL
+pub struct OntologyCache {
+    query_cache: Arc<Mutex<LruCache<String, CacheEntry<QueryResult>>>>,
+    entity_cache: Arc<Mutex<LruCache<String, CacheEntry<String>>>>, // Stores JSON strings
+    stats: Arc<Mutex<CacheStats>>,
+    default_ttl: Duration,
+}
+
+impl OntologyCache {
+    /// Create new cache with default configuration
+    pub fn new() -> Self {
+        Self::with_capacity(1000, Duration::from_secs(3600))
+    }
+
+    /// Create cache with specified capacity and TTL
+    pub fn with_capacity(capacity: usize, default_ttl: Duration) -> Self {
+        let cache_size = NonZeroUsize::new(capacity).unwrap();
+
+        Self {
+            query_cache: Arc::new(Mutex::new(LruCache::new(cache_size))),
+            entity_cache: Arc::new(Mutex::new(LruCache::new(cache_size))),
+            stats: Arc::new(Mutex::new(CacheStats {
+                hits: 0,
+                misses: 0,
+                evictions: 0,
+                size: 0,
+                capacity,
+                hit_rate: 0.0,
+            })),
+            default_ttl,
+        }
+    }
+
+    /// Get query result from cache
+    pub fn get_query_result(&self, key: &str) -> Option<QueryResult> {
+        let mut cache = self.query_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        if let Some(entry) = cache.get_mut(key) {
+            if let Some(result) = entry.get() {
+                stats.hits += 1;
+                stats.hit_rate = stats.hits as f32 / (stats.hits + stats.misses) as f32;
+                debug!("Cache hit for query: {}", key);
+                return Some(result);
+            } else {
+                // Entry expired, remove it
+                cache.pop(key);
+                stats.evictions += 1;
+            }
+        }
+
+        stats.misses += 1;
+        stats.hit_rate = stats.hits as f32 / (stats.hits + stats.misses) as f32;
+        debug!("Cache miss for query: {}", key);
+        None
+    }
+
+    /// Set query result in cache
+    pub fn set_query_result(&self, key: &str, result: &QueryResult) {
+        let mut cache = self.query_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        let entry = CacheEntry::new(result.clone(), self.default_ttl);
+
+        // Check if we're evicting an entry
+        if cache.len() >= cache.cap().get() {
+            stats.evictions += 1;
+        }
+
+        cache.put(key.to_string(), entry);
+        stats.size = cache.len();
+
+        debug!("Cached query result: {}", key);
+    }
+
+    /// Get entity from cache
+    pub fn get_entity(&self, entity_id: &str) -> Option<String> {
+        let mut cache = self.entity_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        if let Some(entry) = cache.get_mut(entity_id) {
+            if let Some(data) = entry.get() {
+                stats.hits += 1;
+                stats.hit_rate = stats.hits as f32 / (stats.hits + stats.misses) as f32;
+                debug!("Cache hit for entity: {}", entity_id);
+                return Some(data);
+            } else {
+                // Entry expired, remove it
+                cache.pop(entity_id);
+                stats.evictions += 1;
+            }
+        }
+
+        stats.misses += 1;
+        stats.hit_rate = stats.hits as f32 / (stats.hits + stats.misses) as f32;
+        debug!("Cache miss for entity: {}", entity_id);
+        None
+    }
+
+    /// Set entity in cache
+    pub fn set_entity(&self, entity_id: &str, data: &str) {
+        let mut cache = self.entity_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        let entry = CacheEntry::new(data.to_string(), self.default_ttl);
+
+        // Check if we're evicting an entry
+        if cache.len() >= cache.cap().get() {
+            stats.evictions += 1;
+        }
+
+        cache.put(entity_id.to_string(), entry);
+        stats.size = cache.len();
+
+        debug!("Cached entity: {}", entity_id);
+    }
+
+    /// Invalidate specific cache key
+    pub fn invalidate(&self, key: &str) {
+        let mut query_cache = self.query_cache.lock().unwrap();
+        let mut entity_cache = self.entity_cache.lock().unwrap();
+
+        query_cache.pop(key);
+        entity_cache.pop(key);
+
+        info!("Invalidated cache key: {}", key);
+    }
+
+    /// Invalidate all entries matching a pattern
+    pub fn invalidate_pattern(&self, pattern: &str) {
+        let mut query_cache = self.query_cache.lock().unwrap();
+        let mut entity_cache = self.entity_cache.lock().unwrap();
+
+        let query_keys: Vec<String> = query_cache
+            .iter()
+            .filter(|(k, _)| k.contains(pattern))
+            .map(|(k, _)| k.clone())
+            .collect();
+
+        let entity_keys: Vec<String> = entity_cache
+            .iter()
+            .filter(|(k, _)| k.contains(pattern))
+            .map(|(k, _)| k.clone())
+            .collect();
+
+        let total_invalidated = query_keys.len() + entity_keys.len();
+
+        for key in query_keys {
+            query_cache.pop(&key);
+        }
+
+        for key in entity_keys {
+            entity_cache.pop(&key);
+        }
+
+        info!("Invalidated {} cache entries matching pattern: {}",
+            total_invalidated, pattern);
+    }
+
+    /// Clear all cache entries
+    pub fn clear(&self) {
+        let mut query_cache = self.query_cache.lock().unwrap();
+        let mut entity_cache = self.entity_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        let total_cleared = query_cache.len() + entity_cache.len();
+
+        query_cache.clear();
+        entity_cache.clear();
+
+        stats.size = 0;
+        stats.evictions += total_cleared as u64;
+
+        info!("Cleared {} cache entries", total_cleared);
+    }
+
+    /// Remove expired entries
+    pub fn evict_expired(&self) {
+        let mut query_cache = self.query_cache.lock().unwrap();
+        let mut entity_cache = self.entity_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        let mut evicted = 0;
+
+        // Collect expired query keys
+        let expired_query_keys: Vec<String> = query_cache
+            .iter()
+            .filter(|(_, entry)| entry.is_expired())
+            .map(|(k, _)| k.clone())
+            .collect();
+
+        // Collect expired entity keys
+        let expired_entity_keys: Vec<String> = entity_cache
+            .iter()
+            .filter(|(_, entry)| entry.is_expired())
+            .map(|(k, _)| k.clone())
+            .collect();
+
+        // Remove expired entries
+        for key in expired_query_keys {
+            query_cache.pop(&key);
+            evicted += 1;
+        }
+
+        for key in expired_entity_keys {
+            entity_cache.pop(&key);
+            evicted += 1;
+        }
+
+        stats.evictions += evicted;
+        stats.size = query_cache.len() + entity_cache.len();
+
+        if evicted > 0 {
+            debug!("Evicted {} expired cache entries", evicted);
+        }
+    }
+
+    /// Get cache statistics
+    pub fn get_stats(&self) -> CacheStats {
+        let stats = self.stats.lock().unwrap();
+        stats.clone()
+    }
+
+    /// Reset statistics
+    pub fn reset_stats(&self) {
+        let mut stats = self.stats.lock().unwrap();
+        stats.hits = 0;
+        stats.misses = 0;
+        stats.evictions = 0;
+        stats.hit_rate = 0.0;
+        info!("Reset cache statistics");
+    }
+}
+
+impl Default for OntologyCache {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+
+/// Background task to periodically evict expired entries
+pub fn start_cache_eviction_task(cache: Arc<OntologyCache>) {
+    use std::thread;
+
+    thread::spawn(move || {
+        loop {
+            thread::sleep(Duration::from_secs(300)); // Every 5 minutes
+            cache.evict_expired();
+        }
+    });
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use chrono::Utc;
+    use uuid::Uuid;
+
+    #[test]
+    fn test_cache_basic_operations() {
+        let cache = OntologyCache::new();
+
+        let query_result = QueryResult {
+            query_id: Uuid::new_v4().to_string(),
+            results: vec![],
+            total_count: 0,
+            execution_time_ms: 100,
+            execution_plan: None,
+            from_cache: false,
+            timestamp: Utc::now(),
+        };
+
+        // Set and get
+        cache.set_query_result("test_query", &query_result);
+        let retrieved = cache.get_query_result("test_query");
+        assert!(retrieved.is_some());
+
+        // Cache miss
+        let missing = cache.get_query_result("nonexistent");
+        assert!(missing.is_none());
+    }
+
+    #[test]
+    fn test_cache_expiration() {
+        let cache = OntologyCache::with_capacity(100, Duration::from_millis(100));
+
+        let query_result = QueryResult {
+            query_id: Uuid::new_v4().to_string(),
+            results: vec![],
+            total_count: 0,
+            execution_time_ms: 100,
+            execution_plan: None,
+            from_cache: false,
+            timestamp: Utc::now(),
+        };
+
+        cache.set_query_result("expiring_query", &query_result);
+
+        // Should exist immediately
+        assert!(cache.get_query_result("expiring_query").is_some());
+
+        // Wait for expiration
+        std::thread::sleep(Duration::from_millis(150));
+
+        // Should be expired
+        assert!(cache.get_query_result("expiring_query").is_none());
+    }
+
+    #[test]
+    fn test_cache_stats() {
+        let cache = OntologyCache::new();
+
+        let query_result = QueryResult {
+            query_id: Uuid::new_v4().to_string(),
+            results: vec![],
+            total_count: 0,
+            execution_time_ms: 100,
+            execution_plan: None,
+            from_cache: false,
+            timestamp: Utc::now(),
+        };
+
+        cache.set_query_result("test", &query_result);
+
+        // Hit
+        cache.get_query_result("test");
+
+        // Miss
+        cache.get_query_result("nonexistent");
+
+        let stats = cache.get_stats();
+        assert_eq!(stats.hits, 1);
+        assert_eq!(stats.misses, 1);
+        assert_eq!(stats.hit_rate, 0.5);
+    }
+
+    #[test]
+    fn test_cache_invalidation() {
+        let cache = OntologyCache::new();
+
+        let query_result = QueryResult {
+            query_id: Uuid::new_v4().to_string(),
+            results: vec![],
+            total_count: 0,
+            execution_time_ms: 100,
+            execution_plan: None,
+            from_cache: false,
+            timestamp: Utc::now(),
+        };
+
+        cache.set_query_result("test", &query_result);
+        assert!(cache.get_query_result("test").is_some());
+
+        cache.invalidate("test");
+        assert!(cache.get_query_result("test").is_none());
+    }
+}
diff --git a/src/handlers/api_handler/ontology_data/db.rs b/src/handlers/api_handler/ontology_data/db.rs
new file mode 100644
index 00000000..caee55bd
--- /dev/null
+++ b/src/handlers/api_handler/ontology_data/db.rs
@@ -0,0 +1,528 @@
+//! SQLite Database Layer for Ontology Data
+//!
+//! Provides persistent storage and querying for ontology metadata, domains, classes,
+//! properties, entities, and relationships. Uses SQLite for lightweight, embedded storage
+//! with full-text search and indexing capabilities.
+
+use chrono::{DateTime, Utc};
+use log::{debug, error, info, warn};
+use serde_json::Value as JsonValue;
+use std::collections::HashMap;
+use std::path::PathBuf;
+use std::sync::{Arc, Mutex};
+
+use super::{
+    CardinalityConstraint, ClassInfo, DomainInfo, EntityInfo, GraphEdge, GraphNode,
+    GraphVisualizationData, PropertyInfo, RelationshipInfo,
+};
+
+/// Ontology database connection
+pub struct OntologyDatabase {
+    db_path: PathBuf,
+    conn: Arc<Mutex<Option<()>>>, // Placeholder for actual SQLite connection
+}
+
+impl OntologyDatabase {
+    /// Create new database instance
+    pub fn new() -> Result<Self, String> {
+        let db_path = Self::get_db_path();
+
+        info!("Initializing ontology database at: {:?}", db_path);
+
+        // Ensure directory exists
+        if let Some(parent) = db_path.parent() {
+            std::fs::create_dir_all(parent)
+                .map_err(|e| format!("Failed to create database directory: {}", e))?;
+        }
+
+        let db = Self {
+            db_path,
+            conn: Arc::new(Mutex::new(None)),
+        };
+
+        db.initialize_schema()?;
+
+        Ok(db)
+    }
+
+    /// Get database file path
+    fn get_db_path() -> PathBuf {
+        let mut path = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
+        path.push(".data");
+        path.push("ontology.db");
+        path
+    }
+
+    /// Initialize database schema
+    fn initialize_schema(&self) -> Result<(), String> {
+        debug!("Initializing database schema");
+
+        // In a real implementation, this would execute SQL to create tables:
+        // - domains (id, name, description, namespace, class_count, property_count, updated_at)
+        // - classes (id, name, description, domain_id, namespace, instance_count, created_at)
+        // - class_hierarchy (parent_id, child_id)
+        // - properties (id, name, description, property_type, domain_id, is_functional, is_inverse_functional, is_transitive, is_symmetric)
+        // - property_domain_constraints (property_id, class_id)
+        // - property_range_constraints (property_id, class_id)
+        // - property_cardinality (property_id, min_cardinality, max_cardinality, exact_cardinality)
+        // - entities (id, label, entity_type, domain_id, properties_json, created_at, updated_at)
+        // - relationships (id, source_id, target_id, relationship_type, properties_json, is_inferred, confidence)
+        // - entity_fts (for full-text search on entity labels)
+
+        // Placeholder: In production, use rusqlite or another SQLite library
+        info!("Database schema initialized (mock implementation)");
+
+        Ok(())
+    }
+
+    /// List all domains
+    pub fn list_domains(
+        &self,
+        filter: Option<&str>,
+        include_stats: bool,
+    ) -> Result<Vec<DomainInfo>, String> {
+        debug!("Listing domains: filter={:?}, include_stats={}", filter, include_stats);
+
+        // Mock data for demonstration
+        let domains = vec![
+            DomainInfo {
+                id: "etsi-nfv".to_string(),
+                name: "ETSI NFV".to_string(),
+                description: "Network Functions Virtualization domain".to_string(),
+                class_count: 125,
+                property_count: 250,
+                namespace: "http://etsi.org/nfv#".to_string(),
+                updated_at: Utc::now(),
+            },
+            DomainInfo {
+                id: "etsi-mec".to_string(),
+                name: "ETSI MEC".to_string(),
+                description: "Multi-access Edge Computing domain".to_string(),
+                class_count: 85,
+                property_count: 175,
+                namespace: "http://etsi.org/mec#".to_string(),
+                updated_at: Utc::now(),
+            },
+            DomainInfo {
+                id: "etsi-core".to_string(),
+                name: "ETSI Core".to_string(),
+                description: "Core ETSI ontology concepts".to_string(),
+                class_count: 50,
+                property_count: 100,
+                namespace: "http://etsi.org/core#".to_string(),
+                updated_at: Utc::now(),
+            },
+        ];
+
+        // Apply filter if provided
+        let filtered = if let Some(filter_str) = filter {
+            domains
+                .into_iter()
+                .filter(|d| {
+                    d.name.to_lowercase().contains(&filter_str.to_lowercase())
+                        || d.id.to_lowercase().contains(&filter_str.to_lowercase())
+                })
+                .collect()
+        } else {
+            domains
+        };
+
+        Ok(filtered)
+    }
+
+    /// List ontology classes
+    pub fn list_classes(
+        &self,
+        domain: Option<&str>,
+        filter: Option<&str>,
+        include_subclasses: bool,
+        include_properties: bool,
+        offset: u32,
+        limit: u32,
+    ) -> Result<(Vec<ClassInfo>, usize), String> {
+        debug!(
+            "Listing classes: domain={:?}, filter={:?}, offset={}, limit={}",
+            domain, filter, offset, limit
+        );
+
+        // Mock data for demonstration
+        let mut classes = vec![
+            ClassInfo {
+                id: "vnf".to_string(),
+                name: "VirtualNetworkFunction".to_string(),
+                description: Some("A virtualized network function".to_string()),
+                parent_classes: vec!["network-function".to_string()],
+                child_classes: if include_subclasses {
+                    vec!["vnfc".to_string(), "vnf-instance".to_string()]
+                } else {
+                    vec![]
+                },
+                domain: "etsi-nfv".to_string(),
+                properties: if include_properties {
+                    vec![PropertyInfo {
+                        id: "has-vnfc".to_string(),
+                        name: "hasVNFC".to_string(),
+                        description: Some("VNF has VNFC components".to_string()),
+                        property_type: "object_property".to_string(),
+                        domain_classes: vec!["vnf".to_string()],
+                        range_classes: vec!["vnfc".to_string()],
+                        is_functional: false,
+                        is_inverse_functional: false,
+                        is_transitive: false,
+                        is_symmetric: false,
+                        cardinality: Some(CardinalityConstraint {
+                            min: Some(1),
+                            max: None,
+                            exact: None,
+                        }),
+                        domain: "etsi-nfv".to_string(),
+                    }]
+                } else {
+                    vec![]
+                },
+                instance_count: 42,
+                namespace: "http://etsi.org/nfv#".to_string(),
+            },
+            ClassInfo {
+                id: "mec-application".to_string(),
+                name: "MECApplication".to_string(),
+                description: Some("An application running at the edge".to_string()),
+                parent_classes: vec!["application".to_string()],
+                child_classes: if include_subclasses {
+                    vec!["mec-service".to_string()]
+                } else {
+                    vec![]
+                },
+                domain: "etsi-mec".to_string(),
+                properties: if include_properties {
+                    vec![PropertyInfo {
+                        id: "has-endpoint".to_string(),
+                        name: "hasEndpoint".to_string(),
+                        description: Some("Application service endpoint".to_string()),
+                        property_type: "data_property".to_string(),
+                        domain_classes: vec!["mec-application".to_string()],
+                        range_classes: vec!["xsd:string".to_string()],
+                        is_functional: false,
+                        is_inverse_functional: false,
+                        is_transitive: false,
+                        is_symmetric: false,
+                        cardinality: None,
+                        domain: "etsi-mec".to_string(),
+                    }]
+                } else {
+                    vec![]
+                },
+                instance_count: 28,
+                namespace: "http://etsi.org/mec#".to_string(),
+            },
+        ];
+
+        // Apply domain filter
+        if let Some(domain_filter) = domain {
+            classes.retain(|c| c.domain == domain_filter);
+        }
+
+        // Apply name filter
+        if let Some(filter_str) = filter {
+            classes.retain(|c| {
+                c.name.to_lowercase().contains(&filter_str.to_lowercase())
+                    || c.id.to_lowercase().contains(&filter_str.to_lowercase())
+            });
+        }
+
+        let total_count = classes.len();
+
+        // Apply pagination
+        let start = offset as usize;
+        let end = (start + limit as usize).min(total_count);
+        let paginated = if start < total_count {
+            classes[start..end].to_vec()
+        } else {
+            vec![]
+        };
+
+        Ok((paginated, total_count))
+    }
+
+    /// List ontology properties
+    pub fn list_properties(
+        &self,
+        domain: Option<&str>,
+        filter: Option<&str>,
+        property_type: Option<&str>,
+        include_constraints: bool,
+        offset: u32,
+        limit: u32,
+    ) -> Result<(Vec<PropertyInfo>, usize), String> {
+        debug!(
+            "Listing properties: domain={:?}, type={:?}, offset={}, limit={}",
+            domain, property_type, offset, limit
+        );
+
+        // Mock data for demonstration
+        let mut properties = vec![
+            PropertyInfo {
+                id: "has-vnfc".to_string(),
+                name: "hasVNFC".to_string(),
+                description: Some("VNF has VNFC components".to_string()),
+                property_type: "object_property".to_string(),
+                domain_classes: vec!["vnf".to_string()],
+                range_classes: vec!["vnfc".to_string()],
+                is_functional: false,
+                is_inverse_functional: false,
+                is_transitive: false,
+                is_symmetric: false,
+                cardinality: if include_constraints {
+                    Some(CardinalityConstraint {
+                        min: Some(1),
+                        max: None,
+                        exact: None,
+                    })
+                } else {
+                    None
+                },
+                domain: "etsi-nfv".to_string(),
+            },
+            PropertyInfo {
+                id: "deployment-status".to_string(),
+                name: "deploymentStatus".to_string(),
+                description: Some("Current deployment status".to_string()),
+                property_type: "data_property".to_string(),
+                domain_classes: vec!["vnf".to_string(), "mec-application".to_string()],
+                range_classes: vec!["xsd:string".to_string()],
+                is_functional: true,
+                is_inverse_functional: false,
+                is_transitive: false,
+                is_symmetric: false,
+                cardinality: if include_constraints {
+                    Some(CardinalityConstraint {
+                        min: None,
+                        max: Some(1),
+                        exact: None,
+                    })
+                } else {
+                    None
+                },
+                domain: "etsi-core".to_string(),
+            },
+            PropertyInfo {
+                id: "connected-to".to_string(),
+                name: "connectedTo".to_string(),
+                description: Some("Network connectivity relationship".to_string()),
+                property_type: "object_property".to_string(),
+                domain_classes: vec!["network-function".to_string()],
+                range_classes: vec!["network-function".to_string()],
+                is_functional: false,
+                is_inverse_functional: false,
+                is_transitive: false,
+                is_symmetric: true,
+                cardinality: None,
+                domain: "etsi-nfv".to_string(),
+            },
+        ];
+
+        // Apply filters
+        if let Some(domain_filter) = domain {
+            properties.retain(|p| p.domain == domain_filter);
+        }
+
+        if let Some(type_filter) = property_type {
+            properties.retain(|p| p.property_type == type_filter);
+        }
+
+        if let Some(filter_str) = filter {
+            properties.retain(|p| {
+                p.name.to_lowercase().contains(&filter_str.to_lowercase())
+                    || p.id.to_lowercase().contains(&filter_str.to_lowercase())
+            });
+        }
+
+        let total_count = properties.len();
+
+        // Apply pagination
+        let start = offset as usize;
+        let end = (start + limit as usize).min(total_count);
+        let paginated = if start < total_count {
+            properties[start..end].to_vec()
+        } else {
+            vec![]
+        };
+
+        Ok((paginated, total_count))
+    }
+
+    /// Get entity by ID with relationships
+    pub fn get_entity(
+        &self,
+        entity_id: &str,
+        include_incoming: bool,
+        include_outgoing: bool,
+        include_inferred: bool,
+        max_depth: u32,
+    ) -> Result<Option<EntityInfo>, String> {
+        debug!("Getting entity: {}, depth={}", entity_id, max_depth);
+
+        // Mock data for demonstration
+        if entity_id == "vnf-123" {
+            let mut properties = HashMap::new();
+            properties.insert("label".to_string(), serde_json::json!("Example VNF Instance"));
+            properties.insert("deploymentStatus".to_string(), serde_json::json!("deployed"));
+            properties.insert("version".to_string(), serde_json::json!("1.2.3"));
+
+            Ok(Some(EntityInfo {
+                id: entity_id.to_string(),
+                label: "Example VNF Instance".to_string(),
+                entity_type: "vnf".to_string(),
+                properties,
+                incoming_relationships: if include_incoming {
+                    vec![RelationshipInfo {
+                        id: "rel-1".to_string(),
+                        source_id: "vnf-manager-1".to_string(),
+                        target_id: entity_id.to_string(),
+                        relationship_type: "manages".to_string(),
+                        properties: HashMap::new(),
+                        is_inferred: false,
+                        confidence: None,
+                    }]
+                } else {
+                    vec![]
+                },
+                outgoing_relationships: if include_outgoing {
+                    vec![
+                        RelationshipInfo {
+                            id: "rel-2".to_string(),
+                            source_id: entity_id.to_string(),
+                            target_id: "vnfc-456".to_string(),
+                            relationship_type: "hasVNFC".to_string(),
+                            properties: HashMap::new(),
+                            is_inferred: false,
+                            confidence: None,
+                        },
+                        RelationshipInfo {
+                            id: "rel-3".to_string(),
+                            source_id: entity_id.to_string(),
+                            target_id: "vnf-789".to_string(),
+                            relationship_type: "connectedTo".to_string(),
+                            properties: HashMap::new(),
+                            is_inferred: false,
+                            confidence: None,
+                        },
+                    ]
+                } else {
+                    vec![]
+                },
+                inferred_relationships: if include_inferred {
+                    vec![RelationshipInfo {
+                        id: "rel-inferred-1".to_string(),
+                        source_id: entity_id.to_string(),
+                        target_id: "network-segment-1".to_string(),
+                        relationship_type: "deployedOn".to_string(),
+                        properties: HashMap::new(),
+                        is_inferred: true,
+                        confidence: Some(0.85),
+                    }]
+                } else {
+                    vec![]
+                },
+                related_entities: vec![
+                    "vnf-manager-1".to_string(),
+                    "vnfc-456".to_string(),
+                    "vnf-789".to_string(),
+                ],
+                domain: "etsi-nfv".to_string(),
+                created_at: Utc::now() - chrono::Duration::days(30),
+                updated_at: Utc::now() - chrono::Duration::hours(2),
+            }))
+        } else {
+            Ok(None)
+        }
+    }
+
+    /// Get graph visualization data
+    pub fn get_graph_visualization(
+        &self,
+        domain_filter: Option<&str>,
+        max_nodes: u32,
+    ) -> Result<GraphVisualizationData, String> {
+        debug!("Getting graph visualization: domain={:?}, max_nodes={}", domain_filter, max_nodes);
+
+        // Mock data for demonstration
+        let nodes = vec![
+            GraphNode {
+                id: "vnf-123".to_string(),
+                label: "Example VNF".to_string(),
+                node_type: "vnf".to_string(),
+                domain: "etsi-nfv".to_string(),
+                size: 1.5,
+                color: "#4a90e2".to_string(),
+                metadata: {
+                    let mut m = HashMap::new();
+                    m.insert("status".to_string(), serde_json::json!("deployed"));
+                    m
+                },
+            },
+            GraphNode {
+                id: "vnfc-456".to_string(),
+                label: "VNF Component".to_string(),
+                node_type: "vnfc".to_string(),
+                domain: "etsi-nfv".to_string(),
+                size: 1.0,
+                color: "#7cb342".to_string(),
+                metadata: HashMap::new(),
+            },
+            GraphNode {
+                id: "mec-app-789".to_string(),
+                label: "Edge Application".to_string(),
+                node_type: "mec-application".to_string(),
+                domain: "etsi-mec".to_string(),
+                size: 1.2,
+                color: "#fb8c00".to_string(),
+                metadata: HashMap::new(),
+            },
+        ];
+
+        let edges = vec![
+            GraphEdge {
+                id: "edge-1".to_string(),
+                source: "vnf-123".to_string(),
+                target: "vnfc-456".to_string(),
+                edge_type: "hasVNFC".to_string(),
+                label: "has component".to_string(),
+                is_inferred: false,
+                weight: 1.0,
+                metadata: HashMap::new(),
+            },
+            GraphEdge {
+                id: "edge-2".to_string(),
+                source: "vnf-123".to_string(),
+                target: "mec-app-789".to_string(),
+                edge_type: "connectedTo".to_string(),
+                label: "connected".to_string(),
+                is_inferred: true,
+                weight: 0.7,
+                metadata: {
+                    let mut m = HashMap::new();
+                    m.insert("confidence".to_string(), serde_json::json!(0.85));
+                    m
+                },
+            },
+        ];
+
+        let mut metadata = HashMap::new();
+        metadata.insert("generated_at".to_string(), serde_json::json!(Utc::now()));
+        metadata.insert("node_count".to_string(), serde_json::json!(nodes.len()));
+        metadata.insert("edge_count".to_string(), serde_json::json!(edges.len()));
+
+        Ok(GraphVisualizationData {
+            nodes,
+            edges,
+            metadata,
+        })
+    }
+}
+
+impl Default for OntologyDatabase {
+    fn default() -> Self {
+        Self::new().expect("Failed to create default OntologyDatabase")
+    }
+}
diff --git a/src/handlers/api_handler/ontology_data/mod.rs b/src/handlers/api_handler/ontology_data/mod.rs
new file mode 100644
index 00000000..d911b54d
--- /dev/null
+++ b/src/handlers/api_handler/ontology_data/mod.rs
@@ -0,0 +1,933 @@
+//! Ontology Data Exposure API
+//!
+//! REST endpoints and WebSocket handlers for exposing ontology data to clients.
+//! Provides domain listings, class hierarchies, property schemas, entity relationships,
+//! advanced query interface, real-time updates, and graph visualization integration.
+//!
+//! ## Endpoints
+//! - GET /api/ontology/domains - List all ETSI domains
+//! - GET /api/ontology/classes - List ontology classes with filters
+//! - GET /api/ontology/properties - List properties with schemas
+//! - GET /api/ontology/entities/:id - Get specific entity with relationships
+//! - POST /api/ontology/query - Advanced query interface
+//! - WebSocket /api/ontology/stream - Real-time ontology updates
+//!
+//! ## Features
+//! - SQLite-based caching for performance
+//! - Integration with graph visualization
+//! - Real-time WebSocket updates
+//! - Comprehensive error handling and validation
+
+use actix::prelude::*;
+use actix_web::{web, HttpRequest, HttpResponse, Responder, Error as ActixError};
+use actix_web_actors::ws;
+use chrono::{DateTime, Utc};
+use log::{info, debug, error, warn};
+use serde::{Deserialize, Serialize};
+use std::collections::HashMap;
+use std::time::Duration as StdDuration;
+use uuid::Uuid;
+
+mod db;
+mod cache;
+mod query;
+
+use crate::AppState;
+use crate::actors::messages::{
+    GetOntologyHealth, OntologyHealth, GetCachedOntologies, CachedOntologyInfo,
+    ValidateOntology, ValidationMode, GetOntologyReport
+};
+use crate::services::owl_validator::PropertyGraph;
+
+// Re-export submodules
+pub use db::OntologyDatabase;
+pub use cache::OntologyCache;
+pub use query::QueryEngine;
+
+// ============================================================================
+// REQUEST/RESPONSE DTOs
+// ============================================================================
+
+/// Request to list ETSI domains
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ListDomainsRequest {
+    /// Filter by domain name pattern
+    pub filter: Option<String>,
+    /// Include domain statistics
+    pub include_stats: Option<bool>,
+}
+
+/// ETSI domain information
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct DomainInfo {
+    /// Domain identifier (e.g., "etsi-nfv", "etsi-mec")
+    pub id: String,
+    /// Human-readable domain name
+    pub name: String,
+    /// Domain description
+    pub description: String,
+    /// Number of classes in domain
+    pub class_count: u32,
+    /// Number of properties in domain
+    pub property_count: u32,
+    /// Domain namespace URI
+    pub namespace: String,
+    /// Last updated timestamp
+    pub updated_at: DateTime<Utc>,
+}
+
+/// Response for domain listing
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct DomainsResponse {
+    pub domains: Vec<DomainInfo>,
+    pub total_count: usize,
+    pub timestamp: DateTime<Utc>,
+}
+
+/// Request to list ontology classes
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ListClassesRequest {
+    /// Filter by domain
+    pub domain: Option<String>,
+    /// Filter by class name pattern
+    pub filter: Option<String>,
+    /// Include subclasses
+    pub include_subclasses: Option<bool>,
+    /// Include properties
+    pub include_properties: Option<bool>,
+    /// Pagination offset
+    pub offset: Option<u32>,
+    /// Pagination limit
+    pub limit: Option<u32>,
+}
+
+/// Ontology class information
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ClassInfo {
+    /// Class identifier
+    pub id: String,
+    /// Class name
+    pub name: String,
+    /// Class description
+    pub description: Option<String>,
+    /// Parent class IDs
+    pub parent_classes: Vec<String>,
+    /// Child class IDs
+    pub child_classes: Vec<String>,
+    /// Domain this class belongs to
+    pub domain: String,
+    /// Class properties
+    pub properties: Vec<PropertyInfo>,
+    /// Number of instances in graph
+    pub instance_count: u32,
+    /// Class namespace URI
+    pub namespace: String,
+}
+
+/// Response for class listing
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ClassesResponse {
+    pub classes: Vec<ClassInfo>,
+    pub total_count: usize,
+    pub offset: u32,
+    pub limit: u32,
+    pub timestamp: DateTime<Utc>,
+}
+
+/// Request to list ontology properties
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ListPropertiesRequest {
+    /// Filter by domain
+    pub domain: Option<String>,
+    /// Filter by property name pattern
+    pub filter: Option<String>,
+    /// Filter by property type (object_property, data_property, annotation_property)
+    pub property_type: Option<String>,
+    /// Include domain/range constraints
+    pub include_constraints: Option<bool>,
+    /// Pagination offset
+    pub offset: Option<u32>,
+    /// Pagination limit
+    pub limit: Option<u32>,
+}
+
+/// Ontology property information
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct PropertyInfo {
+    /// Property identifier
+    pub id: String,
+    /// Property name
+    pub name: String,
+    /// Property description
+    pub description: Option<String>,
+    /// Property type (object_property, data_property, annotation_property)
+    pub property_type: String,
+    /// Domain constraint (classes this property applies to)
+    pub domain_classes: Vec<String>,
+    /// Range constraint (target classes or data types)
+    pub range_classes: Vec<String>,
+    /// Whether property is functional
+    pub is_functional: bool,
+    /// Whether property is inverse functional
+    pub is_inverse_functional: bool,
+    /// Whether property is transitive
+    pub is_transitive: bool,
+    /// Whether property is symmetric
+    pub is_symmetric: bool,
+    /// Cardinality constraints
+    pub cardinality: Option<CardinalityConstraint>,
+    /// Domain this property belongs to
+    pub domain: String,
+}
+
+/// Cardinality constraint for properties
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct CardinalityConstraint {
+    /// Minimum cardinality
+    pub min: Option<u32>,
+    /// Maximum cardinality
+    pub max: Option<u32>,
+    /// Exact cardinality
+    pub exact: Option<u32>,
+}
+
+/// Response for property listing
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct PropertiesResponse {
+    pub properties: Vec<PropertyInfo>,
+    pub total_count: usize,
+    pub offset: u32,
+    pub limit: u32,
+    pub timestamp: DateTime<Utc>,
+}
+
+/// Request to get entity details
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct GetEntityRequest {
+    /// Include incoming relationships
+    pub include_incoming: Option<bool>,
+    /// Include outgoing relationships
+    pub include_outgoing: Option<bool>,
+    /// Include inferred relationships
+    pub include_inferred: Option<bool>,
+    /// Maximum relationship depth
+    pub max_depth: Option<u32>,
+}
+
+/// Entity information with relationships
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct EntityInfo {
+    /// Entity identifier
+    pub id: String,
+    /// Entity label
+    pub label: String,
+    /// Entity type (class)
+    pub entity_type: String,
+    /// Entity properties
+    pub properties: HashMap<String, serde_json::Value>,
+    /// Incoming relationships
+    pub incoming_relationships: Vec<RelationshipInfo>,
+    /// Outgoing relationships
+    pub outgoing_relationships: Vec<RelationshipInfo>,
+    /// Inferred relationships
+    pub inferred_relationships: Vec<RelationshipInfo>,
+    /// Related entities (neighbors)
+    pub related_entities: Vec<String>,
+    /// Domain this entity belongs to
+    pub domain: String,
+    /// Creation timestamp
+    pub created_at: DateTime<Utc>,
+    /// Last updated timestamp
+    pub updated_at: DateTime<Utc>,
+}
+
+/// Relationship information
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct RelationshipInfo {
+    /// Relationship identifier
+    pub id: String,
+    /// Source entity ID
+    pub source_id: String,
+    /// Target entity ID
+    pub target_id: String,
+    /// Relationship type (property)
+    pub relationship_type: String,
+    /// Relationship properties
+    pub properties: HashMap<String, serde_json::Value>,
+    /// Whether this is an inferred relationship
+    pub is_inferred: bool,
+    /// Inference confidence (0.0 - 1.0)
+    pub confidence: Option<f32>,
+}
+
+/// Advanced query request
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct QueryRequest {
+    /// SPARQL-like query string
+    pub query: String,
+    /// Query parameters
+    pub parameters: Option<HashMap<String, serde_json::Value>>,
+    /// Maximum results
+    pub limit: Option<u32>,
+    /// Result offset
+    pub offset: Option<u32>,
+    /// Include query execution plan
+    pub explain: Option<bool>,
+    /// Query timeout in seconds
+    pub timeout_seconds: Option<u32>,
+}
+
+/// Query execution result
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct QueryResult {
+    /// Query execution ID
+    pub query_id: String,
+    /// Result rows
+    pub results: Vec<HashMap<String, serde_json::Value>>,
+    /// Total result count
+    pub total_count: usize,
+    /// Execution time in milliseconds
+    pub execution_time_ms: u64,
+    /// Query execution plan (if requested)
+    pub execution_plan: Option<String>,
+    /// Whether query was served from cache
+    pub from_cache: bool,
+    /// Timestamp
+    pub timestamp: DateTime<Utc>,
+}
+
+/// Graph visualization nodes and edges
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct GraphVisualizationData {
+    /// Graph nodes
+    pub nodes: Vec<GraphNode>,
+    /// Graph edges
+    pub edges: Vec<GraphEdge>,
+    /// Metadata
+    pub metadata: HashMap<String, serde_json::Value>,
+}
+
+/// Graph node for visualization
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct GraphNode {
+    pub id: String,
+    pub label: String,
+    pub node_type: String,
+    pub domain: String,
+    pub size: f32,
+    pub color: String,
+    pub metadata: HashMap<String, serde_json::Value>,
+}
+
+/// Graph edge for visualization
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct GraphEdge {
+    pub id: String,
+    pub source: String,
+    pub target: String,
+    pub edge_type: String,
+    pub label: String,
+    pub is_inferred: bool,
+    pub weight: f32,
+    pub metadata: HashMap<String, serde_json::Value>,
+}
+
+/// WebSocket update message types
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase", tag = "type")]
+pub enum OntologyUpdate {
+    /// Ontology loaded
+    OntologyLoaded {
+        ontology_id: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Validation started
+    ValidationStarted {
+        job_id: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Validation progress
+    ValidationProgress {
+        job_id: String,
+        progress: f32,
+        current_step: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Validation completed
+    ValidationCompleted {
+        job_id: String,
+        report_id: String,
+        violations_count: u32,
+        timestamp: DateTime<Utc>,
+    },
+    /// Validation failed
+    ValidationFailed {
+        job_id: String,
+        error: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Entity added
+    EntityAdded {
+        entity_id: String,
+        entity_type: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Entity updated
+    EntityUpdated {
+        entity_id: String,
+        changes: HashMap<String, serde_json::Value>,
+        timestamp: DateTime<Utc>,
+    },
+    /// Entity removed
+    EntityRemoved {
+        entity_id: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Relationship added
+    RelationshipAdded {
+        relationship_id: String,
+        source_id: String,
+        target_id: String,
+        relationship_type: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Cache cleared
+    CacheCleared {
+        timestamp: DateTime<Utc>,
+    },
+    /// Health status update
+    HealthUpdate {
+        health: OntologyHealthDto,
+        timestamp: DateTime<Utc>,
+    },
+}
+
+/// DTO for ontology health
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct OntologyHealthDto {
+    pub loaded_ontologies: u32,
+    pub cached_reports: u32,
+    pub validation_queue_size: u32,
+    pub last_validation: Option<DateTime<Utc>>,
+    pub cache_hit_rate: f32,
+    pub avg_validation_time_ms: f32,
+    pub active_jobs: u32,
+    pub memory_usage_mb: f32,
+}
+
+impl From<OntologyHealth> for OntologyHealthDto {
+    fn from(health: OntologyHealth) -> Self {
+        OntologyHealthDto {
+            loaded_ontologies: health.loaded_ontologies,
+            cached_reports: health.cached_reports,
+            validation_queue_size: health.validation_queue_size,
+            last_validation: health.last_validation,
+            cache_hit_rate: health.cache_hit_rate,
+            avg_validation_time_ms: health.avg_validation_time_ms,
+            active_jobs: health.active_jobs,
+            memory_usage_mb: health.memory_usage_mb,
+        }
+    }
+}
+
+/// Error response structure
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ErrorResponse {
+    pub error: String,
+    pub code: String,
+    pub details: Option<HashMap<String, serde_json::Value>>,
+    pub timestamp: DateTime<Utc>,
+    pub trace_id: String,
+}
+
+impl ErrorResponse {
+    pub fn new(error: &str, code: &str) -> Self {
+        Self {
+            error: error.to_string(),
+            code: code.to_string(),
+            details: None,
+            timestamp: Utc::now(),
+            trace_id: Uuid::new_v4().to_string(),
+        }
+    }
+
+    pub fn with_details(mut self, details: HashMap<String, serde_json::Value>) -> Self {
+        self.details = Some(details);
+        self
+    }
+}
+
+// ============================================================================
+// REST ENDPOINTS
+// ============================================================================
+
+/// GET /api/ontology/domains - List all ETSI domains
+pub async fn list_domains(
+    state: web::Data<AppState>,
+    req: web::Query<ListDomainsRequest>,
+) -> impl Responder {
+    info!("Listing ontology domains with filter: {:?}", req.filter);
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    match db.list_domains(req.filter.as_deref(), req.include_stats.unwrap_or(false)) {
+        Ok(domains) => {
+            let response = DomainsResponse {
+                total_count: domains.len(),
+                domains,
+                timestamp: Utc::now(),
+            };
+            HttpResponse::Ok().json(response)
+        }
+        Err(e) => {
+            error!("Failed to list domains: {}", e);
+            let error_response = ErrorResponse::new(&e, "DOMAIN_LIST_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+/// GET /api/ontology/classes - List ontology classes with filters
+pub async fn list_classes(
+    state: web::Data<AppState>,
+    req: web::Query<ListClassesRequest>,
+) -> impl Responder {
+    info!("Listing ontology classes: domain={:?}, filter={:?}", req.domain, req.filter);
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    let offset = req.offset.unwrap_or(0);
+    let limit = req.limit.unwrap_or(50).min(500); // Cap at 500
+
+    match db.list_classes(
+        req.domain.as_deref(),
+        req.filter.as_deref(),
+        req.include_subclasses.unwrap_or(false),
+        req.include_properties.unwrap_or(false),
+        offset,
+        limit,
+    ) {
+        Ok((classes, total_count)) => {
+            let response = ClassesResponse {
+                classes,
+                total_count,
+                offset,
+                limit,
+                timestamp: Utc::now(),
+            };
+            HttpResponse::Ok().json(response)
+        }
+        Err(e) => {
+            error!("Failed to list classes: {}", e);
+            let error_response = ErrorResponse::new(&e, "CLASS_LIST_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+/// GET /api/ontology/properties - List properties with schemas
+pub async fn list_properties(
+    state: web::Data<AppState>,
+    req: web::Query<ListPropertiesRequest>,
+) -> impl Responder {
+    info!("Listing ontology properties: domain={:?}, type={:?}", req.domain, req.property_type);
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    let offset = req.offset.unwrap_or(0);
+    let limit = req.limit.unwrap_or(50).min(500); // Cap at 500
+
+    match db.list_properties(
+        req.domain.as_deref(),
+        req.filter.as_deref(),
+        req.property_type.as_deref(),
+        req.include_constraints.unwrap_or(false),
+        offset,
+        limit,
+    ) {
+        Ok((properties, total_count)) => {
+            let response = PropertiesResponse {
+                properties,
+                total_count,
+                offset,
+                limit,
+                timestamp: Utc::now(),
+            };
+            HttpResponse::Ok().json(response)
+        }
+        Err(e) => {
+            error!("Failed to list properties: {}", e);
+            let error_response = ErrorResponse::new(&e, "PROPERTY_LIST_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+/// GET /api/ontology/entities/:id - Get specific entity with relationships
+pub async fn get_entity(
+    state: web::Data<AppState>,
+    path: web::Path<String>,
+    req: web::Query<GetEntityRequest>,
+) -> impl Responder {
+    let entity_id = path.into_inner();
+    info!("Getting entity: {}", entity_id);
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    match db.get_entity(
+        &entity_id,
+        req.include_incoming.unwrap_or(true),
+        req.include_outgoing.unwrap_or(true),
+        req.include_inferred.unwrap_or(false),
+        req.max_depth.unwrap_or(1),
+    ) {
+        Ok(Some(entity)) => HttpResponse::Ok().json(entity),
+        Ok(None) => {
+            warn!("Entity not found: {}", entity_id);
+            let error_response = ErrorResponse::new("Entity not found", "ENTITY_NOT_FOUND");
+            HttpResponse::NotFound().json(error_response)
+        }
+        Err(e) => {
+            error!("Failed to get entity: {}", e);
+            let error_response = ErrorResponse::new(&e, "ENTITY_RETRIEVAL_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+/// POST /api/ontology/query - Advanced query interface
+pub async fn query_ontology(
+    state: web::Data<AppState>,
+    req: web::Json<QueryRequest>,
+) -> impl Responder {
+    info!("Executing ontology query");
+    debug!("Query: {}", req.query);
+
+    let start_time = std::time::Instant::now();
+    let query_id = Uuid::new_v4().to_string();
+
+    // Check cache first
+    let cache = OntologyCache::new();
+    let cache_key = format!("query:{}:{:?}", req.query, req.parameters);
+
+    if let Some(cached_result) = cache.get_query_result(&cache_key) {
+        info!("Serving query from cache");
+        return HttpResponse::Ok().json(cached_result);
+    }
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    let query_engine = QueryEngine::new(db);
+
+    match query_engine.execute_query(
+        &req.query,
+        req.parameters.clone(),
+        req.limit.unwrap_or(100),
+        req.offset.unwrap_or(0),
+        req.timeout_seconds.unwrap_or(30),
+    ) {
+        Ok(results) => {
+            let execution_time_ms = start_time.elapsed().as_millis() as u64;
+
+            let result = QueryResult {
+                query_id,
+                results: results.clone(),
+                total_count: results.len(),
+                execution_time_ms,
+                execution_plan: if req.explain.unwrap_or(false) {
+                    Some("Query execution plan not yet implemented".to_string())
+                } else {
+                    None
+                },
+                from_cache: false,
+                timestamp: Utc::now(),
+            };
+
+            // Cache the result
+            cache.set_query_result(&cache_key, &result);
+
+            HttpResponse::Ok().json(result)
+        }
+        Err(e) => {
+            error!("Query execution failed: {}", e);
+            let error_response = ErrorResponse::new(&e, "QUERY_FAILED");
+            HttpResponse::BadRequest().json(error_response)
+        }
+    }
+}
+
+/// GET /api/ontology/graph - Get graph visualization data
+pub async fn get_graph_visualization(
+    state: web::Data<AppState>,
+    query: web::Query<HashMap<String, String>>,
+) -> impl Responder {
+    info!("Getting graph visualization data");
+
+    let domain_filter = query.get("domain").map(|s| s.as_str());
+    let max_nodes = query.get("max_nodes")
+        .and_then(|s| s.parse::<u32>().ok())
+        .unwrap_or(500)
+        .min(2000); // Cap at 2000 nodes
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    match db.get_graph_visualization(domain_filter, max_nodes) {
+        Ok(graph_data) => HttpResponse::Ok().json(graph_data),
+        Err(e) => {
+            error!("Failed to get graph visualization: {}", e);
+            let error_response = ErrorResponse::new(&e, "GRAPH_VIZ_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+// ============================================================================
+// WEBSOCKET IMPLEMENTATION
+// ============================================================================
+
+/// WebSocket actor for real-time ontology updates
+pub struct OntologyStreamSocket {
+    /// Client ID for tracking
+    client_id: String,
+    /// Subscription filters
+    filters: HashMap<String, String>,
+    /// Last heartbeat
+    last_heartbeat: std::time::Instant,
+}
+
+impl OntologyStreamSocket {
+    pub fn new(client_id: String) -> Self {
+        Self {
+            client_id,
+            filters: HashMap::new(),
+            last_heartbeat: std::time::Instant::now(),
+        }
+    }
+
+    fn send_update(&self, ctx: &mut ws::WebsocketContext<Self>, update: OntologyUpdate) {
+        match serde_json::to_string(&update) {
+            Ok(json) => ctx.text(json),
+            Err(e) => error!("Failed to serialize update: {}", e),
+        }
+    }
+
+    fn heartbeat(&self, ctx: &mut ws::WebsocketContext<Self>) {
+        ctx.run_interval(StdDuration::from_secs(30), |act, ctx| {
+            if std::time::Instant::now().duration_since(act.last_heartbeat) > StdDuration::from_secs(90) {
+                warn!("Client {} heartbeat timeout", act.client_id);
+                ctx.stop();
+            } else {
+                ctx.ping(b"");
+            }
+        });
+    }
+}
+
+impl Actor for OntologyStreamSocket {
+    type Context = ws::WebsocketContext<Self>;
+
+    fn started(&mut self, ctx: &mut Self::Context) {
+        info!("WebSocket connection started for client: {}", self.client_id);
+
+        // Start heartbeat
+        self.heartbeat(ctx);
+
+        // Send initial connection confirmation
+        let msg = serde_json::json!({
+            "type": "connected",
+            "clientId": self.client_id,
+            "timestamp": Utc::now()
+        });
+        ctx.text(msg.to_string());
+    }
+
+    fn stopped(&mut self, _ctx: &mut Self::Context) {
+        info!("WebSocket connection stopped for client: {}", self.client_id);
+    }
+}
+
+impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for OntologyStreamSocket {
+    fn handle(&mut self, msg: Result<ws::Message, ws::ProtocolError>, ctx: &mut Self::Context) {
+        match msg {
+            Ok(ws::Message::Text(text)) => {
+                debug!("Received WebSocket message from {}: {}", self.client_id, text);
+
+                // Parse client commands
+                if let Ok(command) = serde_json::from_str::<serde_json::Value>(&text) {
+                    if let Some(cmd_type) = command.get("type").and_then(|v| v.as_str()) {
+                        match cmd_type {
+                            "subscribe" => {
+                                if let Some(filter) = command.get("filter") {
+                                    info!("Client {} subscribing with filter: {:?}", self.client_id, filter);
+                                }
+                            }
+                            "unsubscribe" => {
+                                info!("Client {} unsubscribing", self.client_id);
+                                self.filters.clear();
+                            }
+                            _ => {
+                                warn!("Unknown command type: {}", cmd_type);
+                            }
+                        }
+                    }
+                }
+            }
+            Ok(ws::Message::Ping(msg)) => {
+                self.last_heartbeat = std::time::Instant::now();
+                ctx.pong(&msg);
+            }
+            Ok(ws::Message::Pong(_)) => {
+                self.last_heartbeat = std::time::Instant::now();
+            }
+            Ok(ws::Message::Close(reason)) => {
+                info!("WebSocket close received from {}: {:?}", self.client_id, reason);
+                ctx.close(reason);
+            }
+            _ => {}
+        }
+    }
+}
+
+/// WebSocket upgrade handler
+pub async fn ontology_stream(
+    req: HttpRequest,
+    stream: web::Payload,
+    query: web::Query<HashMap<String, String>>,
+) -> Result<HttpResponse, ActixError> {
+    info!("WebSocket upgrade request for ontology stream");
+
+    let client_id = query.get("client_id")
+        .cloned()
+        .unwrap_or_else(|| Uuid::new_v4().to_string());
+
+    let websocket = OntologyStreamSocket::new(client_id);
+
+    ws::start(websocket, &req, stream)
+}
+
+// ============================================================================
+// ROUTE CONFIGURATION
+// ============================================================================
+
+/// Configure ontology data API routes
+pub fn config(cfg: &mut web::ServiceConfig) {
+    cfg.service(
+        web::scope("/ontology")
+            // Domain endpoints
+            .route("/domains", web::get().to(list_domains))
+            // Class endpoints
+            .route("/classes", web::get().to(list_classes))
+            // Property endpoints
+            .route("/properties", web::get().to(list_properties))
+            // Entity endpoints
+            .route("/entities/{id}", web::get().to(get_entity))
+            // Query endpoint
+            .route("/query", web::post().to(query_ontology))
+            // Graph visualization endpoint
+            .route("/graph", web::get().to(get_graph_visualization))
+            // WebSocket stream
+            .route("/stream", web::get().to(ontology_stream))
+    );
+}
+
+// ============================================================================
+// TESTS
+// ============================================================================
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_domain_info_serialization() {
+        let domain = DomainInfo {
+            id: "etsi-nfv".to_string(),
+            name: "ETSI NFV".to_string(),
+            description: "Network Functions Virtualization".to_string(),
+            class_count: 150,
+            property_count: 300,
+            namespace: "http://example.org/etsi-nfv#".to_string(),
+            updated_at: Utc::now(),
+        };
+
+        let json = serde_json::to_value(&domain).unwrap();
+        assert!(json.get("id").is_some());
+        assert!(json.get("classCount").is_some());
+    }
+
+    #[test]
+    fn test_query_request_deserialization() {
+        let json = r#"{
+            "query": "SELECT ?s WHERE { ?s a ?type }",
+            "limit": 100,
+            "explain": true
+        }"#;
+
+        let req: QueryRequest = serde_json::from_str(json).unwrap();
+        assert_eq!(req.query, "SELECT ?s WHERE { ?s a ?type }");
+        assert_eq!(req.limit, Some(100));
+        assert_eq!(req.explain, Some(true));
+    }
+}
diff --git a/src/handlers/api_handler/ontology_data/query.rs b/src/handlers/api_handler/ontology_data/query.rs
new file mode 100644
index 00000000..447a6d2a
--- /dev/null
+++ b/src/handlers/api_handler/ontology_data/query.rs
@@ -0,0 +1,284 @@
+//! Query Engine for Ontology Data
+//!
+//! Provides a SPARQL-like query interface for ontology data with support for:
+//! - Entity filtering and selection
+//! - Relationship traversal
+//! - Property-based filtering
+//! - Aggregations
+//! - Sorting and pagination
+
+use log::{debug, error, info, warn};
+use serde_json::Value as JsonValue;
+use std::collections::HashMap;
+
+use super::db::OntologyDatabase;
+
+/// Query execution engine
+pub struct QueryEngine {
+    db: OntologyDatabase,
+}
+
+impl QueryEngine {
+    /// Create new query engine
+    pub fn new(db: OntologyDatabase) -> Self {
+        Self { db }
+    }
+
+    /// Execute a query with parameters
+    pub fn execute_query(
+        &self,
+        query: &str,
+        parameters: Option<HashMap<String, JsonValue>>,
+        limit: u32,
+        offset: u32,
+        timeout_seconds: u32,
+    ) -> Result<Vec<HashMap<String, JsonValue>>, String> {
+        debug!("Executing query: {}", query);
+        debug!("Parameters: {:?}", parameters);
+
+        // Parse query
+        let parsed = self.parse_query(query)?;
+
+        // Execute based on query type
+        match parsed.query_type.as_str() {
+            "SELECT" => self.execute_select(&parsed, parameters, limit, offset),
+            "COUNT" => self.execute_count(&parsed, parameters),
+            "DESCRIBE" => self.execute_describe(&parsed, parameters),
+            _ => Err(format!("Unsupported query type: {}", parsed.query_type)),
+        }
+    }
+
+    /// Parse query string into structured format
+    fn parse_query(&self, query: &str) -> Result<ParsedQuery, String> {
+        let query_lower = query.to_lowercase();
+
+        let query_type = if query_lower.starts_with("select") {
+            "SELECT"
+        } else if query_lower.starts_with("count") {
+            "COUNT"
+        } else if query_lower.starts_with("describe") {
+            "DESCRIBE"
+        } else {
+            return Err("Query must start with SELECT, COUNT, or DESCRIBE".to_string());
+        };
+
+        // Extract WHERE clause
+        let where_clause = if let Some(where_idx) = query_lower.find("where") {
+            Some(&query[where_idx + 5..])
+        } else {
+            None
+        };
+
+        Ok(ParsedQuery {
+            query_type: query_type.to_string(),
+            select_fields: self.extract_select_fields(query),
+            where_clause: where_clause.map(|s| s.trim().to_string()),
+            filters: self.extract_filters(where_clause),
+        })
+    }
+
+    /// Extract SELECT fields from query
+    fn extract_select_fields(&self, query: &str) -> Vec<String> {
+        // Simplified extraction - in production, use a proper SPARQL parser
+        let mut fields = Vec::new();
+
+        if let Some(select_idx) = query.to_lowercase().find("select") {
+            let after_select = &query[select_idx + 6..];
+
+            if let Some(where_idx) = after_select.to_lowercase().find("where") {
+                let fields_str = &after_select[..where_idx];
+                for field in fields_str.split_whitespace() {
+                    if !field.is_empty() && field.starts_with('?') {
+                        fields.push(field[1..].to_string());
+                    }
+                }
+            }
+        }
+
+        if fields.is_empty() {
+            fields.push("*".to_string());
+        }
+
+        fields
+    }
+
+    /// Extract filters from WHERE clause
+    fn extract_filters(&self, where_clause: Option<&str>) -> Vec<QueryFilter> {
+        let mut filters = Vec::new();
+
+        if let Some(clause) = where_clause {
+            // Simplified filter extraction
+            // In production, use a proper SPARQL parser
+            let parts: Vec<&str> = clause.split('.').collect();
+
+            for part in parts {
+                let trimmed = part.trim();
+                if trimmed.is_empty() {
+                    continue;
+                }
+
+                // Look for patterns like "?s a ?type" or "?s rdf:type vnf"
+                let tokens: Vec<&str> = trimmed.split_whitespace().collect();
+
+                if tokens.len() >= 3 {
+                    filters.push(QueryFilter {
+                        subject: tokens[0].to_string(),
+                        predicate: tokens[1].to_string(),
+                        object: tokens[2].to_string(),
+                    });
+                }
+            }
+        }
+
+        filters
+    }
+
+    /// Execute SELECT query
+    fn execute_select(
+        &self,
+        parsed: &ParsedQuery,
+        parameters: Option<HashMap<String, JsonValue>>,
+        limit: u32,
+        offset: u32,
+    ) -> Result<Vec<HashMap<String, JsonValue>>, String> {
+        debug!("Executing SELECT query");
+
+        // Mock implementation - return sample data
+        let mut results = Vec::new();
+
+        // Create sample result rows
+        for i in 0..limit.min(10) {
+            let mut row = HashMap::new();
+
+            for field in &parsed.select_fields {
+                let value = match field.as_str() {
+                    "*" | "entity" => JsonValue::String(format!("entity-{}", i)),
+                    "type" => JsonValue::String("vnf".to_string()),
+                    "label" => JsonValue::String(format!("VNF Instance {}", i)),
+                    "domain" => JsonValue::String("etsi-nfv".to_string()),
+                    "status" => JsonValue::String("deployed".to_string()),
+                    _ => JsonValue::Null,
+                };
+
+                row.insert(field.clone(), value);
+            }
+
+            results.push(row);
+        }
+
+        Ok(results)
+    }
+
+    /// Execute COUNT query
+    fn execute_count(
+        &self,
+        parsed: &ParsedQuery,
+        parameters: Option<HashMap<String, JsonValue>>,
+    ) -> Result<Vec<HashMap<String, JsonValue>>, String> {
+        debug!("Executing COUNT query");
+
+        let mut result = HashMap::new();
+        result.insert("count".to_string(), JsonValue::Number(serde_json::Number::from(42)));
+
+        Ok(vec![result])
+    }
+
+    /// Execute DESCRIBE query
+    fn execute_describe(
+        &self,
+        parsed: &ParsedQuery,
+        parameters: Option<HashMap<String, JsonValue>>,
+    ) -> Result<Vec<HashMap<String, JsonValue>>, String> {
+        debug!("Executing DESCRIBE query");
+
+        // Return entity description
+        let mut result = HashMap::new();
+        result.insert("id".to_string(), JsonValue::String("vnf-123".to_string()));
+        result.insert("type".to_string(), JsonValue::String("vnf".to_string()));
+        result.insert("label".to_string(), JsonValue::String("Example VNF".to_string()));
+        result.insert("properties".to_string(), JsonValue::Object({
+            let mut props = serde_json::Map::new();
+            props.insert("deploymentStatus".to_string(), JsonValue::String("deployed".to_string()));
+            props.insert("version".to_string(), JsonValue::String("1.2.3".to_string()));
+            props
+        }));
+
+        Ok(vec![result])
+    }
+}
+
+/// Parsed query structure
+#[derive(Debug, Clone)]
+struct ParsedQuery {
+    query_type: String,
+    select_fields: Vec<String>,
+    where_clause: Option<String>,
+    filters: Vec<QueryFilter>,
+}
+
+/// Query filter
+#[derive(Debug, Clone)]
+struct QueryFilter {
+    subject: String,
+    predicate: String,
+    object: String,
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_query_parsing() {
+        let db = OntologyDatabase::new().unwrap();
+        let engine = QueryEngine::new(db);
+
+        let query = "SELECT ?entity ?type WHERE { ?entity a ?type }";
+        let parsed = engine.parse_query(query).unwrap();
+
+        assert_eq!(parsed.query_type, "SELECT");
+        assert!(parsed.select_fields.contains(&"entity".to_string()));
+        assert!(parsed.select_fields.contains(&"type".to_string()));
+    }
+
+    #[test]
+    fn test_select_query_execution() {
+        let db = OntologyDatabase::new().unwrap();
+        let engine = QueryEngine::new(db);
+
+        let query = "SELECT ?entity WHERE { ?entity a vnf }";
+        let result = engine.execute_query(query, None, 10, 0, 30);
+
+        assert!(result.is_ok());
+        let rows = result.unwrap();
+        assert!(!rows.is_empty());
+    }
+
+    #[test]
+    fn test_count_query_execution() {
+        let db = OntologyDatabase::new().unwrap();
+        let engine = QueryEngine::new(db);
+
+        let query = "COUNT WHERE { ?entity a vnf }";
+        let result = engine.execute_query(query, None, 10, 0, 30);
+
+        assert!(result.is_ok());
+        let rows = result.unwrap();
+        assert_eq!(rows.len(), 1);
+        assert!(rows[0].contains_key("count"));
+    }
+
+    #[test]
+    fn test_describe_query_execution() {
+        let db = OntologyDatabase::new().unwrap();
+        let engine = QueryEngine::new(db);
+
+        let query = "DESCRIBE vnf-123";
+        let result = engine.execute_query(query, None, 10, 0, 30);
+
+        assert!(result.is_ok());
+        let rows = result.unwrap();
+        assert!(!rows.is_empty());
+        assert!(rows[0].contains_key("id"));
+    }
+}
diff --git a/src/handlers/api_handler/settings/mod.rs b/src/handlers/api_handler/settings/mod.rs
new file mode 100644
index 00000000..27183d8f
--- /dev/null
+++ b/src/handlers/api_handler/settings/mod.rs
@@ -0,0 +1,458 @@
+// REST API for Settings Management
+// Provides complete CRUD operations for settings with validation and permission checks
+
+use actix_web::{web, HttpRequest, HttpResponse, Error as ActixError};
+use serde::{Deserialize, Serialize};
+use serde_json::{json, Value as JsonValue};
+use std::sync::Arc;
+use log::{info, error, debug, warn};
+
+use crate::app_state::AppState;
+use crate::services::settings_service::SettingsService;
+use crate::services::database_service::SettingValue;
+use crate::config::PhysicsSettings;
+
+/// Response DTO for settings list
+#[derive(Debug, Serialize)]
+#[serde(rename_all = "camelCase")]
+pub struct SettingsListResponse {
+    pub settings: Vec<SettingItem>,
+    pub total: usize,
+}
+
+#[derive(Debug, Serialize)]
+#[serde(rename_all = "camelCase")]
+pub struct SettingItem {
+    pub key: String,
+    pub value: JsonValue,
+    pub value_type: String,
+}
+
+/// Request DTO for setting update
+#[derive(Debug, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct UpdateSettingRequest {
+    pub value: JsonValue,
+}
+
+/// Request DTO for validation only
+#[derive(Debug, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ValidateSettingRequest {
+    pub key: String,
+    pub value: JsonValue,
+}
+
+/// Response DTO for validation
+#[derive(Debug, Serialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ValidationResponse {
+    pub is_valid: bool,
+    pub errors: Vec<String>,
+    pub warnings: Vec<String>,
+}
+
+/// Extract user ID from request (from Nostr pubkey or auth token)
+fn extract_user_id(req: &HttpRequest) -> Option<String> {
+    req.headers()
+        .get("x-nostr-pubkey")
+        .and_then(|h| h.to_str().ok())
+        .map(|s| s.to_string())
+}
+
+/// Check if user has power user permissions
+fn check_power_user(app_state: &AppState, user_id: Option<&str>) -> bool {
+    match user_id {
+        Some(uid) => app_state.is_power_user(uid),
+        None => false,
+    }
+}
+
+/// GET /api/settings - List all settings with permission filtering
+pub async fn list_settings(
+    req: HttpRequest,
+    app_state: web::Data<AppState>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let user_id = extract_user_id(&req);
+    let is_power_user = check_power_user(&app_state, user_id.as_deref());
+
+    debug!("Listing settings for user: {:?} (power: {})", user_id, is_power_user);
+
+    match settings_service.list_all_settings().await {
+        Ok(settings) => {
+            let items: Vec<SettingItem> = settings
+                .into_iter()
+                .map(|(key, value)| {
+                    let (json_value, value_type) = convert_setting_value_to_json(&value);
+                    SettingItem {
+                        key,
+                        value: json_value,
+                        value_type,
+                    }
+                })
+                .collect();
+
+            let total = items.len();
+            Ok(HttpResponse::Ok().json(SettingsListResponse {
+                settings: items,
+                total,
+            }))
+        }
+        Err(e) => {
+            error!("Failed to list settings: {}", e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to list settings",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// GET /api/settings/{key} - Get specific setting (supports dots in key)
+pub async fn get_setting(
+    path: web::Path<String>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let key = path.into_inner();
+
+    debug!("Getting setting: {}", key);
+
+    match settings_service.get_setting(&key).await {
+        Ok(Some(value)) => {
+            let (json_value, value_type) = convert_setting_value_to_json(&value);
+            Ok(HttpResponse::Ok().json(json!({
+                "key": key,
+                "value": json_value,
+                "valueType": value_type
+            })))
+        }
+        Ok(None) => Ok(HttpResponse::NotFound().json(json!({
+            "error": "Setting not found",
+            "key": key
+        }))),
+        Err(e) => {
+            error!("Failed to get setting {}: {}", key, e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get setting",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// PUT /api/settings/{key} - Update setting (requires power user)
+pub async fn update_setting(
+    req: HttpRequest,
+    path: web::Path<String>,
+    body: web::Json<UpdateSettingRequest>,
+    app_state: web::Data<AppState>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let key = path.into_inner();
+    let user_id = extract_user_id(&req);
+
+    // Check power user permission
+    if !check_power_user(&app_state, user_id.as_deref()) {
+        warn!("Unauthorized settings update attempt by user: {:?}", user_id);
+        return Ok(HttpResponse::Forbidden().json(json!({
+            "error": "Power user permission required"
+        })));
+    }
+
+    debug!("Updating setting: {} by user: {:?}", key, user_id);
+
+    // Convert JSON value to SettingValue
+    let setting_value = match json_value_to_setting_value(&body.value) {
+        Ok(v) => v,
+        Err(e) => {
+            return Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Invalid value type",
+                "details": e
+            })));
+        }
+    };
+
+    match settings_service
+        .set_setting(&key, setting_value, user_id.as_deref())
+        .await
+    {
+        Ok(()) => {
+            info!("Setting {} updated by user {:?}", key, user_id);
+            Ok(HttpResponse::Ok().json(json!({
+                "success": true,
+                "key": key,
+                "message": "Setting updated successfully"
+            })))
+        }
+        Err(e) => {
+            error!("Failed to update setting {}: {}", key, e);
+            Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Failed to update setting",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// GET /api/settings/tree/{prefix} - Get hierarchical tree
+pub async fn get_settings_tree(
+    path: web::Path<String>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let prefix = path.into_inner();
+
+    debug!("Getting settings tree for prefix: {}", prefix);
+
+    match settings_service.get_settings_tree(&prefix).await {
+        Ok(tree) => Ok(HttpResponse::Ok().json(tree)),
+        Err(e) => {
+            error!("Failed to get settings tree for {}: {}", prefix, e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get settings tree",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// GET /api/settings/physics/{profile} - Get physics profile
+pub async fn get_physics_profile(
+    path: web::Path<String>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let profile_name = path.into_inner();
+
+    debug!("Getting physics profile: {}", profile_name);
+
+    match settings_service.get_physics_profile(&profile_name).await {
+        Ok(profile) => Ok(HttpResponse::Ok().json(profile)),
+        Err(e) => {
+            error!("Failed to get physics profile {}: {}", profile_name, e);
+            Ok(HttpResponse::NotFound().json(json!({
+                "error": "Physics profile not found",
+                "profile": profile_name,
+                "details": e
+            })))
+        }
+    }
+}
+
+/// PUT /api/settings/physics/{profile} - Update physics profile (requires power user)
+pub async fn update_physics_profile(
+    req: HttpRequest,
+    path: web::Path<String>,
+    body: web::Json<PhysicsSettings>,
+    app_state: web::Data<AppState>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let profile_name = path.into_inner();
+    let user_id = extract_user_id(&req);
+
+    // Check power user permission
+    if !check_power_user(&app_state, user_id.as_deref()) {
+        warn!(
+            "Unauthorized physics profile update attempt by user: {:?}",
+            user_id
+        );
+        return Ok(HttpResponse::Forbidden().json(json!({
+            "error": "Power user permission required"
+        })));
+    }
+
+    debug!(
+        "Updating physics profile: {} by user: {:?}",
+        profile_name, user_id
+    );
+
+    match settings_service
+        .update_physics_profile(&profile_name, body.into_inner(), user_id.as_deref())
+        .await
+    {
+        Ok(()) => {
+            info!(
+                "Physics profile {} updated by user {:?}",
+                profile_name, user_id
+            );
+            Ok(HttpResponse::Ok().json(json!({
+                "success": true,
+                "profile": profile_name,
+                "message": "Physics profile updated successfully"
+            })))
+        }
+        Err(e) => {
+            error!("Failed to update physics profile {}: {}", profile_name, e);
+            Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Failed to update physics profile",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// POST /api/settings/validate - Validate settings without saving
+pub async fn validate_setting(
+    body: web::Json<ValidateSettingRequest>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    debug!("Validating setting: {}", body.key);
+
+    // Convert JSON value to SettingValue
+    let setting_value = match json_value_to_setting_value(&body.value) {
+        Ok(v) => v,
+        Err(e) => {
+            return Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Invalid value type",
+                "details": e
+            })));
+        }
+    };
+
+    // Perform validation
+    let validator = &settings_service.validator;
+    match validator.validate_setting(&body.key, &setting_value) {
+        Ok(result) => Ok(HttpResponse::Ok().json(ValidationResponse {
+            is_valid: result.is_valid,
+            errors: result.errors,
+            warnings: result.warnings,
+        })),
+        Err(e) => {
+            error!("Validation error for {}: {}", body.key, e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Validation failed",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// GET /api/settings/search?q=pattern - Search settings
+pub async fn search_settings(
+    query: web::Query<SearchQuery>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let pattern = &query.q;
+
+    debug!("Searching settings with pattern: {}", pattern);
+
+    match settings_service.search_settings(pattern).await {
+        Ok(results) => {
+            let items: Vec<SettingItem> = results
+                .into_iter()
+                .map(|(key, value)| {
+                    let (json_value, value_type) = convert_setting_value_to_json(&value);
+                    SettingItem {
+                        key,
+                        value: json_value,
+                        value_type,
+                    }
+                })
+                .collect();
+
+            Ok(HttpResponse::Ok().json(json!({
+                "results": items,
+                "count": items.len()
+            })))
+        }
+        Err(e) => {
+            error!("Failed to search settings: {}", e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to search settings",
+                "details": e
+            })))
+        }
+    }
+}
+
+#[derive(Debug, Deserialize)]
+pub struct SearchQuery {
+    pub q: String,
+}
+
+/// DELETE /api/settings/{key} - Reset to default (requires power user)
+pub async fn reset_setting(
+    req: HttpRequest,
+    path: web::Path<String>,
+    app_state: web::Data<AppState>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let key = path.into_inner();
+    let user_id = extract_user_id(&req);
+
+    // Check power user permission
+    if !check_power_user(&app_state, user_id.as_deref()) {
+        warn!("Unauthorized reset attempt by user: {:?}", user_id);
+        return Ok(HttpResponse::Forbidden().json(json!({
+            "error": "Power user permission required"
+        })));
+    }
+
+    debug!("Resetting setting: {} by user: {:?}", key, user_id);
+
+    match settings_service
+        .reset_to_default(&key, user_id.as_deref())
+        .await
+    {
+        Ok(()) => {
+            info!("Setting {} reset to default by user {:?}", key, user_id);
+            Ok(HttpResponse::Ok().json(json!({
+                "success": true,
+                "key": key,
+                "message": "Setting reset to default"
+            })))
+        }
+        Err(e) => {
+            error!("Failed to reset setting {}: {}", key, e);
+            Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Failed to reset setting",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// Helper: Convert SettingValue to JSON
+fn convert_setting_value_to_json(value: &SettingValue) -> (JsonValue, String) {
+    match value {
+        SettingValue::String(s) => (json!(s), "string".to_string()),
+        SettingValue::Integer(i) => (json!(i), "integer".to_string()),
+        SettingValue::Float(f) => (json!(f), "float".to_string()),
+        SettingValue::Boolean(b) => (json!(b), "boolean".to_string()),
+        SettingValue::Json(j) => (j.clone(), "json".to_string()),
+    }
+}
+
+/// Helper: Convert JSON value to SettingValue
+fn json_value_to_setting_value(value: &JsonValue) -> Result<SettingValue, String> {
+    match value {
+        JsonValue::String(s) => Ok(SettingValue::String(s.clone())),
+        JsonValue::Number(n) => {
+            if let Some(i) = n.as_i64() {
+                Ok(SettingValue::Integer(i))
+            } else if let Some(f) = n.as_f64() {
+                Ok(SettingValue::Float(f))
+            } else {
+                Err("Invalid number type".to_string())
+            }
+        }
+        JsonValue::Bool(b) => Ok(SettingValue::Boolean(*b)),
+        JsonValue::Object(_) | JsonValue::Array(_) => Ok(SettingValue::Json(value.clone())),
+        JsonValue::Null => Err("Cannot store null values".to_string()),
+    }
+}
+
+/// Configure settings API routes
+pub fn configure_routes(cfg: &mut web::ServiceConfig) {
+    cfg.service(
+        web::scope("/api/settings")
+            .route("", web::get().to(list_settings))
+            .route("/search", web::get().to(search_settings))
+            .route("/validate", web::post().to(validate_setting))
+            .route("/tree/{prefix:.*}", web::get().to(get_settings_tree))
+            .route("/physics/{profile}", web::get().to(get_physics_profile))
+            .route("/physics/{profile}", web::put().to(update_physics_profile))
+            .route("/{key:.*}", web::get().to(get_setting))
+            .route("/{key:.*}", web::put().to(update_setting))
+            .route("/{key:.*}", web::delete().to(reset_setting)),
+    );
+}
diff --git a/src/handlers/api_handler/settings/websocket.rs b/src/handlers/api_handler/settings/websocket.rs
new file mode 100644
index 00000000..030e262a
--- /dev/null
+++ b/src/handlers/api_handler/settings/websocket.rs
@@ -0,0 +1,218 @@
+// WebSocket Broadcast Integration for Settings Changes
+// Notifies all connected clients when settings are modified
+
+use actix::prelude::*;
+use serde::{Deserialize, Serialize};
+use serde_json::Value as JsonValue;
+use std::collections::HashMap;
+use std::sync::Arc;
+use tokio::sync::RwLock;
+use log::{info, debug, warn};
+
+use crate::services::database_service::SettingValue;
+
+/// Manager for broadcasting settings changes to WebSocket clients
+#[derive(Clone)]
+pub struct SettingsBroadcastManager {
+    clients: Arc<RwLock<HashMap<String, SettingsClientInfo>>>,
+    subscriptions: Arc<RwLock<HashMap<String, Vec<String>>>>, // key prefix -> client_ids
+}
+
+#[derive(Clone)]
+struct SettingsClientInfo {
+    client_id: String,
+    subscriptions: Vec<String>, // Prefixes the client is subscribed to
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct SettingsChangeNotification {
+    pub event_type: String,
+    pub key: String,
+    pub value: JsonValue,
+    pub changed_by: Option<String>,
+    pub timestamp: u64,
+}
+
+impl SettingsBroadcastManager {
+    pub fn new() -> Self {
+        Self {
+            clients: Arc::new(RwLock::new(HashMap::new())),
+            subscriptions: Arc::new(RwLock::new(HashMap::new())),
+        }
+    }
+
+    /// Register a new WebSocket client
+    pub async fn register_client(&self, client_id: String) {
+        let mut clients = self.clients.write().await;
+        clients.insert(
+            client_id.clone(),
+            SettingsClientInfo {
+                client_id: client_id.clone(),
+                subscriptions: vec!["*".to_string()], // Default: subscribe to all
+            },
+        );
+        info!("Settings WebSocket client registered: {}", client_id);
+    }
+
+    /// Unregister a WebSocket client
+    pub async fn unregister_client(&self, client_id: &str) {
+        let mut clients = self.clients.write().await;
+        if clients.remove(client_id).is_some() {
+            info!("Settings WebSocket client unregistered: {}", client_id);
+        }
+
+        // Clean up subscriptions
+        let mut subscriptions = self.subscriptions.write().await;
+        for (_, client_ids) in subscriptions.iter_mut() {
+            client_ids.retain(|id| id != client_id);
+        }
+    }
+
+    /// Subscribe client to specific setting prefix
+    pub async fn subscribe(&self, client_id: &str, prefix: String) {
+        let mut clients = self.clients.write().await;
+        if let Some(client) = clients.get_mut(client_id) {
+            if !client.subscriptions.contains(&prefix) {
+                client.subscriptions.push(prefix.clone());
+            }
+        }
+
+        let mut subscriptions = self.subscriptions.write().await;
+        subscriptions
+            .entry(prefix.clone())
+            .or_insert_with(Vec::new)
+            .push(client_id.to_string());
+
+        debug!(
+            "Client {} subscribed to settings prefix: {}",
+            client_id, prefix
+        );
+    }
+
+    /// Unsubscribe client from specific setting prefix
+    pub async fn unsubscribe(&self, client_id: &str, prefix: &str) {
+        let mut clients = self.clients.write().await;
+        if let Some(client) = clients.get_mut(client_id) {
+            client.subscriptions.retain(|p| p != prefix);
+        }
+
+        let mut subscriptions = self.subscriptions.write().await;
+        if let Some(client_ids) = subscriptions.get_mut(prefix) {
+            client_ids.retain(|id| id != client_id);
+        }
+
+        debug!(
+            "Client {} unsubscribed from settings prefix: {}",
+            client_id, prefix
+        );
+    }
+
+    /// Broadcast setting change to all interested clients
+    pub async fn broadcast_change(&self, key: &str, value: &SettingValue, user_id: Option<&str>) {
+        let clients = self.clients.read().await;
+        let interested_clients: Vec<String> = clients
+            .values()
+            .filter(|client| self.is_subscribed_to(client, key))
+            .map(|client| client.client_id.clone())
+            .collect();
+
+        if interested_clients.is_empty() {
+            return;
+        }
+
+        let notification = SettingsChangeNotification {
+            event_type: "settingChanged".to_string(),
+            key: key.to_string(),
+            value: convert_setting_value_to_json(value),
+            changed_by: user_id.map(|s| s.to_string()),
+            timestamp: current_timestamp(),
+        };
+
+        info!(
+            "Broadcasting setting change for '{}' to {} clients",
+            key,
+            interested_clients.len()
+        );
+
+        // In a real implementation, this would send the notification via WebSocket
+        // For now, we just log it
+        debug!("Broadcast notification: {:?}", notification);
+    }
+
+    /// Check if client is subscribed to a specific key
+    fn is_subscribed_to(&self, client: &SettingsClientInfo, key: &str) -> bool {
+        for subscription in &client.subscriptions {
+            if subscription == "*" || key.starts_with(subscription) {
+                return true;
+            }
+        }
+        false
+    }
+
+    /// Get all registered clients
+    pub async fn get_client_count(&self) -> usize {
+        self.clients.read().await.len()
+    }
+
+    /// Get subscription stats
+    pub async fn get_subscription_stats(&self) -> HashMap<String, usize> {
+        let subscriptions = self.subscriptions.read().await;
+        subscriptions
+            .iter()
+            .map(|(prefix, clients)| (prefix.clone(), clients.len()))
+            .collect()
+    }
+}
+
+fn convert_setting_value_to_json(value: &SettingValue) -> JsonValue {
+    match value {
+        SettingValue::String(s) => JsonValue::String(s.clone()),
+        SettingValue::Integer(i) => JsonValue::Number((*i).into()),
+        SettingValue::Float(f) => JsonValue::Number(
+            serde_json::Number::from_f64(*f).unwrap_or_else(|| serde_json::Number::from(0)),
+        ),
+        SettingValue::Boolean(b) => JsonValue::Bool(*b),
+        SettingValue::Json(j) => j.clone(),
+    }
+}
+
+fn current_timestamp() -> u64 {
+    std::time::SystemTime::now()
+        .duration_since(std::time::UNIX_EPOCH)
+        .unwrap_or_default()
+        .as_millis() as u64
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[tokio::test]
+    async fn test_register_unregister_client() {
+        let manager = SettingsBroadcastManager::new();
+
+        manager.register_client("client1".to_string()).await;
+        assert_eq!(manager.get_client_count().await, 1);
+
+        manager.unregister_client("client1").await;
+        assert_eq!(manager.get_client_count().await, 0);
+    }
+
+    #[tokio::test]
+    async fn test_subscribe_unsubscribe() {
+        let manager = SettingsBroadcastManager::new();
+
+        manager.register_client("client1".to_string()).await;
+        manager
+            .subscribe("client1", "visualisation.physics".to_string())
+            .await;
+
+        let stats = manager.get_subscription_stats().await;
+        assert_eq!(stats.get("visualisation.physics"), Some(&1));
+
+        manager.unsubscribe("client1", "visualisation.physics").await;
+        let stats = manager.get_subscription_stats().await;
+        assert_eq!(stats.get("visualisation.physics"), Some(&0));
+    }
+}
diff --git a/src/handlers/bots_handler.rs b/src/handlers/bots_handler.rs
index 63033f14..cc24a5be 100644
--- a/src/handlers/bots_handler.rs
+++ b/src/handlers/bots_handler.rs
@@ -195,7 +195,7 @@ pub async fn update_bots_graph(request: web::Json<BotsDataRequest>, _state: web:

 pub async fn get_bots_data(state: web::Data<AppState>) -> Result<impl Responder> {
     // First try to get data from graph actor if available
-    if let Ok(graph_data) = state.graph_service_addr.send(GetBotsGraphData).await {
+    if let Ok(graph_data) = state.graph_state_addr.send(GetBotsGraphData).await {
         if let Ok(graph) = graph_data {
             let nodes = &graph.nodes;
             let edges = &graph.edges;
diff --git a/src/handlers/client_logs.rs b/src/handlers/client_logs.rs
new file mode 100644
index 00000000..5ca06d25
--- /dev/null
+++ b/src/handlers/client_logs.rs
@@ -0,0 +1,32 @@
+use actix_web::{web, HttpResponse, Error};
+use serde::Deserialize;
+use log::{info, warn, error, debug};
+
+#[derive(Deserialize)]
+pub struct ClientLogEntry {
+    level: String,
+    message: String,
+    timestamp: Option<String>,
+    context: Option<String>,
+}
+
+/// Simple handler to receive client-side logs and forward to server logger
+pub async fn post_client_logs(
+    web::Json(payload): web::Json<ClientLogEntry>
+) -> Result<HttpResponse, Error> {
+    let prefix = "[CLIENT]";
+    let msg = if let Some(ctx) = payload.context {
+        format!("{} {}", payload.message, ctx)
+    } else {
+        payload.message
+    };
+
+    match payload.level.to_lowercase().as_str() {
+        "error" => error!("{} {}", prefix, msg),
+        "warn" => warn!("{} {}", prefix, msg),
+        "debug" => debug!("{} {}", prefix, msg),
+        _ => info!("{} {}", prefix, msg),
+    }
+
+    Ok(HttpResponse::Ok().json(serde_json::json!({"success": true})))
+}
diff --git a/src/handlers/clustering_handler.rs b/src/handlers/clustering_handler.rs
index b627a934..3a8e66a2 100644
--- a/src/handlers/clustering_handler.rs
+++ b/src/handlers/clustering_handler.rs
@@ -300,7 +300,7 @@ async fn get_clustering_results(
         use crate::actors::messages::{GetClusteringResults, GetGraphData};

         // Get graph data for node count
-        let graph_data = match state.graph_service_addr.send(GetGraphData).await {
+        let graph_data = match state.graph_state_addr.send(GetGraphData).await {
             Ok(Ok(data)) => data,
             Ok(Err(e)) => {
                 error!("Failed to get graph data: {}", e);
@@ -510,7 +510,7 @@ async fn export_cluster_assignments(
     }

     // Fallback: try to get data from graph service
-    match state.graph_service_addr.send(crate::actors::messages::GetGraphData).await {
+    match state.graph_state_addr.send(crate::actors::messages::GetGraphData).await {
         Ok(Ok(graph_data)) => {
             if !graph_data.nodes.is_empty() {
                 info!("Using graph data for clustering export with {} nodes", graph_data.nodes.len());
diff --git a/src/handlers/consolidated_health_handler.rs b/src/handlers/consolidated_health_handler.rs
index 357809f7..e314df48 100644
--- a/src/handlers/consolidated_health_handler.rs
+++ b/src/handlers/consolidated_health_handler.rs
@@ -143,7 +143,7 @@ async fn check_service_metrics(
     // Check graph service
     let (nodes_count, edges_count) = match tokio::time::timeout(
         Duration::from_secs(5),
-        app_state.graph_service_addr.send(GetGraphData)
+        app_state.graph_state_addr.send(GetGraphData)
     ).await {
         Ok(Ok(Ok(graph_data))) => (graph_data.nodes.len(), graph_data.edges.len()),
         Ok(Ok(Err(_))) => {
@@ -275,7 +275,7 @@ async fn get_physics_diagnostics(app_state: &web::Data<AppState>) -> Result<(Str
     // Check graph service
     match tokio::time::timeout(
         Duration::from_secs(3),
-        app_state.graph_service_addr.send(GetGraphData)
+        app_state.graph_state_addr.send(GetGraphData)
     ).await {
         Ok(Ok(Ok(graph_data))) => {
             diagnostics.push(format!("Graph: {} nodes, {} edges", graph_data.nodes.len(), graph_data.edges.len()));
diff --git a/src/handlers/graph_state_handler.rs b/src/handlers/graph_state_handler.rs
index c026bb72..685e9f5f 100644
--- a/src/handlers/graph_state_handler.rs
+++ b/src/handlers/graph_state_handler.rs
@@ -28,8 +28,8 @@ pub struct NodePosition {
 pub async fn get_graph_state(state: web::Data<AppState>) -> impl Responder {
     info!("Received request for complete graph state");

-    // Get graph data from GraphServiceActor
-    let graph_data_result = state.graph_service_addr.send(GetGraphData).await;
+    // Get graph data from GraphStateActor
+    let graph_data_result = state.graph_state_addr.send(GetGraphData).await;

     match graph_data_result {
         Ok(Ok(graph_data)) => {
diff --git a/src/handlers/mod.rs b/src/handlers/mod.rs
index 0f26f745..131a910e 100755
--- a/src/handlers/mod.rs
+++ b/src/handlers/mod.rs
@@ -23,7 +23,12 @@ pub mod graph_export_handler;
 pub mod realtime_websocket_handler;
 pub mod websocket_settings_handler;
 pub mod client_log_handler;
+pub mod client_logs; // Simplified client logs handler
 pub mod client_messages_handler;
+#[cfg(feature = "ontology")]
+pub mod admin_handler;
+#[cfg(feature = "ontology")]
+pub mod user_settings_handler;

 #[cfg(test)]
 pub mod tests;
diff --git a/src/handlers/settings_handler.rs b/src/handlers/settings_handler.rs
index c933d8f7..f423123d 100644
--- a/src/handlers/settings_handler.rs
+++ b/src/handlers/settings_handler.rs
@@ -1530,7 +1530,9 @@ impl EnhancedSettingsHandler {
         );

         // Get cache statistics
-        let (cache_entries, cache_ages) = crate::models::user_settings::UserSettings::get_cache_stats();
+        // TODO: Integrate with new user_service-based caching
+        let cache_entries = 0;
+        let cache_ages: Vec<(String, std::time::Duration)> = Vec::new();

         // Calculate cache metrics
         let cache_hit_rate = if cache_entries > 0 {
@@ -1669,13 +1671,9 @@ pub fn config(cfg: &mut web::ServiceConfig) {
     cfg.app_data(handler.clone())
         .service(
             web::scope("/settings")
-                // Modern path-based endpoints
-                .route("/path", web::get().to(get_setting_by_path))
-                .route("/path", web::put().to(update_setting_by_path))
-                // Batch endpoints moved to settings_paths.rs to avoid duplicate route conflicts
-                // .route("/batch", web::post().to(batch_get_settings))
-                // .route("/batch", web::put().to(batch_update_settings))
-                .route("/schema", web::get().to(get_settings_schema))
+                // Path-based batch endpoints from settings_paths module (includes /path, /batch, /schema)
+                .configure(crate::handlers::settings_paths::configure_settings_paths)
+                // Additional endpoints specific to this handler
                 .route("/current", web::get().to(get_current_settings))
                 // Legacy endpoints (kept for compatibility but deprecated)
                 .route("", web::get().to(get_settings))
@@ -2369,7 +2367,7 @@ async fn reset_settings(
     }
 }

-/// Explicitly save current settings to file
+/// Explicitly save current settings to database
 async fn save_settings(
     _req: HttpRequest,
     state: web::Data<AppState>,
@@ -2413,47 +2411,27 @@ async fn save_settings(
         }
     }

-    // Check if persist_settings is enabled
-    if !app_settings.system.persist_settings {
-        return Ok(HttpResponse::BadRequest().json(json!({
-            "error": "Settings persistence is disabled. Enable 'system.persist_settings' to save settings."
-        })));
-    }
-
-    // Save the settings to file
-    match app_settings.save() {
-        Ok(()) => {
-            info!("Settings successfully saved to file");
-
-            // Update the settings in the actor to ensure consistency
-            match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
-                Ok(Ok(())) => {
-                    let response_dto: SettingsResponseDTO = (&app_settings).into();
-                    Ok(HttpResponse::Ok().json(json!({
-                        "message": "Settings saved successfully",
-                        "settings": response_dto
-                    })))
-                }
-                Ok(Err(e)) => {
-                    error!("Failed to update settings in actor after save: {}", e);
-                    Ok(HttpResponse::InternalServerError().json(json!({
-                        "error": "Settings saved to file but failed to update in memory",
-                        "details": e.to_string()
-                    })))
-                }
-                Err(e) => {
-                    error!("Settings actor communication error: {}", e);
-                    Ok(HttpResponse::ServiceUnavailable().json(json!({
-                        "error": "Settings saved to file but service is unavailable"
-                    })))
-                }
-            }
+    // Update the settings in the actor (which automatically saves to database)
+    match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+        Ok(Ok(())) => {
+            info!("Settings successfully saved to database");
+            let response_dto: SettingsResponseDTO = (&app_settings).into();
+            Ok(HttpResponse::Ok().json(json!({
+                "message": "Settings saved successfully to database",
+                "settings": response_dto
+            })))
         }
-        Err(e) => {
-            error!("Failed to save settings to file: {}", e);
+        Ok(Err(e)) => {
+            error!("Failed to save settings to database: {}", e);
             Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "Failed to save settings to file",
-                "details": e
+                "error": "Failed to save settings to database",
+                "details": e.to_string()
+            })))
+        }
+        Err(e) => {
+            error!("Settings actor communication error: {}", e);
+            Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
             })))
         }
     }
@@ -2932,11 +2910,11 @@ async fn propagate_physics_to_gpu(
     }

     // Send to graph service actor
-    info!("[PHYSICS UPDATE] Sending to GraphServiceActor...");
-    if let Err(e) = state.graph_service_addr.send(update_msg).await {
-        error!("[PHYSICS UPDATE] FAILED to update GraphServiceActor: {}", e);
+    info!("[PHYSICS UPDATE] Sending to GraphStateActor...");
+    if let Err(e) = state.graph_state_addr.send(update_msg).await {
+        error!("[PHYSICS UPDATE] FAILED to update GraphStateActor: {}", e);
     } else {
-        info!("[PHYSICS UPDATE] GraphServiceActor updated successfully");
+        info!("[PHYSICS UPDATE] GraphStateActor updated successfully");
     }
 }

@@ -3321,7 +3299,7 @@ async fn get_cluster_analytics(
         use crate::actors::messages::{GetClusteringResults, GetGraphData};

         // First get graph data
-        let graph_data = match state.graph_service_addr.send(GetGraphData).await {
+        let graph_data = match state.graph_state_addr.send(GetGraphData).await {
             Ok(Ok(data)) => data,
             Ok(Err(e)) => {
                 error!("Failed to get graph data for clustering analytics: {}", e);
@@ -3343,7 +3321,7 @@ async fn get_cluster_analytics(
     } else {
         // Get graph data for CPU fallback
         use crate::actors::messages::GetGraphData;
-        match state.graph_service_addr.send(GetGraphData).await {
+        match state.graph_state_addr.send(GetGraphData).await {
             Ok(Ok(graph_data)) => get_cpu_fallback_analytics(&graph_data).await,
             Ok(Err(e)) => {
                 error!("Failed to get graph data: {}", e);
diff --git a/src/handlers/settings_paths.rs b/src/handlers/settings_paths.rs
index 9987ff3f..3a01191d 100644
--- a/src/handlers/settings_paths.rs
+++ b/src/handlers/settings_paths.rs
@@ -617,13 +617,12 @@ fn generate_value_schema(value: &Value, path: &str) -> Value {
 }

 // Configuration for path-based settings routes
+// These are direct routes without a scope wrapper, meant to be included within /settings scope
 pub fn configure_settings_paths(cfg: &mut web::ServiceConfig) {
-    cfg.service(
-        web::scope("/settings")
-            .route("/path", web::get().to(get_settings_by_path))
-            .route("/path", web::put().to(update_settings_by_path))
-            .route("/batch", web::post().to(batch_read_settings_by_path))
-            .route("/batch", web::put().to(batch_update_settings_by_path))
-            .route("/schema", web::get().to(get_settings_schema))
-    );
+    cfg
+        .route("/path", web::get().to(get_settings_by_path))
+        .route("/path", web::put().to(update_settings_by_path))
+        .route("/batch", web::post().to(batch_read_settings_by_path))
+        .route("/batch", web::put().to(batch_update_settings_by_path))
+        .route("/schema", web::get().to(get_settings_schema));
 }
\ No newline at end of file
diff --git a/src/handlers/socket_flow_handler.rs b/src/handlers/socket_flow_handler.rs
index 38bafba9..2149e2d9 100644
--- a/src/handlers/socket_flow_handler.rs
+++ b/src/handlers/socket_flow_handler.rs
@@ -201,17 +201,51 @@ impl SocketFlowServer {
         // Spawn async task to fetch and send state
         actix::spawn(async move {
             // Get graph state
-            if let Ok(Ok(graph_data)) = app_state.graph_service_addr.send(crate::actors::messages::GetGraphData).await {
+            if let Ok(Ok(graph_data)) = app_state.graph_state_addr.send(crate::actors::messages::GetGraphData).await {
                 // Get settings with version
                 if let Ok(Ok(settings)) = app_state.settings_addr.send(crate::actors::messages::GetSettings).await {
-                    // Prepare state sync message
+                    // Prepare nodes with labels for JSON
+                    let nodes_json: Vec<serde_json::Value> = graph_data.nodes.iter().map(|node| {
+                        serde_json::json!({
+                            "id": node.id,
+                            "metadata_id": node.metadata_id,
+                            "label": node.label,
+                            "position": {
+                                "x": node.data.x,
+                                "y": node.data.y,
+                                "z": node.data.z,
+                            },
+                            "velocity": {
+                                "x": node.data.vx,
+                                "y": node.data.vy,
+                                "z": node.data.vz,
+                            },
+                            "type": node.node_type,
+                            "size": node.size,
+                            "color": node.color,
+                            "weight": node.weight,
+                            "group": node.group,
+                        })
+                    }).collect();
+
+                    // Prepare edges for JSON
+                    let edges_json: Vec<serde_json::Value> = graph_data.edges.iter().map(|edge| {
+                        serde_json::json!({
+                            "id": edge.id,
+                            "source": edge.source,
+                            "target": edge.target,
+                            "weight": edge.weight,
+                        })
+                    }).collect();
+
+                    // Prepare state sync message with FULL graph data
                     let state_sync = serde_json::json!({
                         "type": "state_sync",
                         "data": {
                             "graph": {
-                                "nodes_count": graph_data.nodes.len(),
-                                "edges_count": graph_data.edges.len(),
-                                "metadata_count": graph_data.metadata.len(),
+                                "nodes": nodes_json,
+                                "edges": edges_json,
+                                "metadata": graph_data.metadata,
                             },
                             "settings": {
                                 "version": settings.version,
@@ -222,21 +256,31 @@ impl SocketFlowServer {
                                 .as_secs(),
                         }
                     });
-
+
                     // Send state sync as text message through the actor
                     if let Ok(msg_str) = serde_json::to_string(&state_sync) {
                         addr.do_send(SendToClientText(msg_str));
-                        info!("Sent state sync: {} nodes, {} edges, version: {}",
-                            graph_data.nodes.len(),
+                        info!("Sent full state sync: {} nodes, {} edges (with labels), version: {}",
+                            graph_data.nodes.len(),
                             graph_data.edges.len(),
                             settings.version
                         );
                     }
-
-                    // Send initial positions as binary data
+
+                    // Send initial positions as binary data with type classification
                     if !graph_data.nodes.is_empty() {
-                        let node_data: Vec<(u32, BinaryNodeData)> = graph_data.nodes.iter()
-                            .map(|node| (node.id, BinaryNodeData {
+                        let mut node_data: Vec<(u32, BinaryNodeData)> = Vec::new();
+                        let mut agent_ids: Vec<u32> = Vec::new();
+                        let mut knowledge_ids: Vec<u32> = Vec::new();
+                        let mut ontology_class_ids: Vec<u32> = Vec::new();
+                        let mut ontology_individual_ids: Vec<u32> = Vec::new();
+                        let mut ontology_property_ids: Vec<u32> = Vec::new();
+
+                        // Debug: Sample node types
+                        let mut type_samples: std::collections::HashMap<String, u32> = std::collections::HashMap::new();
+
+                        for node in &graph_data.nodes {
+                            node_data.push((node.id, BinaryNodeData {
                                 node_id: node.id,
                                 x: node.data.x,
                                 y: node.data.y,
@@ -244,12 +288,43 @@ impl SocketFlowServer {
                                 vx: node.data.vx,
                                 vy: node.data.vy,
                                 vz: node.data.vz,
-                            }))
-                            .collect();
-
-                        // Send position update through the actor
-                        addr.do_send(BroadcastPositionUpdate(node_data));
-                        debug!("Sent initial node positions for state sync");
+                            }));
+
+                            // Classify node by its type field
+                            let node_type_key = node.node_type.as_ref().map(|s| s.as_str()).unwrap_or("None");
+                            *type_samples.entry(node_type_key.to_string()).or_insert(0) += 1;
+
+                            match node.node_type.as_deref() {
+                                Some("agent") | Some("bot") => agent_ids.push(node.id),
+                                Some("ontology_class") | Some("owl_class") => ontology_class_ids.push(node.id),
+                                Some("ontology_individual") | Some("owl_individual") => ontology_individual_ids.push(node.id),
+                                Some("ontology_property") | Some("owl_property") => ontology_property_ids.push(node.id),
+                                // Default to knowledge graph for backward compatibility
+                                _ => knowledge_ids.push(node.id),
+                            }
+                        }
+
+                        info!("STATE SYNC - Node type distribution: {:?}", type_samples);
+                        info!("STATE SYNC - Total nodes: {}, Knowledge: {}, Agents: {}, Ontology: {}",
+                              graph_data.nodes.len(), knowledge_ids.len(), agent_ids.len(),
+                              ontology_class_ids.len() + ontology_individual_ids.len() + ontology_property_ids.len());
+
+                        // Encode with proper type flags
+                        let binary_data = crate::utils::binary_protocol::encode_node_data_extended(
+                            &node_data,
+                            &agent_ids,
+                            &knowledge_ids,
+                            &ontology_class_ids,
+                            &ontology_individual_ids,
+                            &ontology_property_ids
+                        );
+
+                        // Send binary update to client
+                        addr.do_send(SendToClientBinary(binary_data));
+
+                        info!("Sent initial node positions for state sync: {} knowledge, {} agents, {} ontology",
+                               knowledge_ids.len(), agent_ids.len(),
+                               ontology_class_ids.len() + ontology_individual_ids.len() + ontology_property_ids.len());
                     }
                 }
             }
@@ -475,9 +550,9 @@ async fn fetch_nodes(
     app_state: Arc<AppState>,
     _settings_addr: actix::Addr<crate::actors::optimized_settings_actor::OptimizedSettingsActor>
 ) -> Option<(Vec<(u32, BinaryNodeData)>, bool)> {
-    // Fetch raw nodes asynchronously from GraphServiceActor
+    // Fetch raw nodes asynchronously from GraphStateActor
     use crate::actors::messages::GetGraphData;
-    let graph_data = match app_state.graph_service_addr.send(GetGraphData).await {
+    let graph_data = match app_state.graph_state_addr.send(GetGraphData).await {
         Ok(Ok(data)) => data,
         Ok(Err(e)) => {
             error!("[WebSocket] Failed to get graph data: {}", e);
@@ -576,7 +651,7 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for SocketFlowServer
                                     .map_or(true, |arr| arr.iter().any(|v| v.as_str() == Some("agent")));

                                 // Request snapshot from graph actor
-                                let graph_addr = self.app_state.graph_service_addr.clone();
+                                let graph_addr = self.app_state.graph_state_addr.clone();
                                 let fut = async move {
                                     use crate::actors::messages::RequestPositionSnapshot;
                                     graph_addr.send(RequestPositionSnapshot {
@@ -651,7 +726,7 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for SocketFlowServer
                                 info!("Client requested bots graph - returning optimized position data only");

                                 // Send position-only graph structure + REST API references
-                                let graph_addr = self.app_state.graph_service_addr.clone();
+                                let graph_addr = self.app_state.graph_state_addr.clone();

                                 ctx.spawn(actix::fut::wrap_future::<_, Self>(async move {
                                     // Get minimal graph data from GraphServiceActor
@@ -1084,9 +1159,9 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for SocketFlowServer
                                     debug!("Updated position for node ID {} to [{:.3}, {:.3}, {:.3}]",
                                          node_id, node_data.x, node_data.y, node_data.z);

-                                    // Send update message to GraphServiceActor (now uses u32 directly)
+                                    // Send update message to GraphStateActor (now uses u32 directly)
                                     use crate::actors::messages::UpdateNodePosition;
-                                    if let Err(e) = app_state.graph_service_addr.send(UpdateNodePosition {
+                                    if let Err(e) = app_state.graph_state_addr.send(UpdateNodePosition {
                                         node_id: node_id,
                                         position: node_data.position().into(),
                                         velocity: node_data.velocity().into(),
@@ -1112,9 +1187,9 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for SocketFlowServer
                                 if let Ok(Ok(_iterations_val)) = settings_addr.send(GetSettingByPath { path: "visualisation.graphs.logseq.physics.iterations".to_string() }).await {
                                     if let Ok(Ok(_spring_val)) = settings_addr.send(GetSettingByPath { path: "visualisation.graphs.logseq.physics.spring_k".to_string() }).await {
                                         if let Ok(Ok(_repulsion_val)) = settings_addr.send(GetSettingByPath { path: "visualisation.graphs.logseq.physics.repel_k".to_string() }).await {
-                                            // Send simulation step message to GraphServiceActor
+                                            // Send simulation step message to GraphStateActor
                                             use crate::actors::messages::SimulationStep;
-                                            if let Err(e) = app_state.graph_service_addr.send(SimulationStep).await {
+                                            if let Err(e) = app_state.graph_state_addr.send(SimulationStep).await {
                                                 error!("Failed to trigger simulation step: {}", e);
                                             } else {
                                                 info!("Successfully triggered layout recalculation");
diff --git a/src/handlers/user_settings_handler.rs b/src/handlers/user_settings_handler.rs
new file mode 100644
index 00000000..1d32ea93
--- /dev/null
+++ b/src/handlers/user_settings_handler.rs
@@ -0,0 +1,190 @@
+use actix_web::{web, Error, HttpResponse, HttpRequest};
+use serde::{Deserialize, Serialize};
+use serde_json::Value as JsonValue;
+use log::{error, info};
+
+use crate::services::user_service::{UserService, UserServiceError, SettingValue, UserSetting};
+use crate::middleware::permissions::extract_auth_context;
+
+#[derive(Debug, Serialize)]
+struct UserSettingsResponse {
+    settings: Vec<UserSettingDTO>,
+}
+
+#[derive(Debug, Serialize)]
+struct UserSettingDTO {
+    key: String,
+    value: JsonValue,
+    created_at: i64,
+    updated_at: i64,
+}
+
+#[derive(Debug, Deserialize)]
+struct SetUserSettingRequest {
+    key: String,
+    value: JsonValue,
+}
+
+#[derive(Debug, Serialize)]
+struct MessageResponse {
+    message: String,
+}
+
+#[derive(Debug, Serialize)]
+struct ErrorResponse {
+    error: String,
+}
+
+fn setting_value_to_json(value: &SettingValue) -> JsonValue {
+    match value {
+        SettingValue::String(s) => JsonValue::String(s.clone()),
+        SettingValue::Integer(i) => JsonValue::Number(serde_json::Number::from(*i)),
+        SettingValue::Float(f) => {
+            serde_json::Number::from_f64(*f)
+                .map(JsonValue::Number)
+                .unwrap_or(JsonValue::Null)
+        }
+        SettingValue::Boolean(b) => JsonValue::Bool(*b),
+        SettingValue::Json(j) => j.clone(),
+    }
+}
+
+fn json_to_setting_value(value: JsonValue) -> SettingValue {
+    match value {
+        JsonValue::String(s) => SettingValue::String(s),
+        JsonValue::Number(n) => {
+            if let Some(i) = n.as_i64() {
+                SettingValue::Integer(i)
+            } else if let Some(f) = n.as_f64() {
+                SettingValue::Float(f)
+            } else {
+                SettingValue::String(n.to_string())
+            }
+        }
+        JsonValue::Bool(b) => SettingValue::Boolean(b),
+        _ => SettingValue::Json(value),
+    }
+}
+
+pub async fn get_user_settings(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    match user_service.get_user_settings(auth_context.user_id).await {
+        Ok(settings) => {
+            let dto: Vec<UserSettingDTO> = settings
+                .into_iter()
+                .map(|s| UserSettingDTO {
+                    key: s.key,
+                    value: setting_value_to_json(&s.value),
+                    created_at: s.created_at,
+                    updated_at: s.updated_at,
+                })
+                .collect();
+
+            info!(
+                "Retrieved {} settings for user_id={}",
+                dto.len(),
+                auth_context.user_id
+            );
+            Ok(HttpResponse::Ok().json(UserSettingsResponse { settings: dto }))
+        }
+        Err(e) => {
+            error!("Failed to get user settings: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch user settings".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn set_user_setting(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    payload: web::Json<SetUserSettingRequest>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required to modify settings".to_string(),
+        }));
+    }
+
+    let setting_value = json_to_setting_value(payload.value.clone());
+
+    match user_service
+        .set_user_setting(auth_context.user_id, &payload.key, setting_value)
+        .await
+    {
+        Ok(()) => {
+            info!(
+                "User {} set setting {}",
+                auth_context.nostr_pubkey, payload.key
+            );
+            Ok(HttpResponse::Ok().json(MessageResponse {
+                message: format!("Setting {} updated successfully", payload.key),
+            }))
+        }
+        Err(e) => {
+            error!("Failed to set user setting: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to update setting".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn delete_user_setting(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    path: web::Path<String>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required to delete settings".to_string(),
+        }));
+    }
+
+    let key = path.into_inner();
+
+    match user_service
+        .delete_user_setting(auth_context.user_id, &key)
+        .await
+    {
+        Ok(()) => {
+            info!(
+                "User {} deleted setting {}",
+                auth_context.nostr_pubkey, key
+            );
+            Ok(HttpResponse::Ok().json(MessageResponse {
+                message: format!("Setting {} deleted successfully", key),
+            }))
+        }
+        Err(UserServiceError::UserNotFound) => Ok(HttpResponse::NotFound().json(ErrorResponse {
+            error: "Setting not found".to_string(),
+        })),
+        Err(e) => {
+            error!("Failed to delete user setting: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to delete setting".to_string(),
+            }))
+        }
+    }
+}
+
+pub fn configure_routes(cfg: &mut web::ServiceConfig) {
+    cfg.service(
+        web::scope("/api/user-settings")
+            .route("", web::get().to(get_user_settings))
+            .route("", web::post().to(set_user_setting))
+            .route("/{key}", web::delete().to(delete_user_setting)),
+    );
+}
diff --git a/src/lib.rs b/src/lib.rs
index 9d58fd69..42004ba5 100755
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -1,13 +1,16 @@
 pub mod ontology;
 pub mod actors;
+pub mod adapters;
 pub mod app_state;
 pub mod client;
 pub mod config;
 pub mod errors;
 pub mod gpu;
 pub mod handlers;
+pub mod middleware;
 pub mod models;
 pub mod physics;
+pub mod ports;
 pub mod services;
 pub mod telemetry;
 pub mod types;
@@ -22,4 +25,3 @@ pub use models::metadata::MetadataStore;
 pub use models::protected_settings::ProtectedSettings;
 pub use models::simulation_params::SimulationParams;
 // pub use models::ui_settings::UISettings; // Removed - consolidated into AppFullSettings"
-pub use models::user_settings::UserSettings;
diff --git a/src/main.rs b/src/main.rs
index a8ed724f..99f4cc6f 100755
--- a/src/main.rs
+++ b/src/main.rs
@@ -15,6 +15,7 @@ use webxr::{
         workspace_handler,
         graph_export_handler,
         client_log_handler,
+        client_logs,
         client_messages_handler,
     },
     services::{
@@ -161,11 +162,10 @@ async fn main() -> std::io::Result<()> {
         info!("Telemetry logger initialized with directory: {}", log_dir);
     }

-    // Load settings
+    // Load settings (returns default structure - actual settings managed via SQLite)
     let settings = match AppFullSettings::new() {
         Ok(s) => {
-            info!("✅ AppFullSettings loaded successfully from: {}",
-                std::env::var("SETTINGS_FILE_PATH").unwrap_or_else(|_| "/app/settings.yaml".to_string()));
+            info!("✅ AppFullSettings structure initialized (settings managed via SQLite database)");

             // Test JSON serialization to verify camelCase output works
             match serde_json::to_string(&s.visualisation.rendering) {
@@ -250,6 +250,70 @@ async fn main() -> std::io::Result<()> {
         error!("[main] ragflow_service_option is None after RAGFlowService::new attempt. Chat functionality will be unavailable.");
     }

+    // Initialize database and DevConfig before creating AppState
+    info!("Initializing database service...");
+    let db_path = std::env::var("DATA_ROOT")
+        .unwrap_or_else(|_| "/app/data".to_string());
+    let db_file = std::path::PathBuf::from(&db_path).join("settings.db");
+
+    let db_service = match webxr::services::database_service::DatabaseService::new(&db_file) {
+        Ok(service) => {
+            info!("✅ Database initialized successfully");
+            std::sync::Arc::new(service)
+        }
+        Err(e) => {
+            error!("❌ Failed to initialize database: {}", e);
+            return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Database initialization failed: {}", e)));
+        }
+    };
+
+    // Initialize schema
+    if let Err(e) = db_service.initialize_schema() {
+        error!("❌ Failed to initialize database schema: {}", e);
+        return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Schema initialization failed: {}", e)));
+    }
+
+    // Seed default settings if database is empty
+    info!("Checking if database needs default settings...");
+    match db_service.get_setting("app_full_settings") {
+        Ok(None) => {
+            info!("Database is empty, seeding default settings...");
+            let default_settings = AppFullSettings::default();
+            let settings_json = match serde_json::to_value(&default_settings) {
+                Ok(json) => json,
+                Err(e) => {
+                    error!("❌ Failed to serialize default settings: {}", e);
+                    return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Failed to serialize default settings: {}", e)));
+                }
+            };
+
+            if let Err(e) = db_service.set_setting(
+                "app_full_settings",
+                webxr::services::database_service::SettingValue::Json(settings_json),
+                Some("Default application settings in camelCase format")
+            ) {
+                error!("❌ Failed to seed default settings: {}", e);
+                return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Failed to seed default settings: {}", e)));
+            }
+            info!("✅ Default settings seeded successfully");
+        }
+        Ok(Some(_)) => {
+            info!("✅ Settings already exist in database");
+        }
+        Err(e) => {
+            error!("❌ Failed to check database settings: {}", e);
+            return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Failed to check database settings: {}", e)));
+        }
+    }
+
+    // Initialize DevConfig from database
+    info!("Initializing DevConfig from database...");
+    if let Err(e) = webxr::config::dev_config::DevConfig::initialize(std::sync::Arc::clone(&db_service)) {
+        warn!("DevConfig initialization failed, using defaults: {}", e);
+    } else {
+        info!("✅ DevConfig initialized successfully");
+    }
+
     // Initialize app state asynchronously
     // AppState::new now receives AppFullSettings directly (not Arc<RwLock<>>)
     let settings_value = {
@@ -259,6 +323,7 @@ async fn main() -> std::io::Result<()> {

     let mut app_state = match AppState::new(
             settings_value,
+            db_service.clone(), // Pass database service for settings actor
             github_client.clone(),
             content_api.clone(),
             None, // Perplexity placeholder
@@ -317,7 +382,7 @@ async fn main() -> std::io::Result<()> {
     let content_api_clone = content_api.clone();
     let settings_clone = settings.clone();
     let metadata_addr_clone = app_state.metadata_addr.clone();
-    let graph_service_addr_clone = app_state.graph_service_addr.clone();
+    let graph_state_addr_clone = app_state.graph_state_addr.clone();

     tokio::spawn(async move {
         // Wait a bit for the server to fully initialize
@@ -375,7 +440,7 @@ async fn main() -> std::io::Result<()> {
                 // Update graph with new data
                 use webxr::actors::messages::AddNodesFromMetadata;
                 info!("Background GitHub sync: Updating graph with new data...");
-                match graph_service_addr_clone.send(AddNodesFromMetadata { metadata: metadata_store_copy }).await {
+                match graph_state_addr_clone.send(AddNodesFromMetadata { metadata: metadata_store_copy }).await {
                     Ok(Ok(())) => {
                         info!("Background GitHub sync: Graph updated successfully with new GitHub data");
                     }
@@ -431,14 +496,14 @@ async fn main() -> std::io::Result<()> {

     if let Some(graph_data) = graph_data_option {
         // If we have pre-computed graph data, send it directly to the GraphServiceSupervisor
-        match app_state.graph_service_addr.send(UpdateGraphData { graph_data: StdArc::new(graph_data) }).await {
+        match app_state.graph_state_addr.send(UpdateGraphData { graph_data: StdArc::new(graph_data) }).await {
             Ok(Ok(())) => {
                 info!("Pre-computed graph data loaded successfully into GraphServiceSupervisor");
             },
             Ok(Err(e)) => {
                 error!("Failed to load pre-computed graph data into actor: {}", e);
                 // Fall back to building from metadata
-                match app_state.graph_service_addr.send(BuildGraphFromMetadata { metadata: metadata_store.clone() }).await {
+                match app_state.graph_state_addr.send(BuildGraphFromMetadata { metadata: metadata_store.clone() }).await {
                     Ok(Ok(())) => {
                         info!("Fallback: Graph built from metadata successfully");
                     },
@@ -459,7 +524,7 @@ async fn main() -> std::io::Result<()> {
         }
     } else {
         // No pre-computed graph data, build from metadata
-        match app_state.graph_service_addr.send(BuildGraphFromMetadata { metadata: metadata_store.clone() }).await {
+        match app_state.graph_state_addr.send(BuildGraphFromMetadata { metadata: metadata_store.clone() }).await {
             Ok(Ok(())) => {
                 info!("Graph built successfully using GraphServiceSupervisor - GPU initialization is handled automatically by the supervisor");
             },
@@ -476,7 +541,32 @@ async fn main() -> std::io::Result<()> {

     info!("Waiting for initial physics layout calculation to complete...");
     tokio::time::sleep(Duration::from_millis(500)).await;
-    info!("Initial delay complete. Starting HTTP server...");
+    info!("Initial delay complete.");
+
+    // Initialize ontology system
+    #[cfg(feature = "ontology")]
+    {
+        info!("🔮 Initializing Ontology System...");
+        use webxr::services::ontology_init::initialize_ontology_system;
+
+        match initialize_ontology_system().await {
+            Ok(_db_service) => {
+                info!("✅ Ontology system initialized successfully");
+                info!("📥 Background GitHub sync task spawned - will begin in 10 seconds");
+            }
+            Err(e) => {
+                error!("❌ Ontology system initialization failed: {}", e);
+                info!("⚠️  Continuing without ontology features");
+            }
+        }
+    }
+
+    #[cfg(not(feature = "ontology"))]
+    {
+        info!("Ontology feature not enabled");
+    }
+
+    info!("Starting HTTP server...");

     // Start simulation in GraphServiceSupervisor (Second start attempt commented out for debugging stack overflow)
     // use webxr::actors::messages::StartSimulation;
@@ -494,8 +584,18 @@ async fn main() -> std::io::Result<()> {
     // Start the server
     let bind_address = {
         let settings_read = settings.read().await; // Reads AppFullSettings
-        // Access network settings correctly
-        format!("{}:{}", settings_read.system.network.bind_address, settings_read.system.network.port)
+        // Access network settings correctly, with fallback to env vars
+        let addr = if settings_read.system.network.bind_address.is_empty() {
+            std::env::var("BIND_ADDRESS").unwrap_or_else(|_| "0.0.0.0".to_string())
+        } else {
+            settings_read.system.network.bind_address.clone()
+        };
+        let port = if settings_read.system.network.port == 0 {
+            std::env::var("PORT").unwrap_or_else(|_| "4000".to_string()).parse().unwrap_or(4000)
+        } else {
+            settings_read.system.network.port
+        };
+        format!("{}:{}", addr, port)
     };

     // Pre-read WebSocket settings for SocketFlowServer
@@ -535,7 +635,7 @@ async fn main() -> std::io::Result<()> {
             .app_data(app_state_data.clone()) // Add the complete AppState
             .app_data(pre_read_ws_settings_data.clone()) // Add pre-read WebSocket settings
             // Register actor addresses for handler access
-            .app_data(web::Data::new(app_state_data.graph_service_addr.clone()))
+            .app_data(web::Data::new(app_state_data.graph_state_addr.clone()))
             .app_data(web::Data::new(app_state_data.settings_addr.clone()))
             .app_data(web::Data::new(app_state_data.metadata_addr.clone()))
             .app_data(web::Data::new(app_state_data.client_manager_addr.clone()))
@@ -556,7 +656,8 @@ async fn main() -> std::io::Result<()> {
                     .service(web::scope("/bots").configure(api_handler::bots::config)) // This will now serve /api/bots/data and /api/bots/update
                     .configure(bots_visualization_handler::configure_routes) // Agent visualization endpoints
                     .configure(graph_export_handler::configure_routes) // Graph export and sharing endpoints
-                    .route("/client-logs", web::post().to(client_log_handler::handle_client_logs)) // Client browser logs endpoint
+                    .route("/client-logs", web::post().to(client_log_handler::handle_client_logs)) // Client browser logs endpoint (full telemetry)
+                    .route("/client-logs-simple", web::post().to(client_logs::post_client_logs)) // Simplified client logs endpoint (server logger only)
                     // DEPRECATED: hybrid health routes removed
             );

diff --git a/src/middleware/mod.rs b/src/middleware/mod.rs
new file mode 100644
index 00000000..a527b636
--- /dev/null
+++ b/src/middleware/mod.rs
@@ -0,0 +1 @@
+pub mod permissions;
diff --git a/src/middleware/permissions.rs b/src/middleware/permissions.rs
new file mode 100644
index 00000000..bb8cdca3
--- /dev/null
+++ b/src/middleware/permissions.rs
@@ -0,0 +1,190 @@
+use actix_web::{
+    dev::{forward_ready, Service, ServiceRequest, ServiceResponse, Transform},
+    Error, HttpMessage, HttpResponse,
+};
+use futures_util::future::LocalBoxFuture;
+use std::future::{ready, Ready};
+use std::rc::Rc;
+use log::{debug, warn};
+
+use crate::services::nostr_service::NostrService;
+use crate::services::user_service::{UserService, UserServiceError};
+
+pub struct AuthContext {
+    pub user_id: i64,
+    pub nostr_pubkey: String,
+    pub is_power_user: bool,
+}
+
+pub enum PermissionLevel {
+    Authenticated,
+    PowerUser,
+}
+
+pub struct PermissionMiddleware {
+    nostr_service: Rc<NostrService>,
+    user_service: Rc<UserService>,
+    required_level: PermissionLevel,
+}
+
+impl PermissionMiddleware {
+    pub fn new(
+        nostr_service: Rc<NostrService>,
+        user_service: Rc<UserService>,
+        required_level: PermissionLevel,
+    ) -> Self {
+        Self {
+            nostr_service,
+            user_service,
+            required_level,
+        }
+    }
+
+    pub fn authenticated(nostr_service: Rc<NostrService>, user_service: Rc<UserService>) -> Self {
+        Self::new(nostr_service, user_service, PermissionLevel::Authenticated)
+    }
+
+    pub fn power_user(nostr_service: Rc<NostrService>, user_service: Rc<UserService>) -> Self {
+        Self::new(nostr_service, user_service, PermissionLevel::PowerUser)
+    }
+}
+
+impl<S, B> Transform<S, ServiceRequest> for PermissionMiddleware
+where
+    S: Service<ServiceRequest, Response = ServiceResponse<B>, Error = Error> + 'static,
+    S::Future: 'static,
+    B: 'static,
+{
+    type Response = ServiceResponse<B>;
+    type Error = Error;
+    type InitError = ();
+    type Transform = PermissionMiddlewareService<S>;
+    type Future = Ready<Result<Self::Transform, Self::InitError>>;
+
+    fn new_transform(&self, service: S) -> Self::Future {
+        ready(Ok(PermissionMiddlewareService {
+            service: Rc::new(service),
+            nostr_service: self.nostr_service.clone(),
+            user_service: self.user_service.clone(),
+            required_level: match self.required_level {
+                PermissionLevel::Authenticated => PermissionLevel::Authenticated,
+                PermissionLevel::PowerUser => PermissionLevel::PowerUser,
+            },
+        }))
+    }
+}
+
+pub struct PermissionMiddlewareService<S> {
+    service: Rc<S>,
+    nostr_service: Rc<NostrService>,
+    user_service: Rc<UserService>,
+    required_level: PermissionLevel,
+}
+
+impl<S, B> Service<ServiceRequest> for PermissionMiddlewareService<S>
+where
+    S: Service<ServiceRequest, Response = ServiceResponse<B>, Error = Error> + 'static,
+    S::Future: 'static,
+    B: 'static,
+{
+    type Response = ServiceResponse<B>;
+    type Error = Error;
+    type Future = LocalBoxFuture<'static, Result<Self::Response, Self::Error>>;
+
+    forward_ready!(service);
+
+    fn call(&self, req: ServiceRequest) -> Self::Future {
+        let service = self.service.clone();
+        let nostr_service = self.nostr_service.clone();
+        let user_service = self.user_service.clone();
+        let required_level = match self.required_level {
+            PermissionLevel::Authenticated => PermissionLevel::Authenticated,
+            PermissionLevel::PowerUser => PermissionLevel::PowerUser,
+        };
+
+        Box::pin(async move {
+            let pubkey = match req.headers().get("X-Nostr-Pubkey") {
+                Some(value) => value.to_str().unwrap_or("").to_string(),
+                None => {
+                    warn!("Missing Nostr pubkey in request headers");
+                    return Err(actix_web::error::ErrorForbidden("Authentication required"));
+                }
+            };
+
+            let token = match req.headers().get("X-Nostr-Token") {
+                Some(value) => value.to_str().unwrap_or("").to_string(),
+                None => {
+                    warn!("Missing Nostr token in request headers");
+                    return Err(actix_web::error::ErrorForbidden("Authentication required"));
+                }
+            };
+
+            if !nostr_service.validate_session(&pubkey, &token).await {
+                warn!("Invalid or expired session for user {}", pubkey);
+                return Err(actix_web::error::ErrorUnauthorized(
+                    "Invalid or expired session",
+                ));
+            }
+
+            let user = match user_service.get_user_by_nostr_pubkey(&pubkey).await {
+                Ok(u) => u,
+                Err(UserServiceError::UserNotFound) => {
+                    match user_service
+                        .create_or_update_user(&pubkey, None)
+                        .await
+                    {
+                        Ok(u) => u,
+                        Err(e) => {
+                            warn!("Failed to create user for pubkey {}: {:?}", pubkey, e);
+                            return Err(actix_web::error::ErrorInternalServerError(
+                                "Failed to create user",
+                            ));
+                        }
+                    }
+                }
+                Err(e) => {
+                    warn!("Database error fetching user {}: {:?}", pubkey, e);
+                    return Err(actix_web::error::ErrorInternalServerError(
+                        "Database error",
+                    ));
+                }
+            };
+
+            match required_level {
+                PermissionLevel::Authenticated => {
+                    debug!("Authenticated access granted for user_id={}", user.id);
+                }
+                PermissionLevel::PowerUser => {
+                    if !user.is_power_user {
+                        warn!(
+                            "Non-power user {} attempted restricted operation",
+                            pubkey
+                        );
+                        return Err(actix_web::error::ErrorForbidden(
+                            "This operation requires power user access",
+                        ));
+                    }
+                    debug!("Power user access granted for user_id={}", user.id);
+                }
+            }
+
+            let auth_context = AuthContext {
+                user_id: user.id,
+                nostr_pubkey: pubkey,
+                is_power_user: user.is_power_user,
+            };
+
+            req.extensions_mut().insert(auth_context);
+
+            service.call(req).await
+        })
+    }
+}
+
+pub fn extract_auth_context(req: &actix_web::HttpRequest) -> Option<AuthContext> {
+    req.extensions().get::<AuthContext>().map(|ctx| AuthContext {
+        user_id: ctx.user_id,
+        nostr_pubkey: ctx.nostr_pubkey.clone(),
+        is_power_user: ctx.is_power_user,
+    })
+}
diff --git a/src/models/mod.rs b/src/models/mod.rs
index 4557460a..7c696cda 100755
--- a/src/models/mod.rs
+++ b/src/models/mod.rs
@@ -6,7 +6,6 @@ pub mod node;
 pub mod pagination;
 pub mod protected_settings;
 pub mod simulation_params;
-pub mod user_settings;
 pub mod ragflow_chat;
 pub mod constraints;
 pub mod workspace;
@@ -16,5 +15,4 @@ pub use metadata::MetadataStore;
 pub use pagination::PaginationParams;
 pub use protected_settings::ProtectedSettings;
 pub use simulation_params::SimulationParams;
-pub use user_settings::UserSettings;
 pub use workspace::{Workspace, CreateWorkspaceRequest, UpdateWorkspaceRequest, WorkspaceResponse, WorkspaceListResponse};
diff --git a/src/models/user_settings.rs b/src/models/user_settings.rs
deleted file mode 100644
index f0a4d70d..00000000
--- a/src/models/user_settings.rs
+++ /dev/null
@@ -1,266 +0,0 @@
-use serde::{Deserialize, Serialize};
-use std::fs;
-use std::path::PathBuf;
-use std::sync::{Arc, RwLock};
-use std::collections::HashMap;
-use std::time::{Duration, Instant};
-use log::{info, error, debug, warn};
-use tracing::{debug as trace_debug, info as trace_info};
-use once_cell::sync::Lazy;
-use uuid::Uuid;
-
-use crate::config::AppFullSettings;
-
-// Global cache for user settings
-static USER_SETTINGS_CACHE: Lazy<Arc<RwLock<HashMap<String, CachedUserSettings>>>> =
-    Lazy::new(|| Arc::new(RwLock::new(HashMap::new())));
-
-// Cache expiration time (10 minutes)
-const CACHE_EXPIRATION: Duration = Duration::from_secs(10 * 60);
-
-// Cache entry with timestamp
-struct CachedUserSettings {
-    settings: UserSettings,
-    timestamp: Instant,
-}
-
-#[derive(Debug, Clone, Serialize, Deserialize)]
-pub struct UserSettings {
-    pub pubkey: String,
-    pub settings: AppFullSettings,
-    pub last_modified: i64,
-}
-
-impl UserSettings {
-    pub fn new(pubkey: &str, settings: AppFullSettings) -> Self {
-        Self {
-            pubkey: pubkey.to_string(),
-            settings,
-            last_modified: chrono::Utc::now().timestamp(),
-        }
-    }
-
-    pub fn load(pubkey: &str) -> Option<Self> {
-        let request_id = Uuid::new_v4();
-
-        // First check the cache
-        {
-            let cache = match USER_SETTINGS_CACHE.read() {
-                Ok(cache) => cache,
-                Err(e) => {
-                    error!("Failed to read user settings cache: {}", e);
-                    return None;
-                }
-            };
-            if let Some(cached) = cache.get(pubkey) {
-                // Check if cache is still valid
-                if cached.timestamp.elapsed() < CACHE_EXPIRATION {
-                    debug!("Using cached settings for user {}", pubkey);
-                    trace_debug!(
-                        request_id = %request_id,
-                        user_id = %pubkey,
-                        cache_hit = true,
-                        cache_age_secs = cached.timestamp.elapsed().as_secs(),
-                        "Loading user settings - cache hit"
-                    );
-                    return Some(cached.settings.clone());
-                }
-                // Cache expired, will reload from disk
-                debug!("Cache expired for user {}, reloading from disk", pubkey);
-                trace_debug!(
-                    request_id = %request_id,
-                    user_id = %pubkey,
-                    cache_hit = false,
-                    cache_age_secs = cached.timestamp.elapsed().as_secs(),
-                    reason = "cache_expired",
-                    "Cache expired, reloading from disk"
-                );
-            } else {
-                trace_debug!(
-                    request_id = %request_id,
-                    user_id = %pubkey,
-                    cache_hit = false,
-                    reason = "not_in_cache",
-                    "User not in cache, loading from disk"
-                );
-            }
-        }
-
-        // Not in cache or expired, load from disk
-        let path = Self::get_settings_path(pubkey);
-        match fs::read_to_string(&path) {
-            Ok(content) => {
-                match serde_yaml::from_str::<UserSettings>(&content) {
-                    Ok(settings) => {
-                        // Add to cache
-                        let settings_clone = settings.clone();
-                        {
-                            let mut cache = match USER_SETTINGS_CACHE.write() {
-                                Ok(cache) => cache,
-                                Err(e) => {
-                                    error!("Failed to write to user settings cache: {}", e);
-                                    // Continue without caching
-                                    return Some(settings);
-                                }
-                            };
-                            cache.insert(pubkey.to_string(), CachedUserSettings {
-                                settings: settings_clone,
-                                timestamp: Instant::now(),
-                            });
-                        }
-                        info!("Loaded settings for user {} and added to cache", pubkey);
-                        Some(settings)
-                    }
-                    Err(e) => {
-                        error!("Failed to parse settings for user {}: {}", pubkey, e);
-                        None
-                    }
-                }
-            }
-            Err(e) => {
-                debug!("No settings file found for user {}: {}", pubkey, e);
-                None
-            },
-        }
-    }
-
-    pub fn save(&self) -> Result<(), String> {
-        let path = Self::get_settings_path(&self.pubkey);
-
-        // Update cache first (this is fast and ensures immediate availability)
-        {
-            let mut cache = match USER_SETTINGS_CACHE.write() {
-                Ok(cache) => cache,
-                Err(e) => {
-                    warn!("Failed to write to user settings cache during save: {}", e);
-                    // Continue with save operation even if caching fails
-                    return self.save_to_disk();
-                }
-            };
-            cache.insert(self.pubkey.clone(), CachedUserSettings {
-                settings: self.clone(),
-                timestamp: Instant::now(),
-            });
-            debug!("Updated cache for user {}", self.pubkey);
-        }
-
-        // Ensure directory exists
-        if let Some(parent) = path.parent() {
-            if let Err(e) = fs::create_dir_all(parent) {
-                warn!("Failed to create settings directory: {}", e);
-                return Err(format!("Failed to create settings directory: {}", e));
-            }
-        }
-
-        // Save settings to disk asynchronously to avoid blocking
-        // For now we'll use a simple thread, but this could be improved with a proper async task
-        let pubkey = self.pubkey.clone();
-        let settings_clone = self.clone();
-
-        std::thread::spawn(move || {
-            debug!("Background thread saving settings for user {}", pubkey);
-            match serde_yaml::to_string(&settings_clone) {
-                Ok(yaml) => {
-                    match fs::write(&path, yaml) {
-                        Ok(_) => info!("Saved settings for user {} to disk", pubkey),
-                        Err(e) => error!("Failed to write settings file for {}: {}", pubkey, e)
-                    }
-                }
-                Err(e) => error!("Failed to serialize settings for {}: {}", pubkey, e),
-            }
-        });
-
-        // Return success immediately since we've updated the cache
-        Ok(())
-    }
-
-    fn save_to_disk(&self) -> Result<(), String> {
-        let path = Self::get_settings_path(&self.pubkey);
-
-        // Ensure directory exists
-        if let Some(parent) = path.parent() {
-            if let Err(e) = std::fs::create_dir_all(parent) {
-                return Err(format!("Failed to create settings directory: {}", e));
-            }
-        }
-
-        // Serialize and save settings
-        match serde_yaml::to_string(self) {
-            Ok(content) => {
-                match std::fs::write(&path, content) {
-                    Ok(_) => {
-                        debug!("Saved settings to disk for user {}", self.pubkey);
-                        Ok(())
-                    }
-                    Err(e) => Err(format!("Failed to write settings file: {}", e))
-                }
-            }
-            Err(e) => Err(format!("Failed to serialize settings: {}", e))
-        }
-    }
-
-    fn get_settings_path(pubkey: &str) -> PathBuf {
-        PathBuf::from("/app/user_settings").join(format!("{}.yaml", pubkey))
-    }
-
-    // Clear the cache entry for a specific user
-    pub fn clear_cache(pubkey: &str) {
-        let mut cache = match USER_SETTINGS_CACHE.write() {
-            Ok(cache) => cache,
-            Err(e) => {
-                error!("Failed to write to cache for clearing user {}: {}", pubkey, e);
-                return;
-            }
-        };
-        if cache.remove(pubkey).is_some() {
-            debug!("Cleared cache for user {}", pubkey);
-            trace_info!(
-                user_id = %pubkey,
-                "User settings cache invalidated"
-            );
-        }
-    }
-
-    // Clear all cached settings
-    pub fn clear_all_cache() {
-        let mut cache = match USER_SETTINGS_CACHE.write() {
-            Ok(cache) => cache,
-            Err(e) => {
-                error!("Failed to write to cache for clearing all settings: {}", e);
-                return;
-            }
-        };
-        let count = cache.len();
-        cache.clear();
-        debug!("Cleared all cached settings ({} entries)", count);
-        trace_info!(
-            entries_cleared = count,
-            "All user settings cache cleared"
-        );
-    }
-
-    // Invalidate cache on authentication state change
-    pub fn invalidate_user_cache(pubkey: &str) {
-        Self::clear_cache(pubkey);
-        trace_info!(
-            user_id = %pubkey,
-            "User cache invalidated due to auth state change"
-        );
-    }
-
-    // Get cache statistics
-    pub fn get_cache_stats() -> (usize, Vec<(String, Duration)>) {
-        let cache = match USER_SETTINGS_CACHE.read() {
-            Ok(cache) => cache,
-            Err(_) => return (0, Vec::new()),
-        };
-
-        let entries = cache.len();
-        let ages: Vec<(String, Duration)> = cache
-            .iter()
-            .map(|(key, value)| (key.clone(), value.timestamp.elapsed()))
-            .collect();
-
-        (entries, ages)
-    }
-}
\ No newline at end of file
diff --git a/src/ports/graph_repository.rs b/src/ports/graph_repository.rs
new file mode 100644
index 00000000..b621c02e
--- /dev/null
+++ b/src/ports/graph_repository.rs
@@ -0,0 +1,46 @@
+// Port: GraphRepository
+// Defines the interface for graph state management
+// Future: Add #[derive(HexPort)] when Hexser is available
+
+use async_trait::async_trait;
+use std::sync::Arc;
+use std::collections::HashSet;
+use crate::models::graph::GraphData;
+use crate::models::node::Node;
+use crate::models::edge::Edge;
+use crate::utils::socket_flow_messages::BinaryNodeData;
+
+pub type Result<T> = std::result::Result<T, String>;
+
+#[async_trait]
+pub trait GraphRepository: Send + Sync {
+    /// Get the current graph state
+    async fn get_graph(&self) -> Result<Arc<GraphData>>;
+
+    /// Add nodes to the graph
+    async fn add_nodes(&self, nodes: Vec<Node>) -> Result<Vec<u32>>;
+
+    /// Add edges to the graph
+    async fn add_edges(&self, edges: Vec<Edge>) -> Result<Vec<String>>;
+
+    /// Update node positions (from physics simulation)
+    async fn update_positions(&self, updates: Vec<(u32, BinaryNodeData)>) -> Result<()>;
+
+    /// Get nodes that have changed since last sync
+    async fn get_dirty_nodes(&self) -> Result<HashSet<u32>>;
+
+    /// Clear the dirty nodes list
+    async fn clear_dirty_nodes(&self) -> Result<()>;
+
+    /// Get current graph version for optimistic locking
+    async fn get_version(&self) -> Result<u64>;
+
+    /// Get specific nodes by ID
+    async fn get_nodes(&self, node_ids: Vec<u32>) -> Result<Vec<Node>>;
+
+    /// Remove nodes from the graph
+    async fn remove_nodes(&self, node_ids: Vec<u32>) -> Result<()>;
+
+    /// Clear the entire graph
+    async fn clear(&self) -> Result<()>;
+}
diff --git a/src/ports/mod.rs b/src/ports/mod.rs
new file mode 100644
index 00000000..369a7801
--- /dev/null
+++ b/src/ports/mod.rs
@@ -0,0 +1,10 @@
+// Ports (Interfaces) - Hexagonal Architecture
+// These traits define the boundaries between application and infrastructure
+
+pub mod graph_repository;
+pub mod physics_simulator;
+pub mod semantic_analyzer;
+
+pub use graph_repository::GraphRepository;
+pub use physics_simulator::PhysicsSimulator;
+pub use semantic_analyzer::{SemanticAnalyzer, SSSPResult, ClusteringResult, CommunityResult, ClusterAlgorithm};
diff --git a/src/ports/physics_simulator.rs b/src/ports/physics_simulator.rs
new file mode 100644
index 00000000..f28592ec
--- /dev/null
+++ b/src/ports/physics_simulator.rs
@@ -0,0 +1,35 @@
+// Port: PhysicsSimulator
+// Defines the interface for physics simulation
+// Future: Add #[derive(HexPort)] when Hexser is available
+
+use async_trait::async_trait;
+use crate::models::graph::GraphData;
+use crate::models::simulation_params::SimulationParams;
+use crate::models::constraints::Constraint;
+use crate::utils::socket_flow_messages::BinaryNodeData;
+
+pub type Result<T> = std::result::Result<T, String>;
+
+#[async_trait]
+pub trait PhysicsSimulator: Send + Sync {
+    /// Run a single simulation step and return position updates
+    async fn run_simulation_step(&self, graph: &GraphData) -> Result<Vec<(u32, BinaryNodeData)>>;
+
+    /// Update simulation parameters
+    async fn update_params(&self, params: SimulationParams) -> Result<()>;
+
+    /// Apply constraints from ontology or user-defined
+    async fn apply_constraints(&self, constraints: Vec<Constraint>) -> Result<()>;
+
+    /// Start continuous simulation
+    async fn start_simulation(&self) -> Result<()>;
+
+    /// Stop continuous simulation
+    async fn stop_simulation(&self) -> Result<()>;
+
+    /// Check if simulation is running
+    async fn is_running(&self) -> Result<bool>;
+
+    /// Set SSSP source node for pathfinding visualization
+    async fn set_sssp_source(&self, source: Option<u32>) -> Result<()>;
+}
diff --git a/src/ports/semantic_analyzer.rs b/src/ports/semantic_analyzer.rs
new file mode 100644
index 00000000..5ab31f93
--- /dev/null
+++ b/src/ports/semantic_analyzer.rs
@@ -0,0 +1,54 @@
+// Port: SemanticAnalyzer
+// Defines the interface for graph algorithms and semantic analysis
+// Future: Add #[derive(HexPort)] when Hexser is available
+
+use async_trait::async_trait;
+use std::collections::HashMap;
+use crate::models::graph::GraphData;
+
+pub type Result<T> = std::result::Result<T, String>;
+
+#[derive(Debug, Clone)]
+pub struct SSSPResult {
+    pub distances: HashMap<u32, f32>,
+    pub parents: HashMap<u32, i32>,
+    pub source: u32,
+}
+
+#[derive(Debug, Clone)]
+pub struct ClusteringResult {
+    pub clusters: HashMap<u32, u32>,  // node_id -> cluster_id
+    pub cluster_count: u32,
+    pub algorithm: ClusterAlgorithm,
+}
+
+#[derive(Debug, Clone, Copy)]
+pub enum ClusterAlgorithm {
+    KMeans { k: u32 },
+    DBSCAN { eps: f32, min_samples: u32 },
+    Hierarchical { num_clusters: u32 },
+}
+
+#[derive(Debug, Clone)]
+pub struct CommunityResult {
+    pub communities: HashMap<u32, u32>,  // node_id -> community_id
+    pub modularity: f32,
+}
+
+#[async_trait]
+pub trait SemanticAnalyzer: Send + Sync {
+    /// Run Single-Source Shortest Path from a source node
+    async fn run_sssp(&self, graph: &GraphData, source: u32) -> Result<SSSPResult>;
+
+    /// Run clustering algorithm on the graph
+    async fn run_clustering(&self, graph: &GraphData, algorithm: ClusterAlgorithm) -> Result<ClusteringResult>;
+
+    /// Detect communities using Louvain modularity
+    async fn detect_communities(&self, graph: &GraphData) -> Result<CommunityResult>;
+
+    /// Get shortest path between two nodes
+    async fn get_shortest_path(&self, graph: &GraphData, source: u32, target: u32) -> Result<Vec<u32>>;
+
+    /// Invalidate algorithm caches
+    async fn invalidate_cache(&self) -> Result<()>;
+}
diff --git a/src/services/bots_client.rs b/src/services/bots_client.rs
index 221f525b..04243a69 100644
--- a/src/services/bots_client.rs
+++ b/src/services/bots_client.rs
@@ -4,7 +4,7 @@ use std::sync::Arc;
 use tokio::sync::RwLock;
 use log::{info, error, debug, warn};
 use actix::Addr;
-use crate::actors::graph_service_supervisor::TransitionalGraphSupervisor;
+use crate::actors::graph_state_actor::GraphStateActor;
 use crate::actors::messages::UpdateBotsGraph;
 use crate::utils::mcp_tcp_client::{McpTcpClient, create_mcp_client};
 use crate::services::agent_visualization_protocol::{McpServerType, MultiMcpAgentStatus};
@@ -67,7 +67,7 @@ impl From<MultiMcpAgentStatus> for Agent {
 #[derive(Clone)]
 pub struct BotsClient {
     mcp_client: McpTcpClient,
-    graph_service_addr: Option<Addr<TransitionalGraphSupervisor>>,
+    graph_state_addr: Option<Addr<GraphStateActor>>,
     agents: Arc<RwLock<Vec<Agent>>>,
 }

@@ -83,17 +83,17 @@ impl BotsClient {
             .unwrap_or(9500);

         let mcp_client = create_mcp_client(&McpServerType::ClaudeFlow, &host, port);
-
+
         Self {
             mcp_client,
-            graph_service_addr: None,
+            graph_state_addr: None,
             agents: Arc::new(RwLock::new(Vec::new())),
         }
     }
-
-    pub fn with_graph_service(graph_addr: Addr<TransitionalGraphSupervisor>) -> Self {
+
+    pub fn with_graph_service(graph_addr: Addr<GraphStateActor>) -> Self {
         let mut client = Self::new();
-        client.graph_service_addr = Some(graph_addr);
+        client.graph_state_addr = Some(graph_addr);
         client
     }

@@ -104,7 +104,7 @@ impl BotsClient {
         match self.mcp_client.test_connection().await {
             Ok(true) => {
                 info!("✓ MCP server is reachable");
-
+
                 // Initialize MCP session
                 match self.mcp_client.initialize_session().await {
                     Ok(_) => {
@@ -134,7 +134,7 @@ impl BotsClient {

     async fn start_polling(&self) {
         let mcp_client = self.mcp_client.clone();
-        let graph_service_addr = self.graph_service_addr.clone();
+        let graph_state_addr = self.graph_state_addr.clone();
         let agents = self.agents.clone();

         tokio::spawn(async move {
@@ -161,8 +161,8 @@ impl BotsClient {
                             }

                             // Send to graph if connected
-                            if let Some(ref graph_addr) = graph_service_addr {
-                                info!("📨 BotsClient sending {} agents to graph", converted_agents.len());
+                            if let Some(ref graph_addr) = graph_state_addr {
+                                info!("📨 BotsClient sending {} agents to GraphStateActor", converted_agents.len());

                                 // Send agents directly without conversion
                                 graph_addr.do_send(UpdateBotsGraph { agents: converted_agents.clone() });
@@ -247,4 +247,4 @@ impl BotsClient {
             }
         }
     }
-}
\ No newline at end of file
+}
diff --git a/src/services/database_service.rs b/src/services/database_service.rs
new file mode 100644
index 00000000..c4123809
--- /dev/null
+++ b/src/services/database_service.rs
@@ -0,0 +1,849 @@
+// src/services/database_service.rs
+//! Database service for SQLite-based ontology and metadata storage
+//!
+//! Provides centralized database access for:
+//! - Application settings and physics configuration
+//! - Ontology framework metadata (OWL classes, properties, axioms)
+//! - Markdown file metadata with ontology blocks
+//! - Validation reports and inference results
+//! - Physics constraint generation
+
+use rusqlite::{Connection, OptionalExtension, params, Result as SqliteResult, Transaction};
+use std::collections::HashMap;
+use std::path::Path;
+use std::sync::{Arc, Mutex};
+use chrono::{DateTime, Utc};
+use serde_json::Value as JsonValue;
+
+use crate::models::metadata::Metadata;
+use crate::config::PhysicsSettings;
+
+/// Main database service providing thread-safe access to SQLite
+pub struct DatabaseService {
+    conn: Arc<Mutex<Connection>>,
+}
+
+impl DatabaseService {
+    /// Create new database service, initializing schema if needed
+    pub fn new<P: AsRef<Path>>(db_path: P) -> SqliteResult<Self> {
+        let conn = Connection::open(db_path)?;
+
+        // Configure SQLite for optimal performance
+        conn.pragma_update(None, "journal_mode", "WAL")?;
+        conn.pragma_update(None, "synchronous", "NORMAL")?;
+        conn.pragma_update(None, "cache_size", 10000)?;
+        conn.pragma_update(None, "foreign_keys", true)?;
+
+        Ok(Self {
+            conn: Arc::new(Mutex::new(conn)),
+        })
+    }
+
+    /// Execute schema initialization from SQL file
+    pub fn execute_schema(&self, schema_sql: &str) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+        conn.execute_batch(schema_sql)?;
+        Ok(())
+    }
+
+    /// Get current schema version
+    pub fn get_schema_version(&self) -> SqliteResult<i32> {
+        let conn = self.conn.lock().unwrap();
+        let version: i32 = conn.query_row(
+            "SELECT version FROM schema_version WHERE id = 1",
+            [],
+            |row| row.get(0)
+        )?;
+        Ok(version)
+    }
+
+    /// Initialize database schema from embedded SQL file
+    pub fn initialize_schema(&self) -> SqliteResult<()> {
+        // Load schema from embedded SQL file
+        const SCHEMA_SQL: &str = include_str!("../../schema/ontology_db.sql");
+        self.execute_schema(SCHEMA_SQL)?;
+        Ok(())
+    }
+}
+
+// ================================================================
+// SETTINGS QUERIES
+// ================================================================
+
+impl DatabaseService {
+    /// Convert snake_case to camelCase
+    /// Examples: "spring_k" -> "springK", "max_velocity" -> "maxVelocity"
+    fn to_camel_case(s: &str) -> String {
+        let parts: Vec<&str> = s.split('_').collect();
+        if parts.len() == 1 {
+            return s.to_string();
+        }
+
+        let mut result = parts[0].to_string();
+        for part in &parts[1..] {
+            if !part.is_empty() {
+                let mut chars = part.chars();
+                if let Some(first) = chars.next() {
+                    result.push(first.to_ascii_uppercase());
+                    result.push_str(chars.as_str());
+                }
+            }
+        }
+        result
+    }
+
+    /// Get setting value with exact key match (no fallback)
+    fn get_setting_exact(&self, key: &str) -> SqliteResult<Option<SettingValue>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT value_type, value_text, value_integer, value_float, value_boolean, value_json
+             FROM settings WHERE key = ?1"
+        )?;
+
+        stmt.query_row(params![key], |row| {
+            let value_type: String = row.get(0)?;
+            let value = match value_type.as_str() {
+                "string" => SettingValue::String(row.get(1)?),
+                "integer" => SettingValue::Integer(row.get(2)?),
+                "float" => SettingValue::Float(row.get(3)?),
+                "boolean" => SettingValue::Boolean(row.get::<_, i32>(4)? == 1),
+                "json" => {
+                    let json_str: String = row.get(5)?;
+                    SettingValue::Json(serde_json::from_str(&json_str).unwrap_or(JsonValue::Null))
+                },
+                _ => SettingValue::String(String::new()),
+            };
+            Ok(value)
+        }).optional()
+    }
+
+    /// Get hierarchical settings by key path with intelligent camelCase/snake_case fallback
+    ///
+    /// This method provides smart lookup:
+    /// 1. First tries exact match with the provided key
+    /// 2. If not found and key contains underscore, converts to camelCase and retries
+    ///
+    /// Examples:
+    /// - Database has "springK" = 150.0
+    /// - `get_setting("springK")` -> Direct hit, returns 150.0
+    /// - `get_setting("spring_k")` -> Converts to "springK", returns 150.0
+    pub fn get_setting(&self, key: &str) -> SqliteResult<Option<SettingValue>> {
+        // Try exact match first
+        if let Some(value) = self.get_setting_exact(key)? {
+            return Ok(Some(value));
+        }
+
+        // If not found and key contains underscore, try camelCase conversion
+        if key.contains('_') {
+            let camel_key = Self::to_camel_case(key);
+            if let Some(value) = self.get_setting_exact(&camel_key)? {
+                return Ok(Some(value));
+            }
+        }
+
+        // Not found with either key format
+        Ok(None)
+    }
+
+    /// Set a setting value
+    pub fn set_setting(&self, key: &str, value: SettingValue, description: Option<&str>) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        let (value_type, text, int, float, bool_val, json) = match value {
+            SettingValue::String(s) => ("string", Some(s), None, None, None, None),
+            SettingValue::Integer(i) => ("integer", None, Some(i), None, None, None),
+            SettingValue::Float(f) => ("float", None, None, Some(f), None, None),
+            SettingValue::Boolean(b) => ("boolean", None, None, None, Some(if b { 1 } else { 0 }), None),
+            SettingValue::Json(j) => ("json", None, None, None, None, Some(j.to_string())),
+        };
+
+        conn.execute(
+            "INSERT INTO settings (key, value_type, value_text, value_integer, value_float, value_boolean, value_json, description)
+             VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8)
+             ON CONFLICT(key) DO UPDATE SET
+                value_type = excluded.value_type,
+                value_text = excluded.value_text,
+                value_integer = excluded.value_integer,
+                value_float = excluded.value_float,
+                value_boolean = excluded.value_boolean,
+                value_json = excluded.value_json,
+                description = COALESCE(excluded.description, description),
+                updated_at = CURRENT_TIMESTAMP",
+            params![key, value_type, text, int, float, bool_val, json, description]
+        )?;
+
+        Ok(())
+    }
+
+    /// Get physics settings for a specific profile
+    pub fn get_physics_settings(&self, profile_name: &str) -> SqliteResult<PhysicsSettings> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT damping, dt, iterations, max_velocity, max_force, repel_k, spring_k,
+                    mass_scale, boundary_damping, temperature, gravity, bounds_size, enable_bounds,
+                    rest_length, repulsion_cutoff, repulsion_softening_epsilon, center_gravity_k,
+                    grid_cell_size, warmup_iterations, cooling_rate, constraint_ramp_frames,
+                    constraint_max_force_per_node
+             FROM physics_settings WHERE profile_name = ?1"
+        )?;
+
+        stmt.query_row(params![profile_name], |row| {
+            Ok(PhysicsSettings {
+                damping: row.get(0)?,
+                dt: row.get(1)?,
+                iterations: row.get(2)?,
+                max_velocity: row.get(3)?,
+                max_force: row.get(4)?,
+                repel_k: row.get(5)?,
+                spring_k: row.get(6)?,
+                mass_scale: row.get(7)?,
+                boundary_damping: row.get(8)?,
+                temperature: row.get(9)?,
+                gravity: row.get(10)?,
+                bounds_size: row.get(11)?,
+                enable_bounds: row.get::<_, i32>(12)? == 1,
+                rest_length: row.get(13)?,
+                repulsion_cutoff: row.get(14)?,
+                repulsion_softening_epsilon: row.get(15)?,
+                center_gravity_k: row.get(16)?,
+                grid_cell_size: row.get(17)?,
+                warmup_iterations: row.get(18)?,
+                cooling_rate: row.get(19)?,
+                constraint_ramp_frames: row.get(20)?,
+                constraint_max_force_per_node: row.get(21)?,
+                ..PhysicsSettings::default()
+            })
+        })
+    }
+
+    /// Save physics settings profile
+    pub fn save_physics_settings(&self, profile_name: &str, settings: &PhysicsSettings) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            "INSERT INTO physics_settings (
+                profile_name, damping, dt, iterations, max_velocity, max_force,
+                repel_k, spring_k, mass_scale, boundary_damping, temperature, gravity,
+                bounds_size, enable_bounds, rest_length, repulsion_cutoff,
+                repulsion_softening_epsilon, center_gravity_k, grid_cell_size,
+                warmup_iterations, cooling_rate, constraint_ramp_frames,
+                constraint_max_force_per_node
+             ) VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10, ?11, ?12, ?13, ?14, ?15, ?16, ?17, ?18, ?19, ?20, ?21, ?22, ?23)
+             ON CONFLICT(profile_name) DO UPDATE SET
+                damping = excluded.damping,
+                dt = excluded.dt,
+                iterations = excluded.iterations,
+                max_velocity = excluded.max_velocity,
+                max_force = excluded.max_force,
+                repel_k = excluded.repel_k,
+                spring_k = excluded.spring_k,
+                updated_at = CURRENT_TIMESTAMP",
+            params![
+                profile_name, settings.damping, settings.dt, settings.iterations,
+                settings.max_velocity, settings.max_force, settings.repel_k, settings.spring_k,
+                settings.mass_scale, settings.boundary_damping, settings.temperature,
+                settings.gravity, settings.bounds_size, if settings.enable_bounds { 1 } else { 0 },
+                settings.rest_length, settings.repulsion_cutoff, settings.repulsion_softening_epsilon,
+                settings.center_gravity_k, settings.grid_cell_size, settings.warmup_iterations,
+                settings.cooling_rate, settings.constraint_ramp_frames, settings.constraint_max_force_per_node
+            ]
+        )?;
+
+        Ok(())
+    }
+
+    /// Save complete settings to database as JSON
+    /// Stores all settings categories (rendering, XR, system, auth, etc.)
+    pub fn save_all_settings(&self, settings: &crate::config::AppFullSettings) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        // Serialize all settings as JSON
+        let settings_json = serde_json::to_string(settings)
+            .map_err(|e| rusqlite::Error::ToSqlConversionFailure(Box::new(e)))?;
+
+        // Save as a single JSON blob for simplicity and atomicity
+        conn.execute(
+            "INSERT INTO settings (key, value_type, value_json, description)
+             VALUES ('app_full_settings', 'json', ?1, 'Complete application settings')
+             ON CONFLICT(key) DO UPDATE SET
+                value_json = excluded.value_json,
+                updated_at = CURRENT_TIMESTAMP",
+            params![settings_json]
+        )?;
+
+        // Also save physics settings to dedicated table for fast access
+        self.save_physics_settings("default", &settings.visualisation.graphs.logseq.physics)?;
+
+        Ok(())
+    }
+
+    /// Load complete settings from database
+    /// Returns None if settings not found in database
+    pub fn load_all_settings(&self) -> SqliteResult<Option<crate::config::AppFullSettings>> {
+        let conn = self.conn.lock().unwrap();
+
+        let settings_json: Option<String> = conn.query_row(
+            "SELECT value_json FROM settings WHERE key = 'app_full_settings'",
+            [],
+            |row| row.get(0)
+        ).optional()?;
+
+        if let Some(json_str) = settings_json {
+            let settings: crate::config::AppFullSettings = serde_json::from_str(&json_str)
+                .map_err(|e| rusqlite::Error::FromSqlConversionFailure(
+                    0,
+                    rusqlite::types::Type::Text,
+                    Box::new(e)
+                ))?;
+            Ok(Some(settings))
+        } else {
+            Ok(None)
+        }
+    }
+}
+
+// ================================================================
+// ONTOLOGY QUERIES
+// ================================================================
+
+impl DatabaseService {
+    /// Save ontology metadata
+    pub fn save_ontology(&self, ontology: &OntologyMetadata) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            "INSERT INTO ontologies (
+                ontology_id, source_path, source_type, base_iri, version_iri,
+                title, description, author, version, content_hash, axiom_count,
+                class_count, property_count
+             ) VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10, ?11, ?12, ?13)
+             ON CONFLICT(ontology_id) DO UPDATE SET
+                last_validated_at = CURRENT_TIMESTAMP,
+                updated_at = CURRENT_TIMESTAMP",
+            params![
+                ontology.ontology_id, ontology.source_path, ontology.source_type,
+                ontology.base_iri, ontology.version_iri, ontology.title,
+                ontology.description, ontology.author, ontology.version,
+                ontology.content_hash, ontology.axiom_count, ontology.class_count,
+                ontology.property_count
+            ]
+        )?;
+
+        Ok(())
+    }
+
+    /// Get ontology by ID
+    pub fn get_ontology(&self, ontology_id: &str) -> SqliteResult<Option<OntologyMetadata>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT ontology_id, source_path, source_type, base_iri, version_iri,
+                    title, description, author, version, content_hash, axiom_count,
+                    class_count, property_count, parsed_at, last_validated_at
+             FROM ontologies WHERE ontology_id = ?1"
+        )?;
+
+        stmt.query_row(params![ontology_id], |row| {
+            Ok(OntologyMetadata {
+                ontology_id: row.get(0)?,
+                source_path: row.get(1)?,
+                source_type: row.get(2)?,
+                base_iri: row.get(3)?,
+                version_iri: row.get(4)?,
+                title: row.get(5)?,
+                description: row.get(6)?,
+                author: row.get(7)?,
+                version: row.get(8)?,
+                content_hash: row.get(9)?,
+                axiom_count: row.get(10)?,
+                class_count: row.get(11)?,
+                property_count: row.get(12)?,
+                parsed_at: row.get(13)?,
+                last_validated_at: row.get(14)?,
+            })
+        }).optional()
+    }
+
+    /// Save OWL class definition
+    pub fn save_owl_class(&self, ontology_id: &str, class: &OwlClass) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            "INSERT INTO owl_classes (ontology_id, class_iri, label, comment, parent_class_iri, is_deprecated)
+             VALUES (?1, ?2, ?3, ?4, ?5, ?6)
+             ON CONFLICT(ontology_id, class_iri) DO UPDATE SET
+                label = excluded.label,
+                comment = excluded.comment,
+                parent_class_iri = excluded.parent_class_iri",
+            params![
+                ontology_id, class.class_iri, class.label, class.comment,
+                class.parent_class_iri, if class.is_deprecated { 1 } else { 0 }
+            ]
+        )?;
+
+        Ok(())
+    }
+
+    /// Get all classes for an ontology
+    pub fn get_owl_classes(&self, ontology_id: &str) -> SqliteResult<Vec<OwlClass>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT class_iri, label, comment, parent_class_iri, is_deprecated
+             FROM owl_classes WHERE ontology_id = ?1
+             ORDER BY class_iri"
+        )?;
+
+        let rows = stmt.query_map(params![ontology_id], |row| {
+            Ok(OwlClass {
+                class_iri: row.get(0)?,
+                label: row.get(1)?,
+                comment: row.get(2)?,
+                parent_class_iri: row.get(3)?,
+                is_deprecated: row.get::<_, i32>(4)? == 1,
+            })
+        })?;
+
+        rows.collect()
+    }
+
+    /// Get disjoint class pairs
+    pub fn get_disjoint_classes(&self, ontology_id: &str) -> SqliteResult<Vec<(String, String)>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT class_iri_1, class_iri_2 FROM owl_disjoint_classes WHERE ontology_id = ?1"
+        )?;
+
+        let rows = stmt.query_map(params![ontology_id], |row| {
+            Ok((row.get(0)?, row.get(1)?))
+        })?;
+
+        rows.collect()
+    }
+}
+
+// ================================================================
+// MAPPING CONFIGURATION QUERIES
+// ================================================================
+
+impl DatabaseService {
+    /// Get namespace IRI for prefix
+    pub fn get_namespace(&self, prefix: &str) -> SqliteResult<Option<String>> {
+        let conn = self.conn.lock().unwrap();
+        conn.query_row(
+            "SELECT namespace_iri FROM namespaces WHERE prefix = ?1",
+            params![prefix],
+            |row| row.get(0)
+        ).optional()
+    }
+
+    /// Save namespace
+    pub fn save_namespace(&self, prefix: &str, namespace_iri: &str, is_default: bool) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            "INSERT INTO namespaces (prefix, namespace_iri, is_default)
+             VALUES (?1, ?2, ?3)
+             ON CONFLICT(prefix) DO UPDATE SET
+                namespace_iri = excluded.namespace_iri,
+                is_default = excluded.is_default",
+            params![prefix, namespace_iri, if is_default { 1 } else { 0 }]
+        )?;
+
+        Ok(())
+    }
+
+    /// Get OWL class IRI for graph label
+    pub fn get_class_mapping(&self, graph_label: &str) -> SqliteResult<Option<String>> {
+        let conn = self.conn.lock().unwrap();
+        conn.query_row(
+            "SELECT owl_class_iri FROM class_mappings WHERE graph_label = ?1",
+            params![graph_label],
+            |row| row.get(0)
+        ).optional()
+    }
+
+    /// Get property mapping
+    pub fn get_property_mapping(&self, graph_property: &str) -> SqliteResult<Option<PropertyMapping>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT owl_property_iri, property_type, rdfs_domain, rdfs_range, inverse_property_iri
+             FROM property_mappings WHERE graph_property = ?1"
+        )?;
+
+        stmt.query_row(params![graph_property], |row| {
+            Ok(PropertyMapping {
+                owl_property_iri: row.get(0)?,
+                property_type: row.get(1)?,
+                rdfs_domain: row.get(2)?,
+                rdfs_range: row.get(3)?,
+                inverse_property_iri: row.get(4)?,
+            })
+        }).optional()
+    }
+}
+
+// ================================================================
+// FILE METADATA QUERIES
+// ================================================================
+
+impl DatabaseService {
+    /// Get file metadata
+    pub fn get_file_metadata(&self, file_name: &str) -> SqliteResult<Option<Metadata>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT file_name, file_size, sha1, file_blob_sha, node_id, node_size,
+                    hyperlink_count, perplexity_link, last_modified, last_content_change,
+                    last_commit, last_perplexity_process, change_count
+             FROM file_metadata WHERE file_name = ?1"
+        )?;
+
+        let result = stmt.query_row(params![file_name], |row| {
+            Ok(Metadata {
+                file_name: row.get(0)?,
+                file_size: row.get(1)?,
+                sha1: row.get(2)?,
+                file_blob_sha: row.get(3)?,
+                node_id: row.get(4)?,
+                node_size: row.get(5)?,
+                hyperlink_count: row.get(6)?,
+                perplexity_link: row.get(7)?,
+                last_modified: row.get(8)?,
+                last_content_change: row.get(9)?,
+                last_commit: row.get(10)?,
+                last_perplexity_process: row.get(11)?,
+                change_count: row.get(12)?,
+                topic_counts: HashMap::new(), // Will be populated separately
+            })
+        }).optional()?;
+
+        if let Some(mut metadata) = result {
+            metadata.topic_counts = self.get_file_topics(file_name)?;
+            Ok(Some(metadata))
+        } else {
+            Ok(None)
+        }
+    }
+
+    /// Get topic counts for a file
+    fn get_file_topics(&self, file_name: &str) -> SqliteResult<HashMap<String, usize>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT topic, count FROM file_topics WHERE file_name = ?1"
+        )?;
+
+        let rows = stmt.query_map(params![file_name], |row| {
+            Ok((row.get::<_, String>(0)?, row.get::<_, usize>(1)?))
+        })?;
+
+        let mut topics = HashMap::new();
+        for row in rows {
+            let (topic, count) = row?;
+            topics.insert(topic, count);
+        }
+
+        Ok(topics)
+    }
+
+    /// Get all file metadata
+    pub fn get_all_file_metadata(&self) -> SqliteResult<HashMap<String, Metadata>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT file_name FROM file_metadata ORDER BY file_name"
+        )?;
+
+        let file_names: Vec<String> = stmt.query_map([], |row| row.get(0))?.collect::<SqliteResult<_>>()?;
+
+        drop(stmt);
+        drop(conn);
+
+        let mut metadata_map = HashMap::new();
+        for file_name in file_names {
+            if let Some(metadata) = self.get_file_metadata(&file_name)? {
+                metadata_map.insert(file_name, metadata);
+            }
+        }
+
+        Ok(metadata_map)
+    }
+
+    /// Save file metadata
+    pub fn save_file_metadata(&self, metadata: &Metadata) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            "INSERT INTO file_metadata (
+                file_name, file_path, file_size, sha1, file_blob_sha, node_id,
+                node_size, hyperlink_count, perplexity_link, last_modified,
+                last_content_change, last_commit, last_perplexity_process, change_count
+             ) VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10, ?11, ?12, ?13, ?14)
+             ON CONFLICT(file_name) DO UPDATE SET
+                file_size = excluded.file_size,
+                sha1 = excluded.sha1,
+                node_size = excluded.node_size,
+                hyperlink_count = excluded.hyperlink_count,
+                last_modified = excluded.last_modified,
+                updated_at = CURRENT_TIMESTAMP",
+            params![
+                metadata.file_name, format!("./markdown/{}", metadata.file_name),
+                metadata.file_size, metadata.sha1, metadata.file_blob_sha, metadata.node_id,
+                metadata.node_size, metadata.hyperlink_count, metadata.perplexity_link,
+                metadata.last_modified, metadata.last_content_change, metadata.last_commit,
+                metadata.last_perplexity_process, metadata.change_count
+            ]
+        )?;
+
+        // Save topic counts
+        for (topic, count) in &metadata.topic_counts {
+            conn.execute(
+                "INSERT INTO file_topics (file_name, topic, count) VALUES (?1, ?2, ?3)
+                 ON CONFLICT(file_name, topic) DO UPDATE SET count = excluded.count",
+                params![metadata.file_name, topic, count]
+            )?;
+        }
+
+        Ok(())
+    }
+}
+
+// ================================================================
+// CONSTRAINT QUERIES
+// ================================================================
+
+impl DatabaseService {
+    /// Get constraint groups by physics type
+    pub fn get_constraint_groups(&self, physics_type: &str) -> SqliteResult<Vec<ConstraintGroup>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT id, group_name, kernel_name, physics_type, default_strength, enabled, batch_size
+             FROM constraint_groups
+             WHERE physics_type = ?1 AND enabled = 1"
+        )?;
+
+        let rows = stmt.query_map(params![physics_type], |row| {
+            Ok(ConstraintGroup {
+                id: row.get(0)?,
+                group_name: row.get(1)?,
+                kernel_name: row.get(2)?,
+                physics_type: row.get(3)?,
+                default_strength: row.get(4)?,
+                enabled: row.get::<_, i32>(5)? == 1,
+                batch_size: row.get(6)?,
+            })
+        })?;
+
+        rows.collect()
+    }
+}
+
+// ================================================================
+// TYPE DEFINITIONS
+// ================================================================
+
+#[derive(Debug, Clone)]
+pub enum SettingValue {
+    String(String),
+    Integer(i64),
+    Float(f64),
+    Boolean(bool),
+    Json(JsonValue),
+}
+
+#[derive(Debug, Clone)]
+pub struct OntologyMetadata {
+    pub ontology_id: String,
+    pub source_path: String,
+    pub source_type: String,
+    pub base_iri: String,
+    pub version_iri: Option<String>,
+    pub title: Option<String>,
+    pub description: Option<String>,
+    pub author: Option<String>,
+    pub version: Option<String>,
+    pub content_hash: String,
+    pub axiom_count: i32,
+    pub class_count: i32,
+    pub property_count: i32,
+    pub parsed_at: DateTime<Utc>,
+    pub last_validated_at: Option<DateTime<Utc>>,
+}
+
+#[derive(Debug, Clone)]
+pub struct OwlClass {
+    pub class_iri: String,
+    pub label: Option<String>,
+    pub comment: Option<String>,
+    pub parent_class_iri: Option<String>,
+    pub is_deprecated: bool,
+}
+
+#[derive(Debug, Clone)]
+pub struct PropertyMapping {
+    pub owl_property_iri: String,
+    pub property_type: String,
+    pub rdfs_domain: Option<String>,
+    pub rdfs_range: Option<String>,
+    pub inverse_property_iri: Option<String>,
+}
+
+#[derive(Debug, Clone)]
+pub struct ConstraintGroup {
+    pub id: i64,
+    pub group_name: String,
+    pub kernel_name: String,
+    pub physics_type: String,
+    pub default_strength: f64,
+    pub enabled: bool,
+    pub batch_size: i32,
+}
+
+// ================================================================
+// TESTS
+// ================================================================
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_database_creation() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+        let version = db.get_schema_version();
+        assert!(version.is_ok() || version.is_err()); // Schema not initialized yet
+    }
+
+    #[test]
+    fn test_setting_crud() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+
+        // Initialize schema first
+        let schema = include_str!("../../schema/ontology_db.sql");
+        db.execute_schema(schema).expect("Failed to initialize schema");
+
+        // Create setting
+        db.set_setting("test.key", SettingValue::String("value".to_string()), Some("Test setting"))
+            .expect("Failed to set setting");
+
+        // Read setting
+        let value = db.get_setting("test.key").expect("Failed to get setting");
+        assert!(value.is_some());
+
+        if let Some(SettingValue::String(s)) = value {
+            assert_eq!(s, "value");
+        } else {
+            panic!("Expected string value");
+        }
+    }
+
+    #[test]
+    fn test_to_camel_case() {
+        // Test snake_case to camelCase conversion
+        assert_eq!(DatabaseService::to_camel_case("spring_k"), "springK");
+        assert_eq!(DatabaseService::to_camel_case("max_velocity"), "maxVelocity");
+        assert_eq!(DatabaseService::to_camel_case("repulsion_cutoff"), "repulsionCutoff");
+        assert_eq!(DatabaseService::to_camel_case("center_gravity_k"), "centerGravityK");
+
+        // Test no conversion needed
+        assert_eq!(DatabaseService::to_camel_case("springK"), "springK");
+        assert_eq!(DatabaseService::to_camel_case("damping"), "damping");
+
+        // Test edge cases
+        assert_eq!(DatabaseService::to_camel_case("a_b"), "aB");
+        assert_eq!(DatabaseService::to_camel_case("a_b_c"), "aBC");
+        assert_eq!(DatabaseService::to_camel_case(""), "");
+    }
+
+    #[test]
+    fn test_camel_snake_fallback_lookup() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+        let schema = include_str!("../../schema/ontology_db.sql");
+        db.execute_schema(schema).expect("Failed to initialize schema");
+
+        // Store setting in camelCase format (preferred format)
+        db.set_setting("springK", SettingValue::Float(150.0), Some("Spring constant"))
+            .expect("Failed to set springK");
+
+        // Test direct camelCase lookup (exact match)
+        let value = db.get_setting("springK").expect("Failed to get springK");
+        assert!(value.is_some());
+        if let Some(SettingValue::Float(f)) = value {
+            assert_eq!(f, 150.0);
+        } else {
+            panic!("Expected float value for springK");
+        }
+
+        // Test snake_case lookup with camelCase fallback
+        let value = db.get_setting("spring_k").expect("Failed to get spring_k");
+        assert!(value.is_some());
+        if let Some(SettingValue::Float(f)) = value {
+            assert_eq!(f, 150.0);
+        } else {
+            panic!("Expected float value for spring_k");
+        }
+
+        // Test non-existent key returns None
+        let value = db.get_setting("nonexistent_key").expect("Failed to get nonexistent");
+        assert!(value.is_none());
+    }
+
+    #[test]
+    fn test_multiple_physics_settings_fallback() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+        let schema = include_str!("../../schema/ontology_db.sql");
+        db.execute_schema(schema).expect("Failed to initialize schema");
+
+        // Store multiple physics settings in camelCase
+        db.set_setting("repelK", SettingValue::Float(50.0), None).unwrap();
+        db.set_setting("maxVelocity", SettingValue::Float(10.0), None).unwrap();
+        db.set_setting("centerGravityK", SettingValue::Float(0.1), None).unwrap();
+
+        // Test snake_case lookups
+        assert_eq!(
+            match db.get_setting("repel_k").unwrap() {
+                Some(SettingValue::Float(f)) => f,
+                _ => panic!("Expected float"),
+            },
+            50.0
+        );
+
+        assert_eq!(
+            match db.get_setting("max_velocity").unwrap() {
+                Some(SettingValue::Float(f)) => f,
+                _ => panic!("Expected float"),
+            },
+            10.0
+        );
+
+        assert_eq!(
+            match db.get_setting("center_gravity_k").unwrap() {
+                Some(SettingValue::Float(f)) => f,
+                _ => panic!("Expected float"),
+            },
+            0.1
+        );
+    }
+
+    #[test]
+    fn test_exact_match_priority() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+        let schema = include_str!("../../schema/ontology_db.sql");
+        db.execute_schema(schema).expect("Failed to initialize schema");
+
+        // Store both snake_case and camelCase versions
+        db.set_setting("spring_k", SettingValue::Float(100.0), None).unwrap();
+        db.set_setting("springK", SettingValue::Float(150.0), None).unwrap();
+
+        // Exact match should always take priority
+        let value1 = db.get_setting("spring_k").unwrap();
+        if let Some(SettingValue::Float(f)) = value1 {
+            assert_eq!(f, 100.0); // Gets exact match
+        } else {
+            panic!("Expected float");
+        }
+
+        let value2 = db.get_setting("springK").unwrap();
+        if let Some(SettingValue::Float(f)) = value2 {
+            assert_eq!(f, 150.0); // Gets exact match
+        } else {
+            panic!("Expected float");
+        }
+    }
+}
diff --git a/src/services/mod.rs b/src/services/mod.rs
index fb8b369d..30e0bb2f 100755
--- a/src/services/mod.rs
+++ b/src/services/mod.rs
@@ -13,6 +13,24 @@ pub mod mcp_relay_manager;
 pub mod nostr_service;
 #[cfg(feature = "ontology")]
 pub mod owl_validator;
+#[cfg(feature = "ontology")]
+pub mod ontology_downloader;
+#[cfg(feature = "ontology")]
+pub mod ontology_storage;
+#[cfg(feature = "ontology")]
+pub mod ontology_sync;
+#[cfg(feature = "ontology")]
+pub mod database_service;
+#[cfg(feature = "ontology")]
+pub mod ontology_init;
+#[cfg(feature = "ontology")]
+pub mod settings_migration;
+#[cfg(feature = "ontology")]
+pub mod settings_service;
+#[cfg(feature = "ontology")]
+pub mod settings_validator;
+#[cfg(feature = "ontology")]
+pub mod user_service;
 pub mod perplexity_service;
 pub mod ragflow_service;
 pub mod semantic_analyzer;
diff --git a/src/services/ontology_downloader.rs b/src/services/ontology_downloader.rs
new file mode 100644
index 00000000..5123f291
--- /dev/null
+++ b/src/services/ontology_downloader.rs
@@ -0,0 +1,847 @@
+use anyhow::{Context, Result, anyhow};
+use chrono::{DateTime, Utc};
+use log::{debug, info, warn, error};
+use regex::Regex;
+use reqwest::{Client, StatusCode};
+use serde::{Deserialize, Serialize};
+use std::collections::{HashMap, HashSet};
+use std::path::PathBuf;
+use std::sync::Arc;
+use std::time::Duration;
+use thiserror::Error;
+use tokio::sync::RwLock;
+use tokio::time::sleep;
+
+#[derive(Error, Debug)]
+pub enum DownloaderError {
+    #[error("GitHub API error: {0}")]
+    GitHubApi(String),
+
+    #[error("Network error: {0}")]
+    Network(#[from] reqwest::Error),
+
+    #[error("Parse error: {0}")]
+    Parse(String),
+
+    #[error("Database error: {0}")]
+    Database(String),
+
+    #[error("Rate limit exceeded, retry after: {0:?}")]
+    RateLimit(Duration),
+
+    #[error("Authentication error: {0}")]
+    Auth(String),
+
+    #[error("Configuration error: {0}")]
+    Config(String),
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct OntologyDownloaderConfig {
+    pub github_token: String,
+    pub repo_owner: String,
+    pub repo_name: String,
+    pub base_path: String,
+    pub max_retries: u32,
+    pub initial_retry_delay_ms: u64,
+    pub max_retry_delay_ms: u64,
+    pub request_timeout_secs: u64,
+    pub respect_rate_limits: bool,
+}
+
+impl Default for OntologyDownloaderConfig {
+    fn default() -> Self {
+        Self {
+            github_token: String::new(),
+            repo_owner: String::from("jjohare"),
+            repo_name: String::from("logseq"),
+            base_path: String::from("mainKnowledgeGraph/pages"),
+            max_retries: 3,
+            initial_retry_delay_ms: 1000,
+            max_retry_delay_ms: 30000,
+            request_timeout_secs: 30,
+            respect_rate_limits: true,
+        }
+    }
+}
+
+impl OntologyDownloaderConfig {
+    pub fn from_env() -> Result<Self> {
+        let github_token = std::env::var("GITHUB_TOKEN")
+            .or_else(|_| std::env::var("GH_TOKEN"))
+            .context("GITHUB_TOKEN or GH_TOKEN environment variable not set")?;
+
+        if github_token.is_empty() {
+            return Err(anyhow!("GitHub token cannot be empty"));
+        }
+
+        Ok(Self {
+            github_token,
+            ..Default::default()
+        })
+    }
+
+    pub fn with_token(token: String) -> Self {
+        Self {
+            github_token: token,
+            ..Default::default()
+        }
+    }
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct GitHubFile {
+    pub name: String,
+    pub path: String,
+    pub sha: String,
+    pub size: u64,
+    #[serde(rename = "type")]
+    pub file_type: String,
+    pub download_url: Option<String>,
+    pub url: String,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct OntologyBlock {
+    pub id: String,
+    pub source_file: String,
+    pub title: String,
+    pub properties: HashMap<String, Vec<String>>,
+    pub owl_content: Vec<String>,
+    pub classes: Vec<String>,
+    pub properties_list: Vec<String>,
+    pub relationships: Vec<OntologyRelationship>,
+    pub downloaded_at: DateTime<Utc>,
+    pub content_hash: String,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct OntologyRelationship {
+    pub subject: String,
+    pub predicate: String,
+    pub object: String,
+    pub relationship_type: RelationshipType,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
+pub enum RelationshipType {
+    SubClassOf,
+    ObjectProperty,
+    DataProperty,
+    DisjointWith,
+    EquivalentTo,
+    InverseOf,
+    Domain,
+    Range,
+    Other(String),
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct DownloadProgress {
+    pub total_files: usize,
+    pub processed_files: usize,
+    pub ontology_blocks_found: usize,
+    pub errors: Vec<String>,
+    pub started_at: DateTime<Utc>,
+    pub completed_at: Option<DateTime<Utc>>,
+    pub current_file: Option<String>,
+}
+
+impl DownloadProgress {
+    pub fn new(total_files: usize) -> Self {
+        Self {
+            total_files,
+            processed_files: 0,
+            ontology_blocks_found: 0,
+            errors: Vec::new(),
+            started_at: Utc::now(),
+            completed_at: None,
+            current_file: None,
+        }
+    }
+
+    pub fn percentage(&self) -> f64 {
+        if self.total_files == 0 {
+            return 0.0;
+        }
+        (self.processed_files as f64 / self.total_files as f64) * 100.0
+    }
+}
+
+pub struct OntologyDownloader {
+    config: OntologyDownloaderConfig,
+    client: Client,
+    progress: Arc<RwLock<DownloadProgress>>,
+    framework_files: HashSet<String>,
+}
+
+impl OntologyDownloader {
+    pub fn new(config: OntologyDownloaderConfig) -> Result<Self> {
+        if config.github_token.is_empty() {
+            return Err(DownloaderError::Config("GitHub token is required".to_string()).into());
+        }
+
+        let client = Client::builder()
+            .user_agent("ontology-downloader/1.0")
+            .timeout(Duration::from_secs(config.request_timeout_secs))
+            .build()?;
+
+        let mut framework_files = HashSet::new();
+        framework_files.insert("ETSI.md".to_string());
+        framework_files.insert("OntologyDefinition.md".to_string());
+        framework_files.insert("PropertySchema.md".to_string());
+
+        Ok(Self {
+            config,
+            client,
+            progress: Arc::new(RwLock::new(DownloadProgress::new(0))),
+            framework_files,
+        })
+    }
+
+    pub async fn get_progress(&self) -> DownloadProgress {
+        self.progress.read().await.clone()
+    }
+
+    pub async fn download_all(&self) -> Result<Vec<OntologyBlock>> {
+        info!("Starting ontology download from GitHub: {}/{}/{}",
+              self.config.repo_owner, self.config.repo_name, self.config.base_path);
+
+        let files = self.list_all_files().await?;
+
+        {
+            let mut progress = self.progress.write().await;
+            progress.total_files = files.len();
+            progress.started_at = Utc::now();
+        }
+
+        info!("Found {} files to process", files.len());
+
+        let mut all_blocks = Vec::new();
+
+        for file in &files {
+            {
+                let mut progress = self.progress.write().await;
+                progress.current_file = Some(file.name.clone());
+            }
+
+            match self.download_and_parse_file(file).await {
+                Ok(blocks) => {
+                    let block_count = blocks.len();
+                    if block_count > 0 {
+                        info!("Found {} ontology blocks in {}", block_count, file.name);
+                        all_blocks.extend(blocks);
+
+                        let mut progress = self.progress.write().await;
+                        progress.ontology_blocks_found += block_count;
+                    }
+                }
+                Err(e) => {
+                    warn!("Error processing file {}: {}", file.name, e);
+                    let mut progress = self.progress.write().await;
+                    progress.errors.push(format!("{}: {}", file.name, e));
+                }
+            }
+
+            {
+                let mut progress = self.progress.write().await;
+                progress.processed_files += 1;
+            }
+        }
+
+        {
+            let mut progress = self.progress.write().await;
+            progress.completed_at = Some(Utc::now());
+        }
+
+        info!("Download complete: {} ontology blocks from {} files",
+              all_blocks.len(), files.len());
+
+        Ok(all_blocks)
+    }
+
+    async fn list_all_files(&self) -> Result<Vec<GitHubFile>> {
+        info!("Listing files in {}", self.config.base_path);
+
+        let mut all_files = Vec::new();
+        let mut directories = vec![self.config.base_path.clone()];
+
+        while let Some(dir) = directories.pop() {
+            let files = self.list_directory(&dir).await?;
+
+            for file in files {
+                match file.file_type.as_str() {
+                    "file" => {
+                        if file.name.ends_with(".md") {
+                            all_files.push(file);
+                        }
+                    }
+                    "dir" => {
+                        directories.push(file.path.clone());
+                    }
+                    _ => {}
+                }
+            }
+        }
+
+        Ok(all_files)
+    }
+
+    async fn list_directory(&self, path: &str) -> Result<Vec<GitHubFile>> {
+        let url = format!(
+            "https://api.github.com/repos/{}/{}/contents/{}",
+            self.config.repo_owner,
+            self.config.repo_name,
+            path
+        );
+
+        let files = self.github_request::<Vec<GitHubFile>>(&url).await?;
+        Ok(files)
+    }
+
+    async fn download_and_parse_file(&self, file: &GitHubFile) -> Result<Vec<OntologyBlock>> {
+        debug!("Processing file: {}", file.name);
+
+        let content = if let Some(ref download_url) = file.download_url {
+            self.download_file_content(download_url).await?
+        } else {
+            self.get_file_content(&file.url).await?
+        };
+
+        if !self.should_process_file(&content) {
+            debug!("Skipping file {} - no public ontology blocks", file.name);
+            return Ok(Vec::new());
+        }
+
+        self.parse_ontology_file(&file.name, &file.path, &content)
+    }
+
+    fn should_process_file(&self, content: &str) -> bool {
+        let has_ontology_marker = content.contains("- ### OntologyBlock")
+            || content.contains("OntologyBlock");
+
+        if !has_ontology_marker {
+            return false;
+        }
+
+        let has_public_gate = content.contains("public:: true");
+
+        has_public_gate
+    }
+
+    fn parse_ontology_file(&self, filename: &str, filepath: &str, content: &str) -> Result<Vec<OntologyBlock>> {
+        let mut blocks = Vec::new();
+
+        let title = self.extract_title(filename, content);
+        let properties = self.extract_properties(content);
+        let owl_contents = self.extract_owl_blocks(content)?;
+
+        for (idx, owl_content) in owl_contents.iter().enumerate() {
+            let classes = self.extract_classes(owl_content);
+            let properties_list = self.extract_owl_properties(owl_content);
+            let relationships = self.extract_relationships(owl_content);
+
+            let block_id = format!("{}:block:{}", filepath, idx);
+            let content_hash = self.calculate_hash(&format!("{}{}", title, owl_content));
+
+            let block = OntologyBlock {
+                id: block_id,
+                source_file: filepath.to_string(),
+                title: title.clone(),
+                properties: properties.clone(),
+                owl_content: vec![owl_content.clone()],
+                classes,
+                properties_list,
+                relationships,
+                downloaded_at: Utc::now(),
+                content_hash,
+            };
+
+            blocks.push(block);
+        }
+
+        Ok(blocks)
+    }
+
+    fn extract_title(&self, filename: &str, content: &str) -> String {
+        let heading_re = Regex::new(r"^#\s+(.+)$").unwrap();
+        for line in content.lines() {
+            if let Some(cap) = heading_re.captures(line) {
+                return cap[1].trim().to_string();
+            }
+        }
+
+        filename.trim_end_matches(".md").to_string()
+    }
+
+    fn extract_properties(&self, content: &str) -> HashMap<String, Vec<String>> {
+        let mut properties = HashMap::new();
+        let property_re = Regex::new(r"^([a-zA-Z][a-zA-Z0-9-_]*)::\s*(.+)$").unwrap();
+
+        for line in content.lines() {
+            if let Some(cap) = property_re.captures(line.trim()) {
+                let key = cap[1].to_string();
+                let value = cap[2].to_string();
+
+                let values: Vec<String> = value
+                    .split(',')
+                    .map(|v| v.trim().to_string())
+                    .filter(|v| !v.is_empty())
+                    .collect();
+
+                properties.entry(key).or_insert_with(Vec::new).extend(values);
+            }
+        }
+
+        properties
+    }
+
+    fn extract_owl_blocks(&self, content: &str) -> Result<Vec<String>> {
+        let mut blocks = Vec::new();
+        let lines: Vec<&str> = content.lines().collect();
+        let mut i = 0;
+
+        while i < lines.len() {
+            let line = lines[i].trim();
+
+            let fence_match = if line.starts_with("```") {
+                Some(line)
+            } else if line.starts_with("- ```") {
+                Some(&line[2..])
+            } else {
+                None
+            };
+
+            if let Some(fence_line) = fence_match {
+                let language = fence_line.trim_start_matches("```").trim();
+
+                if language == "clojure" || language.is_empty() {
+                    i += 1;
+                    if i >= lines.len() {
+                        break;
+                    }
+
+                    let should_extract = if language == "clojure" {
+                        true
+                    } else if lines[i].trim().starts_with("owl:functional-syntax::") {
+                        i += 1;
+                        true
+                    } else {
+                        false
+                    };
+
+                    if should_extract {
+                        let mut block_lines = Vec::new();
+                        while i < lines.len() {
+                            let current_line = lines[i];
+                            if current_line.trim().starts_with("```") {
+                                break;
+                            }
+                            let trimmed = current_line.trim_start();
+                            if !trimmed.is_empty()
+                                && !trimmed.starts_with(";;")
+                                && !trimmed.starts_with("#")
+                                && trimmed != "|" {
+                                block_lines.push(trimmed);
+                            }
+                            i += 1;
+                        }
+
+                        let block_text = block_lines.join("\n");
+                        let is_owl = block_text.contains("Declaration(")
+                            || block_text.contains("SubClassOf(")
+                            || block_text.contains("EquivalentClasses(")
+                            || block_text.contains("DisjointClasses(")
+                            || block_text.contains("ObjectProperty(")
+                            || block_text.contains("DataProperty(");
+
+                        if is_owl && !block_lines.is_empty() {
+                            blocks.push(block_text);
+                        }
+                    }
+                }
+                i += 1;
+                continue;
+            }
+
+            if line.starts_with("owl:functional-syntax::") {
+                i += 1;
+                if i >= lines.len() {
+                    break;
+                }
+
+                if !lines[i].trim().starts_with('|') {
+                    i += 1;
+                    continue;
+                }
+
+                i += 1;
+
+                let mut block_lines = Vec::new();
+                let base_indent = if i < lines.len() {
+                    lines[i].len() - lines[i].trim_start().len()
+                } else {
+                    0
+                };
+
+                while i < lines.len() {
+                    let current_line = lines[i];
+                    let current_indent = current_line.len() - current_line.trim_start().len();
+
+                    if !current_line.trim().is_empty() && current_indent < base_indent {
+                        break;
+                    }
+
+                    if current_line.trim_start().starts_with('#')
+                        || current_line.trim().starts_with("```")
+                        || (current_line.contains("::") && !current_line.trim().starts_with("//"))
+                    {
+                        break;
+                    }
+
+                    if current_indent >= base_indent && !current_line.trim().is_empty() {
+                        let trimmed = if current_indent >= base_indent {
+                            &current_line[base_indent..]
+                        } else {
+                            current_line.trim_start()
+                        };
+                        block_lines.push(trimmed);
+                    }
+
+                    i += 1;
+                }
+
+                if !block_lines.is_empty() {
+                    blocks.push(block_lines.join("\n"));
+                }
+            } else {
+                i += 1;
+            }
+        }
+
+        Ok(blocks)
+    }
+
+    fn extract_classes(&self, owl_content: &str) -> Vec<String> {
+        let mut classes = Vec::new();
+        let class_re = Regex::new(r"Declaration\(Class\(([^)]+)\)\)").unwrap();
+
+        for cap in class_re.captures_iter(owl_content) {
+            classes.push(cap[1].to_string());
+        }
+
+        let subclass_re = Regex::new(r"SubClassOf\(([^\s]+)\s+[^)]+\)").unwrap();
+        for cap in subclass_re.captures_iter(owl_content) {
+            let class = cap[1].to_string();
+            if !classes.contains(&class) {
+                classes.push(class);
+            }
+        }
+
+        classes
+    }
+
+    fn extract_owl_properties(&self, owl_content: &str) -> Vec<String> {
+        let mut properties = Vec::new();
+
+        let obj_prop_re = Regex::new(r"Declaration\(ObjectProperty\(([^)]+)\)\)").unwrap();
+        for cap in obj_prop_re.captures_iter(owl_content) {
+            properties.push(cap[1].to_string());
+        }
+
+        let data_prop_re = Regex::new(r"Declaration\(DataProperty\(([^)]+)\)\)").unwrap();
+        for cap in data_prop_re.captures_iter(owl_content) {
+            properties.push(cap[1].to_string());
+        }
+
+        properties
+    }
+
+    fn extract_relationships(&self, owl_content: &str) -> Vec<OntologyRelationship> {
+        let mut relationships = Vec::new();
+
+        let subclass_re = Regex::new(r"SubClassOf\(([^\s]+)\s+([^)]+)\)").unwrap();
+        for cap in subclass_re.captures_iter(owl_content) {
+            relationships.push(OntologyRelationship {
+                subject: cap[1].to_string(),
+                predicate: "rdfs:subClassOf".to_string(),
+                object: cap[2].to_string(),
+                relationship_type: RelationshipType::SubClassOf,
+            });
+        }
+
+        let domain_re = Regex::new(r"ObjectPropertyDomain\(([^\s]+)\s+([^)]+)\)").unwrap();
+        for cap in domain_re.captures_iter(owl_content) {
+            relationships.push(OntologyRelationship {
+                subject: cap[1].to_string(),
+                predicate: "rdfs:domain".to_string(),
+                object: cap[2].to_string(),
+                relationship_type: RelationshipType::Domain,
+            });
+        }
+
+        let range_re = Regex::new(r"ObjectPropertyRange\(([^\s]+)\s+([^)]+)\)").unwrap();
+        for cap in range_re.captures_iter(owl_content) {
+            relationships.push(OntologyRelationship {
+                subject: cap[1].to_string(),
+                predicate: "rdfs:range".to_string(),
+                object: cap[2].to_string(),
+                relationship_type: RelationshipType::Range,
+            });
+        }
+
+        let disjoint_re = Regex::new(r"DisjointClasses\(([^\s]+)\s+([^)]+)\)").unwrap();
+        for cap in disjoint_re.captures_iter(owl_content) {
+            relationships.push(OntologyRelationship {
+                subject: cap[1].to_string(),
+                predicate: "owl:disjointWith".to_string(),
+                object: cap[2].to_string(),
+                relationship_type: RelationshipType::DisjointWith,
+            });
+        }
+
+        relationships
+    }
+
+    fn calculate_hash(&self, content: &str) -> String {
+        use blake3::Hasher;
+        let mut hasher = Hasher::new();
+        hasher.update(content.as_bytes());
+        hasher.finalize().to_hex().to_string()
+    }
+
+    async fn download_file_content(&self, url: &str) -> Result<String> {
+        let content = self.retry_with_backoff(|| async {
+            self.client
+                .get(url)
+                .header("Authorization", format!("token {}", self.config.github_token))
+                .send()
+                .await?
+                .error_for_status()?
+                .text()
+                .await
+                .map_err(|e| e.into())
+        })
+        .await?;
+
+        Ok(content)
+    }
+
+    async fn get_file_content(&self, api_url: &str) -> Result<String> {
+        #[derive(Deserialize)]
+        struct ContentResponse {
+            content: String,
+            encoding: String,
+        }
+
+        let response: ContentResponse = self.github_request(api_url).await?;
+
+        if response.encoding == "base64" {
+            let decoded = base64::decode(&response.content.replace('\n', ""))
+                .context("Failed to decode base64 content")?;
+            String::from_utf8(decoded).context("Invalid UTF-8 in file content")
+        } else {
+            Ok(response.content)
+        }
+    }
+
+    async fn github_request<T: for<'de> Deserialize<'de>>(&self, url: &str) -> Result<T> {
+        self.retry_with_backoff(|| async {
+            let response = self.client
+                .get(url)
+                .header("Authorization", format!("token {}", self.config.github_token))
+                .header("Accept", "application/vnd.github.v3+json")
+                .send()
+                .await?;
+
+            if response.status() == StatusCode::FORBIDDEN {
+                if let Some(retry_after) = response.headers().get("Retry-After") {
+                    if let Ok(retry_str) = retry_after.to_str() {
+                        if let Ok(seconds) = retry_str.parse::<u64>() {
+                            return Err(DownloaderError::RateLimit(Duration::from_secs(seconds)).into());
+                        }
+                    }
+                }
+
+                if let Some(remaining) = response.headers().get("X-RateLimit-Remaining") {
+                    if let Ok(remaining_str) = remaining.to_str() {
+                        if remaining_str == "0" {
+                            if let Some(reset) = response.headers().get("X-RateLimit-Reset") {
+                                if let Ok(reset_str) = reset.to_str() {
+                                    if let Ok(reset_timestamp) = reset_str.parse::<i64>() {
+                                        let now = Utc::now().timestamp();
+                                        let wait_seconds = (reset_timestamp - now).max(0) as u64;
+                                        return Err(DownloaderError::RateLimit(Duration::from_secs(wait_seconds)).into());
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+
+            if response.status() == StatusCode::UNAUTHORIZED {
+                return Err(DownloaderError::Auth("Invalid GitHub token".to_string()).into());
+            }
+
+            let response = response.error_for_status()?;
+            response.json::<T>().await.map_err(|e| e.into())
+        })
+        .await
+    }
+
+    async fn retry_with_backoff<F, Fut, T>(&self, mut operation: F) -> Result<T>
+    where
+        F: FnMut() -> Fut,
+        Fut: std::future::Future<Output = Result<T>>,
+    {
+        let mut attempt = 0;
+        let mut delay = Duration::from_millis(self.config.initial_retry_delay_ms);
+
+        loop {
+            match operation().await {
+                Ok(result) => return Ok(result),
+                Err(e) => {
+                    if let Some(rate_limit_err) = e.downcast_ref::<DownloaderError>() {
+                        if let DownloaderError::RateLimit(wait_duration) = rate_limit_err {
+                            if self.config.respect_rate_limits {
+                                warn!("Rate limit hit, waiting {:?}", wait_duration);
+                                sleep(*wait_duration).await;
+                                continue;
+                            }
+                        }
+                    }
+
+                    attempt += 1;
+                    if attempt >= self.config.max_retries {
+                        error!("Max retries ({}) exceeded", self.config.max_retries);
+                        return Err(e);
+                    }
+
+                    warn!("Request failed (attempt {}/{}): {}", attempt, self.config.max_retries, e);
+
+                    sleep(delay).await;
+
+                    delay = Duration::from_millis(
+                        (delay.as_millis() as u64 * 2).min(self.config.max_retry_delay_ms)
+                    );
+                }
+            }
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_extract_title() {
+        let downloader = create_test_downloader();
+
+        let content = "# Test Title\nSome content";
+        let title = downloader.extract_title("test.md", content);
+        assert_eq!(title, "Test Title");
+
+        let content_no_heading = "Some content without heading";
+        let title = downloader.extract_title("myfile.md", content_no_heading);
+        assert_eq!(title, "myfile");
+    }
+
+    #[test]
+    fn test_extract_properties() {
+        let downloader = create_test_downloader();
+
+        let content = r#"
+term-id:: 20067
+maturity:: mature
+tags:: ontology, test
+"#;
+
+        let props = downloader.extract_properties(content);
+        assert_eq!(props.get("term-id").unwrap()[0], "20067");
+        assert_eq!(props.get("maturity").unwrap()[0], "mature");
+        assert_eq!(props.get("tags").unwrap().len(), 2);
+    }
+
+    #[test]
+    fn test_extract_classes() {
+        let downloader = create_test_downloader();
+
+        let owl_content = r#"
+Declaration(Class(mv:Avatar))
+SubClassOf(mv:Avatar mv:VirtualEntity)
+"#;
+
+        let classes = downloader.extract_classes(owl_content);
+        assert!(classes.contains(&"mv:Avatar".to_string()));
+        assert!(classes.len() >= 1);
+    }
+
+    #[test]
+    fn test_extract_relationships() {
+        let downloader = create_test_downloader();
+
+        let owl_content = r#"
+SubClassOf(mv:Avatar mv:VirtualEntity)
+ObjectPropertyDomain(mv:hasProperty mv:Avatar)
+"#;
+
+        let relationships = downloader.extract_relationships(owl_content);
+        assert_eq!(relationships.len(), 2);
+        assert_eq!(relationships[0].relationship_type, RelationshipType::SubClassOf);
+        assert_eq!(relationships[1].relationship_type, RelationshipType::Domain);
+    }
+
+    #[test]
+    fn test_should_process_file() {
+        let downloader = create_test_downloader();
+
+        let content_with_public = r#"
+- ### OntologyBlock
+public:: true
+Some OWL content
+"#;
+        assert!(downloader.should_process_file(content_with_public));
+
+        let content_no_public = r#"
+- ### OntologyBlock
+Some OWL content
+"#;
+        assert!(!downloader.should_process_file(content_no_public));
+
+        let content_no_ontology = r#"
+public:: true
+Some regular content
+"#;
+        assert!(!downloader.should_process_file(content_no_ontology));
+    }
+
+    #[test]
+    fn test_config_from_token() {
+        let config = OntologyDownloaderConfig::with_token("test_token".to_string());
+        assert_eq!(config.github_token, "test_token");
+        assert_eq!(config.repo_owner, "jjohare");
+        assert_eq!(config.repo_name, "logseq");
+    }
+
+    #[test]
+    fn test_progress_percentage() {
+        let progress = DownloadProgress {
+            total_files: 100,
+            processed_files: 25,
+            ontology_blocks_found: 10,
+            errors: Vec::new(),
+            started_at: Utc::now(),
+            completed_at: None,
+            current_file: None,
+        };
+
+        assert_eq!(progress.percentage(), 25.0);
+    }
+
+    fn create_test_downloader() -> OntologyDownloader {
+        let config = OntologyDownloaderConfig::with_token("test_token".to_string());
+        OntologyDownloader::new(config).unwrap()
+    }
+}
diff --git a/src/services/ontology_init.rs b/src/services/ontology_init.rs
new file mode 100644
index 00000000..966d81dc
--- /dev/null
+++ b/src/services/ontology_init.rs
@@ -0,0 +1,176 @@
+//! Ontology System Initialization
+//!
+//! Handles complete initialization of the ontology system including:
+//! - SQLite database setup
+//! - GitHub repository scanning
+//! - Ontology data download and parsing
+//! - Database population
+//! - Integration with graph visualization
+
+use std::path::PathBuf;
+use std::sync::Arc;
+use tokio::time::Duration;
+use log::{info, error, warn, debug};
+
+#[cfg(feature = "ontology")]
+use crate::services::database_service::DatabaseService;
+#[cfg(feature = "ontology")]
+use crate::services::ontology_downloader::{OntologyDownloader, OntologyDownloaderConfig};
+#[cfg(feature = "ontology")]
+use crate::services::ontology_storage::OntologyStorage;
+#[cfg(feature = "ontology")]
+use crate::services::ontology_sync::OntologySync;
+
+/// Initialize the complete ontology system
+#[cfg(feature = "ontology")]
+pub async fn initialize_ontology_system() -> Result<Arc<DatabaseService>, String> {
+    info!("🔮 Initializing Ontology System");
+
+    // 1. Setup database path
+    let db_path = std::env::var("DATA_ROOT")
+        .unwrap_or_else(|_| "/app/data".to_string());
+    let db_file = PathBuf::from(db_path).join("ontology_db.sqlite3");
+
+    info!("📊 Database path: {}", db_file.display());
+
+    // 2. Initialize database with schema
+    let db_service = match DatabaseService::new(&db_file) {
+        Ok(service) => {
+            info!("✅ Database initialized successfully");
+            Arc::new(service)
+        }
+        Err(e) => {
+            error!("❌ Failed to initialize database: {}", e);
+            return Err(format!("Database initialization failed: {}", e));
+        }
+    };
+
+    // 3. Initialize database schema
+    match db_service.initialize_schema() {
+        Ok(_) => info!("✅ Database schema created/verified"),
+        Err(e) => {
+            error!("❌ Schema initialization failed: {}", e);
+            return Err(format!("Schema initialization failed: {}", e));
+        }
+    }
+
+    // 4. Run settings migration from YAML to SQLite (if not already done)
+    let migration_service = crate::services::settings_migration::SettingsMigration::new(Arc::clone(&db_service));
+    if !migration_service.is_migrated() {
+        info!("⚙️  Running settings migration from YAML to SQLite");
+        match migration_service.migrate_from_yaml_files() {
+            Ok(result) => {
+                info!("✅ Settings migration completed successfully");
+                info!("   📝 Settings migrated: {}", result.settings_migrated);
+                info!("   ⚡ Physics profiles: {}", result.physics_profiles_migrated);
+                info!("   🔧 Dev config params: {}", result.dev_config_params_migrated);
+                info!("   ⏱️  Duration: {:?}", result.duration);
+                if !result.errors.is_empty() {
+                    warn!("   ⚠️  Errors: {} (migration continues)", result.errors.len());
+                }
+            }
+            Err(e) => {
+                warn!("⚠️  Settings migration failed (continuing with defaults): {}", e);
+            }
+        }
+    } else {
+        info!("✅ Settings already migrated, skipping");
+    }
+
+    // 5. Initialize DevConfig from database
+    info!("🔧 Initializing DevConfig from database");
+    match crate::config::dev_config::DevConfig::initialize(Arc::clone(&db_service)) {
+        Ok(_) => {
+            info!("✅ DevConfig initialized successfully from database");
+        }
+        Err(e) => {
+            warn!("⚠️  DevConfig initialization failed (using defaults): {}", e);
+        }
+    }
+
+    // 6. Spawn background task for ontology download
+    let db_service_clone = Arc::clone(&db_service);
+    tokio::spawn(async move {
+        // Wait for server to fully start
+        info!("⏳ Waiting 10 seconds for server initialization before starting ontology sync");
+        tokio::time::sleep(Duration::from_secs(10)).await;
+
+        if let Err(e) = download_and_process_ontology(db_service_clone).await {
+            error!("❌ Ontology download/processing failed: {}", e);
+        }
+    });
+
+    Ok(db_service)
+}
+
+/// Download and process ontology data from GitHub
+#[cfg(feature = "ontology")]
+async fn download_and_process_ontology(db_service: Arc<DatabaseService>) -> Result<(), String> {
+    info!("🚀 Starting ontology download and processing");
+
+    // 1. Create downloader with configuration from environment
+    let github_token = std::env::var("GITHUB_TOKEN")
+        .map_err(|_| "GITHUB_TOKEN not set in environment".to_string())?;
+
+    let config = OntologyDownloaderConfig {
+        repo_owner: std::env::var("GITHUB_OWNER")
+            .unwrap_or_else(|_| "jjohare".to_string()),
+        repo_name: std::env::var("GITHUB_REPO")
+            .unwrap_or_else(|_| "logseq".to_string()),
+        base_path: std::env::var("GITHUB_BASE_PATH")
+            .unwrap_or_else(|_| "mainKnowledgeGraph/pages".to_string()),
+        github_token,
+        max_retries: 3,
+        initial_retry_delay_ms: 1000,
+        max_retry_delay_ms: 30000,
+        request_timeout_secs: 30,
+        respect_rate_limits: true,
+    };
+
+    // 2. Create storage (separate database for ontology blocks)
+    let ontology_db_path = std::env::var("DATA_ROOT")
+        .unwrap_or_else(|_| "/app/data".to_string());
+    let ontology_db_file = PathBuf::from(ontology_db_path).join("ontology_blocks.sqlite3");
+
+    let storage = OntologyStorage::new(&ontology_db_file)
+        .map_err(|e| format!("Failed to create storage: {}", e))?;
+
+    // 3. Create sync orchestrator
+    let sync = OntologySync::new(config, storage)
+        .map_err(|e| format!("Failed to create sync: {}", e))?;
+
+    // 4. Execute sync with progress tracking
+    info!("📥 Beginning ontology sync from GitHub");
+
+    match sync.sync().await {
+        Ok(result) => {
+            info!("✅ Ontology sync completed successfully!");
+            info!("   📥 Blocks downloaded: {}", result.blocks_downloaded);
+            info!("   📝 Blocks saved: {}", result.blocks_saved);
+            info!("   ⏱️  Duration: {} seconds", result.duration_seconds);
+
+            if !result.errors.is_empty() {
+                warn!("⚠️  {} errors occurred during sync:", result.errors.len());
+                for (idx, error) in result.errors.iter().enumerate().take(5) {
+                    warn!("   {}. {}", idx + 1, error);
+                }
+                if result.errors.len() > 5 {
+                    warn!("   ... and {} more errors", result.errors.len() - 5);
+                }
+            }
+
+            Ok(())
+        }
+        Err(e) => {
+            error!("❌ Ontology sync failed: {}", e);
+            Err(format!("Ontology sync failed: {}", e))
+        }
+    }
+}
+
+/// No-op initialization when ontology feature is disabled
+#[cfg(not(feature = "ontology"))]
+pub async fn initialize_ontology_system() -> Result<(), String> {
+    debug!("Ontology feature not enabled, skipping initialization");
+    Ok(())
+}
diff --git a/src/services/ontology_storage.rs b/src/services/ontology_storage.rs
new file mode 100644
index 00000000..5db8c0ab
--- /dev/null
+++ b/src/services/ontology_storage.rs
@@ -0,0 +1,754 @@
+use anyhow::{Context, Result};
+use chrono::{DateTime, Utc};
+use log::{debug, info};
+use rusqlite::{Connection, params, OptionalExtension};
+use std::path::Path;
+use std::sync::{Arc, Mutex};
+use thiserror::Error;
+
+use super::ontology_downloader::{OntologyBlock, OntologyRelationship, RelationshipType};
+
+#[derive(Error, Debug)]
+pub enum StorageError {
+    #[error("Database error: {0}")]
+    Database(#[from] rusqlite::Error),
+
+    #[error("Serialization error: {0}")]
+    Serialization(String),
+
+    #[error("Not found: {0}")]
+    NotFound(String),
+}
+
+pub struct OntologyStorage {
+    conn: Arc<Mutex<Connection>>,
+}
+
+impl OntologyStorage {
+    pub fn new<P: AsRef<Path>>(db_path: P) -> Result<Self> {
+        let conn = Connection::open(db_path)?;
+        let storage = Self {
+            conn: Arc::new(Mutex::new(conn)),
+        };
+
+        storage.initialize_schema()?;
+        Ok(storage)
+    }
+
+    pub fn in_memory() -> Result<Self> {
+        let conn = Connection::open_in_memory()?;
+        let storage = Self {
+            conn: Arc::new(Mutex::new(conn)),
+        };
+
+        storage.initialize_schema()?;
+        Ok(storage)
+    }
+
+    fn initialize_schema(&self) -> Result<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute_batch(
+            r#"
+            CREATE TABLE IF NOT EXISTS ontology_blocks (
+                id TEXT PRIMARY KEY,
+                source_file TEXT NOT NULL,
+                title TEXT NOT NULL,
+                content_hash TEXT NOT NULL,
+                downloaded_at TEXT NOT NULL,
+                created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
+                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_blocks_source_file
+                ON ontology_blocks(source_file);
+            CREATE INDEX IF NOT EXISTS idx_ontology_blocks_content_hash
+                ON ontology_blocks(content_hash);
+
+            CREATE TABLE IF NOT EXISTS ontology_properties (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                property_key TEXT NOT NULL,
+                property_value TEXT NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_properties_block_id
+                ON ontology_properties(block_id);
+            CREATE INDEX IF NOT EXISTS idx_ontology_properties_key
+                ON ontology_properties(property_key);
+
+            CREATE TABLE IF NOT EXISTS ontology_owl_content (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                content TEXT NOT NULL,
+                content_order INTEGER NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_owl_content_block_id
+                ON ontology_owl_content(block_id);
+
+            CREATE TABLE IF NOT EXISTS ontology_classes (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                class_name TEXT NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_classes_block_id
+                ON ontology_classes(block_id);
+            CREATE INDEX IF NOT EXISTS idx_ontology_classes_name
+                ON ontology_classes(class_name);
+
+            CREATE TABLE IF NOT EXISTS ontology_owl_properties (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                property_name TEXT NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_owl_properties_block_id
+                ON ontology_owl_properties(block_id);
+            CREATE INDEX IF NOT EXISTS idx_ontology_owl_properties_name
+                ON ontology_owl_properties(property_name);
+
+            CREATE TABLE IF NOT EXISTS ontology_relationships (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                subject TEXT NOT NULL,
+                predicate TEXT NOT NULL,
+                object TEXT NOT NULL,
+                relationship_type TEXT NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_relationships_block_id
+                ON ontology_relationships(block_id);
+            CREATE INDEX IF NOT EXISTS idx_ontology_relationships_subject
+                ON ontology_relationships(subject);
+            CREATE INDEX IF NOT EXISTS idx_ontology_relationships_object
+                ON ontology_relationships(object);
+            CREATE INDEX IF NOT EXISTS idx_ontology_relationships_type
+                ON ontology_relationships(relationship_type);
+
+            CREATE TABLE IF NOT EXISTS sync_metadata (
+                key TEXT PRIMARY KEY,
+                value TEXT NOT NULL,
+                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
+            );
+            "#,
+        )?;
+
+        info!("Database schema initialized successfully");
+        Ok(())
+    }
+
+    pub fn save_block(&self, block: &OntologyBlock) -> Result<()> {
+        let conn = self.conn.lock().unwrap();
+
+        let tx = conn.unchecked_transaction()?;
+
+        tx.execute(
+            r#"
+            INSERT OR REPLACE INTO ontology_blocks
+                (id, source_file, title, content_hash, downloaded_at, updated_at)
+            VALUES (?1, ?2, ?3, ?4, ?5, ?6)
+            "#,
+            params![
+                &block.id,
+                &block.source_file,
+                &block.title,
+                &block.content_hash,
+                &block.downloaded_at.to_rfc3339(),
+                Utc::now().to_rfc3339(),
+            ],
+        )?;
+
+        tx.execute(
+            "DELETE FROM ontology_properties WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for (key, values) in &block.properties {
+            for value in values {
+                tx.execute(
+                    r#"
+                    INSERT INTO ontology_properties (block_id, property_key, property_value)
+                    VALUES (?1, ?2, ?3)
+                    "#,
+                    params![&block.id, key, value],
+                )?;
+            }
+        }
+
+        tx.execute(
+            "DELETE FROM ontology_owl_content WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for (idx, content) in block.owl_content.iter().enumerate() {
+            tx.execute(
+                r#"
+                INSERT INTO ontology_owl_content (block_id, content, content_order)
+                VALUES (?1, ?2, ?3)
+                "#,
+                params![&block.id, content, idx as i32],
+            )?;
+        }
+
+        tx.execute(
+            "DELETE FROM ontology_classes WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for class in &block.classes {
+            tx.execute(
+                r#"
+                INSERT INTO ontology_classes (block_id, class_name)
+                VALUES (?1, ?2)
+                "#,
+                params![&block.id, class],
+            )?;
+        }
+
+        tx.execute(
+            "DELETE FROM ontology_owl_properties WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for property in &block.properties_list {
+            tx.execute(
+                r#"
+                INSERT INTO ontology_owl_properties (block_id, property_name)
+                VALUES (?1, ?2)
+                "#,
+                params![&block.id, property],
+            )?;
+        }
+
+        tx.execute(
+            "DELETE FROM ontology_relationships WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for rel in &block.relationships {
+            let rel_type = relationship_type_to_string(&rel.relationship_type);
+            tx.execute(
+                r#"
+                INSERT INTO ontology_relationships
+                    (block_id, subject, predicate, object, relationship_type)
+                VALUES (?1, ?2, ?3, ?4, ?5)
+                "#,
+                params![&block.id, &rel.subject, &rel.predicate, &rel.object, rel_type],
+            )?;
+        }
+
+        tx.commit()?;
+
+        debug!("Saved ontology block: {}", block.id);
+        Ok(())
+    }
+
+    pub fn save_blocks(&self, blocks: &[OntologyBlock]) -> Result<usize> {
+        let mut count = 0;
+        for block in blocks {
+            self.save_block(block)?;
+            count += 1;
+        }
+
+        info!("Saved {} ontology blocks to database", count);
+        Ok(count)
+    }
+
+    pub fn get_block(&self, id: &str) -> Result<Option<OntologyBlock>> {
+        let conn = self.conn.lock().unwrap();
+
+        let block_data: Option<(String, String, String, String, String)> = conn
+            .query_row(
+                r#"
+                SELECT id, source_file, title, content_hash, downloaded_at
+                FROM ontology_blocks
+                WHERE id = ?1
+                "#,
+                params![id],
+                |row| Ok((row.get(0)?, row.get(1)?, row.get(2)?, row.get(3)?, row.get(4)?)),
+            )
+            .optional()?;
+
+        if let Some((id, source_file, title, content_hash, downloaded_at)) = block_data {
+            let downloaded_at = DateTime::parse_from_rfc3339(&downloaded_at)?
+                .with_timezone(&Utc);
+
+            let properties = self.get_properties(&conn, &id)?;
+            let owl_content = self.get_owl_content(&conn, &id)?;
+            let classes = self.get_classes(&conn, &id)?;
+            let properties_list = self.get_owl_properties(&conn, &id)?;
+            let relationships = self.get_relationships(&conn, &id)?;
+
+            Ok(Some(OntologyBlock {
+                id,
+                source_file,
+                title,
+                properties,
+                owl_content,
+                classes,
+                properties_list,
+                relationships,
+                downloaded_at,
+                content_hash,
+            }))
+        } else {
+            Ok(None)
+        }
+    }
+
+    pub fn list_all_blocks(&self) -> Result<Vec<OntologyBlock>> {
+        let conn = self.conn.lock().unwrap();
+
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT id, source_file, title, content_hash, downloaded_at
+            FROM ontology_blocks
+            ORDER BY downloaded_at DESC
+            "#,
+        )?;
+
+        let block_ids: Vec<String> = stmt
+            .query_map([], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        drop(stmt);
+        drop(conn);
+
+        let mut blocks = Vec::new();
+        for id in block_ids {
+            if let Some(block) = self.get_block(&id)? {
+                blocks.push(block);
+            }
+        }
+
+        Ok(blocks)
+    }
+
+    pub fn search_by_class(&self, class_name: &str) -> Result<Vec<OntologyBlock>> {
+        let conn = self.conn.lock().unwrap();
+
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT DISTINCT block_id
+            FROM ontology_classes
+            WHERE class_name LIKE ?1
+            "#,
+        )?;
+
+        let pattern = format!("%{}%", class_name);
+        let block_ids: Vec<String> = stmt
+            .query_map(params![pattern], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        drop(stmt);
+        drop(conn);
+
+        let mut blocks = Vec::new();
+        for id in block_ids {
+            if let Some(block) = self.get_block(&id)? {
+                blocks.push(block);
+            }
+        }
+
+        Ok(blocks)
+    }
+
+    pub fn search_by_property(&self, property_key: &str) -> Result<Vec<OntologyBlock>> {
+        let conn = self.conn.lock().unwrap();
+
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT DISTINCT block_id
+            FROM ontology_properties
+            WHERE property_key = ?1
+            "#,
+        )?;
+
+        let block_ids: Vec<String> = stmt
+            .query_map(params![property_key], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        drop(stmt);
+        drop(conn);
+
+        let mut blocks = Vec::new();
+        for id in block_ids {
+            if let Some(block) = self.get_block(&id)? {
+                blocks.push(block);
+            }
+        }
+
+        Ok(blocks)
+    }
+
+    pub fn delete_block(&self, id: &str) -> Result<bool> {
+        let conn = self.conn.lock().unwrap();
+
+        let rows_affected = conn.execute(
+            "DELETE FROM ontology_blocks WHERE id = ?1",
+            params![id],
+        )?;
+
+        Ok(rows_affected > 0)
+    }
+
+    pub fn clear_all(&self) -> Result<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute_batch(
+            r#"
+            DELETE FROM ontology_relationships;
+            DELETE FROM ontology_owl_properties;
+            DELETE FROM ontology_classes;
+            DELETE FROM ontology_owl_content;
+            DELETE FROM ontology_properties;
+            DELETE FROM ontology_blocks;
+            DELETE FROM sync_metadata;
+            "#,
+        )?;
+
+        info!("Cleared all ontology data from database");
+        Ok(())
+    }
+
+    pub fn set_sync_metadata(&self, key: &str, value: &str) -> Result<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            r#"
+            INSERT OR REPLACE INTO sync_metadata (key, value, updated_at)
+            VALUES (?1, ?2, ?3)
+            "#,
+            params![key, value, Utc::now().to_rfc3339()],
+        )?;
+
+        Ok(())
+    }
+
+    pub fn get_sync_metadata(&self, key: &str) -> Result<Option<String>> {
+        let conn = self.conn.lock().unwrap();
+
+        let value: Option<String> = conn
+            .query_row(
+                "SELECT value FROM sync_metadata WHERE key = ?1",
+                params![key],
+                |row| row.get(0),
+            )
+            .optional()?;
+
+        Ok(value)
+    }
+
+    pub fn get_statistics(&self) -> Result<DatabaseStatistics> {
+        let conn = self.conn.lock().unwrap();
+
+        let total_blocks: i64 = conn.query_row(
+            "SELECT COUNT(*) FROM ontology_blocks",
+            [],
+            |row| row.get(0),
+        )?;
+
+        let total_classes: i64 = conn.query_row(
+            "SELECT COUNT(DISTINCT class_name) FROM ontology_classes",
+            [],
+            |row| row.get(0),
+        )?;
+
+        let total_properties: i64 = conn.query_row(
+            "SELECT COUNT(DISTINCT property_name) FROM ontology_owl_properties",
+            [],
+            |row| row.get(0),
+        )?;
+
+        let total_relationships: i64 = conn.query_row(
+            "SELECT COUNT(*) FROM ontology_relationships",
+            [],
+            |row| row.get(0),
+        )?;
+
+        let last_sync: Option<String> = self.get_sync_metadata("last_sync_time")?;
+        let last_sync_time = last_sync
+            .and_then(|s| DateTime::parse_from_rfc3339(&s).ok())
+            .map(|dt| dt.with_timezone(&Utc));
+
+        Ok(DatabaseStatistics {
+            total_blocks: total_blocks as usize,
+            total_classes: total_classes as usize,
+            total_properties: total_properties as usize,
+            total_relationships: total_relationships as usize,
+            last_sync_time,
+        })
+    }
+
+    fn get_properties(
+        &self,
+        conn: &Connection,
+        block_id: &str,
+    ) -> Result<std::collections::HashMap<String, Vec<String>>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT property_key, property_value
+            FROM ontology_properties
+            WHERE block_id = ?1
+            "#,
+        )?;
+
+        let mut properties = std::collections::HashMap::new();
+        let rows = stmt.query_map(params![block_id], |row| {
+            Ok((row.get::<_, String>(0)?, row.get::<_, String>(1)?))
+        })?;
+
+        for row in rows {
+            let (key, value) = row?;
+            properties
+                .entry(key)
+                .or_insert_with(Vec::new)
+                .push(value);
+        }
+
+        Ok(properties)
+    }
+
+    fn get_owl_content(&self, conn: &Connection, block_id: &str) -> Result<Vec<String>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT content
+            FROM ontology_owl_content
+            WHERE block_id = ?1
+            ORDER BY content_order
+            "#,
+        )?;
+
+        let contents: Vec<String> = stmt
+            .query_map(params![block_id], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        Ok(contents)
+    }
+
+    fn get_classes(&self, conn: &Connection, block_id: &str) -> Result<Vec<String>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT class_name
+            FROM ontology_classes
+            WHERE block_id = ?1
+            "#,
+        )?;
+
+        let classes: Vec<String> = stmt
+            .query_map(params![block_id], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        Ok(classes)
+    }
+
+    fn get_owl_properties(&self, conn: &Connection, block_id: &str) -> Result<Vec<String>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT property_name
+            FROM ontology_owl_properties
+            WHERE block_id = ?1
+            "#,
+        )?;
+
+        let properties: Vec<String> = stmt
+            .query_map(params![block_id], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        Ok(properties)
+    }
+
+    fn get_relationships(
+        &self,
+        conn: &Connection,
+        block_id: &str,
+    ) -> Result<Vec<OntologyRelationship>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT subject, predicate, object, relationship_type
+            FROM ontology_relationships
+            WHERE block_id = ?1
+            "#,
+        )?;
+
+        let relationships: Vec<OntologyRelationship> = stmt
+            .query_map(params![block_id], |row| {
+                Ok(OntologyRelationship {
+                    subject: row.get(0)?,
+                    predicate: row.get(1)?,
+                    object: row.get(2)?,
+                    relationship_type: string_to_relationship_type(&row.get::<_, String>(3)?),
+                })
+            })?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        Ok(relationships)
+    }
+}
+
+#[derive(Debug, Clone, serde::Serialize)]
+pub struct DatabaseStatistics {
+    pub total_blocks: usize,
+    pub total_classes: usize,
+    pub total_properties: usize,
+    pub total_relationships: usize,
+    pub last_sync_time: Option<DateTime<Utc>>,
+}
+
+fn relationship_type_to_string(rel_type: &RelationshipType) -> String {
+    match rel_type {
+        RelationshipType::SubClassOf => "SubClassOf".to_string(),
+        RelationshipType::ObjectProperty => "ObjectProperty".to_string(),
+        RelationshipType::DataProperty => "DataProperty".to_string(),
+        RelationshipType::DisjointWith => "DisjointWith".to_string(),
+        RelationshipType::EquivalentTo => "EquivalentTo".to_string(),
+        RelationshipType::InverseOf => "InverseOf".to_string(),
+        RelationshipType::Domain => "Domain".to_string(),
+        RelationshipType::Range => "Range".to_string(),
+        RelationshipType::Other(s) => s.clone(),
+    }
+}
+
+fn string_to_relationship_type(s: &str) -> RelationshipType {
+    match s {
+        "SubClassOf" => RelationshipType::SubClassOf,
+        "ObjectProperty" => RelationshipType::ObjectProperty,
+        "DataProperty" => RelationshipType::DataProperty,
+        "DisjointWith" => RelationshipType::DisjointWith,
+        "EquivalentTo" => RelationshipType::EquivalentTo,
+        "InverseOf" => RelationshipType::InverseOf,
+        "Domain" => RelationshipType::Domain,
+        "Range" => RelationshipType::Range,
+        _ => RelationshipType::Other(s.to_string()),
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use std::collections::HashMap;
+
+    #[test]
+    fn test_create_storage() {
+        let storage = OntologyStorage::in_memory().unwrap();
+        let stats = storage.get_statistics().unwrap();
+        assert_eq!(stats.total_blocks, 0);
+    }
+
+    #[test]
+    fn test_save_and_retrieve_block() {
+        let storage = OntologyStorage::in_memory().unwrap();
+
+        let mut properties = HashMap::new();
+        properties.insert("term-id".to_string(), vec!["123".to_string()]);
+
+        let block = OntologyBlock {
+            id: "test:block:1".to_string(),
+            source_file: "test.md".to_string(),
+            title: "Test Block".to_string(),
+            properties,
+            owl_content: vec!["Declaration(Class(test:Class))".to_string()],
+            classes: vec!["test:Class".to_string()],
+            properties_list: vec!["test:property".to_string()],
+            relationships: vec![OntologyRelationship {
+                subject: "test:A".to_string(),
+                predicate: "rdfs:subClassOf".to_string(),
+                object: "test:B".to_string(),
+                relationship_type: RelationshipType::SubClassOf,
+            }],
+            downloaded_at: Utc::now(),
+            content_hash: "abc123".to_string(),
+        };
+
+        storage.save_block(&block).unwrap();
+
+        let retrieved = storage.get_block("test:block:1").unwrap();
+        assert!(retrieved.is_some());
+
+        let retrieved_block = retrieved.unwrap();
+        assert_eq!(retrieved_block.id, "test:block:1");
+        assert_eq!(retrieved_block.title, "Test Block");
+        assert_eq!(retrieved_block.classes.len(), 1);
+        assert_eq!(retrieved_block.relationships.len(), 1);
+    }
+
+    #[test]
+    fn test_search_by_class() {
+        let storage = OntologyStorage::in_memory().unwrap();
+
+        let block = OntologyBlock {
+            id: "test:block:1".to_string(),
+            source_file: "test.md".to_string(),
+            title: "Test Block".to_string(),
+            properties: HashMap::new(),
+            owl_content: vec![],
+            classes: vec!["mv:Avatar".to_string()],
+            properties_list: vec![],
+            relationships: vec![],
+            downloaded_at: Utc::now(),
+            content_hash: "abc123".to_string(),
+        };
+
+        storage.save_block(&block).unwrap();
+
+        let results = storage.search_by_class("Avatar").unwrap();
+        assert_eq!(results.len(), 1);
+        assert_eq!(results[0].id, "test:block:1");
+    }
+
+    #[test]
+    fn test_statistics() {
+        let storage = OntologyStorage::in_memory().unwrap();
+
+        let block = OntologyBlock {
+            id: "test:block:1".to_string(),
+            source_file: "test.md".to_string(),
+            title: "Test Block".to_string(),
+            properties: HashMap::new(),
+            owl_content: vec![],
+            classes: vec!["test:Class1".to_string(), "test:Class2".to_string()],
+            properties_list: vec!["test:prop1".to_string()],
+            relationships: vec![],
+            downloaded_at: Utc::now(),
+            content_hash: "abc123".to_string(),
+        };
+
+        storage.save_block(&block).unwrap();
+
+        let stats = storage.get_statistics().unwrap();
+        assert_eq!(stats.total_blocks, 1);
+        assert_eq!(stats.total_classes, 2);
+        assert_eq!(stats.total_properties, 1);
+    }
+
+    #[test]
+    fn test_clear_all() {
+        let storage = OntologyStorage::in_memory().unwrap();
+
+        let block = OntologyBlock {
+            id: "test:block:1".to_string(),
+            source_file: "test.md".to_string(),
+            title: "Test Block".to_string(),
+            properties: HashMap::new(),
+            owl_content: vec![],
+            classes: vec![],
+            properties_list: vec![],
+            relationships: vec![],
+            downloaded_at: Utc::now(),
+            content_hash: "abc123".to_string(),
+        };
+
+        storage.save_block(&block).unwrap();
+        assert_eq!(storage.get_statistics().unwrap().total_blocks, 1);
+
+        storage.clear_all().unwrap();
+        assert_eq!(storage.get_statistics().unwrap().total_blocks, 0);
+    }
+}
diff --git a/src/services/ontology_sync.rs b/src/services/ontology_sync.rs
new file mode 100644
index 00000000..c4a699e7
--- /dev/null
+++ b/src/services/ontology_sync.rs
@@ -0,0 +1,93 @@
+use anyhow::Result;
+use chrono::Utc;
+use log::{info, warn};
+use std::sync::Arc;
+use tokio::sync::RwLock;
+
+use super::ontology_downloader::{OntologyDownloader, OntologyDownloaderConfig, DownloadProgress};
+use super::ontology_storage::{OntologyStorage, DatabaseStatistics};
+
+pub struct OntologySync {
+    downloader: OntologyDownloader,
+    storage: Arc<OntologyStorage>,
+}
+
+impl OntologySync {
+    pub fn new(
+        downloader_config: OntologyDownloaderConfig,
+        storage: OntologyStorage,
+    ) -> Result<Self> {
+        let downloader = OntologyDownloader::new(downloader_config)?;
+
+        Ok(Self {
+            downloader,
+            storage: Arc::new(storage),
+        })
+    }
+
+    pub async fn sync(&self) -> Result<SyncResult> {
+        info!("Starting ontology synchronization");
+
+        let start_time = Utc::now();
+
+        let blocks = self.downloader.download_all().await?;
+
+        let saved_count = self.storage.save_blocks(&blocks)?;
+
+        self.storage.set_sync_metadata("last_sync_time", &Utc::now().to_rfc3339())?;
+        self.storage.set_sync_metadata("last_sync_blocks", &saved_count.to_string())?;
+
+        let progress = self.downloader.get_progress().await;
+        let stats = self.storage.get_statistics()?;
+
+        let duration = Utc::now().signed_duration_since(start_time);
+
+        info!(
+            "Synchronization complete: {} blocks saved in {:?}",
+            saved_count, duration
+        );
+
+        Ok(SyncResult {
+            blocks_downloaded: blocks.len(),
+            blocks_saved: saved_count,
+            errors: progress.errors,
+            duration_seconds: duration.num_seconds() as u64,
+            statistics: stats,
+        })
+    }
+
+    pub async fn get_progress(&self) -> DownloadProgress {
+        self.downloader.get_progress().await
+    }
+
+    pub fn get_statistics(&self) -> Result<DatabaseStatistics> {
+        self.storage.get_statistics()
+    }
+
+    pub fn storage(&self) -> Arc<OntologyStorage> {
+        Arc::clone(&self.storage)
+    }
+}
+
+#[derive(Debug, Clone, serde::Serialize)]
+pub struct SyncResult {
+    pub blocks_downloaded: usize,
+    pub blocks_saved: usize,
+    pub errors: Vec<String>,
+    pub duration_seconds: u64,
+    pub statistics: DatabaseStatistics,
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[tokio::test]
+    async fn test_sync_creation() {
+        let config = OntologyDownloaderConfig::with_token("test_token".to_string());
+        let storage = OntologyStorage::in_memory().unwrap();
+        let sync = OntologySync::new(config, storage);
+
+        assert!(sync.is_ok());
+    }
+}
diff --git a/src/services/settings_migration.rs b/src/services/settings_migration.rs
new file mode 100644
index 00000000..b81030dd
--- /dev/null
+++ b/src/services/settings_migration.rs
@@ -0,0 +1,657 @@
+//! Settings Migration Service
+//!
+//! Migrates YAML-based settings to SQLite database with dual key format support.
+//! Supports both camelCase (client) and snake_case (server) key formats simultaneously.
+
+use std::path::Path;
+use std::collections::HashMap;
+use serde_yaml::Value as YamlValue;
+use log::{info, warn, error, debug};
+use rusqlite::{Connection, params, Result as SqliteResult};
+
+use crate::services::database_service::{DatabaseService, SettingValue};
+
+/// Settings migration service
+pub struct SettingsMigration {
+    db_service: std::sync::Arc<DatabaseService>,
+}
+
+impl SettingsMigration {
+    /// Create new migration service
+    pub fn new(db_service: std::sync::Arc<DatabaseService>) -> Self {
+        Self { db_service }
+    }
+
+    /// Execute complete migration from YAML files to database
+    pub fn migrate_from_yaml_files(&self) -> Result<MigrationResult, String> {
+        info!("Starting settings migration from YAML to SQLite");
+        let start_time = std::time::Instant::now();
+
+        let mut result = MigrationResult::default();
+
+        // Load and merge YAML files
+        let main_yaml_path = std::env::var("DATA_ROOT")
+            .unwrap_or_else(|_| "/app/data".to_string()) + "/settings.yaml";
+        let ontology_yaml_path = std::env::var("DATA_ROOT")
+            .unwrap_or_else(|_| "/app/data".to_string()) + "/settings_ontology_extension.yaml";
+
+        // Parse main settings
+        let main_settings = match self.load_yaml_file(&main_yaml_path) {
+            Ok(value) => {
+                info!("Loaded main settings from: {}", main_yaml_path);
+                value
+            }
+            Err(e) => {
+                error!("Failed to load main settings: {}", e);
+                return Err(format!("Failed to load main settings: {}", e));
+            }
+        };
+
+        // Parse ontology extension settings
+        let ontology_settings = match self.load_yaml_file(&ontology_yaml_path) {
+            Ok(value) => {
+                info!("Loaded ontology extension from: {}", ontology_yaml_path);
+                value
+            }
+            Err(e) => {
+                warn!("Failed to load ontology extension (continuing without it): {}", e);
+                YamlValue::Mapping(serde_yaml::Mapping::new())
+            }
+        };
+
+        // Merge settings
+        let merged = self.merge_yaml_values(vec![main_settings, ontology_settings]);
+
+        // Flatten YAML hierarchy into key-value pairs
+        let flattened = self.flatten_yaml(&merged, "");
+        info!("Flattened {} settings keys", flattened.len());
+
+        // Migrate each setting with dual key format
+        for (key, value) in flattened.iter() {
+            match self.migrate_setting(key, value) {
+                Ok(_) => {
+                    result.settings_migrated += 1;
+                }
+                Err(e) => {
+                    error!("Failed to migrate setting '{}': {}", key, e);
+                    result.errors.push(format!("Setting '{}': {}", key, e));
+                }
+            }
+        }
+
+        // Extract and migrate physics settings
+        match self.migrate_physics_profiles(&merged) {
+            Ok(count) => {
+                info!("Migrated {} physics profiles", count);
+                result.physics_profiles_migrated = count;
+            }
+            Err(e) => {
+                error!("Failed to migrate physics profiles: {}", e);
+                result.errors.push(format!("Physics profiles: {}", e));
+            }
+        }
+
+        // Migrate dev_config.toml
+        match self.migrate_dev_config_to_sqlite() {
+            Ok(count) => {
+                info!("Migrated {} dev_config parameters", count);
+                result.dev_config_params_migrated = count;
+            }
+            Err(e) => {
+                error!("Failed to migrate dev_config.toml: {}", e);
+                result.errors.push(format!("Dev config: {}", e));
+            }
+        }
+
+        result.duration = start_time.elapsed();
+        info!("Migration completed in {:?}", result.duration);
+        info!("  Settings migrated: {}", result.settings_migrated);
+        info!("  Physics profiles: {}", result.physics_profiles_migrated);
+        info!("  Dev config params: {}", result.dev_config_params_migrated);
+        if !result.errors.is_empty() {
+            warn!("  Errors encountered: {}", result.errors.len());
+        }
+
+        Ok(result)
+    }
+
+    /// Load YAML file from disk
+    fn load_yaml_file(&self, path: &str) -> Result<YamlValue, String> {
+        let contents = std::fs::read_to_string(path)
+            .map_err(|e| format!("Failed to read file: {}", e))?;
+
+        serde_yaml::from_str(&contents)
+            .map_err(|e| format!("Failed to parse YAML: {}", e))
+    }
+
+    /// Merge multiple YAML values (deep merge)
+    fn merge_yaml_values(&self, values: Vec<YamlValue>) -> YamlValue {
+        let mut result = YamlValue::Mapping(serde_yaml::Mapping::new());
+
+        for value in values {
+            if let YamlValue::Mapping(map) = value {
+                if let YamlValue::Mapping(result_map) = &mut result {
+                    for (k, v) in map {
+                        result_map.insert(k, v);
+                    }
+                }
+            }
+        }
+
+        result
+    }
+
+    /// Flatten nested YAML into hierarchical keys
+    fn flatten_yaml(&self, value: &YamlValue, prefix: &str) -> HashMap<String, YamlValue> {
+        let mut result = HashMap::new();
+
+        match value {
+            YamlValue::Mapping(map) => {
+                for (key, val) in map {
+                    if let Some(key_str) = key.as_str() {
+                        let new_prefix = if prefix.is_empty() {
+                            key_str.to_string()
+                        } else {
+                            format!("{}.{}", prefix, key_str)
+                        };
+
+                        match val {
+                            YamlValue::Mapping(_) => {
+                                // Recursively flatten nested objects
+                                let nested = self.flatten_yaml(val, &new_prefix);
+                                result.extend(nested);
+                            }
+                            YamlValue::Sequence(_) => {
+                                // Store arrays as JSON
+                                result.insert(new_prefix, val.clone());
+                            }
+                            _ => {
+                                // Store primitive values
+                                result.insert(new_prefix, val.clone());
+                            }
+                        }
+                    }
+                }
+            }
+            _ => {
+                // Non-mapping root value
+                if !prefix.is_empty() {
+                    result.insert(prefix.to_string(), value.clone());
+                }
+            }
+        }
+
+        result
+    }
+
+    /// Migrate a single setting to database
+    fn migrate_setting(&self, key: &str, value: &YamlValue) -> Result<(), String> {
+        // Convert YAML value to SettingValue
+        let setting_value = self.yaml_to_setting_value(value)?;
+
+        // Store with camelCase key only
+        self.db_service.set_setting(key, setting_value, None)
+            .map_err(|e| format!("Failed to store setting: {}", e))?;
+
+        debug!("Migrated: {}", key);
+        Ok(())
+    }
+
+    /// Convert YAML value to SettingValue
+    fn yaml_to_setting_value(&self, value: &YamlValue) -> Result<SettingValue, String> {
+        match value {
+            YamlValue::String(s) => Ok(SettingValue::String(s.clone())),
+            YamlValue::Number(n) => {
+                if let Some(i) = n.as_i64() {
+                    Ok(SettingValue::Integer(i))
+                } else if let Some(f) = n.as_f64() {
+                    Ok(SettingValue::Float(f))
+                } else {
+                    Err("Unsupported number type".to_string())
+                }
+            }
+            YamlValue::Bool(b) => Ok(SettingValue::Boolean(*b)),
+            YamlValue::Sequence(_) | YamlValue::Mapping(_) => {
+                // Convert to JSON for complex types
+                let json = serde_json::to_value(value)
+                    .map_err(|e| format!("Failed to convert to JSON: {}", e))?;
+                Ok(SettingValue::Json(json))
+            }
+            YamlValue::Null => Ok(SettingValue::String("".to_string())),
+            _ => Err("Unsupported YAML type".to_string()),
+        }
+    }
+
+    /// Convert hierarchical key to snake_case
+    fn to_snake_case_key(&self, key: &str) -> String {
+        key.split('.')
+            .map(|part| Self::to_snake_case_part(part))
+            .collect::<Vec<_>>()
+            .join(".")
+    }
+
+    /// Convert hierarchical key to camelCase
+    fn to_camel_case_key(&self, key: &str) -> String {
+        key.split('.')
+            .map(|part| Self::to_camel_case_part(part))
+            .collect::<Vec<_>>()
+            .join(".")
+    }
+
+    /// Convert a single part to snake_case
+    fn to_snake_case_part(s: &str) -> String {
+        let mut result = String::new();
+        let mut prev_is_upper = false;
+
+        for (i, ch) in s.chars().enumerate() {
+            if ch.is_uppercase() {
+                if i > 0 && !prev_is_upper {
+                    result.push('_');
+                }
+                result.push(ch.to_lowercase().next().unwrap());
+                prev_is_upper = true;
+            } else {
+                result.push(ch);
+                prev_is_upper = false;
+            }
+        }
+        result
+    }
+
+    /// Convert a single part to camelCase
+    fn to_camel_case_part(s: &str) -> String {
+        let mut result = String::new();
+        let mut capitalize_next = false;
+        let mut first = true;
+
+        for ch in s.chars() {
+            if ch == '_' {
+                capitalize_next = true;
+            } else if capitalize_next {
+                result.push(ch.to_uppercase().next().unwrap());
+                capitalize_next = false;
+                first = false;
+            } else {
+                if first {
+                    result.push(ch.to_lowercase().next().unwrap());
+                    first = false;
+                } else {
+                    result.push(ch);
+                }
+            }
+        }
+        result
+    }
+
+    /// Migrate physics settings profiles
+    fn migrate_physics_profiles(&self, yaml: &YamlValue) -> Result<usize, String> {
+        let mut count = 0;
+
+        // Extract physics settings from nested structure
+        // Path: visualisation.graphs.{profile}.physics
+        if let Some(vis) = yaml.get("visualisation") {
+            if let Some(graphs) = vis.get("graphs") {
+                if let YamlValue::Mapping(graphs_map) = graphs {
+                    for (profile_name, profile_config) in graphs_map {
+                        if let Some(profile_name_str) = profile_name.as_str() {
+                            if let Some(physics) = profile_config.get("physics") {
+                                match self.migrate_physics_profile(profile_name_str, physics) {
+                                    Ok(_) => {
+                                        count += 1;
+                                        debug!("Migrated physics profile: {}", profile_name_str);
+                                    }
+                                    Err(e) => {
+                                        warn!("Failed to migrate physics profile '{}': {}",
+                                              profile_name_str, e);
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        }
+
+        Ok(count)
+    }
+
+    /// Migrate a single physics profile
+    fn migrate_physics_profile(&self, profile_name: &str, physics: &YamlValue) -> Result<(), String> {
+        use crate::config::PhysicsSettings;
+
+        // Extract physics settings from YAML
+        let settings = PhysicsSettings {
+            damping: self.get_f32(physics, "damping").unwrap_or(0.95),
+            dt: self.get_f32(physics, "dt").unwrap_or(0.016),
+            iterations: self.get_u32(physics, "iterations").unwrap_or(100),
+            max_velocity: self.get_f32(physics, "maxVelocity").unwrap_or(1.0),
+            max_force: self.get_f32(physics, "maxForce").unwrap_or(100.0),
+            repel_k: self.get_f32(physics, "repelK").unwrap_or(50.0),
+            spring_k: self.get_f32(physics, "springK").unwrap_or(0.005),
+            mass_scale: self.get_f32(physics, "massScale").unwrap_or(1.0),
+            boundary_damping: self.get_f32(physics, "boundaryDamping").unwrap_or(0.95),
+            temperature: self.get_f32(physics, "temperature").unwrap_or(0.01),
+            gravity: self.get_f32(physics, "gravity").unwrap_or(0.0001),
+            bounds_size: self.get_f32(physics, "boundsSize").unwrap_or(500.0),
+            enable_bounds: self.get_bool(physics, "enableBounds").unwrap_or(false),
+            rest_length: self.get_f32(physics, "restLength").unwrap_or(50.0),
+            repulsion_cutoff: self.get_f32(physics, "repulsionCutoff").unwrap_or(50.0),
+            repulsion_softening_epsilon: self.get_f32(physics, "repulsionSofteningEpsilon").unwrap_or(0.0001),
+            center_gravity_k: self.get_f32(physics, "centerGravityK").unwrap_or(0.0),
+            grid_cell_size: self.get_f32(physics, "gridCellSize").unwrap_or(50.0),
+            warmup_iterations: self.get_u32(physics, "warmupIterations").unwrap_or(100),
+            cooling_rate: self.get_f32(physics, "coolingRate").unwrap_or(0.001),
+            constraint_ramp_frames: self.get_u32(physics, "constraintRampFrames").unwrap_or(60),
+            constraint_max_force_per_node: self.get_f32(physics, "constraintMaxForcePerNode").unwrap_or(50.0),
+            ..PhysicsSettings::default()
+        };
+
+        self.db_service.save_physics_settings(profile_name, &settings)
+            .map_err(|e| format!("Database error: {}", e))
+    }
+
+    /// Helper to get f32 from YAML
+    fn get_f32(&self, yaml: &YamlValue, key: &str) -> Option<f32> {
+        yaml.get(key)?.as_f64().map(|v| v as f32)
+    }
+
+    /// Helper to get u32 from YAML
+    fn get_u32(&self, yaml: &YamlValue, key: &str) -> Option<u32> {
+        yaml.get(key)?.as_i64().map(|v| v as u32)
+    }
+
+    /// Helper to get bool from YAML
+    fn get_bool(&self, yaml: &YamlValue, key: &str) -> Option<bool> {
+        yaml.get(key)?.as_bool()
+    }
+
+    /// Check if migration has been run
+    pub fn is_migrated(&self) -> bool {
+        // Check if migrated settings exist (use full settings key instead of version)
+        match self.db_service.get_setting("app_full_settings") {
+            Ok(Some(_)) => true,
+            _ => false,
+        }
+    }
+
+    /// Rollback migration (delete all settings)
+    pub fn rollback(&self) -> Result<(), String> {
+        warn!("Rolling back settings migration - this will delete all settings!");
+
+        // This would require direct database access
+        // For now, we'll just log a warning
+        warn!("Rollback not fully implemented - manual database cleanup required");
+
+        Ok(())
+    }
+
+    /// Migrate dev_config.toml to SQLite database
+    pub fn migrate_dev_config_to_sqlite(&self) -> Result<usize, String> {
+        info!("Starting dev_config.toml migration to SQLite");
+
+        let dev_config_path = "data/dev_config.toml";
+
+        // Check if file exists
+        if !std::path::Path::new(dev_config_path).exists() {
+            warn!("dev_config.toml not found, skipping migration");
+            return Ok(0);
+        }
+
+        // Load TOML file
+        let content = std::fs::read_to_string(dev_config_path)
+            .map_err(|e| format!("Failed to read dev_config.toml: {}", e))?;
+
+        let toml_value: toml::Value = toml::from_str(&content)
+            .map_err(|e| format!("Failed to parse dev_config.toml: {}", e))?;
+
+        let mut count = 0;
+
+        // Migrate each section
+        if let toml::Value::Table(table) = toml_value {
+            // Physics section (32 parameters)
+            if let Some(toml::Value::Table(physics)) = table.get("physics") {
+                count += self.migrate_toml_section("dev.physics", physics)?;
+            }
+
+            // CUDA section (11 parameters)
+            if let Some(toml::Value::Table(cuda)) = table.get("cuda") {
+                count += self.migrate_toml_section("dev.cuda", cuda)?;
+            }
+
+            // Network section (13 parameters)
+            if let Some(toml::Value::Table(network)) = table.get("network") {
+                count += self.migrate_toml_section("dev.network", network)?;
+            }
+
+            // Rendering section (with nested agent_colors)
+            if let Some(toml::Value::Table(rendering)) = table.get("rendering") {
+                // Separate agent_colors from other rendering params
+                let mut rendering_copy = rendering.clone();
+
+                // Handle nested agent_colors
+                if let Some(toml::Value::Table(agent_colors)) = rendering_copy.remove("agent_colors") {
+                    count += self.migrate_toml_section("dev.rendering.agent_colors", &agent_colors)?;
+                }
+
+                // Migrate remaining rendering params
+                count += self.migrate_toml_section("dev.rendering", &rendering_copy)?;
+            }
+
+            // Performance section (11 parameters)
+            if let Some(toml::Value::Table(performance)) = table.get("performance") {
+                count += self.migrate_toml_section("dev.performance", performance)?;
+            }
+
+            // Debug section (8 parameters)
+            if let Some(toml::Value::Table(debug)) = table.get("debug") {
+                count += self.migrate_toml_section("dev.debug", debug)?;
+            }
+        }
+
+        info!("Migrated {} dev_config parameters to database", count);
+        Ok(count)
+    }
+
+    /// Migrate a TOML section to database with hierarchical keys
+    fn migrate_toml_section(&self, prefix: &str, table: &toml::map::Map<String, toml::Value>) -> Result<usize, String> {
+        let mut count = 0;
+
+        for (key, value) in table {
+            let full_key = format!("{}.{}", prefix, key);
+
+            // Convert TOML value to SettingValue
+            let setting_value = self.toml_to_setting_value(value)?;
+
+            // Store with camelCase key only
+            self.db_service.set_setting(&full_key, setting_value, None)
+                .map_err(|e| format!("Failed to store key '{}': {}", full_key, e))?;
+
+            count += 1;
+            debug!("Migrated dev_config parameter: {}", full_key);
+        }
+
+        Ok(count)
+    }
+
+    /// Convert TOML value to SettingValue
+    fn toml_to_setting_value(&self, value: &toml::Value) -> Result<SettingValue, String> {
+        match value {
+            toml::Value::String(s) => Ok(SettingValue::String(s.clone())),
+            toml::Value::Integer(i) => Ok(SettingValue::Integer(*i)),
+            toml::Value::Float(f) => Ok(SettingValue::Float(*f)),
+            toml::Value::Boolean(b) => Ok(SettingValue::Boolean(*b)),
+            toml::Value::Array(_) | toml::Value::Table(_) => {
+                // Convert complex types to JSON
+                let json = serde_json::to_value(value)
+                    .map_err(|e| format!("Failed to convert TOML to JSON: {}", e))?;
+                Ok(SettingValue::Json(json))
+            }
+            toml::Value::Datetime(dt) => Ok(SettingValue::String(dt.to_string())),
+        }
+    }
+}
+
+/// Key format converter utility
+pub struct KeyFormatConverter;
+
+impl KeyFormatConverter {
+    /// Convert any key between camelCase and snake_case
+    pub fn to_snake_case(key: &str) -> String {
+        key.split('.')
+            .map(|part| Self::to_snake_case_part(part))
+            .collect::<Vec<_>>()
+            .join(".")
+    }
+
+    /// Convert any key to camelCase
+    pub fn to_camel_case(key: &str) -> String {
+        key.split('.')
+            .map(|part| Self::to_camel_case_part(part))
+            .collect::<Vec<_>>()
+            .join(".")
+    }
+
+    /// Get both key formats for a given key
+    pub fn both_formats(key: &str) -> (String, String) {
+        (Self::to_camel_case(key), Self::to_snake_case(key))
+    }
+
+    /// Convert a single part to snake_case
+    fn to_snake_case_part(s: &str) -> String {
+        let mut result = String::new();
+        let mut prev_is_upper = false;
+
+        for (i, ch) in s.chars().enumerate() {
+            if ch.is_uppercase() {
+                if i > 0 && !prev_is_upper {
+                    result.push('_');
+                }
+                result.push(ch.to_lowercase().next().unwrap());
+                prev_is_upper = true;
+            } else {
+                result.push(ch);
+                prev_is_upper = false;
+            }
+        }
+        result
+    }
+
+    /// Convert a single part to camelCase
+    fn to_camel_case_part(s: &str) -> String {
+        let mut result = String::new();
+        let mut capitalize_next = false;
+        let mut first = true;
+
+        for ch in s.chars() {
+            if ch == '_' {
+                capitalize_next = true;
+            } else if capitalize_next {
+                result.push(ch.to_uppercase().next().unwrap());
+                capitalize_next = false;
+                first = false;
+            } else {
+                if first {
+                    result.push(ch.to_lowercase().next().unwrap());
+                    first = false;
+                } else {
+                    result.push(ch);
+                }
+            }
+        }
+        result
+    }
+}
+
+/// Migration result statistics
+#[derive(Debug, Default)]
+pub struct MigrationResult {
+    pub settings_migrated: usize,
+    pub physics_profiles_migrated: usize,
+    pub dev_config_params_migrated: usize,
+    pub errors: Vec<String>,
+    pub duration: std::time::Duration,
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_key_conversion() {
+        assert_eq!(
+            KeyFormatConverter::to_snake_case("visualisation.graphs.logseq.nodes.baseColor"),
+            "visualisation.graphs.logseq.nodes.base_color"
+        );
+
+        assert_eq!(
+            KeyFormatConverter::to_camel_case("visualisation.graphs.logseq.nodes.base_color"),
+            "visualisation.graphs.logseq.nodes.baseColor"
+        );
+    }
+
+    #[test]
+    fn test_both_formats() {
+        let (camel, snake) = KeyFormatConverter::both_formats("visualisation.enableBounds");
+        assert_eq!(camel, "visualisation.enableBounds");
+        assert_eq!(snake, "visualisation.enable_bounds");
+    }
+
+    #[test]
+    fn test_yaml_flattening() {
+        let yaml_str = r#"
+        root:
+          nested:
+            value: 42
+            flag: true
+          array: [1, 2, 3]
+        "#;
+
+        let yaml: YamlValue = serde_yaml::from_str(yaml_str).unwrap();
+        let db = std::sync::Arc::new(
+            DatabaseService::new(":memory:").unwrap()
+        );
+        let migration = SettingsMigration::new(db);
+
+        let flattened = migration.flatten_yaml(&yaml, "");
+
+        assert!(flattened.contains_key("root.nested.value"));
+        assert!(flattened.contains_key("root.nested.flag"));
+        assert!(flattened.contains_key("root.array"));
+    }
+
+    #[test]
+    fn test_yaml_to_setting_value() {
+        let db = std::sync::Arc::new(
+            DatabaseService::new(":memory:").unwrap()
+        );
+        let migration = SettingsMigration::new(db);
+
+        // String
+        let yaml = YamlValue::String("test".to_string());
+        match migration.yaml_to_setting_value(&yaml).unwrap() {
+            SettingValue::String(s) => assert_eq!(s, "test"),
+            _ => panic!("Expected String"),
+        }
+
+        // Integer
+        let yaml = serde_yaml::to_value(42).unwrap();
+        match migration.yaml_to_setting_value(&yaml).unwrap() {
+            SettingValue::Integer(i) => assert_eq!(i, 42),
+            _ => panic!("Expected Integer"),
+        }
+
+        // Float
+        let yaml = serde_yaml::to_value(3.14).unwrap();
+        match migration.yaml_to_setting_value(&yaml).unwrap() {
+            SettingValue::Float(f) => assert!((f - 3.14).abs() < 0.001),
+            _ => panic!("Expected Float"),
+        }
+
+        // Boolean
+        let yaml = YamlValue::Bool(true);
+        match migration.yaml_to_setting_value(&yaml).unwrap() {
+            SettingValue::Boolean(b) => assert!(b),
+            _ => panic!("Expected Boolean"),
+        }
+    }
+}
diff --git a/src/services/settings_service.rs b/src/services/settings_service.rs
new file mode 100644
index 00000000..eb9fa0c4
--- /dev/null
+++ b/src/services/settings_service.rs
@@ -0,0 +1,376 @@
+// SQLite-backed Settings Service
+// Replaces file-based settings with database-backed configuration
+// Supports user-specific overrides and hierarchical settings
+
+use std::collections::HashMap;
+use std::sync::Arc;
+use tokio::sync::RwLock;
+use serde_json::Value as JsonValue;
+use log::{info, error, debug, warn};
+
+use crate::services::database_service::{DatabaseService, SettingValue};
+use crate::services::settings_validator::{SettingsValidator, ValidationResult};
+use crate::config::{AppFullSettings, PhysicsSettings};
+
+#[derive(Clone)]
+pub struct SettingsService {
+    db: Arc<DatabaseService>,
+    validator: Arc<SettingsValidator>,
+    cache: Arc<RwLock<SettingsCache>>,
+    change_listeners: Arc<RwLock<Vec<ChangeListener>>>,
+}
+
+#[derive(Clone)]
+struct SettingsCache {
+    settings: HashMap<String, CachedSetting>,
+    last_updated: std::time::Instant,
+}
+
+#[derive(Clone)]
+struct CachedSetting {
+    value: SettingValue,
+    timestamp: std::time::Instant,
+}
+
+type ChangeListener = Arc<dyn Fn(&str, &SettingValue, Option<&str>) + Send + Sync>;
+
+#[derive(Debug, Clone)]
+pub struct SettingsTreeNode {
+    pub key: String,
+    pub value: Option<SettingValue>,
+    pub children: HashMap<String, SettingsTreeNode>,
+}
+
+impl SettingsService {
+    pub fn new(db: Arc<DatabaseService>) -> Result<Self, String> {
+        let validator = Arc::new(SettingsValidator::new());
+
+        Ok(Self {
+            db,
+            validator,
+            cache: Arc::new(RwLock::new(SettingsCache {
+                settings: HashMap::new(),
+                last_updated: std::time::Instant::now(),
+            })),
+            change_listeners: Arc::new(RwLock::new(Vec::new())),
+        })
+    }
+
+    /// Get setting by key (uses camelCase format, database handles snake_case fallback)
+    pub async fn get_setting(&self, key: &str) -> Result<Option<SettingValue>, String> {
+        // Use key as-is (camelCase) - database service has smart lookup with fallback
+
+        // Check cache first
+        {
+            let cache = self.cache.read().await;
+            if let Some(cached) = cache.settings.get(key) {
+                if cached.timestamp.elapsed().as_secs() < 300 { // 5 min TTL
+                    debug!("Cache hit for setting: {}", key);
+                    return Ok(Some(cached.value.clone()));
+                }
+            }
+        }
+
+        // Query database (smart lookup handles both camelCase and snake_case)
+        match self.db.get_setting(key) {
+            Ok(Some(value)) => {
+                // Update cache
+                let mut cache = self.cache.write().await;
+                cache.settings.insert(key.to_string(), CachedSetting {
+                    value: value.clone(),
+                    timestamp: std::time::Instant::now(),
+                });
+                Ok(Some(value))
+            }
+            Ok(None) => Ok(None),
+            Err(e) => {
+                error!("Database error getting setting {}: {}", key, e);
+                Err(format!("Database error: {}", e))
+            }
+        }
+    }
+
+    /// Set setting value with permission check
+    pub async fn set_setting(
+        &self,
+        key: &str,
+        value: SettingValue,
+        user_id: Option<&str>,
+    ) -> Result<(), String> {
+        // Use key as-is (camelCase format) - no normalization needed
+
+        // Validate the setting
+        let validation = self.validator.validate_setting(key, &value)?;
+        if !validation.is_valid {
+            return Err(format!(
+                "Validation failed for {}: {}",
+                key,
+                validation.errors.join(", ")
+            ));
+        }
+
+        // Store in database (camelCase format)
+        self.db.set_setting(key, value.clone(), None)
+            .map_err(|e| format!("Database error: {}", e))?;
+
+        // Invalidate cache
+        {
+            let mut cache = self.cache.write().await;
+            cache.settings.remove(key);
+            cache.last_updated = std::time::Instant::now();
+        }
+
+        // Notify listeners
+        self.notify_change(key, &value, user_id).await;
+
+        info!("Setting updated: {} by user {:?}", key, user_id);
+        Ok(())
+    }
+
+    /// Get settings tree by prefix
+    pub async fn get_settings_tree(&self, prefix: &str) -> Result<SettingsTreeNode, String> {
+        // Use prefix as-is (camelCase format)
+        let all_settings = self.list_all_settings().await?;
+
+        let mut root = SettingsTreeNode {
+            key: prefix.to_string(),
+            value: None,
+            children: HashMap::new(),
+        };
+
+        // Build tree from flat settings
+        for (key, value) in all_settings {
+            if key.starts_with(prefix) {
+                self.insert_into_tree(&mut root, &key, value, prefix);
+            }
+        }
+
+        Ok(root)
+    }
+
+    fn insert_into_tree(
+        &self,
+        node: &mut SettingsTreeNode,
+        key: &str,
+        value: SettingValue,
+        prefix: &str,
+    ) {
+        let relative_key = if key.starts_with(prefix) {
+            &key[prefix.len()..].trim_start_matches('.')
+        } else {
+            key
+        };
+
+        let parts: Vec<&str> = relative_key.split('.').collect();
+        if parts.is_empty() {
+            return;
+        }
+
+        let mut current = node;
+        for (i, part) in parts.iter().enumerate() {
+            if i == parts.len() - 1 {
+                // Leaf node
+                current.children.insert(
+                    part.to_string(),
+                    SettingsTreeNode {
+                        key: key.to_string(),
+                        value: Some(value.clone()),
+                        children: HashMap::new(),
+                    },
+                );
+            } else {
+                // Intermediate node
+                current = current
+                    .children
+                    .entry(part.to_string())
+                    .or_insert_with(|| SettingsTreeNode {
+                        key: format!("{}.{}", prefix, parts[..=i].join(".")),
+                        value: None,
+                        children: HashMap::new(),
+                    });
+            }
+        }
+    }
+
+    /// Get physics profile
+    pub async fn get_physics_profile(&self, profile_name: &str) -> Result<PhysicsSettings, String> {
+        self.db
+            .get_physics_settings(profile_name)
+            .map_err(|e| format!("Database error: {}", e))
+    }
+
+    /// Update physics profile with permission check
+    pub async fn update_physics_profile(
+        &self,
+        profile_name: &str,
+        params: PhysicsSettings,
+        user_id: Option<&str>,
+    ) -> Result<(), String> {
+        // Validate physics settings
+        let validation = self.validator.validate_physics_settings(&params)?;
+        if !validation.is_valid {
+            return Err(format!(
+                "Physics validation failed: {}",
+                validation.errors.join(", ")
+            ));
+        }
+
+        // Save to database
+        self.db
+            .save_physics_settings(profile_name, &params)
+            .map_err(|e| format!("Database error: {}", e))?;
+
+        info!(
+            "Physics profile {} updated by user {:?}",
+            profile_name, user_id
+        );
+        Ok(())
+    }
+
+    /// List all settings (for power users)
+    pub async fn list_all_settings(&self) -> Result<HashMap<String, SettingValue>, String> {
+        // For now, return a simple implementation
+        // In production, this would query all settings from the database
+        let cache = self.cache.read().await;
+        let mut result = HashMap::new();
+
+        for (key, cached) in cache.settings.iter() {
+            result.insert(key.clone(), cached.value.clone());
+        }
+
+        Ok(result)
+    }
+
+    /// Search settings by pattern
+    pub async fn search_settings(&self, pattern: &str) -> Result<Vec<(String, SettingValue)>, String> {
+        let all_settings = self.list_all_settings().await?;
+        let pattern_lower = pattern.to_lowercase();
+
+        let results: Vec<(String, SettingValue)> = all_settings
+            .into_iter()
+            .filter(|(key, _)| key.to_lowercase().contains(&pattern_lower))
+            .collect();
+
+        Ok(results)
+    }
+
+    /// Reset setting to default
+    pub async fn reset_to_default(&self, key: &str, user_id: Option<&str>) -> Result<(), String> {
+        // Use key as-is (camelCase format)
+
+        // Get default value from AppFullSettings
+        let defaults = AppFullSettings::default();
+        let default_value = self.extract_default_value(&defaults, key)?;
+
+        self.set_setting(key, default_value, user_id).await
+    }
+
+    /// Register change listener for WebSocket broadcasts
+    pub async fn register_change_listener<F>(&self, listener: F)
+    where
+        F: Fn(&str, &SettingValue, Option<&str>) + Send + Sync + 'static,
+    {
+        let mut listeners = self.change_listeners.write().await;
+        listeners.push(Arc::new(listener));
+    }
+
+    /// Notify all listeners of a setting change
+    async fn notify_change(&self, key: &str, value: &SettingValue, user_id: Option<&str>) {
+        let listeners = self.change_listeners.read().await;
+        for listener in listeners.iter() {
+            listener(key, value, user_id);
+        }
+    }
+
+    /// Extract default value from AppFullSettings
+    fn extract_default_value(
+        &self,
+        defaults: &AppFullSettings,
+        key: &str,
+    ) -> Result<SettingValue, String> {
+        // Convert settings to JSON and extract the value
+        let json = serde_json::to_value(defaults)
+            .map_err(|e| format!("Failed to serialize defaults: {}", e))?;
+
+        let parts: Vec<&str> = key.split('.').collect();
+        let mut current = &json;
+
+        for part in parts {
+            match current.get(part) {
+                Some(v) => current = v,
+                None => {
+                    return Err(format!("Key not found in defaults: {}", key));
+                }
+            }
+        }
+
+        // Convert JSON value to SettingValue
+        match current {
+            JsonValue::String(s) => Ok(SettingValue::String(s.clone())),
+            JsonValue::Number(n) => {
+                if let Some(i) = n.as_i64() {
+                    Ok(SettingValue::Integer(i))
+                } else if let Some(f) = n.as_f64() {
+                    Ok(SettingValue::Float(f))
+                } else {
+                    Err("Invalid number type".to_string())
+                }
+            }
+            JsonValue::Bool(b) => Ok(SettingValue::Boolean(*b)),
+            JsonValue::Object(_) | JsonValue::Array(_) => {
+                Ok(SettingValue::Json(current.clone()))
+            }
+            JsonValue::Null => Err("Cannot reset to null value".to_string()),
+        }
+    }
+
+    /// Clear cache
+    pub async fn clear_cache(&self) {
+        let mut cache = self.cache.write().await;
+        cache.settings.clear();
+        cache.last_updated = std::time::Instant::now();
+        info!("Settings cache cleared");
+    }
+
+    /// Warm cache with common settings
+    pub async fn warm_cache(&self) {
+        let common_keys = vec![
+            "visualisation.graphs.logseq.physics",
+            "visualisation.rendering",
+            "system",
+        ];
+
+        for key in common_keys {
+            if let Err(e) = self.get_setting(key).await {
+                warn!("Failed to warm cache for {}: {}", key, e);
+            }
+        }
+
+        info!("Settings cache warmed");
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[tokio::test]
+    async fn test_camel_case_keys() {
+        // Verify that settings service uses camelCase keys directly
+        let db = Arc::new(DatabaseService::new(":memory:").unwrap());
+        db.initialize_schema().unwrap();
+
+        let service = SettingsService::new(db).unwrap();
+
+        // Set a camelCase setting
+        service.set_setting(
+            "testSetting",
+            SettingValue::String("test_value".to_string()),
+            None
+        ).await.unwrap();
+
+        // Should retrieve with same camelCase key
+        let value = service.get_setting("testSetting").await.unwrap();
+        assert!(value.is_some());
+    }
+}
diff --git a/src/services/settings_validator.rs b/src/services/settings_validator.rs
new file mode 100644
index 00000000..b93d1826
--- /dev/null
+++ b/src/services/settings_validator.rs
@@ -0,0 +1,590 @@
+// Settings Validation Layer
+// Schema-based validation for all settings categories
+
+use std::collections::HashMap;
+use serde_json::Value as JsonValue;
+
+use crate::services::database_service::SettingValue;
+use crate::config::PhysicsSettings;
+
+#[derive(Debug, Clone)]
+pub struct ValidationResult {
+    pub is_valid: bool,
+    pub errors: Vec<String>,
+    pub warnings: Vec<String>,
+}
+
+impl ValidationResult {
+    pub fn valid() -> Self {
+        Self {
+            is_valid: true,
+            errors: Vec::new(),
+            warnings: Vec::new(),
+        }
+    }
+
+    pub fn invalid(error: String) -> Self {
+        Self {
+            is_valid: false,
+            errors: vec![error],
+            warnings: Vec::new(),
+        }
+    }
+
+    pub fn with_warning(mut self, warning: String) -> Self {
+        self.warnings.push(warning);
+        self
+    }
+}
+
+pub struct SettingsValidator {
+    rules: HashMap<String, ValidationRule>,
+}
+
+#[derive(Clone)]
+struct ValidationRule {
+    value_type: ValueType,
+    min: Option<f64>,
+    max: Option<f64>,
+    allowed_values: Option<Vec<String>>,
+    pattern: Option<String>,
+    required: bool,
+}
+
+#[derive(Clone, PartialEq)]
+enum ValueType {
+    Float,
+    Integer,
+    Boolean,
+    String,
+    Object,
+    Array,
+}
+
+impl SettingsValidator {
+    pub fn new() -> Self {
+        let mut rules = HashMap::new();
+
+        // Visualization settings rules
+        Self::register_visualization_rules(&mut rules);
+
+        // Physics settings rules
+        Self::register_physics_rules(&mut rules);
+
+        // System settings rules
+        Self::register_system_rules(&mut rules);
+
+        // Ontology settings rules
+        Self::register_ontology_rules(&mut rules);
+
+        Self { rules }
+    }
+
+    fn register_visualization_rules(rules: &mut HashMap<String, ValidationRule>) {
+        // Rendering settings
+        rules.insert(
+            "visualisation.rendering.ambient_light_intensity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(2.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.rendering.environment_intensity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(2.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.rendering.enable_shadows".to_string(),
+            ValidationRule {
+                value_type: ValueType::Boolean,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        // Glow settings
+        rules.insert(
+            "visualisation.glow.opacity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.glow.intensity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(5.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        // Node settings
+        rules.insert(
+            "visualisation.graphs.logseq.nodes.base_size".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.1),
+                max: Some(100.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+    }
+
+    fn register_physics_rules(rules: &mut HashMap<String, ValidationRule>) {
+        rules.insert(
+            "visualisation.graphs.logseq.physics.damping".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.dt".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.001),
+                max: Some(0.1),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.iterations".to_string(),
+            ValidationRule {
+                value_type: ValueType::Integer,
+                min: Some(1.0),
+                max: Some(10000.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.max_velocity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.1),
+                max: Some(100.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.max_force".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.1),
+                max: Some(1000.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.repel_k".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1000.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.spring_k".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(10.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.gravity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.temperature".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.bounds_size".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(100.0),
+                max: Some(10000.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.enabled".to_string(),
+            ValidationRule {
+                value_type: ValueType::Boolean,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+    }
+
+    fn register_system_rules(rules: &mut HashMap<String, ValidationRule>) {
+        rules.insert(
+            "system.port".to_string(),
+            ValidationRule {
+                value_type: ValueType::Integer,
+                min: Some(1024.0),
+                max: Some(65535.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "system.host".to_string(),
+            ValidationRule {
+                value_type: ValueType::String,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "system.persist_settings".to_string(),
+            ValidationRule {
+                value_type: ValueType::Boolean,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+    }
+
+    fn register_ontology_rules(rules: &mut HashMap<String, ValidationRule>) {
+        rules.insert(
+            "ontology.reasoner.engine".to_string(),
+            ValidationRule {
+                value_type: ValueType::String,
+                min: None,
+                max: None,
+                allowed_values: Some(vec![
+                    "horned-owl".to_string(),
+                    "whelk".to_string(),
+                    "custom".to_string(),
+                ]),
+                pattern: None,
+                required: false,
+            },
+        );
+
+        rules.insert(
+            "ontology.gpu.enabled".to_string(),
+            ValidationRule {
+                value_type: ValueType::Boolean,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: false,
+            },
+        );
+
+        rules.insert(
+            "ontology.validation.mode".to_string(),
+            ValidationRule {
+                value_type: ValueType::String,
+                min: None,
+                max: None,
+                allowed_values: Some(vec![
+                    "quick".to_string(),
+                    "full".to_string(),
+                    "incremental".to_string(),
+                ]),
+                pattern: None,
+                required: false,
+            },
+        );
+    }
+
+    /// Validate a single setting
+    pub fn validate_setting(&self, key: &str, value: &SettingValue) -> Result<ValidationResult, String> {
+        // Find matching rule
+        let rule = match self.rules.get(key) {
+            Some(r) => r,
+            None => {
+                // No explicit rule, use basic type checking
+                return Ok(ValidationResult::valid().with_warning(format!(
+                    "No validation rule found for key: {}",
+                    key
+                )));
+            }
+        };
+
+        // Type validation
+        if !self.validate_type(value, &rule.value_type) {
+            return Ok(ValidationResult::invalid(format!(
+                "Type mismatch for {}: expected {:?}, got {:?}",
+                key, rule.value_type, value
+            )));
+        }
+
+        // Range validation for numeric types
+        if let Some(min) = rule.min {
+            if let Some(num) = self.extract_number(value) {
+                if num < min {
+                    return Ok(ValidationResult::invalid(format!(
+                        "Value {} is below minimum {} for {}",
+                        num, min, key
+                    )));
+                }
+            }
+        }
+
+        if let Some(max) = rule.max {
+            if let Some(num) = self.extract_number(value) {
+                if num > max {
+                    return Ok(ValidationResult::invalid(format!(
+                        "Value {} is above maximum {} for {}",
+                        num, max, key
+                    )));
+                }
+            }
+        }
+
+        // Allowed values validation
+        if let Some(allowed) = &rule.allowed_values {
+            if let SettingValue::String(s) = value {
+                if !allowed.contains(s) {
+                    return Ok(ValidationResult::invalid(format!(
+                        "Value '{}' is not in allowed values for {}: {:?}",
+                        s, key, allowed
+                    )));
+                }
+            }
+        }
+
+        Ok(ValidationResult::valid())
+    }
+
+    /// Validate physics settings
+    pub fn validate_physics_settings(&self, settings: &PhysicsSettings) -> Result<ValidationResult, String> {
+        let mut result = ValidationResult::valid();
+
+        // Validate damping
+        if settings.damping < 0.0 || settings.damping > 1.0 {
+            result.is_valid = false;
+            result
+                .errors
+                .push(format!("Damping must be between 0.0 and 1.0, got {}", settings.damping));
+        }
+
+        // Validate dt
+        if settings.dt < 0.001 || settings.dt > 0.1 {
+            result.is_valid = false;
+            result
+                .errors
+                .push(format!("dt must be between 0.001 and 0.1, got {}", settings.dt));
+        }
+
+        // Validate iterations
+        if settings.iterations < 1 || settings.iterations > 10000 {
+            result.is_valid = false;
+            result.errors.push(format!(
+                "Iterations must be between 1 and 10000, got {}",
+                settings.iterations
+            ));
+        }
+
+        // Validate max_velocity
+        if settings.max_velocity <= 0.0 {
+            result.is_valid = false;
+            result.errors.push(format!(
+                "Max velocity must be positive, got {}",
+                settings.max_velocity
+            ));
+        }
+
+        // Validate repel_k and spring_k (must be non-negative)
+        if settings.repel_k < 0.0 {
+            result.is_valid = false;
+            result
+                .errors
+                .push(format!("Repel_k must be non-negative, got {}", settings.repel_k));
+        }
+
+        if settings.spring_k < 0.0 {
+            result.is_valid = false;
+            result.errors.push(format!(
+                "Spring_k must be non-negative, got {}",
+                settings.spring_k
+            ));
+        }
+
+        // Cross-field validation: temperature and cooling_rate
+        if settings.temperature > 0.5 && settings.cooling_rate < 0.0001 {
+            result.warnings.push(
+                "High temperature with low cooling rate may cause instability".to_string(),
+            );
+        }
+
+        // Constraint validation
+        if settings.constraint_max_force_per_node <= 0.0 {
+            result.is_valid = false;
+            result.errors.push(format!(
+                "Constraint max force must be positive, got {}",
+                settings.constraint_max_force_per_node
+            ));
+        }
+
+        Ok(result)
+    }
+
+    fn validate_type(&self, value: &SettingValue, expected: &ValueType) -> bool {
+        match (value, expected) {
+            (SettingValue::Float(_), ValueType::Float) => true,
+            (SettingValue::Integer(_), ValueType::Integer) => true,
+            (SettingValue::Boolean(_), ValueType::Boolean) => true,
+            (SettingValue::String(_), ValueType::String) => true,
+            (SettingValue::Json(v), ValueType::Object) => v.is_object(),
+            (SettingValue::Json(v), ValueType::Array) => v.is_array(),
+            _ => false,
+        }
+    }
+
+    fn extract_number(&self, value: &SettingValue) -> Option<f64> {
+        match value {
+            SettingValue::Float(f) => Some(*f),
+            SettingValue::Integer(i) => Some(*i as f64),
+            _ => None,
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_validate_physics_damping() {
+        let validator = SettingsValidator::new();
+
+        // Valid damping
+        let result = validator
+            .validate_setting(
+                "visualisation.graphs.logseq.physics.damping",
+                &SettingValue::Float(0.95),
+            )
+            .unwrap();
+        assert!(result.is_valid);
+
+        // Invalid damping (too high)
+        let result = validator
+            .validate_setting(
+                "visualisation.graphs.logseq.physics.damping",
+                &SettingValue::Float(1.5),
+            )
+            .unwrap();
+        assert!(!result.is_valid);
+
+        // Invalid damping (negative)
+        let result = validator
+            .validate_setting(
+                "visualisation.graphs.logseq.physics.damping",
+                &SettingValue::Float(-0.1),
+            )
+            .unwrap();
+        assert!(!result.is_valid);
+    }
+
+    #[test]
+    fn test_validate_port() {
+        let validator = SettingsValidator::new();
+
+        // Valid port
+        let result = validator
+            .validate_setting("system.port", &SettingValue::Integer(8080))
+            .unwrap();
+        assert!(result.is_valid);
+
+        // Invalid port (too low)
+        let result = validator
+            .validate_setting("system.port", &SettingValue::Integer(80))
+            .unwrap();
+        assert!(!result.is_valid);
+
+        // Invalid port (too high)
+        let result = validator
+            .validate_setting("system.port", &SettingValue::Integer(70000))
+            .unwrap();
+        assert!(!result.is_valid);
+    }
+}
diff --git a/src/services/user_service.rs b/src/services/user_service.rs
new file mode 100644
index 00000000..364ed57d
--- /dev/null
+++ b/src/services/user_service.rs
@@ -0,0 +1,471 @@
+use rusqlite::{params, Connection, Result as SqliteResult};
+use serde::{Deserialize, Serialize};
+use serde_json::Value as JsonValue;
+use std::sync::Arc;
+use tokio::sync::RwLock;
+use chrono::Utc;
+use thiserror::Error;
+use log::{debug, error, info, warn};
+
+#[derive(Debug, Error)]
+pub enum UserServiceError {
+    #[error("Database error: {0}")]
+    DatabaseError(String),
+    #[error("User not found")]
+    UserNotFound,
+    #[error("Invalid setting value")]
+    InvalidSettingValue,
+    #[error("Permission denied")]
+    PermissionDenied,
+    #[error("User already exists")]
+    UserAlreadyExists,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct User {
+    pub id: i64,
+    pub nostr_pubkey: String,
+    pub username: Option<String>,
+    pub is_power_user: bool,
+    pub created_at: i64,
+    pub last_seen: i64,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(tag = "type", content = "value")]
+pub enum SettingValue {
+    String(String),
+    Integer(i64),
+    Float(f64),
+    Boolean(bool),
+    Json(JsonValue),
+}
+
+impl SettingValue {
+    fn value_type(&self) -> &'static str {
+        match self {
+            SettingValue::String(_) => "string",
+            SettingValue::Integer(_) => "integer",
+            SettingValue::Float(_) => "float",
+            SettingValue::Boolean(_) => "boolean",
+            SettingValue::Json(_) => "json",
+        }
+    }
+
+    fn to_json(&self) -> String {
+        match self {
+            SettingValue::String(s) => s.clone(),
+            SettingValue::Integer(i) => i.to_string(),
+            SettingValue::Float(f) => f.to_string(),
+            SettingValue::Boolean(b) => b.to_string(),
+            SettingValue::Json(v) => serde_json::to_string(v).unwrap_or_default(),
+        }
+    }
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct UserSetting {
+    pub id: i64,
+    pub user_id: i64,
+    pub key: String,
+    pub value: SettingValue,
+    pub created_at: i64,
+    pub updated_at: i64,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct AuditLogEntry {
+    pub id: i64,
+    pub user_id: Option<i64>,
+    pub key: String,
+    pub old_value: Option<String>,
+    pub new_value: Option<String>,
+    pub action: String,
+    pub timestamp: i64,
+}
+
+pub struct UserService {
+    db_path: String,
+    conn: Arc<RwLock<Connection>>,
+}
+
+impl UserService {
+    pub async fn new(db_path: String) -> Result<Self, UserServiceError> {
+        let conn = Connection::open(&db_path)
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        Ok(Self {
+            db_path,
+            conn: Arc::new(RwLock::new(conn)),
+        })
+    }
+
+    pub async fn get_user_by_nostr_pubkey(&self, pubkey: &str) -> Result<User, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare("SELECT id, nostr_pubkey, username, is_power_user, strftime('%s', created_at) as created_at, strftime('%s', last_seen) as last_seen FROM users WHERE nostr_pubkey = ?1")
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let user = stmt
+            .query_row(params![pubkey], |row| {
+                Ok(User {
+                    id: row.get(0)?,
+                    nostr_pubkey: row.get(1)?,
+                    username: row.get(2)?,
+                    is_power_user: row.get::<_, i64>(3)? == 1,
+                    created_at: row.get::<_, String>(4)?.parse().unwrap_or(0),
+                    last_seen: row.get::<_, String>(5)?.parse().unwrap_or(0),
+                })
+            })
+            .map_err(|_| UserServiceError::UserNotFound)?;
+
+        Ok(user)
+    }
+
+    pub async fn get_user_by_id(&self, user_id: i64) -> Result<User, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare("SELECT id, nostr_pubkey, username, is_power_user, strftime('%s', created_at) as created_at, strftime('%s', last_seen) as last_seen FROM users WHERE id = ?1")
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let user = stmt
+            .query_row(params![user_id], |row| {
+                Ok(User {
+                    id: row.get(0)?,
+                    nostr_pubkey: row.get(1)?,
+                    username: row.get(2)?,
+                    is_power_user: row.get::<_, i64>(3)? == 1,
+                    created_at: row.get::<_, String>(4)?.parse().unwrap_or(0),
+                    last_seen: row.get::<_, String>(5)?.parse().unwrap_or(0),
+                })
+            })
+            .map_err(|_| UserServiceError::UserNotFound)?;
+
+        Ok(user)
+    }
+
+    pub async fn create_or_update_user(
+        &self,
+        pubkey: &str,
+        username: Option<String>,
+    ) -> Result<User, UserServiceError> {
+        let mut conn = self.conn.write().await;
+        let now = Utc::now().timestamp();
+
+        let tx = conn.transaction()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let result = tx.execute(
+            "INSERT INTO users (nostr_pubkey, username, created_at, last_seen)
+             VALUES (?1, ?2, datetime('now'), datetime('now'))
+             ON CONFLICT(nostr_pubkey) DO UPDATE SET
+             username = COALESCE(?2, username),
+             last_seen = datetime('now')",
+            params![pubkey, username],
+        );
+
+        match result {
+            Ok(_) => {
+                let user: User = tx.query_row(
+                    "SELECT id, nostr_pubkey, username, is_power_user, strftime('%s', created_at) as created_at, strftime('%s', last_seen) as last_seen FROM users WHERE nostr_pubkey = ?1",
+                    params![pubkey],
+                    |row| {
+                        Ok(User {
+                            id: row.get(0)?,
+                            nostr_pubkey: row.get(1)?,
+                            username: row.get(2)?,
+                            is_power_user: row.get::<_, i64>(3)? == 1,
+                            created_at: row.get::<_, String>(4)?.parse().unwrap_or(0),
+                            last_seen: row.get::<_, String>(5)?.parse().unwrap_or(0),
+                        })
+                    }
+                ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+                tx.commit().map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+                info!("Created/updated user: pubkey={}, id={}", pubkey, user.id);
+                Ok(user)
+            }
+            Err(e) => {
+                error!("Failed to create/update user {}: {}", pubkey, e);
+                Err(UserServiceError::DatabaseError(e.to_string()))
+            }
+        }
+    }
+
+    pub async fn is_power_user(&self, user_id: i64) -> Result<bool, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare("SELECT is_power_user FROM users WHERE id = ?1")
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let is_power = stmt
+            .query_row(params![user_id], |row| {
+                let val: i64 = row.get(0)?;
+                Ok(val == 1)
+            })
+            .map_err(|_| UserServiceError::UserNotFound)?;
+
+        Ok(is_power)
+    }
+
+    pub async fn grant_power_user(&self, user_id: i64) -> Result<(), UserServiceError> {
+        let mut conn = self.conn.write().await;
+        conn.execute(
+            "UPDATE users SET is_power_user = 1 WHERE id = ?1",
+            params![user_id],
+        )
+        .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        info!("Granted power user to user_id={}", user_id);
+        Ok(())
+    }
+
+    pub async fn revoke_power_user(&self, user_id: i64) -> Result<(), UserServiceError> {
+        let mut conn = self.conn.write().await;
+        conn.execute(
+            "UPDATE users SET is_power_user = 0 WHERE id = ?1",
+            params![user_id],
+        )
+        .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        info!("Revoked power user from user_id={}", user_id);
+        Ok(())
+    }
+
+    pub async fn get_user_settings(&self, user_id: i64) -> Result<Vec<UserSetting>, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare(
+                "SELECT id, user_id, key, value_type, value_text, value_integer, value_float, value_boolean, value_json,
+                 strftime('%s', created_at) as created_at, strftime('%s', updated_at) as updated_at
+                 FROM user_settings WHERE user_id = ?1"
+            )
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let settings = stmt
+            .query_map(params![user_id], |row| {
+                let value_type: String = row.get(3)?;
+                let value = match value_type.as_str() {
+                    "string" => SettingValue::String(row.get(4)?),
+                    "integer" => SettingValue::Integer(row.get(5)?),
+                    "float" => SettingValue::Float(row.get(6)?),
+                    "boolean" => SettingValue::Boolean(row.get::<_, i64>(7)? == 1),
+                    "json" => {
+                        let json_str: String = row.get(8)?;
+                        let json_val: JsonValue = serde_json::from_str(&json_str).unwrap_or(JsonValue::Null);
+                        SettingValue::Json(json_val)
+                    }
+                    _ => SettingValue::String(String::new()),
+                };
+
+                Ok(UserSetting {
+                    id: row.get(0)?,
+                    user_id: row.get(1)?,
+                    key: row.get(2)?,
+                    value,
+                    created_at: row.get::<_, String>(9)?.parse().unwrap_or(0),
+                    updated_at: row.get::<_, String>(10)?.parse().unwrap_or(0),
+                })
+            })
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?
+            .collect::<Result<Vec<_>, _>>()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        Ok(settings)
+    }
+
+    pub async fn set_user_setting(
+        &self,
+        user_id: i64,
+        key: &str,
+        value: SettingValue,
+    ) -> Result<(), UserServiceError> {
+        let mut conn = self.conn.write().await;
+        let tx = conn.transaction()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let old_value: Option<String> = tx
+            .query_row(
+                "SELECT value_text, value_integer, value_float, value_boolean, value_json, value_type FROM user_settings WHERE user_id = ?1 AND key = ?2",
+                params![user_id, key],
+                |row| {
+                    let value_type: String = row.get(5)?;
+                    let val = match value_type.as_str() {
+                        "string" => row.get::<_, String>(0).ok(),
+                        "integer" => row.get::<_, i64>(1).ok().map(|v| v.to_string()),
+                        "float" => row.get::<_, f64>(2).ok().map(|v| v.to_string()),
+                        "boolean" => row.get::<_, i64>(3).ok().map(|v| (v == 1).to_string()),
+                        "json" => row.get::<_, String>(4).ok(),
+                        _ => None,
+                    };
+                    Ok(val)
+                }
+            )
+            .ok()
+            .flatten();
+
+        let (value_text, value_integer, value_float, value_boolean, value_json) = match &value {
+            SettingValue::String(s) => (Some(s.clone()), None, None, None, None),
+            SettingValue::Integer(i) => (None, Some(*i), None, None, None),
+            SettingValue::Float(f) => (None, None, Some(*f), None, None),
+            SettingValue::Boolean(b) => (None, None, None, Some(if *b { 1 } else { 0 }), None),
+            SettingValue::Json(j) => (None, None, None, None, Some(serde_json::to_string(j).unwrap_or_default())),
+        };
+
+        let action = if old_value.is_some() { "update" } else { "create" };
+
+        tx.execute(
+            "INSERT INTO user_settings (user_id, key, value_type, value_text, value_integer, value_float, value_boolean, value_json, created_at, updated_at)
+             VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, datetime('now'), datetime('now'))
+             ON CONFLICT(user_id, key) DO UPDATE SET
+             value_type = ?3,
+             value_text = ?4,
+             value_integer = ?5,
+             value_float = ?6,
+             value_boolean = ?7,
+             value_json = ?8,
+             updated_at = datetime('now')",
+            params![user_id, key, value.value_type(), value_text, value_integer, value_float, value_boolean, value_json],
+        ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        tx.execute(
+            "INSERT INTO settings_audit_log (user_id, key, old_value, new_value, action, timestamp)
+             VALUES (?1, ?2, ?3, ?4, ?5, datetime('now'))",
+            params![user_id, key, old_value, value.to_json(), action],
+        ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        tx.commit().map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        info!("Set user setting: user_id={}, key={}, action={}", user_id, key, action);
+        Ok(())
+    }
+
+    pub async fn delete_user_setting(&self, user_id: i64, key: &str) -> Result<(), UserServiceError> {
+        let mut conn = self.conn.write().await;
+        let tx = conn.transaction()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let old_value: Option<String> = tx
+            .query_row(
+                "SELECT value_text, value_integer, value_float, value_boolean, value_json, value_type FROM user_settings WHERE user_id = ?1 AND key = ?2",
+                params![user_id, key],
+                |row| {
+                    let value_type: String = row.get(5)?;
+                    let val = match value_type.as_str() {
+                        "string" => row.get::<_, String>(0).ok(),
+                        "integer" => row.get::<_, i64>(1).ok().map(|v| v.to_string()),
+                        "float" => row.get::<_, f64>(2).ok().map(|v| v.to_string()),
+                        "boolean" => row.get::<_, i64>(3).ok().map(|v| (v == 1).to_string()),
+                        "json" => row.get::<_, String>(4).ok(),
+                        _ => None,
+                    };
+                    Ok(val)
+                }
+            )
+            .ok()
+            .flatten();
+
+        tx.execute(
+            "DELETE FROM user_settings WHERE user_id = ?1 AND key = ?2",
+            params![user_id, key],
+        ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        tx.execute(
+            "INSERT INTO settings_audit_log (user_id, key, old_value, new_value, action, timestamp)
+             VALUES (?1, ?2, ?3, NULL, 'delete', datetime('now'))",
+            params![user_id, key, old_value],
+        ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        tx.commit().map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        info!("Deleted user setting: user_id={}, key={}", user_id, key);
+        Ok(())
+    }
+
+    pub async fn get_audit_log(
+        &self,
+        key: Option<String>,
+        user_id: Option<i64>,
+        limit: i64,
+    ) -> Result<Vec<AuditLogEntry>, UserServiceError> {
+        let conn = self.conn.read().await;
+
+        let query = match (key.as_ref(), user_id) {
+            (Some(_), Some(_)) => {
+                "SELECT id, user_id, key, old_value, new_value, action, strftime('%s', timestamp) as timestamp
+                 FROM settings_audit_log WHERE key = ?1 AND user_id = ?2 ORDER BY timestamp DESC LIMIT ?3"
+            }
+            (Some(_), None) => {
+                "SELECT id, user_id, key, old_value, new_value, action, strftime('%s', timestamp) as timestamp
+                 FROM settings_audit_log WHERE key = ?1 ORDER BY timestamp DESC LIMIT ?2"
+            }
+            (None, Some(_)) => {
+                "SELECT id, user_id, key, old_value, new_value, action, strftime('%s', timestamp) as timestamp
+                 FROM settings_audit_log WHERE user_id = ?1 ORDER BY timestamp DESC LIMIT ?2"
+            }
+            (None, None) => {
+                "SELECT id, user_id, key, old_value, new_value, action, strftime('%s', timestamp) as timestamp
+                 FROM settings_audit_log ORDER BY timestamp DESC LIMIT ?1"
+            }
+        };
+
+        let mut stmt = conn.prepare(query)
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let entries = match (key, user_id) {
+            (Some(k), Some(u)) => {
+                stmt.query_map(params![k, u, limit], Self::map_audit_row)
+            }
+            (Some(k), None) => {
+                stmt.query_map(params![k, limit], Self::map_audit_row)
+            }
+            (None, Some(u)) => {
+                stmt.query_map(params![u, limit], Self::map_audit_row)
+            }
+            (None, None) => {
+                stmt.query_map(params![limit], Self::map_audit_row)
+            }
+        }
+        .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?
+        .collect::<Result<Vec<_>, _>>()
+        .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        Ok(entries)
+    }
+
+    fn map_audit_row(row: &rusqlite::Row) -> rusqlite::Result<AuditLogEntry> {
+        Ok(AuditLogEntry {
+            id: row.get(0)?,
+            user_id: row.get(1)?,
+            key: row.get(2)?,
+            old_value: row.get(3)?,
+            new_value: row.get(4)?,
+            action: row.get(5)?,
+            timestamp: row.get::<_, String>(6)?.parse().unwrap_or(0),
+        })
+    }
+
+    pub async fn list_all_users(&self) -> Result<Vec<User>, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare("SELECT id, nostr_pubkey, username, is_power_user, strftime('%s', created_at) as created_at, strftime('%s', last_seen) as last_seen FROM users ORDER BY created_at DESC")
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let users = stmt
+            .query_map([], |row| {
+                Ok(User {
+                    id: row.get(0)?,
+                    nostr_pubkey: row.get(1)?,
+                    username: row.get(2)?,
+                    is_power_user: row.get::<_, i64>(3)? == 1,
+                    created_at: row.get::<_, String>(4)?.parse().unwrap_or(0),
+                    last_seen: row.get::<_, String>(5)?.parse().unwrap_or(0),
+                })
+            })
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?
+            .collect::<Result<Vec<_>, _>>()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        Ok(users)
+    }
+}
diff --git a/src/utils/binary_protocol.rs b/src/utils/binary_protocol.rs
index b58ac71d..627e0667 100644
--- a/src/utils/binary_protocol.rs
+++ b/src/utils/binary_protocol.rs
@@ -1,7 +1,7 @@
 use crate::utils::socket_flow_messages::BinaryNodeData;
 use crate::types::vec3::Vec3Data;
 use crate::models::constraints::{Constraint, AdvancedParams};
-use log::{trace, debug};
+use log::{trace, debug, warn};
 use serde::{Serialize, Deserialize};
 use serde_json;

@@ -21,30 +21,11 @@ const ONTOLOGY_PROPERTY_FLAG: u32 = 0x10000000;   // Bit 28: OWL Property

 const NODE_ID_MASK: u32 = 0x3FFFFFFF;        // Mask to extract actual node ID (bits 0-29)

-// Node type flag constants for u16 (wire format v1 - DEPRECATED)
-// BUG: These constants truncate node IDs > 16383, causing collisions
-// FIXED: Use PROTOCOL_V2 with full u32 IDs for node_id > 16383
-const WIRE_V1_AGENT_FLAG: u16 = 0x8000;         // Bit 15 indicates agent node
-const WIRE_V1_KNOWLEDGE_FLAG: u16 = 0x4000;     // Bit 14 indicates knowledge graph node
-const WIRE_V1_NODE_ID_MASK: u16 = 0x3FFF;       // Mask to extract actual node ID (bits 0-13)
-
 // Node type flag constants for u32 (wire format v2)
 const WIRE_V2_AGENT_FLAG: u32 = 0x80000000;     // Bit 31 indicates agent node
 const WIRE_V2_KNOWLEDGE_FLAG: u32 = 0x40000000; // Bit 30 indicates knowledge graph node
 const WIRE_V2_NODE_ID_MASK: u32 = 0x3FFFFFFF;   // Mask to extract actual node ID (bits 0-29)

-/// Wire format v1 struct (LEGACY - 34 bytes)
-/// BUG: Truncates node IDs to 14 bits (max 16383), causing collisions
-/// DEPRECATED: Use WireNodeDataItemV2 for new implementations
-pub struct WireNodeDataItemV1 {
-    pub id: u16,                // 2 bytes - TRUNCATED to 14 bits + 2 flag bits
-    pub position: Vec3Data,     // 12 bytes
-    pub velocity: Vec3Data,     // 12 bytes
-    pub sssp_distance: f32,     // 4 bytes - SSSP distance from source
-    pub sssp_parent: i32,       // 4 bytes - Parent node for path reconstruction
-    // Total: 34 bytes
-}
-
 /// Wire format v2 struct (FIXED - 38 bytes)
 /// FIXES: Uses full 32-bit node IDs (30 bits + 2 flag bits)
 /// Supports node IDs up to 1,073,741,823 (2^30 - 1)
@@ -61,12 +42,11 @@ pub struct WireNodeDataItemV2 {
 pub type WireNodeDataItem = WireNodeDataItemV2;

 // Constants for wire format sizes
-const WIRE_V1_ID_SIZE: usize = 2;  // u16 (LEGACY)
 const WIRE_V2_ID_SIZE: usize = 4;  // u32 (FIXED)
 const WIRE_VEC3_SIZE: usize = 12;  // 3 * f32
 const WIRE_F32_SIZE: usize = 4;    // f32
 const WIRE_I32_SIZE: usize = 4;    // i32
-const WIRE_V1_ITEM_SIZE: usize = WIRE_V1_ID_SIZE + WIRE_VEC3_SIZE + WIRE_VEC3_SIZE + WIRE_F32_SIZE + WIRE_I32_SIZE; // 34 bytes (2+12+12+4+4)
+const WIRE_V1_ITEM_SIZE: usize = 34; // V1 legacy size (2+12+12+4+4) - kept for decoder only
 const WIRE_V2_ITEM_SIZE: usize = WIRE_V2_ID_SIZE + WIRE_VEC3_SIZE + WIRE_VEC3_SIZE + WIRE_F32_SIZE + WIRE_I32_SIZE; // 36 bytes (4+12+12+4+4) NOT 38!

 // Backwards compatibility alias - DEPRECATED
@@ -191,40 +171,6 @@ pub fn is_ontology_node(node_id: u32) -> bool {
     (node_id & ONTOLOGY_TYPE_MASK) != 0
 }

-/// Convert u32 node ID with flags to u16 wire format (V1 - LEGACY)
-/// BUG: Truncates node IDs to 14 bits! Use to_wire_id_v2 instead.
-/// DEPRECATED: Only kept for backwards compatibility with old clients
-#[deprecated(note = "Use to_wire_id_v2 for full 32-bit node ID support")]
-pub fn to_wire_id_v1(node_id: u32) -> u16 {
-    let actual_id = get_actual_node_id(node_id);
-    let wire_id = (actual_id & 0x3FFF) as u16; // BUG: Truncates to 14 bits!
-
-    // Preserve node type flags
-    if is_agent_node(node_id) {
-        wire_id | WIRE_V1_AGENT_FLAG
-    } else if is_knowledge_node(node_id) {
-        wire_id | WIRE_V1_KNOWLEDGE_FLAG
-    } else {
-        wire_id
-    }
-}
-
-/// Convert u16 wire ID back to u32 preserving flags (V1 - LEGACY)
-/// DEPRECATED: Only kept for backwards compatibility with old clients
-#[deprecated(note = "Use from_wire_id_v2 for full 32-bit node ID support")]
-pub fn from_wire_id_v1(wire_id: u16) -> u32 {
-    let actual_id = (wire_id & WIRE_V1_NODE_ID_MASK) as u32;
-
-    // Restore node type flags
-    if (wire_id & WIRE_V1_AGENT_FLAG) != 0 {
-        actual_id | AGENT_NODE_FLAG
-    } else if (wire_id & WIRE_V1_KNOWLEDGE_FLAG) != 0 {
-        actual_id | KNOWLEDGE_NODE_FLAG
-    } else {
-        actual_id
-    }
-}
-
 /// Convert u32 node ID with flags to u32 wire format (V2 - FIXED)
 /// FIXED: Preserves full 32-bit node ID without truncation
 pub fn to_wire_id_v2(node_id: u32) -> u32 {
@@ -291,8 +237,8 @@ pub fn encode_node_data_extended(
     ontology_individual_ids: &[u32],
     ontology_property_ids: &[u32]
 ) -> Vec<u8> {
-    // Always use V2 protocol to prevent truncation bugs (V1 is only for backwards compat)
-    let use_v2 = true; // Force V2 for all new encoding
+    // Always use V2 protocol to prevent truncation bugs
+    // V1 encoding removed - decoder kept for backward compatibility only
     let item_size = WIRE_V2_ITEM_SIZE;
     let protocol_version = PROTOCOL_V2;

@@ -317,6 +263,24 @@ pub fn encode_node_data_extended(
     for (node_id, node) in nodes {
         // Check node type and set the appropriate flag
         // Priority: Agent > Knowledge > Ontology types > None
+
+        // Validate exclusive type assignment in debug mode
+        #[cfg(debug_assertions)]
+        {
+            let type_count = [
+                agent_node_ids.contains(node_id),
+                knowledge_node_ids.contains(node_id),
+                ontology_class_ids.contains(node_id),
+                ontology_individual_ids.contains(node_id),
+                ontology_property_ids.contains(node_id),
+            ].iter().filter(|&&x| x).count();
+
+            if type_count > 1 {
+                warn!("Node {} appears in {} type lists - using priority order (Agent > Knowledge > Ontology)",
+                      node_id, type_count);
+            }
+        }
+
         let flagged_id = if agent_node_ids.contains(node_id) {
             set_agent_flag(*node_id)
         } else if knowledge_node_ids.contains(node_id) {
@@ -340,16 +304,10 @@ pub fn encode_node_data_extended(
                 agent_node_ids.contains(node_id));
         }

-        if use_v2 {
-            // V2: Write u32 ID (4 bytes) - NO TRUNCATION
-            let wire_id = to_wire_id_v2(flagged_id);
-            buffer.extend_from_slice(&wire_id.to_le_bytes());
-        } else {
-            // V1: Write u16 ID (2 bytes) - TRUNCATES (legacy support only)
-            #[allow(deprecated)]
-            let wire_id = to_wire_id_v1(flagged_id);
-            buffer.extend_from_slice(&wire_id.to_le_bytes());
-        }
+        // V2: Write u32 ID (4 bytes) - NO TRUNCATION
+        // Note: use_v2 is always true (line 295), V1 encoding removed as dead code
+        let wire_id = to_wire_id_v2(flagged_id);
+        buffer.extend_from_slice(&wire_id.to_le_bytes());

         // Write position (12 bytes = 3 * f32)
         buffer.extend_from_slice(&node.x.to_le_bytes());
@@ -456,9 +414,16 @@ fn decode_node_data_v1(data: &[u8]) -> Result<Vec<(u32, BinaryNodeData)>, String
         cursor += 4;
         let _sssp_parent = i32::from_le_bytes([chunk[cursor], chunk[cursor + 1], chunk[cursor + 2], chunk[cursor + 3]]);

-        // Convert V1 wire ID back to u32 with flags
-        #[allow(deprecated)]
-        let full_node_id = from_wire_id_v1(wire_id);
+        // Convert V1 wire ID back to u32 with flags (inlined for backward compatibility)
+        // V1 constants: AGENT_FLAG=0x8000, KNOWLEDGE_FLAG=0x4000, ID_MASK=0x3FFF
+        let actual_id = (wire_id & 0x3FFF) as u32;
+        let full_node_id = if (wire_id & 0x8000) != 0 {
+            actual_id | AGENT_NODE_FLAG
+        } else if (wire_id & 0x4000) != 0 {
+            actual_id | KNOWLEDGE_NODE_FLAG
+        } else {
+            actual_id
+        };

         if samples_logged < max_samples {
             let is_agent = is_agent_node(full_node_id);
@@ -883,23 +848,15 @@ mod tests {
     }

     #[test]
-    fn test_v1_backwards_compatibility() {
-        // Test that small node IDs can use V1 if needed
-        let small_nodes = vec![
-            (100u32, BinaryNodeData {
-                node_id: 100,
-                x: 1.0, y: 2.0, z: 3.0,
-                vx: 0.1, vy: 0.2, vz: 0.3,
-            }),
-        ];
-
-        // V1 not needed, but should decode correctly if received
+    fn test_v1_backwards_compatibility_decoder() {
+        // Test V1 decoder for backward compatibility with old clients
+        // V1 encoding removed, but decoder kept to support legacy data
         let mut v1_encoded = vec![PROTOCOL_V1];
-        #[allow(deprecated)]
-        {
-            let wire_id = to_wire_id_v1(100);
-            v1_encoded.extend_from_slice(&wire_id.to_le_bytes());
-        }
+
+        // Manually construct V1 wire format (u16 ID with flags)
+        let wire_id: u16 = 100; // Small node ID without flags
+        v1_encoded.extend_from_slice(&wire_id.to_le_bytes());
+
         // Add position
         v1_encoded.extend_from_slice(&1.0f32.to_le_bytes());
         v1_encoded.extend_from_slice(&2.0f32.to_le_bytes());
