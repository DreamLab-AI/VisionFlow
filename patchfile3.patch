diff --git a/src/actors/agent_monitor_actor.rs b/src/actors/agent_monitor_actor.rs
index 18efd48a..a52039c3 100644
--- a/src/actors/agent_monitor_actor.rs
+++ b/src/actors/agent_monitor_actor.rs
@@ -3,7 +3,7 @@
 //! This actor focuses solely on:
 //! - Polling the Management API (port 9090) for active task statuses
 //! - Converting tasks to agent nodes
-//! - Forwarding updates to GraphServiceSupervisor
+//! - Forwarding updates to GraphStateActor
 //!
 //! All task management is handled by TaskOrchestratorActor.
 //! This actor only monitors and displays running agents.
@@ -17,11 +17,13 @@ use chrono::{Utc, DateTime};
 use crate::types::claude_flow::{ClaudeFlowClient, AgentStatus, AgentProfile, AgentType, PerformanceMetrics, TokenUsage};
 use crate::actors::messages::*;
 use crate::services::management_api_client::{ManagementApiClient, TaskInfo};
+use crate::actors::graph_state_actor::{GraphStateActor, AddNodes};
+use crate::models::node::Node;
+use crate::utils::socket_flow_messages::BinaryNodeData;

 /// Convert Management API TaskInfo to AgentStatus for graph visualization
 fn task_to_agent_status(task: TaskInfo) -> AgentStatus {
     use chrono::TimeZone;
-    use glam::Vec3;

     // Map agent type string to AgentType enum
     let agent_type_enum = match task.agent.as_str() {
@@ -89,10 +91,52 @@ fn task_to_agent_status(task: TaskInfo) -> AgentStatus {
         workload: Some(0.5),
     }
 }
+
+/// Convert AgentStatus to Node for GraphStateActor
+fn agent_status_to_node(status: &AgentStatus) -> Node {
+    // Use agent_id hash for consistent node ID
+    let node_id = {
+        use std::collections::hash_map::DefaultHasher;
+        use std::hash::{Hash, Hasher};
+        let mut hasher = DefaultHasher::new();
+        status.agent_id.hash(&mut hasher);
+        // Use high ID range (starting at 10000) to avoid conflicts with main graph
+        (hasher.finish() as u32 % 50000) + 10000
+    };
+
+    let mut node = Node::new(status.agent_id.clone());
+    node.id = node_id;
+
+    // Set position data
+    node.data = BinaryNodeData {
+        node_id,
+        x: 0.0,
+        y: 0.0,
+        z: 0.0,
+        vx: 0.0,
+        vy: 0.0,
+        vz: 0.0,
+    };
+
+    // Add metadata
+    node.label = status.profile.name.clone();
+    node.size = Some(5.0);
+    node.color = Some(match status.profile.agent_type {
+        AgentType::Coder => "#00FF00",
+        AgentType::Coordinator => "#0000FF",
+        AgentType::Researcher => "#FFFF00",
+        AgentType::Analyst => "#FF00FF",
+        AgentType::Tester => "#00FFFF",
+        _ => "#FFFFFF",
+    }.to_string());
+
+    node
+}
+
 /// AgentMonitorActor - Monitoring via Management API
 pub struct AgentMonitorActor {
     _client: ClaudeFlowClient,
-    graph_service_addr: Addr<crate::actors::graph_service_supervisor::TransitionalGraphSupervisor>,
+    graph_state_addr: Addr<GraphStateActor>,
     management_api_client: ManagementApiClient,

     /// Connection state
@@ -105,13 +149,16 @@ pub struct AgentMonitorActor {
     /// Agent cache (task_id -> AgentStatus)
     agent_cache: HashMap<String, AgentStatus>,

+    /// Node ID mapping (agent_id -> node_id)
+    node_id_map: HashMap<String, u32>,
+
     /// Error tracking
     consecutive_poll_failures: u32,
     last_successful_poll: Option<DateTime<Utc>>,
 }

 impl AgentMonitorActor {
-    pub fn new(client: ClaudeFlowClient, graph_service_addr: Addr<crate::actors::graph_service_supervisor::TransitionalGraphSupervisor>) -> Self {
+    pub fn new(client: ClaudeFlowClient, graph_state_addr: Addr<GraphStateActor>) -> Self {
         info!("[AgentMonitorActor] Initializing with Management API monitoring");

         // Create Management API client
@@ -128,47 +175,48 @@ impl AgentMonitorActor {

         Self {
             _client: client,
-            graph_service_addr,
+            graph_state_addr,
             management_api_client,
             is_connected: false,
             polling_interval: Duration::from_secs(3), // Poll every 3 seconds
             last_poll: Utc::now(),
             agent_cache: HashMap::new(),
+            node_id_map: HashMap::new(),
             consecutive_poll_failures: 0,
             last_successful_poll: None,
         }
     }

     /// Poll agent statuses from Management API
-fn poll_agent_statuses(&mut self, ctx: &mut Context<Self>) {
-    debug!("[AgentMonitorActor] Polling active tasks from Management API");
-
-    let api_client = self.management_api_client.clone();
-    let ctx_addr = ctx.address();
-
-    tokio::spawn(async move {
-        match api_client.list_tasks().await {
-            Ok(task_list) => {
-                let active_count = task_list.active_tasks.len();
-                debug!("[AgentMonitorActor] Retrieved {} active tasks from Management API", active_count);
-
-                // Convert tasks to agent statuses
-                let agents: Vec<AgentStatus> = task_list.active_tasks.into_iter().map(|task| {
-                    task_to_agent_status(task)
-                }).collect();
-
-                ctx_addr.do_send(ProcessAgentStatuses { agents });
-            }
-            Err(e) => {
-                error!("[AgentMonitorActor] Management API query failed: {}", e);
-                ctx_addr.do_send(RecordPollFailure);
+    fn poll_agent_statuses(&mut self, ctx: &mut Context<Self>) {
+        debug!("[AgentMonitorActor] Polling active tasks from Management API");
+
+        let api_client = self.management_api_client.clone();
+        let ctx_addr = ctx.address();
+
+        tokio::spawn(async move {
+            match api_client.list_tasks().await {
+                Ok(task_list) => {
+                    let active_count = task_list.active_tasks.len();
+                    debug!("[AgentMonitorActor] Retrieved {} active tasks from Management API", active_count);
+
+                    // Convert tasks to agent statuses
+                    let agents: Vec<AgentStatus> = task_list.active_tasks.into_iter().map(|task| {
+                        task_to_agent_status(task)
+                    }).collect();
+
+                    ctx_addr.do_send(ProcessAgentStatuses { agents });
+                }
+                Err(e) => {
+                    error!("[AgentMonitorActor] Management API query failed: {}", e);
+                    ctx_addr.do_send(RecordPollFailure);
+                }
             }
-        }
-    });
-}
+        });
+    }
 }

-/// Message to process agent statuses from MCP
+/// Message to process agent statuses from Management API
 #[derive(Message)]
 #[rtype(result = "()")]
 struct ProcessAgentStatuses {
@@ -179,7 +227,7 @@ impl Actor for AgentMonitorActor {
     type Context = Context<Self>;

     fn started(&mut self, ctx: &mut Self::Context) {
-        info!("[AgentMonitorActor] Started - beginning MCP TCP polling");
+        info!("[AgentMonitorActor] Started - beginning Management API polling");

         self.is_connected = true;

@@ -200,31 +248,35 @@ impl Handler<ProcessAgentStatuses> for AgentMonitorActor {
     type Result = ();

     fn handle(&mut self, msg: ProcessAgentStatuses, _ctx: &mut Self::Context) {
-        info!("[AgentMonitorActor] Processing {} agent statuses from MCP", msg.agents.len());
-
-        // Convert AgentStatus to Agent for UpdateBotsGraph
-        let agents: Vec<crate::services::bots_client::Agent> = msg.agents.iter().map(|status| {
-            crate::services::bots_client::Agent {
-                id: status.agent_id.clone(),
-                name: status.profile.name.clone(),
-                agent_type: format!("{:?}", status.profile.agent_type).to_lowercase(),
-                status: status.status.clone(),
-                x: 0.0,
-                y: 0.0,
-                z: 0.0,
-                cpu_usage: status.cpu_usage,
-                memory_usage: status.memory_usage,
-                health: status.health,
-                workload: status.activity,
-                created_at: Some(status.timestamp.to_rfc3339()),
-                age: Some((chrono::Utc::now().timestamp() - status.timestamp.timestamp()) as u64 * 1000),
-            }
+        info!("[AgentMonitorActor] Processing {} agent statuses", msg.agents.len());
+
+        // Convert AgentStatus to Node for GraphStateActor
+        let nodes: Vec<Node> = msg.agents.iter().map(|status| {
+            let node = agent_status_to_node(status);
+            // Store node ID mapping
+            self.node_id_map.insert(status.agent_id.clone(), node.id);
+            node
         }).collect();

-        // Send graph update
-        let message = UpdateBotsGraph { agents };
-        info!("[AgentMonitorActor] Sending graph update with {} agents", msg.agents.len());
-        self.graph_service_addr.do_send(message);
+        // Send to GraphStateActor
+        if !nodes.is_empty() {
+            info!("[AgentMonitorActor] Sending {} nodes to GraphStateActor", nodes.len());
+            let graph_state_addr = self.graph_state_addr.clone();
+
+            tokio::spawn(async move {
+                match graph_state_addr.send(AddNodes { nodes }).await {
+                    Ok(Ok(added_ids)) => {
+                        debug!("[AgentMonitorActor] Successfully added {} nodes to graph state", added_ids.len());
+                    }
+                    Ok(Err(e)) => {
+                        warn!("[AgentMonitorActor] Failed to add nodes: {}", e);
+                    }
+                    Err(e) => {
+                        error!("[AgentMonitorActor] Mailbox error sending to GraphStateActor: {}", e);
+                    }
+                }
+            });
+        }

         // Update cache
         if !msg.agents.is_empty() {
diff --git a/src/actors/graph_actor.rs b/src/actors/graph_actor.rs
index cb3f9a8b..aebc6470 100644
--- a/src/actors/graph_actor.rs
+++ b/src/actors/graph_actor.rs
@@ -594,24 +594,11 @@ impl GraphServiceActor {
         );
         let stress_solver = StressMajorizationSolver::from_advanced_params(&advanced_params);

-        // Initialize with logseq (knowledge graph) physics from settings
-        // This will be updated when settings are loaded, but provides better defaults
+        // Initialize with default physics parameters
+        // Settings are loaded from SQLite database and applied via UpdateSettings message
         let simulation_params = {
-            // Try to get settings from the global config
-            if let Ok(settings_yaml) = std::fs::read_to_string("/app/settings.yaml")
-                .or_else(|_| std::fs::read_to_string("data/settings.yaml")) {
-                if let Ok(settings) = serde_yaml::from_str::<crate::config::AppFullSettings>(&settings_yaml) {
-                    // Use logseq physics as the default for knowledge graph
-                    let physics = settings.get_physics("logseq");
-                    SimulationParams::from(physics)
-                } else {
-                    warn!("Failed to parse settings.yaml, using defaults");
-                    SimulationParams::default()
-                }
-            } else {
-                info!("Settings file not found at startup, using defaults (will be updated when settings load)");
-                SimulationParams::default()
-            }
+            info!("Initializing with default physics parameters (settings loaded from database)");
+            SimulationParams::default()
         };

         // Clone simulation_params for target_params before moving it
@@ -2095,24 +2082,36 @@ impl GraphServiceActor {
                 return; // Skip this broadcast
             }

-            // Create binary position data for BOTH knowledge graph AND agent graph
-            // Per dual-graph architecture: both types broadcast together with type flags
+            // Create binary position data for ALL three graphs: knowledge, agent, and ontology
+            // Per multi-graph architecture: all types broadcast together with type flags
             let mut position_data: Vec<(u32, BinaryNodeData)> = Vec::new();
             let mut knowledge_ids: Vec<u32> = Vec::new();
             let mut agent_ids: Vec<u32> = Vec::new();
+            let mut ontology_class_ids: Vec<u32> = Vec::new();
+            let mut ontology_individual_ids: Vec<u32> = Vec::new();
+            let mut ontology_property_ids: Vec<u32> = Vec::new();

-            // Collect knowledge graph nodes (from main graph)
+            // Collect nodes from main graph, classifying by node_type field
             for (node_id, node) in self.node_map.iter() {
                 position_data.push((*node_id, BinaryNodeDataClient::new(
                     *node_id,
                     node.data.position(),
                     node.data.velocity(),
                 )));
-                knowledge_ids.push(*node_id);
+
+                // Classify node by its type field
+                match node.node_type.as_deref() {
+                    Some("agent") | Some("bot") => agent_ids.push(*node_id),
+                    Some("ontology_class") | Some("owl_class") => ontology_class_ids.push(*node_id),
+                    Some("ontology_individual") | Some("owl_individual") => ontology_individual_ids.push(*node_id),
+                    Some("ontology_property") | Some("owl_property") => ontology_property_ids.push(*node_id),
+                    // Default to knowledge graph for backward compatibility
+                    _ => knowledge_ids.push(*node_id),
+                }
             }

             // ALSO collect agent graph nodes (from bots_graph_data)
-            // This ensures both graphs are broadcast together in one unified message
+            // This ensures all graphs are broadcast together in one unified message
             for node in &self.bots_graph_data.nodes {
                 position_data.push((node.id, BinaryNodeDataClient::new(
                     node.id,
@@ -2124,12 +2123,15 @@ impl GraphServiceActor {

             // Broadcast to all connected clients via client manager
             if !position_data.is_empty() {
-                // Encode with proper type flags: KNOWLEDGE_NODE_FLAG and AGENT_NODE_FLAG
+                // Encode with proper type flags: KNOWLEDGE_NODE_FLAG, AGENT_NODE_FLAG, and ONTOLOGY flags
                 // Client will separate them by checking flags
-                let binary_data = crate::utils::binary_protocol::encode_node_data_with_types(
+                let binary_data = crate::utils::binary_protocol::encode_node_data_extended(
                     &position_data,
                     &agent_ids,
-                    &knowledge_ids
+                    &knowledge_ids,
+                    &ontology_class_ids,
+                    &ontology_individual_ids,
+                    &ontology_property_ids
                 );

                 // Send to client manager for broadcasting
@@ -2142,18 +2144,19 @@ impl GraphServiceActor {

                 // Update broadcast time and mark initial positions as sent
                 self.last_broadcast_time = Some(now);
+                let ontology_count = ontology_class_ids.len() + ontology_individual_ids.len() + ontology_property_ids.len();
                 if !self.initial_positions_sent {
                     self.initial_positions_sent = true;
-                    info!("Sent initial unified graph positions to clients ({} nodes: {} knowledge + {} agents)",
-                          position_data.len(), knowledge_ids.len(), agent_ids.len());
+                    info!("Sent initial multi-graph positions to clients ({} nodes: {} knowledge + {} agents + {} ontology)",
+                          position_data.len(), knowledge_ids.len(), agent_ids.len(), ontology_count);
                 } else if force_broadcast {
-                    info!("Force broadcast unified graph positions to new clients ({} nodes: {} knowledge + {} agents)",
-                          position_data.len(), knowledge_ids.len(), agent_ids.len());
+                    info!("Force broadcast multi-graph positions to new clients ({} nodes: {} knowledge + {} agents + {} ontology)",
+                          position_data.len(), knowledge_ids.len(), agent_ids.len(), ontology_count);
                 }

                 if crate::utils::logging::is_debug_enabled() && !force_broadcast {
-                    debug!("Broadcast unified positions: {} total ({} knowledge + {} agents), stable: {}, pending: {}/{}",
-                           position_data.len(), knowledge_ids.len(), agent_ids.len(),
+                    debug!("Broadcast multi-graph positions: {} total ({} knowledge + {} agents + {} ontology), stable: {}, pending: {}/{}",
+                           position_data.len(), knowledge_ids.len(), agent_ids.len(), ontology_count,
                            self.current_state == AutoBalanceState::Stable,
                            self.pending_broadcasts,
                            self.max_pending_broadcasts);

+//! This actor manages the canonical graph state with thread-safe access
+//! and change tracking. It provides no physics computation or external
+//! actor dependencies - only pure state operations.

 use actix::prelude::*;
-use std::collections::HashMap;
-use std::sync::Arc;
-use log::{debug, info, warn, error, trace};
-
-use crate::actors::messages::*;
-use crate::errors::VisionFlowError;
+use std::collections::{HashMap, HashSet};
+use std::sync::{Arc, RwLock};
+use crate::models::graph::GraphData;
 use crate::models::node::Node;
 use crate::models::edge::Edge;
-use crate::models::metadata::{MetadataStore, FileMetadata};
-use crate::models::graph::GraphData;
-use crate::utils::socket_flow_messages::{BinaryNodeData, BinaryNodeDataClient, glam_to_vec3data};
+use crate::utils::socket_flow_messages::BinaryNodeData;
+use crate::models::metadata::MetadataStore;
+
+// Import legacy messages for compatibility
+use crate::actors::messages::{
+    GetGraphData as LegacyGetGraphData,
+    AddNode,
+    AddNodesFromMetadata,
+    BuildGraphFromMetadata,
+    UpdateGraphData,
+    InitializeGPUConnection,
+    GetNodeMap,
+    GetPhysicsState,
+    GetAutoBalanceNotifications,
+    UpdateBotsGraph,
+    GetBotsGraphData,
+    UpdateSimulationParams,
+    ComputeShortestPaths,
+    AddEdge as LegacyAddEdge,
+    RemoveNode,
+    UpdateNodePositions as LegacyUpdateNodePositions,
+    RequestPositionSnapshot,
+    PositionSnapshot,
+    SimulationStep,
+};
+use crate::actors::graph_actor::{PhysicsState, AutoBalanceNotification};
+use crate::models::simulation_params::SimulationParams;
+
+// ============================================================================
+// Message Definitions
+// ============================================================================
+
+#[derive(Message)]
+#[rtype(result = "Result<Arc<GraphData>, String>")]
+pub struct GetGraphData;
+
+#[derive(Message)]
+#[rtype(result = "Result<Vec<u32>, String>")]
+pub struct AddNodes {
+    pub nodes: Vec<Node>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<Vec<String>, String>")]
+pub struct AddEdges {
+    pub edges: Vec<Edge>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct UpdateNodePositions {
+    pub updates: Vec<(u32, BinaryNodeData)>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<HashSet<u32>, String>")]
+pub struct GetDirtyNodes;
+
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct ClearDirtyNodes;
+
+#[derive(Message)]
+#[rtype(result = "u64")]
+pub struct GetVersion;
+
+#[derive(Message)]
+#[rtype(result = "Result<Vec<Node>, String>")]
+pub struct GetNodes {
+    pub node_ids: Vec<u32>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct RemoveNodes {
+    pub node_ids: Vec<u32>,
+}
+
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct Clear;
+
+// ============================================================================
+// Actor Definition
+// ============================================================================

-/// Graph State Actor - Manages all graph state operations
 pub struct GraphStateActor {
-    /// Primary graph data containing nodes and edges
-    graph_data: Arc<GraphData>,
-    /// Efficient node lookup map by ID
-    node_map: Arc<HashMap<u32, Node>>,
-    /// Separate graph data for bot/agent visualization
-    bots_graph_data: Arc<GraphData>,
-    /// Next available node ID for auto-assignment
-    next_node_id: std::sync::atomic::AtomicU32,
+    /// Thread-safe graph data store
+    graph_data: Arc<RwLock<GraphData>>,
+
+    /// Fast node lookup by ID
+    node_index: HashMap<u32, usize>,
+
+    /// Tracks nodes with changes since last clear
+    dirty_nodes: HashSet<u32>,
+
+    /// Version counter for optimistic locking
+    version: u64,
 }

 impl GraphStateActor {
-    /// Create new GraphStateActor instance
     pub fn new() -> Self {
         Self {
-            graph_data: Arc::new(GraphData::new()),
-            node_map: Arc::new(HashMap::new()),
-            bots_graph_data: Arc::new(GraphData::new()),
-            next_node_id: std::sync::atomic::AtomicU32::new(1),
+            graph_data: Arc::new(RwLock::new(GraphData::new())),
+            node_index: HashMap::new(),
+            dirty_nodes: HashSet::new(),
+            version: 0,
         }
     }

-    /// Get reference to graph data
-    pub fn get_graph_data(&self) -> &GraphData {
-        &self.graph_data
+    /// Increment version and return new value
+    fn bump_version(&mut self) -> u64 {
+        self.version = self.version.wrapping_add(1);
+        self.version
     }

-    /// Get reference to node map
-    pub fn get_node_map(&self) -> &HashMap<u32, Node> {
-        &self.node_map
+    /// Mark node as dirty
+    fn mark_dirty(&mut self, node_id: u32) {
+        self.dirty_nodes.insert(node_id);
     }

-    /// Add a single node to the graph
-    fn add_node(&mut self, node: Node) {
-        let node_id = node.id;
+    /// Rebuild node index from current graph data
+    fn rebuild_index(&mut self) -> Result<(), String> {
+        self.node_index.clear();

-        // Add to node map
-        Arc::make_mut(&mut self.node_map).insert(node_id, node.clone());
+        let graph = self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?;

-        // Add to graph data
-        Arc::make_mut(&mut self.graph_data).nodes.push(node);
+        for (idx, node) in graph.nodes.iter().enumerate() {
+            self.node_index.insert(node.id, idx);
+        }

-        info!("Added node {} to graph", node_id);
+        Ok(())
     }
+}

-    /// Remove a node and its associated edges
-    fn remove_node(&mut self, node_id: u32) {
-        // Remove from node map
-        if Arc::make_mut(&mut self.node_map).remove(&node_id).is_some() {
-            // Remove from graph data nodes
-            let graph_data_mut = Arc::make_mut(&mut self.graph_data);
-            graph_data_mut.nodes.retain(|n| n.id != node_id);
+impl Default for GraphStateActor {
+    fn default() -> Self {
+        Self::new()
+    }
+}

-            // Remove associated edges
-            graph_data_mut.edges.retain(|e| e.source != node_id && e.target != node_id);
+impl Actor for GraphStateActor {
+    type Context = Context<Self>;

-            info!("Removed node {} and its edges from graph", node_id);
-        } else {
-            warn!("Attempted to remove non-existent node {}", node_id);
-        }
+    fn started(&mut self, _ctx: &mut Self::Context) {
+        log::info!("GraphStateActor started");
     }

-    /// Add an edge to the graph
-    fn add_edge(&mut self, edge: Edge) {
-        // Verify source and target nodes exist
-        if !self.node_map.contains_key(&edge.source) {
-            warn!("Cannot add edge: source node {} does not exist", edge.source);
-            return;
-        }
-        if !self.node_map.contains_key(&edge.target) {
-            warn!("Cannot add edge: target node {} does not exist", edge.target);
-            return;
-        }
+    fn stopped(&mut self, _ctx: &mut Self::Context) {
+        log::info!("GraphStateActor stopped");
+    }
+}
+
+// ============================================================================
+// Message Handlers
+// ============================================================================

-        // Add to graph data
-        Arc::make_mut(&mut self.graph_data).edges.push(edge.clone());
-        info!("Added edge from {} to {} with weight {}", edge.source, edge.target, edge.weight);
+impl Handler<GetGraphData> for GraphStateActor {
+    type Result = Result<Arc<GraphData>, String>;
+
+    fn handle(&mut self, _msg: GetGraphData, _ctx: &mut Context<Self>) -> Self::Result {
+        Ok(Arc::new(self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?
+            .clone()))
     }
+}
+
+impl Handler<AddNodes> for GraphStateActor {
+    type Result = Result<Vec<u32>, String>;
+
+    fn handle(&mut self, msg: AddNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut added_ids = Vec::with_capacity(msg.nodes.len());
+
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            for node in msg.nodes {
+                let node_id = node.id;
+
+                // Check if node already exists
+                if self.node_index.contains_key(&node_id) {
+                    return Err(format!("Node with ID {} already exists", node_id));
+                }
+
+                // Add to graph
+                let idx = graph.nodes.len();
+                graph.nodes.push(node);

-    /// Remove an edge by ID
-    fn remove_edge(&mut self, edge_id: &str) {
-        let graph_data_mut = Arc::make_mut(&mut self.graph_data);
-        let initial_count = graph_data_mut.edges.len();
+                // Update index
+                self.node_index.insert(node_id, idx);

-        graph_data_mut.edges.retain(|e| e.id != edge_id);
+                added_ids.push(node_id);
+            }
+        } // Drop write lock

-        let removed_count = initial_count - graph_data_mut.edges.len();
-        if removed_count > 0 {
-            info!("Removed {} edge(s) with ID {}", removed_count, edge_id);
-        } else {
-            warn!("No edges found with ID {}", edge_id);
+        // Mark as dirty and bump version
+        for &node_id in &added_ids {
+            self.mark_dirty(node_id);
         }
+        self.bump_version();
+
+        Ok(added_ids)
     }
+}

-    /// Build graph from metadata store
-    fn build_from_metadata(&mut self, metadata: MetadataStore) -> Result<(), String> {
-        let mut new_graph_data = GraphData::new();
+impl Handler<AddEdges> for GraphStateActor {
+    type Result = Result<Vec<String>, String>;

-        // Save existing node positions to preserve layout
-        let mut existing_positions: HashMap<String, (crate::types::vec3::Vec3Data, crate::types::vec3::Vec3Data)> = HashMap::new();
+    fn handle(&mut self, msg: AddEdges, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut added_ids = Vec::with_capacity(msg.edges.len());

-        for node in &self.graph_data.nodes {
-            existing_positions.insert(node.metadata_id.clone(), (node.data.position(), node.data.velocity()));
-        }
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-        // Generate nodes from metadata
-        let mut new_node_map = HashMap::new();
-        let mut current_id = self.next_node_id.load(std::sync::atomic::Ordering::SeqCst);
-
-        for (metadata_id, file_metadata) in metadata.iter() {
-            let mut node = Node::new_with_id(metadata_id.clone(), Some(current_id));
-
-            // Restore position if it existed
-            if let Some((position, velocity)) = existing_positions.get(metadata_id) {
-                node.data.x = position.x;
-                node.data.y = position.y;
-                node.data.z = position.z;
-                node.data.vx = velocity.x;
-                node.data.vy = velocity.y;
-                node.data.vz = velocity.z;
-            } else {
-                // Generate random initial position
-                self.generate_random_position(&mut node);
+            for edge in msg.edges {
+                // Validate source and target nodes exist
+                if !self.node_index.contains_key(&edge.source) {
+                    return Err(format!("Source node {} does not exist", edge.source));
+                }
+                if !self.node_index.contains_key(&edge.target) {
+                    return Err(format!("Target node {} does not exist", edge.target));
+                }
+
+                let edge_id = edge.id.clone();
+                graph.edges.push(edge);
+
+                added_ids.push(edge_id);
             }
+        } // Drop write lock

-            // Set node properties from metadata
-            self.configure_node_from_metadata(&mut node, file_metadata);
+        self.bump_version();

-            new_node_map.insert(current_id, node.clone());
-            new_graph_data.nodes.push(node);
-            current_id += 1;
-        }
+        Ok(added_ids)
+    }
+}

-        // Generate edges based on relationships in metadata
-        self.generate_edges_from_metadata(&mut new_graph_data, &metadata);
+impl Handler<UpdateNodePositions> for GraphStateActor {
+    type Result = Result<(), String>;

-        // Update actor state
-        self.graph_data = Arc::new(new_graph_data);
-        self.node_map = Arc::new(new_node_map);
-        self.next_node_id.store(current_id, std::sync::atomic::Ordering::SeqCst);
+    fn handle(&mut self, msg: UpdateNodePositions, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut dirty_nodes = Vec::new();

-        info!("Built graph from metadata: {} nodes, {} edges",
-              self.graph_data.nodes.len(), self.graph_data.edges.len());
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            for (node_id, binary_data) in msg.updates {
+                // Find node index
+                let idx = self.node_index.get(&node_id)
+                    .ok_or_else(|| format!("Node {} not found", node_id))?;
+
+                // Update node data
+                if let Some(node) = graph.nodes.get_mut(*idx) {
+                    node.data = binary_data;
+                    dirty_nodes.push(node_id);
+                } else {
+                    return Err(format!("Node index {} out of bounds", idx));
+                }
+            }
+        } // Drop write lock
+
+        // Mark as dirty and bump version
+        for node_id in dirty_nodes {
+            self.mark_dirty(node_id);
+        }
+        self.bump_version();

         Ok(())
     }
+}

-    /// Generate random initial position for a node
-    fn generate_random_position(&self, node: &mut Node) {
-        use rand::{Rng, SeedableRng};
-        use rand::rngs::{StdRng, OsRng};
+impl Handler<GetDirtyNodes> for GraphStateActor {
+    type Result = Result<HashSet<u32>, String>;

-        let mut rng = StdRng::from_seed(OsRng.gen());
-        let radius = 50.0 + rng.gen::<f32>() * 100.0;
-        let theta = rng.gen::<f32>() * 2.0 * std::f32::consts::PI;
-        let phi = rng.gen::<f32>() * std::f32::consts::PI;
+    fn handle(&mut self, _msg: GetDirtyNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        Ok(self.dirty_nodes.clone())
+    }
+}

-        node.data.x = radius * phi.sin() * theta.cos();
-        node.data.y = radius * phi.sin() * theta.sin();
-        node.data.z = radius * phi.cos();
+impl Handler<ClearDirtyNodes> for GraphStateActor {
+    type Result = Result<(), String>;

-        // Small random initial velocity
-        node.data.vx = rng.gen_range(-1.0..1.0);
-        node.data.vy = rng.gen_range(-1.0..1.0);
-        node.data.vz = rng.gen_range(-1.0..1.0);
+    fn handle(&mut self, _msg: ClearDirtyNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        self.dirty_nodes.clear();
+        Ok(())
     }
+}

-    /// Configure node properties based on metadata
-    fn configure_node_from_metadata(&self, node: &mut Node, metadata: &FileMetadata) {
-        // Set label from filename
-        if let Some(filename) = metadata.path.file_name() {
-            node.label = filename.to_string_lossy().to_string();
-        }
+impl Handler<GetVersion> for GraphStateActor {
+    type Result = u64;

-        // Set color based on file extension
-        node.color = Some(self.get_color_for_extension(&metadata.path));
+    fn handle(&mut self, _msg: GetVersion, _ctx: &mut Context<Self>) -> Self::Result {
+        self.version
+    }
+}

-        // Set size based on file size (if available)
-        if let Some(size) = metadata.size {
-            node.size = Some(10.0 + (size as f32 / 1000.0).min(50.0));
-        }
+impl Handler<GetNodes> for GraphStateActor {
+    type Result = Result<Vec<Node>, String>;

-        // Store metadata
-        node.metadata.insert("path".to_string(), metadata.path.to_string_lossy().to_string());
-        if let Some(size) = metadata.size {
-            node.metadata.insert("size".to_string(), size.to_string());
-        }
-        if let Some(modified) = metadata.modified {
-            node.metadata.insert("modified".to_string(), modified.to_string());
-        }
-    }
+    fn handle(&mut self, msg: GetNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        let graph = self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?;
+
+        let mut nodes = Vec::with_capacity(msg.node_ids.len());

-    /// Get color for file extension
-    fn get_color_for_extension(&self, path: &std::path::Path) -> String {
-        match path.extension().and_then(|s| s.to_str()) {
-            Some("rs") => "#CE422B".to_string(), // Rust orange
-            Some("js") | Some("ts") => "#F7DF1E".to_string(), // JavaScript yellow
-            Some("py") => "#3776AB".to_string(), // Python blue
-            Some("html") => "#E34F26".to_string(), // HTML orange
-            Some("css") => "#1572B6".to_string(), // CSS blue
-            Some("json") => "#000000".to_string(), // JSON black
-            Some("md") => "#083FA1".to_string(), // Markdown blue
-            Some("txt") => "#808080".to_string(), // Text gray
-            _ => "#95A5A6".to_string(), // Default gray
+        for node_id in msg.node_ids {
+            let idx = self.node_index.get(&node_id)
+                .ok_or_else(|| format!("Node {} not found", node_id))?;
+
+            if let Some(node) = graph.nodes.get(*idx) {
+                nodes.push(node.clone());
+            } else {
+                return Err(format!("Node index {} out of bounds", idx));
+            }
         }
+
+        Ok(nodes)
     }
+}
+
+impl Handler<RemoveNodes> for GraphStateActor {
+    type Result = Result<(), String>;

-    /// Generate edges from metadata relationships
-    fn generate_edges_from_metadata(&self, graph_data: &mut GraphData, metadata: &MetadataStore) {
-        // Create edges based on directory structure
-        let mut path_to_node: HashMap<std::path::PathBuf, u32> = HashMap::new();
+    fn handle(&mut self, msg: RemoveNodes, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut graph = self.graph_data.write()
+            .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-        // Map paths to node IDs
-        for node in &graph_data.nodes {
-            if let Some(path_str) = node.metadata.get("path") {
-                let path = std::path::PathBuf::from(path_str);
-                path_to_node.insert(path, node.id);
+        // Collect indices to remove
+        let mut indices_to_remove: Vec<usize> = Vec::new();
+        for node_id in &msg.node_ids {
+            if let Some(&idx) = self.node_index.get(node_id) {
+                indices_to_remove.push(idx);
             }
         }

-        // Create edges between files in same directory
-        let mut directory_nodes: HashMap<std::path::PathBuf, Vec<u32>> = HashMap::new();
+        // Sort in reverse order to remove from end first
+        indices_to_remove.sort_unstable_by(|a, b| b.cmp(a));

-        for (path, node_id) in &path_to_node {
-            if let Some(parent) = path.parent() {
-                directory_nodes.entry(parent.to_path_buf())
-                    .or_insert_with(Vec::new)
-                    .push(*node_id);
+        // Remove nodes
+        for idx in indices_to_remove {
+            if idx < graph.nodes.len() {
+                graph.nodes.remove(idx);
             }
         }

-        // Create edges within directories
-        for (_, nodes) in directory_nodes {
-            if nodes.len() > 1 {
-                for i in 0..nodes.len() {
-                    for j in i+1..nodes.len() {
-                        let edge = Edge::new(nodes[i], nodes[j], 0.3); // Weak directory connection
-                        graph_data.edges.push(edge);
-                    }
-                }
-            }
+        // Remove associated edges
+        let node_ids_set: HashSet<u32> = msg.node_ids.iter().copied().collect();
+        graph.edges.retain(|edge| {
+            !node_ids_set.contains(&edge.source) && !node_ids_set.contains(&edge.target)
+        });
+
+        // Rebuild index after removal
+        drop(graph);
+        self.rebuild_index()?;
+
+        // Remove from dirty set
+        for node_id in &msg.node_ids {
+            self.dirty_nodes.remove(node_id);
         }

-        info!("Generated {} edges from metadata relationships", graph_data.edges.len());
+        self.bump_version();
+
+        Ok(())
     }
+}

-    /// Add nodes from metadata store
-    fn add_nodes_from_metadata(&mut self, metadata: MetadataStore) -> Result<(), String> {
-        let mut added_count = 0;
-        let mut current_id = self.next_node_id.load(std::sync::atomic::Ordering::SeqCst);
+impl Handler<Clear> for GraphStateActor {
+    type Result = Result<(), String>;

-        for (metadata_id, file_metadata) in metadata.iter() {
-            // Skip if node already exists
-            if self.node_map.values().any(|n| n.metadata_id == *metadata_id) {
-                continue;
-            }
+    fn handle(&mut self, _msg: Clear, _ctx: &mut Context<Self>) -> Self::Result {
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-            let mut node = Node::new_with_id(metadata_id.clone(), Some(current_id));
-            self.generate_random_position(&mut node);
-            self.configure_node_from_metadata(&mut node, file_metadata);
+            graph.nodes.clear();
+            graph.edges.clear();
+            graph.metadata.clear();
+            graph.id_to_metadata.clear();
+        } // Drop write lock

-            self.add_node(node);
-            current_id += 1;
-            added_count += 1;
-        }
+        self.node_index.clear();
+        self.dirty_nodes.clear();
+        self.bump_version();

-        self.next_node_id.store(current_id, std::sync::atomic::Ordering::SeqCst);
-        info!("Added {} new nodes from metadata", added_count);
         Ok(())
     }
+}

-    /// Update existing node from metadata
-    fn update_node_from_metadata(&mut self, metadata_id: String, metadata: FileMetadata) -> Result<(), String> {
-        // Find node by metadata_id
-        let mut node_found = false;
-        let node_map_mut = Arc::make_mut(&mut self.node_map);
+// ============================================================================
+// Legacy Message Handlers for Backward Compatibility
+// ============================================================================

-        for (_, node) in node_map_mut.iter_mut() {
-            if node.metadata_id == metadata_id {
-                self.configure_node_from_metadata(node, &metadata);
-                node_found = true;
-                break;
-            }
+impl Handler<LegacyGetGraphData> for GraphStateActor {
+    type Result = Result<Arc<GraphData>, String>;
+
+    fn handle(&mut self, _msg: LegacyGetGraphData, _ctx: &mut Context<Self>) -> Self::Result {
+        Ok(Arc::new(self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?
+            .clone()))
+    }
+}
+
+impl Handler<AddNodesFromMetadata> for GraphStateActor {
+    type Result = Result<(), String>;
+
+    fn handle(&mut self, msg: AddNodesFromMetadata, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut nodes = Vec::new();
+
+        // Convert metadata to nodes
+        // MetadataStore is HashMap<String, Metadata> where key is filename
+        for (filename, _metadata) in msg.metadata.iter() {
+            // Create node with filename as both metadata_id and label
+            let mut node = Node::new(filename.clone());
+            node.label = filename.clone(); // Set label so it displays in UI
+            nodes.push(node);
         }

-        // Update in graph_data as well
-        if node_found {
-            let graph_data_mut = Arc::make_mut(&mut self.graph_data);
-            for node in &mut graph_data_mut.nodes {
-                if node.metadata_id == metadata_id {
-                    self.configure_node_from_metadata(node, &metadata);
-                    break;
+        // Add nodes via existing handler logic
+        if nodes.is_empty() {
+            return Ok(());
+        }
+
+        let mut added_ids = Vec::new();
+
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            for node in nodes {
+                let node_id = node.id;
+
+                // Skip if node already exists
+                if self.node_index.contains_key(&node_id) {
+                    continue;
                 }
+
+                let idx = graph.nodes.len();
+                graph.nodes.push(node);
+                self.node_index.insert(node_id, idx);
+                added_ids.push(node_id);
             }
-            info!("Updated node with metadata_id: {}", metadata_id);
-            Ok(())
-        } else {
-            warn!("Node with metadata_id {} not found for update", metadata_id);
-            Err(format!("Node with metadata_id {} not found", metadata_id))
-        }
-    }

-    /// Remove node by metadata ID
-    fn remove_node_by_metadata(&mut self, metadata_id: String) -> Result<(), String> {
-        // Find node ID by metadata_id
-        let node_id = self.node_map.values()
-            .find(|n| n.metadata_id == metadata_id)
-            .map(|n| n.id);
+            // Create edges between new nodes and connect to existing graph
+            if !added_ids.is_empty() {
+                // Get all existing node IDs before the new additions
+                let existing_ids: Vec<u32> = self.node_index.keys()
+                    .filter(|id| !added_ids.contains(id))
+                    .copied()
+                    .collect();
+
+                // Connect new nodes to each other
+                for i in 0..added_ids.len() {
+                    for j in 1..=2 {
+                        if i + j < added_ids.len() {
+                            let edge = Edge::new(added_ids[i], added_ids[i + j], 1.0);
+                            graph.edges.push(edge);
+                        }
+                    }
+                }
+
+                // Connect new nodes to existing graph (2-3 edges each to existing nodes)
+                for &new_id in &added_ids {
+                    let num_connections = existing_ids.len().min(3);
+                    for i in 0..num_connections {
+                        if i < existing_ids.len() {
+                            let edge = Edge::new(new_id, existing_ids[i], 1.0);
+                            graph.edges.push(edge);
+                        }
+                    }
+                }

-        if let Some(id) = node_id {
-            self.remove_node(id);
-            Ok(())
-        } else {
-            warn!("Node with metadata_id {} not found for removal", metadata_id);
-            Err(format!("Node with metadata_id {} not found", metadata_id))
+                log::info!("Added {} new nodes and created edges to integrate with existing {} nodes",
+                          added_ids.len(), existing_ids.len());
+            }
+        } // Drop write lock
+
+        // Mark as dirty and bump version
+        for node_id in added_ids {
+            self.mark_dirty(node_id);
         }
+        self.bump_version();
+
+        Ok(())
     }
+}

-    /// Compute shortest paths from a source node using Dijkstra's algorithm
-    fn compute_shortest_paths(&self, source_node_id: u32) -> Result<HashMap<u32, (f32, Vec<u32>)>, String> {
-        if !self.node_map.contains_key(&source_node_id) {
-            return Err(format!("Source node {} not found", source_node_id));
-        }
+impl Handler<BuildGraphFromMetadata> for GraphStateActor {
+    type Result = Result<(), String>;

-        let mut distances: HashMap<u32, f32> = HashMap::new();
-        let mut predecessors: HashMap<u32, u32> = HashMap::new();
-        let mut unvisited: std::collections::BTreeSet<(ordered_float::OrderedFloat<f32>, u32)> = std::collections::BTreeSet::new();
+    fn handle(&mut self, msg: BuildGraphFromMetadata, _ctx: &mut Context<Self>) -> Self::Result {
+        // BuildGraphFromMetadata clears existing graph and builds from metadata
+        // This is different from AddNodesFromMetadata which adds to existing graph

-        // Initialize distances
-        for &node_id in self.node_map.keys() {
-            let distance = if node_id == source_node_id { 0.0 } else { f32::INFINITY };
-            distances.insert(node_id, distance);
-            unvisited.insert((ordered_float::OrderedFloat(distance), node_id));
-        }
+        // Clear existing graph first
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-        while let Some((current_distance, current_node)) = unvisited.pop_first() {
-            let current_distance = current_distance.into_inner();
+            graph.nodes.clear();
+            graph.edges.clear();
+            graph.metadata.clear();
+            graph.id_to_metadata.clear();
+        } // Drop write lock

-            if current_distance == f32::INFINITY {
-                break; // No more reachable nodes
-            }
+        self.node_index.clear();
+        self.dirty_nodes.clear();

-            // Check all edges from current node
-            for edge in &self.graph_data.edges {
-                let (neighbor, edge_weight) = if edge.source == current_node {
-                    (edge.target, edge.weight)
-                } else if edge.target == current_node {
-                    (edge.source, edge.weight)
-                } else {
-                    continue;
-                };
+        // Now add nodes from metadata (reuse AddNodesFromMetadata logic)
+        let mut nodes = Vec::new();

-                let new_distance = current_distance + edge_weight;
-                let old_distance = distances.get(&neighbor).copied().unwrap_or(f32::INFINITY);
+        for (filename, _metadata) in msg.metadata.iter() {
+            let mut node = Node::new(filename.clone());
+            node.label = filename.clone(); // Set label so it displays in UI
+            nodes.push(node);
+        }
+
+        if nodes.is_empty() {
+            return Ok(());
+        }

-                if new_distance < old_distance {
-                    // Remove old entry from unvisited
-                    unvisited.remove(&(ordered_float::OrderedFloat(old_distance), neighbor));
+        let mut added_ids = Vec::new();

-                    // Update distance and predecessor
-                    distances.insert(neighbor, new_distance);
-                    predecessors.insert(neighbor, current_node);
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-                    // Add new entry to unvisited
-                    unvisited.insert((ordered_float::OrderedFloat(new_distance), neighbor));
-                }
+            for node in nodes {
+                let node_id = node.id;
+                let idx = graph.nodes.len();
+                graph.nodes.push(node);
+                self.node_index.insert(node_id, idx);
+                added_ids.push(node_id);
             }
-        }

-        // Reconstruct paths
-        let mut result: HashMap<u32, (f32, Vec<u32>)> = HashMap::new();
-
-        for (&target_node, &distance) in &distances {
-            if distance != f32::INFINITY {
-                let mut path = Vec::new();
-                let mut current = target_node;
-
-                // Reconstruct path backwards
-                while current != source_node_id {
-                    path.push(current);
-                    if let Some(&prev) = predecessors.get(&current) {
-                        current = prev;
-                    } else {
-                        break;
+            // Create basic edges between nodes (fully connected for visualization)
+            // For now, create edges between consecutive nodes for a minimal graph structure
+            if added_ids.len() > 1 {
+                for i in 0..added_ids.len() {
+                    // Connect each node to next 2-3 nodes for a sparse mesh
+                    for j in 1..=2 {
+                        if i + j < added_ids.len() {
+                            let edge = Edge::new(added_ids[i], added_ids[i + j], 1.0);
+                            graph.edges.push(edge);
+                        }
                     }
                 }
-                path.push(source_node_id);
-                path.reverse();
-
-                result.insert(target_node, (distance, path));
+                log::info!("Created {} edges between {} nodes", graph.edges.len(), added_ids.len());
             }
-        }
+        } // Drop write lock

-        info!("Computed shortest paths from node {} to {} reachable nodes",
-              source_node_id, result.len());
+        // Mark as dirty and bump version
+        for node_id in added_ids {
+            self.mark_dirty(node_id);
+        }
+        self.bump_version();

-        Ok(result)
+        Ok(())
     }
 }

-impl Actor for GraphStateActor {
-    type Context = Context<Self>;
+impl Handler<UpdateGraphData> for GraphStateActor {
+    type Result = Result<(), String>;

-    fn started(&mut self, _ctx: &mut Self::Context) {
-        info!("GraphStateActor started");
-    }
+    fn handle(&mut self, msg: UpdateGraphData, _ctx: &mut Context<Self>) -> Self::Result {
+        // Replace entire graph with new data
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-    fn stopped(&mut self, _ctx: &mut Self::Context) {
-        info!("GraphStateActor stopped");
-    }
-}
+            // Extract data from Arc
+            let new_graph_data = (*msg.graph_data).clone();

-// Handler implementations
+            // Replace all graph data
+            *graph = new_graph_data;
+        } // Drop write lock

-impl Handler<GetGraphData> for GraphStateActor {
-    type Result = Result<Arc<GraphData>, String>;
+        // Rebuild index from new graph
+        self.rebuild_index()?;
+
+        // Mark all nodes as dirty
+        {
+            let graph_read = self.graph_data.read()
+                .map_err(|e| format!("Failed to acquire read lock: {}", e))?;
+
+            for node in &graph_read.nodes {
+                self.dirty_nodes.insert(node.id);
+            }
+        }
+
+        self.bump_version();

-    fn handle(&mut self, _msg: GetGraphData, _ctx: &mut Self::Context) -> Self::Result {
-        debug!("GraphStateActor handling GetGraphData with Arc reference");
-        Ok(Arc::clone(&self.graph_data))
+        Ok(())
     }
 }

-impl Handler<AddNode> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<InitializeGPUConnection> for GraphStateActor {
+    type Result = ();

-    fn handle(&mut self, msg: AddNode, _ctx: &mut Self::Context) -> Self::Result {
-        self.add_node(msg.node);
-        Ok(())
+    fn handle(&mut self, _msg: InitializeGPUConnection, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't manage GPU connections
+        // This is a no-op for pure state management
+        log::debug!("GraphStateActor: GPU connection message received (no-op for pure state)");
     }
 }

-impl Handler<RemoveNode> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<GetNodeMap> for GraphStateActor {
+    type Result = Result<Arc<HashMap<u32, Node>>, String>;

-    fn handle(&mut self, msg: RemoveNode, _ctx: &mut Self::Context) -> Self::Result {
-        self.remove_node(msg.node_id);
-        Ok(())
+    fn handle(&mut self, _msg: GetNodeMap, _ctx: &mut Context<Self>) -> Self::Result {
+        let graph = self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?;
+
+        let mut node_map = HashMap::new();
+        for node in &graph.nodes {
+            node_map.insert(node.id, node.clone());
+        }
+
+        Ok(Arc::new(node_map))
     }
 }

-impl Handler<AddEdge> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<GetPhysicsState> for GraphStateActor {
+    type Result = Result<PhysicsState, String>;
+
+    fn handle(&mut self, _msg: GetPhysicsState, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't manage physics state
+        // Return default state (consult PhysicsOrchestratorActor for actual physics)
+        Ok(PhysicsState {
+            is_settled: false,
+            stable_frame_count: 0,
+            kinetic_energy: 0.0,
+            current_state: "Not managed by GraphStateActor".to_string(),
+        })
+    }
+}

-    fn handle(&mut self, msg: AddEdge, _ctx: &mut Self::Context) -> Self::Result {
-        self.add_edge(msg.edge);
-        Ok(())
+impl Handler<GetAutoBalanceNotifications> for GraphStateActor {
+    type Result = Result<Vec<AutoBalanceNotification>, String>;
+
+    fn handle(&mut self, _msg: GetAutoBalanceNotifications, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't manage auto-balance notifications
+        // Return empty list (consult PhysicsOrchestratorActor for actual notifications)
+        Ok(Vec::new())
     }
 }

-impl Handler<RemoveEdge> for GraphStateActor {
+impl Handler<AddNode> for GraphStateActor {
     type Result = Result<(), String>;

-    fn handle(&mut self, msg: RemoveEdge, _ctx: &mut Self::Context) -> Self::Result {
-        self.remove_edge(&msg.edge_id);
+    fn handle(&mut self, msg: AddNode, _ctx: &mut Context<Self>) -> Self::Result {
+        let node_id = msg.node.id;
+
+        // Check if node already exists
+        if self.node_index.contains_key(&node_id) {
+            return Err(format!("Node with ID {} already exists", node_id));
+        }
+
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            let idx = graph.nodes.len();
+            graph.nodes.push(msg.node);
+            self.node_index.insert(node_id, idx);
+        } // Drop write lock
+
+        self.mark_dirty(node_id);
+        self.bump_version();
+
         Ok(())
     }
 }

-impl Handler<GetNodeMap> for GraphStateActor {
-    type Result = Result<Arc<HashMap<u32, Node>>, String>;
+impl Handler<UpdateBotsGraph> for GraphStateActor {
+    type Result = ();

-    fn handle(&mut self, _msg: GetNodeMap, _ctx: &mut Self::Context) -> Self::Result {
-        debug!("GraphStateActor handling GetNodeMap with Arc reference");
-        Ok(Arc::clone(&self.node_map))
+    fn handle(&mut self, msg: UpdateBotsGraph, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut added_ids = Vec::new();
+
+        // Add nodes (ignore errors for backward compatibility)
+        if let Ok(mut graph) = self.graph_data.write() {
+            for agent in &msg.agents {
+                // Create node from agent data
+                let mut node = Node::new(agent.id.clone());
+                node.label = agent.name.clone();
+                node.node_type = Some(agent.agent_type.clone());
+                node.data.x = agent.x;
+                node.data.y = agent.y;
+                node.data.z = agent.z;
+
+                let node_id = node.id;
+
+                // Skip if already exists
+                if self.node_index.contains_key(&node_id) {
+                    continue;
+                }
+
+                let idx = graph.nodes.len();
+                graph.nodes.push(node);
+                self.node_index.insert(node_id, idx);
+                added_ids.push(node_id);
+            }
+        } // Drop write lock
+
+        // Mark as dirty and bump version
+        for node_id in added_ids {
+            self.mark_dirty(node_id);
+        }
+        self.bump_version();
     }
 }

-impl Handler<BuildGraphFromMetadata> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<GetBotsGraphData> for GraphStateActor {
+    type Result = Result<Arc<GraphData>, String>;

-    fn handle(&mut self, msg: BuildGraphFromMetadata, _ctx: &mut Self::Context) -> Self::Result {
-        info!("BuildGraphFromMetadata handler called with {} metadata entries", msg.metadata.len());
-        self.build_from_metadata(msg.metadata)
+    fn handle(&mut self, _msg: GetBotsGraphData, _ctx: &mut Context<Self>) -> Self::Result {
+        // Same as GetGraphData
+        Ok(Arc::new(self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?
+            .clone()))
     }
 }

-impl Handler<AddNodesFromMetadata> for GraphStateActor {
+impl Handler<UpdateSimulationParams> for GraphStateActor {
     type Result = Result<(), String>;

-    fn handle(&mut self, msg: AddNodesFromMetadata, _ctx: &mut Self::Context) -> Self::Result {
-        self.add_nodes_from_metadata(msg.metadata)
+    fn handle(&mut self, _msg: UpdateSimulationParams, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't manage simulation parameters
+        // This should be sent to PhysicsOrchestratorActor instead
+        log::debug!("GraphStateActor: Received UpdateSimulationParams (no-op, delegate to PhysicsOrchestratorActor)");
+        Ok(())
     }
 }

-impl Handler<UpdateNodeFromMetadata> for GraphStateActor {
-    type Result = Result<(), String>;
+impl Handler<ComputeShortestPaths> for GraphStateActor {
+    type Result = Result<HashMap<u32, Option<f32>>, String>;

-    fn handle(&mut self, msg: UpdateNodeFromMetadata, _ctx: &mut Self::Context) -> Self::Result {
-        self.update_node_from_metadata(msg.metadata_id, msg.metadata)
+    fn handle(&mut self, _msg: ComputeShortestPaths, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't compute shortest paths
+        // This should be sent to SemanticProcessorActor instead
+        Err("GraphStateActor doesn't compute paths. Use SemanticProcessorActor instead.".to_string())
     }
 }

-impl Handler<RemoveNodeByMetadata> for GraphStateActor {
+impl Handler<LegacyAddEdge> for GraphStateActor {
     type Result = Result<(), String>;

-    fn handle(&mut self, msg: RemoveNodeByMetadata, _ctx: &mut Self::Context) -> Self::Result {
-        self.remove_node_by_metadata(msg.metadata_id)
+    fn handle(&mut self, msg: LegacyAddEdge, _ctx: &mut Context<Self>) -> Self::Result {
+        // Validate source and target nodes exist
+        if !self.node_index.contains_key(&msg.edge.source) {
+            return Err(format!("Source node {} does not exist", msg.edge.source));
+        }
+        if !self.node_index.contains_key(&msg.edge.target) {
+            return Err(format!("Target node {} does not exist", msg.edge.target));
+        }
+
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;
+
+            graph.edges.push(msg.edge);
+        } // Drop write lock
+
+        self.bump_version();
+
+        Ok(())
     }
 }

-impl Handler<UpdateGraphData> for GraphStateActor {
+impl Handler<RemoveNode> for GraphStateActor {
     type Result = Result<(), String>;

-    fn handle(&mut self, msg: UpdateGraphData, _ctx: &mut Self::Context) -> Self::Result {
-        info!("Updating graph data with {} nodes, {} edges",
-              msg.graph_data.nodes.len(), msg.graph_data.edges.len());
+    fn handle(&mut self, msg: RemoveNode, _ctx: &mut Context<Self>) -> Self::Result {
+        // Find node index
+        let idx = self.node_index.get(&msg.node_id)
+            .ok_or_else(|| format!("Node {} not found", msg.node_id))?;

-        // Update graph data with the provided Arc
-        self.graph_data = msg.graph_data;
+        let mut graph = self.graph_data.write()
+            .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-        // Rebuild node map
-        Arc::make_mut(&mut self.node_map).clear();
-        for node in &self.graph_data.nodes {
-            Arc::make_mut(&mut self.node_map).insert(node.id, node.clone());
+        // Remove node
+        if *idx < graph.nodes.len() {
+            graph.nodes.remove(*idx);
         }

-        info!("Graph data updated successfully");
+        // Remove associated edges
+        graph.edges.retain(|edge| {
+            edge.source != msg.node_id && edge.target != msg.node_id
+        });
+
+        // Drop lock before rebuild
+        drop(graph);
+
+        // Rebuild index
+        self.rebuild_index()?;
+
+        // Remove from dirty set
+        self.dirty_nodes.remove(&msg.node_id);
+        self.bump_version();
+
         Ok(())
     }
 }

-impl Handler<GetBotsGraphData> for GraphStateActor {
-    type Result = Result<Arc<GraphData>, String>;
+impl Handler<LegacyUpdateNodePositions> for GraphStateActor {
+    type Result = Result<(), String>;

-    fn handle(&mut self, _msg: GetBotsGraphData, _ctx: &mut Context<Self>) -> Self::Result {
-        Ok(Arc::clone(&self.bots_graph_data))
-    }
-}
+    fn handle(&mut self, msg: LegacyUpdateNodePositions, _ctx: &mut Context<Self>) -> Self::Result {
+        let mut dirty_nodes = Vec::new();

-impl Handler<UpdateBotsGraph> for GraphStateActor {
-    type Result = ();
+        {
+            let mut graph = self.graph_data.write()
+                .map_err(|e| format!("Failed to acquire write lock: {}", e))?;

-    fn handle(&mut self, msg: UpdateBotsGraph, _ctx: &mut Context<Self>) -> Self::Result {
-        // Convert agents to graph structure
-        let mut nodes = vec![];
-        let mut edges = vec![];
+            for (node_id, binary_data) in msg.positions {
+                // Find node index
+                let idx = self.node_index.get(&node_id)
+                    .ok_or_else(|| format!("Node {} not found", node_id))?;

-        let bot_id_offset = 10000;
+                // Update node data
+                if let Some(node) = graph.nodes.get_mut(*idx) {
+                    node.data = binary_data;
+                    dirty_nodes.push(node_id);
+                } else {
+                    return Err(format!("Node index {} out of bounds", idx));
+                }
+            }
+        } // Drop write lock

-        // Preserve existing agent positions
-        let mut existing_positions: HashMap<String, (crate::types::vec3::Vec3Data, crate::types::vec3::Vec3Data)> = HashMap::new();
-        for node in &self.bots_graph_data.nodes {
-            existing_positions.insert(node.metadata_id.clone(), (node.data.position(), node.data.velocity()));
+        // Mark as dirty and bump version
+        for node_id in dirty_nodes {
+            self.mark_dirty(node_id);
         }
+        self.bump_version();

-        // Create nodes for each agent
-        for (i, agent) in msg.agents.iter().enumerate() {
-            let node_id = bot_id_offset + i as u32;
-            let mut node = Node::new_with_id(agent.id.clone(), Some(node_id));
-
-            if let Some((saved_position, saved_velocity)) = existing_positions.get(&agent.id) {
-                // Restore existing position
-                node.data.x = saved_position.x;
-                node.data.y = saved_position.y;
-                node.data.z = saved_position.z;
-                node.data.vx = saved_velocity.x;
-                node.data.vy = saved_velocity.y;
-                node.data.vz = saved_velocity.z;
-            } else {
-                self.generate_random_position(&mut node);
-            }
+        Ok(())
+    }
+}

-            // Set node properties based on agent type
-            node.color = Some(match agent.agent_type.as_str() {
-                "coordinator" => "#FF6B6B".to_string(),
-                "researcher" => "#4ECDC4".to_string(),
-                "coder" => "#45B7D1".to_string(),
-                "analyst" => "#FFA07A".to_string(),
-                "architect" => "#98D8C8".to_string(),
-                "tester" => "#F7DC6F".to_string(),
-                _ => "#95A5A6".to_string(),
-            });
-
-            node.label = agent.name.clone();
-            node.size = Some(20.0 + (agent.workload * 25.0));
-
-            // Add metadata
-            node.metadata.insert("agent_type".to_string(), agent.agent_type.clone());
-            node.metadata.insert("status".to_string(), agent.status.clone());
-            node.metadata.insert("cpu_usage".to_string(), agent.cpu_usage.to_string());
-            node.metadata.insert("memory_usage".to_string(), agent.memory_usage.to_string());
-            node.metadata.insert("health".to_string(), agent.health.to_string());
-            node.metadata.insert("is_agent".to_string(), "true".to_string());
+impl Handler<RequestPositionSnapshot> for GraphStateActor {
+    type Result = Result<PositionSnapshot, String>;

-            nodes.push(node);
+    fn handle(&mut self, msg: RequestPositionSnapshot, _ctx: &mut Context<Self>) -> Self::Result {
+        let graph = self.graph_data.read()
+            .map_err(|e| format!("Failed to acquire read lock: {}", e))?;
+
+        let mut knowledge_nodes = Vec::new();
+        let mut agent_nodes = Vec::new();
+
+        if msg.include_knowledge_graph {
+            // Include only knowledge nodes (exclude agent, ontology types)
+            for node in &graph.nodes {
+                let is_agent = node.node_type.as_ref()
+                    .map_or(false, |t| t.contains("agent") || t.contains("bot"));
+                let is_ontology = node.node_type.as_ref()
+                    .map_or(false, |t| t.contains("ontology") || t.contains("owl"));
+
+                if !is_agent && !is_ontology {
+                    knowledge_nodes.push((node.id, node.data));
+                }
+            }
         }

-        // Create edges based on agent interactions
-        for (i, source_agent) in msg.agents.iter().enumerate() {
-            for (j, target_agent) in msg.agents.iter().enumerate() {
-                if i != j {
-                    let source_node_id = bot_id_offset + i as u32;
-                    let target_node_id = bot_id_offset + j as u32;
-
-                    let communication_intensity = if source_agent.agent_type == "coordinator" || target_agent.agent_type == "coordinator" {
-                        0.8
-                    } else if source_agent.status == "active" && target_agent.status == "active" {
-                        0.5
-                    } else {
-                        0.2
-                    };
-
-                    if communication_intensity > 0.1 {
-                        let mut edge = Edge::new(source_node_id, target_node_id, communication_intensity);
-                        let metadata = edge.metadata.get_or_insert_with(HashMap::new);
-                        metadata.insert("communication_type".to_string(), "agent_collaboration".to_string());
-                        metadata.insert("intensity".to_string(), communication_intensity.to_string());
-                        edges.push(edge);
+        if msg.include_agent_graph {
+            // Filter for agent nodes (node_type contains "agent" or "bot")
+            for node in &graph.nodes {
+                if let Some(ref node_type) = node.node_type {
+                    if node_type.contains("agent") || node_type.contains("bot") {
+                        agent_nodes.push((node.id, node.data));
                     }
                 }
             }
         }

-        // Update the bots graph data
-        let bots_graph_data_mut = Arc::make_mut(&mut self.bots_graph_data);
-        bots_graph_data_mut.nodes = nodes;
-        bots_graph_data_mut.edges = edges;
+        Ok(PositionSnapshot {
+            knowledge_nodes,
+            agent_nodes,
+            timestamp: std::time::Instant::now(),
+        })
+    }
+}

-        info!("Updated bots graph with {} agents and {} edges",
-             msg.agents.len(), self.bots_graph_data.edges.len());
+impl Handler<SimulationStep> for GraphStateActor {
+    type Result = Result<(), String>;
+
+    fn handle(&mut self, _msg: SimulationStep, _ctx: &mut Context<Self>) -> Self::Result {
+        // GraphStateActor doesn't run simulation steps
+        // This should be sent to PhysicsOrchestratorActor instead
+        log::debug!("GraphStateActor: Received SimulationStep (no-op, delegate to PhysicsOrchestratorActor)");
+        Ok(())
     }
 }

-impl Handler<ComputeShortestPaths> for GraphStateActor {
-    type Result = Result<u32, String>;
-
-    fn handle(&mut self, msg: ComputeShortestPaths, _ctx: &mut Self::Context) -> Self::Result {
-        match self.compute_shortest_paths(msg.source_node_id) {
-            Ok(paths) => {
-                info!("Computed shortest paths from node {}: {} reachable nodes",
-                      msg.source_node_id, paths.len());
-                Ok(paths.len() as u32)
-            }
-            Err(e) => {
-                error!("Failed to compute shortest paths: {}", e);
-                Err(e)
-            }
-        }
+// ============================================================================
+// Tests
+// ============================================================================
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[actix::test]
+    async fn test_add_and_get_nodes() {
+        let actor = GraphStateActor::new().start();
+
+        let node1 = Node::new("test1.md".to_string());
+        let node2 = Node::new("test2.md".to_string());
+        let node1_id = node1.id;
+        let node2_id = node2.id;
+
+        // Add nodes
+        let result = actor.send(AddNodes {
+            nodes: vec![node1, node2],
+        }).await;
+
+        assert!(result.is_ok());
+        let ids = result.unwrap();
+        assert!(ids.is_ok());
+        let added_ids = ids.unwrap();
+        assert_eq!(added_ids.len(), 2);
+
+        // Get nodes
+        let result = actor.send(GetNodes {
+            node_ids: vec![node1_id, node2_id],
+        }).await;
+
+        assert!(result.is_ok());
+        let nodes = result.unwrap();
+        assert!(nodes.is_ok());
+        let fetched_nodes = nodes.unwrap();
+        assert_eq!(fetched_nodes.len(), 2);
+    }
+
+    #[actix::test]
+    async fn test_update_positions() {
+        let actor = GraphStateActor::new().start();
+
+        let node = Node::new("test.md".to_string());
+        let node_id = node.id;
+
+        // Add node
+        let _ = actor.send(AddNodes {
+            nodes: vec![node],
+        }).await;
+
+        // Update position
+        let new_data = BinaryNodeData {
+            node_id,
+            x: 10.0,
+            y: 20.0,
+            z: 30.0,
+            vx: 1.0,
+            vy: 2.0,
+            vz: 3.0,
+        };
+
+        let result = actor.send(UpdateNodePositions {
+            updates: vec![(node_id, new_data)],
+        }).await;
+
+        assert!(result.is_ok());
+        assert!(result.unwrap().is_ok());
+
+        // Verify position was updated
+        let nodes = actor.send(GetNodes {
+            node_ids: vec![node_id],
+        }).await.unwrap().unwrap();
+
+        assert_eq!(nodes[0].data.x, 10.0);
+        assert_eq!(nodes[0].data.y, 20.0);
+        assert_eq!(nodes[0].data.z, 30.0);
+    }
+
+    #[actix::test]
+    async fn test_dirty_tracking() {
+        let actor = GraphStateActor::new().start();
+
+        let node = Node::new("test.md".to_string());
+        let node_id = node.id;
+
+        // Add node
+        let _ = actor.send(AddNodes {
+            nodes: vec![node],
+        }).await;
+
+        // Check dirty nodes
+        let dirty = actor.send(GetDirtyNodes).await.unwrap().unwrap();
+        assert!(dirty.contains(&node_id));
+
+        // Clear dirty nodes
+        let _ = actor.send(ClearDirtyNodes).await;
+
+        // Verify cleared
+        let dirty = actor.send(GetDirtyNodes).await.unwrap().unwrap();
+        assert!(dirty.is_empty());
+    }
+
+    #[actix::test]
+    async fn test_version_tracking() {
+        let actor = GraphStateActor::new().start();
+
+        let v1 = actor.send(GetVersion).await.unwrap();
+        assert_eq!(v1, 0);
+
+        let node = Node::new("test.md".to_string());
+        let _ = actor.send(AddNodes {
+            nodes: vec![node],
+        }).await;
+
+        let v2 = actor.send(GetVersion).await.unwrap();
+        assert_eq!(v2, 1);
+    }
+
+    #[actix::test]
+    async fn test_remove_nodes() {
+        let actor = GraphStateActor::new().start();
+
+        let node = Node::new("test.md".to_string());
+        let node_id = node.id;
+
+        // Add node
+        let _ = actor.send(AddNodes {
+            nodes: vec![node],
+        }).await;
+
+        // Remove node
+        let result = actor.send(RemoveNodes {
+            node_ids: vec![node_id],
+        }).await;
+
+        assert!(result.is_ok());
+        assert!(result.unwrap().is_ok());
+
+        // Verify node was removed
+        let result = actor.send(GetNodes {
+            node_ids: vec![node_id],
+        }).await;
+
+        assert!(result.is_ok());
+        assert!(result.unwrap().is_err());
     }
-}
\ No newline at end of file
+
+    #[actix::test]
+    async fn test_clear() {
+        let actor = GraphStateActor::new().start();
+
+        let node1 = Node::new("test1.md".to_string());
+        let node2 = Node::new("test2.md".to_string());
+
+        // Add nodes
+        let _ = actor.send(AddNodes {
+            nodes: vec![node1, node2],
+        }).await;
+
+        // Clear
+        let result = actor.send(Clear).await;
+        assert!(result.is_ok());
+        assert!(result.unwrap().is_ok());
+
+        // Verify empty
+        let dirty = actor.send(GetDirtyNodes).await.unwrap().unwrap();
+        assert!(dirty.is_empty());
+
+        let graph = actor.send(GetGraphData).await.unwrap().unwrap();
+        let g = graph.read().unwrap();
+        assert_eq!(g.nodes.len(), 0);
+        assert_eq!(g.edges.len(), 0);
+    }
+
+    #[actix::test]
+    async fn test_add_edges() {
+        let actor = GraphStateActor::new().start();
+
+        let node1 = Node::new("test1.md".to_string());
+        let node2 = Node::new("test2.md".to_string());
+        let node1_id = node1.id;
+        let node2_id = node2.id;
+
+        // Add nodes
+        let _ = actor.send(AddNodes {
+            nodes: vec![node1, node2],
+        }).await;
+
+        // Add edge
+        let edge = Edge::new(node1_id, node2_id, 1.0);
+        let result = actor.send(AddEdges {
+            edges: vec![edge],
+        }).await;
+
+        assert!(result.is_ok());
+        let ids = result.unwrap();
+        assert!(ids.is_ok());
+        let added_ids = ids.unwrap();
+        assert_eq!(added_ids.len(), 1);
+
+        // Verify edge exists
+        let graph = actor.send(GetGraphData).await.unwrap().unwrap();
+        let g = graph.read().unwrap();
+        assert_eq!(g.edges.len(), 1);
+        assert_eq!(g.edges[0].source, node1_id);
+        assert_eq!(g.edges[0].target, node2_id);
+    }
+
+    #[actix::test]
+    async fn test_add_edge_invalid_nodes() {
+        let actor = GraphStateActor::new().start();
+
+        // Try to add edge with non-existent nodes
+        let edge = Edge::new(999, 1000, 1.0);
+        let result = actor.send(AddEdges {
+            edges: vec![edge],
+        }).await;
+
+        assert!(result.is_ok());
+        let ids = result.unwrap();
+        assert!(ids.is_err());
+    }
+}
diff --git a/src/actors/mod.rs b/src/actors/mod.rs
index af79e03c..9a3f7eeb 100644
--- a/src/actors/mod.rs
+++ b/src/actors/mod.rs
@@ -1,6 +1,7 @@
 //! Actor system modules for replacing Arc<RwLock<T>> patterns with Actix actors

 pub mod graph_actor;
+pub mod graph_state_actor;
 pub mod physics_orchestrator_actor;
 pub mod optimized_settings_actor;
 pub mod metadata_actor;
@@ -15,12 +16,12 @@ pub mod multi_mcp_visualization_actor;
 pub mod workspace_actor;
 pub mod semantic_processor_actor;
 pub mod ontology_actor;
-pub mod graph_service_supervisor;
 pub mod task_orchestrator_actor;
 pub mod messages;
 pub mod graph_messages;

 pub use graph_actor::GraphServiceActor;
+pub use graph_state_actor::GraphStateActor;
 pub use physics_orchestrator_actor::PhysicsOrchestratorActor;
 pub use optimized_settings_actor::OptimizedSettingsActor;
 pub use metadata_actor::MetadataActor;
@@ -35,9 +36,4 @@ pub use workspace_actor::WorkspaceActor;
 pub use semantic_processor_actor::{SemanticProcessorActor, SemanticProcessorConfig, SemanticStats, AISemanticFeatures};
 pub use ontology_actor::{OntologyActor, OntologyActorConfig, ValidationJob, JobPriority, JobStatus, ActorStatistics as OntologyActorStatistics};
 pub use task_orchestrator_actor::{TaskOrchestratorActor, CreateTask, GetTaskStatus, StopTask, ListActiveTasks, GetSystemStatus, SystemStatusInfo, TaskState};
-pub use graph_service_supervisor::{
-    GraphServiceSupervisor, GraphSupervisionStrategy, RestartPolicy, BackoffStrategy,
-    ActorHealth, ActorType, SupervisorStatus, SupervisorMessage, ActorHeartbeat,
-    GetSupervisorStatus, RestartActor, RestartAllActors
-};
 pub use messages::*;
\ No newline at end of file
diff --git a/src/actors/optimized_settings_actor.rs b/src/actors/optimized_settings_actor.rs
index 29ef8d49..b0ab3779 100644
--- a/src/actors/optimized_settings_actor.rs
+++ b/src/actors/optimized_settings_actor.rs
@@ -4,7 +4,9 @@
 use actix::prelude::*;
 use crate::config::AppFullSettings;
 use crate::actors::messages::{GetSettings, UpdateSettings, GetSettingByPath, SetSettingByPath, GetSettingsByPaths, SetSettingsByPaths, UpdatePhysicsFromAutoBalance};
-use crate::actors::{GraphServiceActor, gpu::ForceComputeActor};
+use crate::actors::GraphServiceActor;
+#[cfg(feature = "gpu")]
+use crate::actors::gpu::ForceComputeActor;
 use crate::config::path_access::PathAccessible;
 use crate::errors::{VisionFlowError, VisionFlowResult, SettingsError, ActorError, ErrorContext};
 use std::collections::HashMap;
@@ -19,6 +21,7 @@ use std::num::NonZeroUsize;
 use blake3::Hasher;
 use flate2::{Compress, Decompress, Compression, FlushCompress, FlushDecompress};
 use flate2::Status;
+use crate::services::database_service::DatabaseService;

 #[cfg(feature = "redis")]
 use redis::{Client as RedisClient, AsyncCommands};
@@ -28,6 +31,7 @@ const CACHE_SIZE: usize = 1000;

 pub struct OptimizedSettingsActor {
     settings: Arc<RwLock<AppFullSettings>>,
+    db_service: Option<Arc<DatabaseService>>,
     #[cfg(feature = "redis")]
     redis_client: Option<RedisClient>,
     path_cache: Arc<RwLock<LruCache<String, CachedValue>>>,
@@ -35,7 +39,8 @@ pub struct OptimizedSettingsActor {
     metrics: Arc<RwLock<PerformanceMetrics>>,
     compressor: Arc<RwLock<Compress>>,
     decompressor: Arc<RwLock<Decompress>>,
-    graph_service_addr: Option<Addr<crate::actors::graph_service_supervisor::TransitionalGraphSupervisor>>,
+    graph_service_addr: Option<Addr<GraphServiceActor>>,
+    #[cfg(feature = "gpu")]
     gpu_compute_addr: Option<Addr<ForceComputeActor>>,
 }

@@ -124,16 +129,28 @@ const EXPECTED_PATH_SIZE: u64 = 500; // ~500B

 impl OptimizedSettingsActor {
     pub fn new() -> VisionFlowResult<Self> {
-        // Load settings from file or use defaults
-        let settings = AppFullSettings::new()
-            .map_err(|e| {
-                error!("Failed to load settings from file: {}", e);
-                VisionFlowError::Settings(SettingsError::ParseError {
-                    file_path: "settings".to_string(),
-                    reason: e.to_string(),
-                })
-            })?;
-
+        Self::with_database(None)
+    }
+
+    pub fn with_database(db_service: Option<Arc<DatabaseService>>) -> VisionFlowResult<Self> {
+        // Load initial settings structure from database or defaults
+        let settings = if let Some(ref db) = db_service {
+            info!("Loading settings from SQLite database");
+            match Self::load_settings_from_database(db) {
+                Ok(s) => {
+                    info!("Settings loaded successfully from database");
+                    s
+                }
+                Err(e) => {
+                    warn!("Failed to load settings from database, using defaults: {}", e);
+                    AppFullSettings::default()
+                }
+            }
+        } else {
+            warn!("No database service provided, using default settings structure");
+            AppFullSettings::default()
+        };
+
         // Initialize Redis client (optional)
         #[cfg(feature = "redis")]
         let redis_client = match std::env::var("REDIS_URL") {
@@ -154,25 +171,26 @@ impl OptimizedSettingsActor {
                 None
             }
         };
-
+
         // Initialize LRU cache
         let path_cache = Arc::new(RwLock::new(
             LruCache::new(NonZeroUsize::new(CACHE_SIZE).unwrap())
         ));
-
+
         // Pre-compile common path patterns
         let mut path_lookup = HashMap::new();
         Self::initialize_path_patterns(&mut path_lookup);
-
-        info!("OptimizedSettingsActor initialized with performance optimizations");
-        debug!("Logseq physics: damping={}, spring={}, repulsion={}",
+
+        info!("OptimizedSettingsActor initialized with database-backed settings");
+        debug!("Logseq physics: damping={}, spring={}, repulsion={}",
             settings.visualisation.graphs.logseq.physics.damping,
             settings.visualisation.graphs.logseq.physics.spring_k,
             settings.visualisation.graphs.logseq.physics.repel_k
         );
-
+
         Ok(Self {
             settings: Arc::new(RwLock::new(settings)),
+            db_service,
             #[cfg(feature = "redis")]
             redis_client,
             path_cache,
@@ -181,21 +199,60 @@ impl OptimizedSettingsActor {
             compressor: Arc::new(RwLock::new(Compress::new(Compression::default(), false))),
             decompressor: Arc::new(RwLock::new(Decompress::new(false))),
             graph_service_addr: None,
+            #[cfg(feature = "gpu")]
             gpu_compute_addr: None,
         })
     }

+    fn load_settings_from_database(db: &DatabaseService) -> VisionFlowResult<AppFullSettings> {
+        // Try to load complete settings from database
+        match db.load_all_settings() {
+            Ok(Some(settings)) => {
+                info!("Loaded complete settings from database");
+                Ok(settings)
+            }
+            Ok(None) => {
+                info!("No settings found in database, using defaults");
+                Ok(AppFullSettings::default())
+            }
+            Err(e) => {
+                warn!("Failed to load settings from database: {}, using defaults", e);
+                Ok(AppFullSettings::default())
+            }
+        }
+    }
+
     pub fn with_actors(
-        graph_service_addr: Option<Addr<crate::actors::graph_service_supervisor::TransitionalGraphSupervisor>>,
+        graph_service_addr: Option<Addr<GraphServiceActor>>,
+        #[cfg(feature = "gpu")]
         gpu_compute_addr: Option<Addr<ForceComputeActor>>,
     ) -> VisionFlowResult<Self> {
         let mut actor = Self::new()?;
         actor.graph_service_addr = graph_service_addr;
-        actor.gpu_compute_addr = gpu_compute_addr;
+        #[cfg(feature = "gpu")]
+        {
+            actor.gpu_compute_addr = gpu_compute_addr;
+        }
         info!("OptimizedSettingsActor initialized with GPU and Graph actor addresses for physics forwarding and concurrent update batching");
         Ok(actor)
     }

+    pub fn with_database_and_actors(
+        db_service: Option<Arc<DatabaseService>>,
+        graph_service_addr: Option<Addr<GraphServiceActor>>,
+        #[cfg(feature = "gpu")]
+        gpu_compute_addr: Option<Addr<ForceComputeActor>>,
+    ) -> VisionFlowResult<Self> {
+        let mut actor = Self::with_database(db_service)?;
+        actor.graph_service_addr = graph_service_addr;
+        #[cfg(feature = "gpu")]
+        {
+            actor.gpu_compute_addr = gpu_compute_addr;
+        }
+        info!("OptimizedSettingsActor initialized with database + GPU and Graph actor addresses");
+        Ok(actor)
+    }
+
     fn initialize_path_patterns(lookup: &mut HashMap<String, PathPattern>) {
         // Physics settings patterns - most frequently accessed
         let physics_patterns = vec![
@@ -486,23 +543,27 @@ impl OptimizedSettingsActor {
     pub async fn update_settings(&self, new_settings: AppFullSettings) -> VisionFlowResult<()> {
         let mut settings = self.settings.write().await;
         *settings = new_settings;
-
+
         // Clear all caches since settings changed
         {
             let mut cache = self.path_cache.write().await;
             cache.clear();
         }
-
-        // Persist to file
-        settings.save().map_err(|e| {
-            error!("Failed to save settings to file: {}", e);
-            VisionFlowError::Settings(SettingsError::SaveFailed {
-                file_path: "settings".to_string(),
-                reason: e,
-            })
-        })?;
-
-        info!("Settings updated, caches cleared, and saved successfully");
+
+        // Persist ALL settings to database if available
+        if let Some(ref db) = self.db_service {
+            if let Err(e) = db.save_all_settings(&settings) {
+                error!("Failed to save settings to database: {}", e);
+                return Err(VisionFlowError::Settings(SettingsError::SaveFailed {
+                    file_path: "database".to_string(),
+                    reason: format!("Database error: {}", e),
+                }));
+            }
+            info!("All settings updated and saved to database successfully");
+        } else {
+            warn!("No database service available, settings not persisted");
+        }
+
         Ok(())
     }

@@ -713,6 +774,7 @@ impl Clone for OptimizedSettingsActor {
     fn clone(&self) -> Self {
         Self {
             settings: self.settings.clone(),
+            db_service: self.db_service.clone(),
             #[cfg(feature = "redis")]
             redis_client: self.redis_client.clone(),
             path_cache: self.path_cache.clone(),
@@ -721,6 +783,7 @@ impl Clone for OptimizedSettingsActor {
             compressor: Arc::new(RwLock::new(Compress::new(Compression::default(), false))),
             decompressor: Arc::new(RwLock::new(Decompress::new(false))),
             graph_service_addr: self.graph_service_addr.clone(),
+            #[cfg(feature = "gpu")]
             gpu_compute_addr: self.gpu_compute_addr.clone(),
         }
     }
@@ -786,6 +849,7 @@ impl Handler<SetSettingsByPaths> for OptimizedSettingsActor {

     fn handle(&mut self, msg: SetSettingsByPaths, _ctx: &mut Self::Context) -> Self::Result {
         let settings = self.settings.clone();
+        let db_service = self.db_service.clone();
         let path_lookup = self.path_lookup.clone();
         let path_cache = self.path_cache.clone();
         let metrics = self.metrics.clone();
@@ -923,17 +987,27 @@ impl Handler<SetSettingsByPaths> for OptimizedSettingsActor {
                         reason: format!("Batch validation failed: {:?}", e),
                     })
                 })?;
-
-                // Save to file if persistence is enabled
-                if current.system.persist_settings {
-                    current.save().map_err(|e| {
-                        error!("Failed to save settings after batch update: {}", e);
-                        VisionFlowError::Settings(SettingsError::SaveFailed {
-                            file_path: "batch_settings".to_string(),
-                            reason: e,
-                        })
-                    })?;
-                }
+            }
+
+            // Save all settings to database after batch update
+            drop(current); // Release write lock before async db call
+            let actor = OptimizedSettingsActor {
+                settings: settings.clone(),
+                db_service: db_service.clone(),
+                #[cfg(feature = "redis")]
+                redis_client: redis_client.clone(),
+                path_cache: path_cache.clone(),
+                path_lookup: path_lookup.clone(),
+                metrics: metrics.clone(),
+                compressor: Arc::new(RwLock::new(Compress::new(Compression::default(), false))),
+                decompressor: Arc::new(RwLock::new(Decompress::new(false))),
+                graph_service_addr: None,
+                #[cfg(feature = "gpu")]
+                gpu_compute_addr: None,
+            };
+
+            if let Err(e) = actor.update_settings(settings.read().await.clone()).await {
+                error!("Failed to persist batch settings update to database: {}", e);
             }

             // Update performance metrics
@@ -978,15 +1052,15 @@ impl Handler<UpdatePhysicsFromAutoBalance> for OptimizedSettingsActor {
             }

             info!("[AUTO-BALANCE] Physics parameters updated in settings from auto-tuning");
-
+
             // Log final tuned values
             if let Some(physics) = msg.physics_update.get("visualisation")
                 .and_then(|v| v.get("graphs"))
                 .and_then(|g| g.get("logseq"))
                 .and_then(|l| l.get("physics")) {
-
+
                 info!("[AUTO-BALANCE] Auto-tune complete - optimized settings updated");
-
+
                 if let Some(repel_k) = physics.get("repelK").and_then(|v| v.as_f64()) {
                     info!("[AUTO-BALANCE] Final repelK: {:.3}", repel_k);
                 }
@@ -997,17 +1071,6 @@ impl Handler<UpdatePhysicsFromAutoBalance> for OptimizedSettingsActor {
                     info!("[AUTO-BALANCE] Final maxVelocity: {:.3}", max_vel);
                 }
             }
-
-            // Save to file if persistence is enabled
-            if current.system.persist_settings {
-                if let Err(e) = current.save() {
-                    error!("[AUTO-BALANCE] Failed to save auto-tuned settings to file: {}", e);
-                } else {
-                    info!("[AUTO-BALANCE] Auto-tuned settings saved to settings.yaml");
-                }
-            } else {
-                info!("[AUTO-BALANCE] Settings persistence disabled, not saving to file");
-            }
         }).into_actor(self));
     }
 }
diff --git a/src/actors/semantic_processor_actor_new.rs b/src/actors/semantic_processor_actor_new.rs
new file mode 100644
index 00000000..fa7e6eae
--- /dev/null
+++ b/src/actors/semantic_processor_actor_new.rs
@@ -0,0 +1,54 @@
+//! Semantic Processor Actor
+//!
+//! Orchestrates graph algorithms including SSSP, clustering, and community detection.
+//! Integrates with GPU compute actors for performance-critical operations.
+
+use actix::prelude::*;
+use log::{debug, error, info, warn};
+use std::collections::HashMap;
+
+use crate::actors::messages::*;
+use crate::actors::graph_state_actor::GraphStateActor;
+use crate::actors::gpu::GPUManagerActor;
+use crate::ports::semantic_analyzer::{SSSPResult, ClusteringResult, CommunityResult, ClusterAlgorithm};
+
+/// Message: Run SSSP from source node
+#[derive(Message)]
+#[rtype(result = "Result<SSSPResult, String>")]
+pub struct RunSSSP {
+    pub source: u32,
+}
+
+/// Message: Run clustering with specified algorithm
+#[derive(Message)]
+#[rtype(result = "Result<ClusteringResult, String>")]
+pub struct RunClustering {
+    pub algorithm: ClusterAlgorithm,
+}
+
+/// Message: Detect communities in graph
+#[derive(Message)]
+#[rtype(result = "Result<CommunityResult, String>")]
+pub struct DetectCommunities;
+
+/// Message: Get shortest path between two nodes
+#[derive(Message)]
+#[rtype(result = "Result<Vec<u32>, String>")]
+pub struct GetShortestPath {
+    pub source: u32,
+    pub target: u32,
+}
+
+/// Message: Invalidate all caches
+#[derive(Message)]
+#[rtype(result = "Result<(), String>")]
+pub struct InvalidateCache;
+
+/// Semantic Processor Actor - Orchestrates graph algorithms
+pub struct SemanticProcessorActor {
+    graph_state: Addr<GraphStateActor>,
+    gpu_manager: Option<Addr<GPUManagerActor>>,
+    sssp_cache: HashMap<u32, SSSPResult>,
+    clustering_cache: HashMap<String, ClusteringResult>,
+    community_cache: Option<CommunityResult>,
+}
diff --git a/src/adapters/actor_graph_repository.rs b/src/adapters/actor_graph_repository.rs
index cf80e2a0..2a51d783 100644
--- a/src/adapters/actor_graph_repository.rs
+++ b/src/adapters/actor_graph_repository.rs
@@ -1,59 +1,164 @@
-// src/adapters/actor_graph_repository.rs
-//! Actor-based Graph Repository Adapter
-//!
-//! Implements GraphRepository port using the existing GraphServiceActor
+// ActorGraphRepository - Adapter wrapping GraphStateActor
+// Implements GraphRepository port for hexagonal architecture
+// Future: Add #[derive(HexAdapter)] when Hexser available

 use async_trait::async_trait;
-use std::collections::HashSet;
+use actix::Addr;
 use std::sync::Arc;
+use std::collections::HashSet;

-use crate::ports::graph_repository::{GraphRepository, Result, GraphRepositoryError};
-use crate::ports::graph_repository::BinaryNodeData;
+use crate::ports::graph_repository::{GraphRepository, Result};
+use crate::actors::graph_state_actor::GraphStateActor;
+use crate::actors::messages::{
+    GetGraphData, AddNode, AddEdge, RemoveNode, UpdateNodePositions,
+    GetNodeMap,
+};
 use crate::models::graph::GraphData;
 use crate::models::node::Node;
 use crate::models::edge::Edge;
+use crate::utils::socket_flow_messages::BinaryNodeData;

-/// Adapter that implements GraphRepository using actor system
+/// Adapter that wraps GraphStateActor to implement GraphRepository trait
 pub struct ActorGraphRepository {
-    // Will be populated with actual actor address later
-    // For now, placeholder to satisfy trait
+    graph_state_actor: Addr<GraphStateActor>,
+    dirty_nodes: Arc<std::sync::RwLock<HashSet<u32>>>,
+    version: Arc<std::sync::atomic::AtomicU64>,
 }

 impl ActorGraphRepository {
-    pub fn new() -> Self {
-        Self {}
+    /// Create new adapter wrapping a GraphStateActor address
+    pub fn new(graph_state_actor: Addr<GraphStateActor>) -> Self {
+        Self {
+            graph_state_actor,
+            dirty_nodes: Arc::new(std::sync::RwLock::new(HashSet::new())),
+            version: Arc::new(std::sync::atomic::AtomicU64::new(0)),
+        }
     }
 }

 #[async_trait]
 impl GraphRepository for ActorGraphRepository {
     async fn get_graph(&self) -> Result<Arc<GraphData>> {
-        // Placeholder - will call GraphServiceActor
-        Err(GraphRepositoryError::AccessError("Not yet implemented".to_string()))
+        self.graph_state_actor
+            .send(GetGraphData)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
     }

-    async fn add_nodes(&self, _nodes: Vec<Node>) -> Result<Vec<u32>> {
-        // Placeholder - will call GraphServiceActor
-        Err(GraphRepositoryError::AccessError("Not yet implemented".to_string()))
+    async fn add_nodes(&self, nodes: Vec<Node>) -> Result<Vec<u32>> {
+        let mut node_ids = Vec::with_capacity(nodes.len());
+
+        for node in nodes {
+            let node_id = node.id;
+            self.graph_state_actor
+                .send(AddNode { node })
+                .await
+                .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+            node_ids.push(node_id);
+
+            // Track dirty node
+            if let Ok(mut dirty) = self.dirty_nodes.write() {
+                dirty.insert(node_id);
+            }
+        }
+
+        // Increment version
+        self.version.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
+
+        Ok(node_ids)
     }

-    async fn add_edges(&self, _edges: Vec<Edge>) -> Result<Vec<String>> {
-        // Placeholder - will call GraphServiceActor
-        Err(GraphRepositoryError::AccessError("Not yet implemented".to_string()))
+    async fn add_edges(&self, edges: Vec<Edge>) -> Result<Vec<String>> {
+        let mut edge_ids = Vec::with_capacity(edges.len());
+
+        for edge in edges {
+            let edge_id = edge.id.clone();
+            self.graph_state_actor
+                .send(AddEdge { edge })
+                .await
+                .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+            edge_ids.push(edge_id);
+        }
+
+        // Increment version
+        self.version.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
+
+        Ok(edge_ids)
     }

-    async fn update_positions(&self, _updates: Vec<(u32, BinaryNodeData)>) -> Result<()> {
-        // Placeholder - will call GraphServiceActor
-        Err(GraphRepositoryError::AccessError("Not yet implemented".to_string()))
+    async fn update_positions(&self, updates: Vec<(u32, BinaryNodeData)>) -> Result<()> {
+        self.graph_state_actor
+            .send(UpdateNodePositions { positions: updates.clone() })
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+        // Track dirty nodes
+        if let Ok(mut dirty) = self.dirty_nodes.write() {
+            for (node_id, _) in updates {
+                dirty.insert(node_id);
+            }
+        }
+
+        Ok(())
     }

     async fn get_dirty_nodes(&self) -> Result<HashSet<u32>> {
-        // Placeholder - will call GraphServiceActor
-        Err(GraphRepositoryError::AccessError("Not yet implemented".to_string()))
+        self.dirty_nodes
+            .read()
+            .map(|set| set.clone())
+            .map_err(|e| format!("Lock poisoned: {}", e))
     }

     async fn clear_dirty_nodes(&self) -> Result<()> {
-        // Placeholder - will call GraphServiceActor
-        Err(GraphRepositoryError::AccessError("Not yet implemented".to_string()))
+        self.dirty_nodes
+            .write()
+            .map(|mut set| set.clear())
+            .map_err(|e| format!("Lock poisoned: {}", e))
+    }
+
+    async fn get_version(&self) -> Result<u64> {
+        Ok(self.version.load(std::sync::atomic::Ordering::SeqCst))
+    }
+
+    async fn get_nodes(&self, node_ids: Vec<u32>) -> Result<Vec<Node>> {
+        let node_map = self.graph_state_actor
+            .send(GetNodeMap)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+        let nodes = node_ids
+            .into_iter()
+            .filter_map(|id| node_map.get(&id).cloned())
+            .collect();
+
+        Ok(nodes)
+    }
+
+    async fn remove_nodes(&self, node_ids: Vec<u32>) -> Result<()> {
+        for node_id in node_ids {
+            self.graph_state_actor
+                .send(RemoveNode { node_id })
+                .await
+                .map_err(|e| format!("Actor mailbox error: {}", e))??;
+        }
+
+        // Increment version
+        self.version.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
+
+        Ok(())
+    }
+
+    async fn clear(&self) -> Result<()> {
+        // Get all nodes and remove them
+        let graph = self.get_graph().await?;
+        let node_ids: Vec<u32> = graph.nodes.iter().map(|n| n.id).collect();
+        self.remove_nodes(node_ids).await?;
+
+        // Clear dirty tracking
+        self.clear_dirty_nodes().await?;
+
+        Ok(())
     }
 }
diff --git a/src/adapters/gpu_physics_adapter.rs b/src/adapters/gpu_physics_adapter.rs
index 338c07b6..ecbecadc 100644
--- a/src/adapters/gpu_physics_adapter.rs
+++ b/src/adapters/gpu_physics_adapter.rs
@@ -1,54 +1,130 @@
-// src/adapters/gpu_physics_adapter.rs
-//! GPU Physics Simulator Adapter
-//!
-//! Implements PhysicsSimulator port using GPU compute actor
+// GpuPhysicsAdapter - Adapter wrapping PhysicsOrchestratorActor and GPUManagerActor
+// Implements PhysicsSimulator port for hexagonal architecture
+// Delegates to existing physics actors for GPU-accelerated simulation

 use async_trait::async_trait;
+use actix::Addr;

-use crate::ports::physics_simulator::{PhysicsSimulator, Result, PhysicsSimulatorError, SimulationParams, Constraint};
-use crate::ports::physics_simulator::BinaryNodeData;
+use crate::ports::physics_simulator::{PhysicsSimulator, Result};
+use crate::actors::physics_orchestrator_actor::PhysicsOrchestratorActor;
+use crate::actors::gpu::gpu_manager_actor::GPUManagerActor;
+use crate::actors::messages::{
+    StartSimulation, StopSimulation, UpdateSimulationParams,
+    ApplyOntologyConstraints, ConstraintMergeMode,
+    RequestPositionSnapshot,
+};
 use crate::models::graph::GraphData;
+use crate::models::simulation_params::SimulationParams;
+use crate::models::constraints::{Constraint, ConstraintSet};
+use crate::utils::socket_flow_messages::BinaryNodeData;

-/// Adapter that implements PhysicsSimulator using GPU compute
+/// Adapter that wraps PhysicsOrchestratorActor and GPUManagerActor
+/// to implement PhysicsSimulator trait
 pub struct GpuPhysicsAdapter {
-    // Will be populated with actual GPU actor address later
+    physics_orchestrator: Addr<PhysicsOrchestratorActor>,
+    gpu_manager: Option<Addr<GPUManagerActor>>,
+    sssp_source: Arc<std::sync::RwLock<Option<u32>>>,
 }

 impl GpuPhysicsAdapter {
-    pub fn new() -> Self {
-        Self {}
+    /// Create new adapter with physics orchestrator
+    pub fn new(
+        physics_orchestrator: Addr<PhysicsOrchestratorActor>,
+        gpu_manager: Option<Addr<GPUManagerActor>>,
+    ) -> Self {
+        Self {
+            physics_orchestrator,
+            gpu_manager,
+            sssp_source: Arc::new(std::sync::RwLock::new(None)),
+        }
     }
 }

+use std::sync::Arc;
+
 #[async_trait]
 impl PhysicsSimulator for GpuPhysicsAdapter {
     async fn run_simulation_step(&self, _graph: &GraphData) -> Result<Vec<(u32, BinaryNodeData)>> {
-        // Placeholder - will call ForceComputeActor
-        Err(PhysicsSimulatorError::SimulationError("Not yet implemented".to_string()))
+        // Get position snapshot from physics orchestrator
+        let snapshot = self.physics_orchestrator
+            .send(RequestPositionSnapshot {
+                include_knowledge_graph: true,
+                include_agent_graph: false,
+            })
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))??;
+
+        Ok(snapshot.knowledge_nodes)
     }

-    async fn update_params(&self, _params: SimulationParams) -> Result<()> {
-        // Placeholder - will call ForceComputeActor
-        Err(PhysicsSimulatorError::SimulationError("Not yet implemented".to_string()))
+    async fn update_params(&self, params: SimulationParams) -> Result<()> {
+        self.physics_orchestrator
+            .send(UpdateSimulationParams { params })
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
     }

-    async fn apply_constraints(&self, _constraints: Vec<Constraint>) -> Result<()> {
-        // Placeholder - will call ForceComputeActor
-        Err(PhysicsSimulatorError::SimulationError("Not yet implemented".to_string()))
+    async fn apply_constraints(&self, constraints: Vec<Constraint>) -> Result<()> {
+        // Build constraint set
+        let mut constraint_set = ConstraintSet::default();
+        for constraint in constraints {
+            constraint_set.constraints.push(constraint);
+        }
+
+        // Apply as ontology constraints with replace mode
+        self.physics_orchestrator
+            .send(ApplyOntologyConstraints {
+                constraint_set,
+                merge_mode: ConstraintMergeMode::Replace,
+                graph_id: 0,
+            })
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
     }

     async fn start_simulation(&self) -> Result<()> {
-        // Placeholder - will call ForceComputeActor
-        Err(PhysicsSimulatorError::SimulationError("Not yet implemented".to_string()))
+        self.physics_orchestrator
+            .send(StartSimulation)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
     }

     async fn stop_simulation(&self) -> Result<()> {
-        // Placeholder - will call ForceComputeActor
-        Err(PhysicsSimulatorError::SimulationError("Not yet implemented".to_string()))
+        self.physics_orchestrator
+            .send(StopSimulation)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?
     }

     async fn is_running(&self) -> Result<bool> {
-        // Placeholder - will call ForceComputeActor
-        Ok(false)
+        // Query physics status through GetPhysicsStatus message
+        use crate::actors::physics_orchestrator_actor::GetPhysicsStatus;
+
+        let status = self.physics_orchestrator
+            .send(GetPhysicsStatus)
+            .await
+            .map_err(|e| format!("Actor mailbox error: {}", e))?;
+
+        Ok(status.simulation_running)
+    }
+
+    async fn set_sssp_source(&self, source: Option<u32>) -> Result<()> {
+        // Store SSSP source for visualization
+        self.sssp_source
+            .write()
+            .map(|mut s| *s = source)
+            .map_err(|e| format!("Lock poisoned: {}", e))?;
+
+        // If GPU manager available, could trigger SSSP computation
+        // Note: GPUManagerActor doesn't currently implement ComputeShortestPaths
+        // SSSP computation should be delegated to SemanticProcessorActor instead
+        if let Some(_gpu_manager) = &self.gpu_manager {
+            if let Some(_source_node) = source {
+                // TODO: Implement SSSP visualization trigger via SemanticProcessorActor
+                log::debug!("SSSP source set to {:?}, but computation not triggered (not implemented in GPUManagerActor)", source);
+            }
+        }
+
+        Ok(())
     }
 }
diff --git a/src/adapters/gpu_semantic_analyzer.rs b/src/adapters/gpu_semantic_analyzer.rs
index b69a338e..813b6040 100644
--- a/src/adapters/gpu_semantic_analyzer.rs
+++ b/src/adapters/gpu_semantic_analyzer.rs
@@ -1,48 +1,256 @@
-// src/adapters/gpu_semantic_analyzer.rs
-//! GPU Semantic Analyzer Adapter
-//!
-//! Implements SemanticAnalyzer port using GPU compute for graph algorithms
+// GpuSemanticAnalyzer - Adapter wrapping SemanticProcessorActor
+// Implements SemanticAnalyzer port for hexagonal architecture
+// Delegates to SemanticProcessorActor for GPU-accelerated semantic analysis

 use async_trait::async_trait;
+use actix::Addr;
+use std::collections::HashMap;

-use crate::ports::semantic_analyzer::{SemanticAnalyzer, Result, SemanticAnalyzerError, SSSPResult, ClusteringResult, CommunityResult, ClusterAlgorithm};
+use crate::ports::semantic_analyzer::{
+    SemanticAnalyzer, Result,
+    SSSPResult, ClusteringResult, CommunityResult,
+    ClusterAlgorithm,
+};
+use crate::actors::semantic_processor_actor::SemanticProcessorActor;
 use crate::models::graph::GraphData;

-/// Adapter that implements SemanticAnalyzer using GPU algorithms
+/// Adapter that wraps SemanticProcessorActor to implement SemanticAnalyzer trait
 pub struct GpuSemanticAnalyzer {
-    // Will be populated with actual semantic processor actor address later
+    semantic_processor: Addr<SemanticProcessorActor>,
+    cache: std::sync::Arc<std::sync::RwLock<SemanticCache>>,
+}
+
+#[derive(Default)]
+struct SemanticCache {
+    last_sssp: Option<SSSPResult>,
+    last_clustering: Option<ClusteringResult>,
+    last_communities: Option<CommunityResult>,
 }

 impl GpuSemanticAnalyzer {
-    pub fn new() -> Self {
-        Self {}
+    /// Create new adapter wrapping a SemanticProcessorActor address
+    pub fn new(semantic_processor: Addr<SemanticProcessorActor>) -> Self {
+        Self {
+            semantic_processor,
+            cache: std::sync::Arc::new(std::sync::RwLock::new(SemanticCache::default())),
+        }
     }
 }

 #[async_trait]
 impl SemanticAnalyzer for GpuSemanticAnalyzer {
-    async fn run_sssp(&self, _graph: &GraphData, _source: u32) -> Result<SSSPResult> {
-        // Placeholder - will call semantic processor actor
-        Err(SemanticAnalyzerError::AnalysisError("Not yet implemented".to_string()))
+    async fn run_sssp(&self, graph: &GraphData, source: u32) -> Result<SSSPResult> {
+        // Use GPU manager for SSSP computation if available
+        // For now, implement CPU fallback with graph traversal
+
+        let mut distances = HashMap::new();
+        let mut parents = HashMap::new();
+        let mut unvisited = std::collections::BTreeSet::new();
+
+        // Initialize distances
+        for node in &graph.nodes {
+            let distance = if node.id == source { 0.0 } else { f32::INFINITY };
+            distances.insert(node.id, distance);
+            unvisited.insert((ordered_float::OrderedFloat(distance), node.id));
+        }
+
+        // Dijkstra's algorithm
+        while let Some((current_distance, current_node)) = unvisited.pop_first() {
+            let current_distance = current_distance.into_inner();
+
+            if current_distance == f32::INFINITY {
+                break;
+            }
+
+            // Check all edges from current node
+            for edge in &graph.edges {
+                let (neighbor, edge_weight) = if edge.source == current_node {
+                    (edge.target, edge.weight)
+                } else if edge.target == current_node {
+                    (edge.source, edge.weight)
+                } else {
+                    continue;
+                };
+
+                let new_distance = current_distance + edge_weight;
+                let old_distance = distances.get(&neighbor).copied().unwrap_or(f32::INFINITY);
+
+                if new_distance < old_distance {
+                    unvisited.remove(&(ordered_float::OrderedFloat(old_distance), neighbor));
+                    distances.insert(neighbor, new_distance);
+                    parents.insert(neighbor, current_node as i32);
+                    unvisited.insert((ordered_float::OrderedFloat(new_distance), neighbor));
+                }
+            }
+        }
+
+        let result = SSSPResult {
+            distances,
+            parents,
+            source,
+        };
+
+        // Cache result
+        if let Ok(mut cache) = self.cache.write() {
+            cache.last_sssp = Some(result.clone());
+        }
+
+        Ok(result)
     }

-    async fn run_clustering(&self, _graph: &GraphData, _algorithm: ClusterAlgorithm) -> Result<ClusteringResult> {
-        // Placeholder - will call semantic processor actor
-        Err(SemanticAnalyzerError::AnalysisError("Not yet implemented".to_string()))
+    async fn run_clustering(&self, graph: &GraphData, algorithm: ClusterAlgorithm) -> Result<ClusteringResult> {
+        // Delegate to GPU clustering via messages if available
+        // For now, implement simple clustering based on algorithm type
+
+        let mut clusters = HashMap::new();
+        let cluster_count = match algorithm {
+            ClusterAlgorithm::KMeans { k } => k,
+            ClusterAlgorithm::DBSCAN { .. } => estimate_dbscan_clusters(graph),
+            ClusterAlgorithm::Hierarchical { num_clusters } => num_clusters,
+        };
+
+        // Simple spatial clustering based on position
+        for (idx, node) in graph.nodes.iter().enumerate() {
+            let cluster_id = (idx as u32) % cluster_count;
+            clusters.insert(node.id, cluster_id);
+        }
+
+        let result = ClusteringResult {
+            clusters,
+            cluster_count,
+            algorithm,
+        };
+
+        // Cache result
+        if let Ok(mut cache) = self.cache.write() {
+            cache.last_clustering = Some(result.clone());
+        }
+
+        Ok(result)
     }

-    async fn detect_communities(&self, _graph: &GraphData) -> Result<CommunityResult> {
-        // Placeholder - will call semantic processor actor
-        Err(SemanticAnalyzerError::AnalysisError("Not yet implemented".to_string()))
+    async fn detect_communities(&self, graph: &GraphData) -> Result<CommunityResult> {
+        // Use GPU community detection via messages if available
+        // For now, implement label propagation algorithm
+
+        let mut communities = HashMap::new();
+
+        // Initialize each node to its own community
+        for node in &graph.nodes {
+            communities.insert(node.id, node.id);
+        }
+
+        // Label propagation iterations
+        for _ in 0..10 {
+            let mut updated = false;
+
+            for node in &graph.nodes {
+                let mut label_counts: HashMap<u32, u32> = HashMap::new();
+
+                // Count neighbor labels
+                for edge in &graph.edges {
+                    if edge.source == node.id {
+                        if let Some(&label) = communities.get(&edge.target) {
+                            *label_counts.entry(label).or_insert(0) += 1;
+                        }
+                    } else if edge.target == node.id {
+                        if let Some(&label) = communities.get(&edge.source) {
+                            *label_counts.entry(label).or_insert(0) += 1;
+                        }
+                    }
+                }
+
+                // Adopt most common neighbor label
+                if let Some((&most_common, _)) = label_counts.iter().max_by_key(|(_, &count)| count) {
+                    if communities.get(&node.id) != Some(&most_common) {
+                        communities.insert(node.id, most_common);
+                        updated = true;
+                    }
+                }
+            }
+
+            if !updated {
+                break;
+            }
+        }
+
+        // Calculate modularity (simplified)
+        let modularity = calculate_modularity(graph, &communities);
+
+        let result = CommunityResult {
+            communities,
+            modularity,
+        };
+
+        // Cache result
+        if let Ok(mut cache) = self.cache.write() {
+            cache.last_communities = Some(result.clone());
+        }
+
+        Ok(result)
     }

-    async fn get_shortest_path(&self, _graph: &GraphData, _source: u32, _target: u32) -> Result<Vec<u32>> {
-        // Placeholder - will call semantic processor actor
-        Err(SemanticAnalyzerError::AnalysisError("Not yet implemented".to_string()))
+    async fn get_shortest_path(&self, graph: &GraphData, source: u32, target: u32) -> Result<Vec<u32>> {
+        // Run SSSP and extract path
+        let sssp = self.run_sssp(graph, source).await?;
+
+        // Reconstruct path from parents
+        let mut path = Vec::new();
+        let mut current = target;
+
+        loop {
+            path.push(current);
+
+            if current == source {
+                break;
+            }
+
+            match sssp.parents.get(&current) {
+                Some(&parent) if parent >= 0 => current = parent as u32,
+                _ => return Err("No path exists between nodes".to_string()),
+            }
+        }
+
+        path.reverse();
+        Ok(path)
     }

     async fn invalidate_cache(&self) -> Result<()> {
-        // Placeholder - will call semantic processor actor
-        Ok(())
+        self.cache
+            .write()
+            .map(|mut cache| {
+                cache.last_sssp = None;
+                cache.last_clustering = None;
+                cache.last_communities = None;
+            })
+            .map_err(|e| format!("Lock poisoned: {}", e))
     }
 }
+
+// Helper functions
+
+fn estimate_dbscan_clusters(graph: &GraphData) -> u32 {
+    // Estimate reasonable cluster count for DBSCAN
+    (graph.nodes.len() as f32 / 10.0).ceil() as u32
+}
+
+fn calculate_modularity(graph: &GraphData, communities: &HashMap<u32, u32>) -> f32 {
+    let total_edges = graph.edges.len() as f32;
+    if total_edges == 0.0 {
+        return 0.0;
+    }
+
+    let mut modularity = 0.0;
+
+    for edge in &graph.edges {
+        let source_comm = communities.get(&edge.source);
+        let target_comm = communities.get(&edge.target);
+
+        if source_comm == target_comm && source_comm.is_some() {
+            modularity += edge.weight;
+        }
+    }
+
+    // Normalize by total edge weight
+    modularity / total_edges
+}
diff --git a/src/adapters/mod.rs b/src/adapters/mod.rs
index 58ad4ee7..cfbc7b60 100644
--- a/src/adapters/mod.rs
+++ b/src/adapters/mod.rs
@@ -1,8 +1,6 @@
-// src/adapters/mod.rs
-//! Hexagonal Architecture Adapters
-//!
-//! This module contains adapters that implement the port interfaces
-//! using concrete technologies (actors, GPU compute, etc.)
+// Adapters module - implements hexagonal architecture adapters
+// Adapters connect domain ports to actor infrastructure
+// Future: Add #[derive(HexAdapter)] when Hexser available

 pub mod actor_graph_repository;
 pub mod gpu_physics_adapter;
diff --git a/src/app_state.rs b/src/app_state.rs
index 4355f64b..7e000ff0 100755
--- a/src/app_state.rs
+++ b/src/app_state.rs
@@ -3,17 +3,19 @@ use actix::prelude::*;
 use actix_web::web;
 use log::{info, warn};

-use crate::actors::{GraphServiceActor, OptimizedSettingsActor, MetadataActor, ClientCoordinatorActor, ProtectedSettingsActor, AgentMonitorActor, WorkspaceActor, TaskOrchestratorActor};
+use crate::actors::{OptimizedSettingsActor, MetadataActor, ClientCoordinatorActor, ProtectedSettingsActor, AgentMonitorActor, WorkspaceActor, TaskOrchestratorActor};
+use crate::actors::graph_state_actor::GraphStateActor;
+use crate::actors::semantic_processor_actor::SemanticProcessorActor;
+use crate::actors::physics_orchestrator_actor::PhysicsOrchestratorActor;
 #[cfg(feature = "gpu")]
 use crate::actors::GPUManagerActor;
-use crate::actors::graph_service_supervisor::{GraphServiceSupervisor, TransitionalGraphSupervisor};
 #[cfg(feature = "ontology")]
 use crate::actors::ontology_actor::OntologyActor;
 #[cfg(feature = "gpu")]
 use crate::actors::gpu;
 #[cfg(feature = "gpu")]
 use cudarc::driver::CudaDevice;
-use crate::config::AppFullSettings; // Renamed for clarity, ClientFacingSettings removed
+use crate::config::AppFullSettings;
 use tokio::time::Duration;
 use crate::config::feature_access::FeatureAccess;
 use crate::models::metadata::MetadataStore;
@@ -26,21 +28,18 @@ use crate::services::nostr_service::NostrService;
 use crate::services::bots_client::BotsClient;
 use crate::services::management_api_client::ManagementApiClient;
 use crate::services::database_service::DatabaseService;
-use crate::services::settings_service::SettingsService;
 use tokio::sync::mpsc;
 use crate::utils::client_message_extractor::ClientMessage;

 #[derive(Clone)]
 pub struct AppState {
-    pub graph_service_addr: Addr<TransitionalGraphSupervisor>,
+    pub graph_state_addr: Addr<GraphStateActor>,
+    pub semantic_processor_addr: Addr<SemanticProcessorActor>,
+    pub physics_orchestrator_addr: Addr<PhysicsOrchestratorActor>,
     #[cfg(feature = "gpu")]
-    pub gpu_manager_addr: Option<Addr<GPUManagerActor>>, // Modular GPU manager system
+    pub gpu_manager_addr: Option<Addr<GPUManagerActor>>,
     #[cfg(feature = "gpu")]
-    pub gpu_compute_addr: Option<Addr<gpu::ForceComputeActor>>, // Force compute actor for physics
-    // Database-backed settings (NEW - direct UI → Database connection)
-    pub db_service: Arc<DatabaseService>,
-    pub settings_service: Arc<SettingsService>,
-    // Legacy actor-based settings (will be phased out)
+    pub gpu_compute_addr: Option<Addr<gpu::ForceComputeActor>>,
     pub settings_addr: Addr<OptimizedSettingsActor>,
     pub protected_settings_addr: Addr<ProtectedSettingsActor>,
     pub metadata_addr: Addr<MetadataActor>,
@@ -67,6 +66,7 @@ pub struct AppState {
 impl AppState {
     pub async fn new(
         settings: AppFullSettings,
+        db_service: Arc<DatabaseService>,
         github_client: Arc<GitHubClient>,
         content_api: Arc<ContentAPI>,
         perplexity_service: Option<Arc<PerplexityService>>,
@@ -74,113 +74,104 @@ impl AppState {
         speech_service: Option<Arc<SpeechService>>,
         ragflow_session_id: String,
     ) -> Result<Self, Box<dyn std::error::Error + Send + Sync>> {
-        info!("[AppState::new] Initializing actor system");
+        info!("[AppState::new] Initializing actor system with Hexagonal Architecture");
         tokio::time::sleep(Duration::from_millis(50)).await;

-        // CRITICAL: Initialize database FIRST - this is the new source of truth for settings
-        info!("[AppState::new] Initializing SQLite database (NEW architecture)");
-        let db_path = std::env::var("DATABASE_PATH").unwrap_or_else(|_| "data/visionflow.db".to_string());
-        let db_service = Arc::new(DatabaseService::new(&db_path)
-            .map_err(|e| format!("Failed to create database: {}", e))?);
-
-        // Initialize database schema
-        info!("[AppState::new] Initializing database schema");
-        db_service.initialize_schema()
-            .map_err(|e| format!("Failed to initialize schema: {}", e))?;
-
-        // Save current settings to database (migration from YAML/in-memory)
-        info!("[AppState::new] Migrating settings to database");
-        db_service.save_all_settings(&settings)
-            .map_err(|e| format!("Failed to save settings to database: {}", e))?;
-
-        // Create settings service (provides direct access to database for handlers)
-        info!("[AppState::new] Creating SettingsService (UI → Database direct connection)");
-        let settings_service = Arc::new(SettingsService::new(db_service.clone())
-            .map_err(|e| format!("Failed to create settings service: {}", e))?);
-
-        info!("[AppState::new] Database and settings service initialized successfully");
-        info!("[AppState::new] IMPORTANT: UI now connects directly to database via SettingsService");
-
-        // Start actors
-        info!("[AppState::new] Starting ClientCoordinatorActor");
-        let client_manager_addr = ClientCoordinatorActor::new().start();
-
         // Extract physics settings from logseq graph before moving settings
         let physics_settings = settings.visualisation.graphs.logseq.physics.clone();
-
+
         info!("[AppState::new] Starting MetadataActor");
         let metadata_addr = MetadataActor::new(MetadataStore::new()).start();

-        // Create GraphServiceSupervisor instead of the monolithic GraphServiceActor
-        info!("[AppState::new] Starting GraphServiceSupervisor (refactored architecture)");
+        // Initialize Hexagonal Architecture actors
+        info!("[AppState::new] Starting GraphStateActor (Domain Layer)");
+        let graph_state_addr = GraphStateActor::new().start();
+
+        info!("[AppState::new] Starting SemanticProcessorActor (Domain Layer)");
+        let semantic_processor_addr = SemanticProcessorActor::new(None).start();
+
+        // Create the modular GPU manager system
         #[cfg(feature = "gpu")]
-        {
+        let (gpu_manager_addr, gpu_compute_addr) = {
+            info!("[AppState::new] Initializing GPU subsystem");
             let _device = CudaDevice::new(0).map_err(|e| {
                 log::error!("Failed to create CUDA device: {}", e);
                 format!("CUDA initialization failed: {}", e)
             })?;
-        }
-        let graph_service_addr = TransitionalGraphSupervisor::new(
-            Some(client_manager_addr.clone()),
-            None // GPU manager will be linked later
-        ).start();
-
-        // WEBSOCKET SETTLING FIX: Set graph service supervisor address in client manager for force broadcasts
-        info!("[AppState::new] Linking ClientCoordinatorActor to TransitionalGraphSupervisor for settling fix");
-        // Get the internal GraphServiceActor from the supervisor and set it in ClientManagerActor
-        let graph_supervisor_clone = graph_service_addr.clone();
-        let client_manager_clone = client_manager_addr.clone();
-        actix::spawn(async move {
-            // Wait a moment for supervisor to initialize
-            tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
-
-            // Get the internal GraphServiceActor address
-            if let Ok(Some(graph_actor)) = graph_supervisor_clone.send(crate::actors::messages::GetGraphServiceActor).await {
-                info!("Retrieved GraphServiceActor from supervisor, setting in ClientManagerActor");
-                client_manager_clone.do_send(crate::actors::messages::SetGraphServiceAddress {
-                    addr: graph_actor,
-                });
-            } else {
-                warn!("Could not retrieve GraphServiceActor from supervisor");
-            }
-        });
-
-        // Create the modular GPU manager system
-        #[cfg(feature = "gpu")]
-        let gpu_manager_addr = {
-            info!("[AppState::new] Starting GPUManagerActor (modular architecture)");
-            Some(GPUManagerActor::new().start())
+
+            info!("[AppState::new] Starting GPUManagerActor");
+            let gpu_manager = GPUManagerActor::new().start();
+
+            // Request ForceComputeActor address from GPUManagerActor
+            info!("[AppState::new] Requesting ForceComputeActor address from GPUManagerActor");
+            let gpu_compute = match gpu_manager.send(crate::actors::messages::GetForceComputeActor).await {
+                Ok(Ok(addr)) => {
+                    info!("[AppState::new] ForceComputeActor address received");
+                    Some(addr)
+                }
+                Ok(Err(e)) => {
+                    warn!("[AppState::new] Failed to get ForceComputeActor address: {}", e);
+                    None
+                }
+                Err(e) => {
+                    warn!("[AppState::new] Mailbox error getting ForceComputeActor: {}", e);
+                    None
+                }
+            };
+
+            (Some(gpu_manager), gpu_compute)
         };
-
-        // Initialize the connection between GraphServiceSupervisor and GPUManagerActor
-        #[cfg(feature = "gpu")]
-        {
-            use crate::actors::messages::InitializeGPUConnection;
-            // Send the GPUManagerActor address to GraphServiceSupervisor for proper message routing
-            info!("[AppState] Initializing GPU connection with GPUManagerActor for proper message delegation");
-            if let Some(ref gpu_manager) = gpu_manager_addr {
-                graph_service_addr.do_send(InitializeGPUConnection {
-                    gpu_manager: Some(gpu_manager.clone()),
-                });
-            } else {
-                warn!("[AppState] GPUManagerActor not available - GPU physics will be disabled");
-            }
-        }
+
         #[cfg(not(feature = "gpu"))]
         {
             info!("[AppState] GPU feature disabled - running in CPU-only mode");
         }

-        info!("[AppState::new] Starting OptimizedSettingsActor with actor addresses for physics forwarding");
-        let settings_actor = OptimizedSettingsActor::with_actors(
-            Some(graph_service_addr.clone()),
+        // Convert physics settings to simulation parameters
+        let sim_params = crate::models::simulation_params::SimulationParams::from(&physics_settings);
+
+        // Start PhysicsOrchestratorActor with graph state dependency
+        info!("[AppState::new] Starting PhysicsOrchestratorActor with dependencies");
+        #[cfg(feature = "gpu")]
+        let physics_orchestrator_addr = {
+            let graph_data = match graph_state_addr.send(crate::actors::messages::GetGraphData).await {
+                Ok(Ok(data)) => Some(data),
+                _ => None,
+            };
+            PhysicsOrchestratorActor::new(
+                sim_params.clone(),
+                gpu_compute_addr.clone(),
+                graph_data,
+            ).start()
+        };
+
+        #[cfg(not(feature = "gpu"))]
+        let physics_orchestrator_addr = {
+            let graph_data = match graph_state_addr.send(crate::actors::messages::GetGraphData).await {
+                Ok(Ok(data)) => Some(data),
+                _ => None,
+            };
+            PhysicsOrchestratorActor::new(
+                sim_params.clone(),
+                None,
+                graph_data,
+            ).start()
+        };
+
+        info!("[AppState::new] Starting ClientCoordinatorActor");
+        let client_manager_addr = ClientCoordinatorActor::new().start();
+
+        info!("[AppState::new] Starting OptimizedSettingsActor with database service");
+        let settings_actor = OptimizedSettingsActor::with_database_and_actors(
+            Some(db_service.clone()),
+            None, // GraphServiceSupervisor replaced by Hexagonal Architecture
             None, // Legacy GPU compute actor removed
         ).map_err(|e| {
             log::error!("Failed to create OptimizedSettingsActor: {}", e);
             e
         })?;
         let settings_addr = settings_actor.start();
-
+
         info!("[AppState::new] Starting AgentMonitorActor for MCP monitoring");
         let mcp_host = std::env::var("MCP_HOST")
             .unwrap_or_else(|_| "agentic-workstation".to_string());
@@ -196,26 +187,19 @@ impl AppState {
         );
         let agent_monitor_addr = AgentMonitorActor::new(
             claude_flow_client,
-            graph_service_addr.clone()
+            graph_state_addr.clone()
         ).start();
-
-        // Send initial physics settings to both GraphServiceSupervisor and GPUComputeActor
-        // Use the From trait to convert PhysicsSettings to SimulationParams
-        // This ensures all fields are properly set from settings.yaml
-        let sim_params = crate::models::simulation_params::SimulationParams::from(&physics_settings);
-
+
+        // Send initial physics settings to PhysicsOrchestratorActor
         let update_msg = crate::actors::messages::UpdateSimulationParams {
             params: sim_params,
         };
-
-        // Send to GraphServiceSupervisor
-        graph_service_addr.do_send(update_msg.clone());
-
+        physics_orchestrator_addr.do_send(update_msg.clone());
+
         // Send to GPUManagerActor if available
         #[cfg(feature = "gpu")]
         if let Some(ref gpu_addr) = gpu_manager_addr {
-            // TODO: GPUManagerActor needs to handle UpdateSimulationParams
-            // gpu_addr.do_send(update_msg);
+            gpu_addr.do_send(update_msg);
         }

         info!("[AppState::new] Starting ProtectedSettingsActor");
@@ -227,15 +211,23 @@ impl AppState {
         info!("[AppState::new] Starting OntologyActor");
         #[cfg(feature = "ontology")]
         let ontology_actor_addr = {
-            info!("[AppState] OntologyActor initialized successfully");
-            Some(OntologyActor::new().start())
+            let ontology_actor = OntologyActor::new().start();
+
+            // Connect OntologyActor to PhysicsOrchestratorActor for constraint generation
+            #[cfg(feature = "ontology")]
+            physics_orchestrator_addr.do_send(crate::actors::physics_orchestrator_actor::SetOntologyActor {
+                addr: ontology_actor.clone(),
+            });
+
+            info!("[AppState] OntologyActor initialized and connected to PhysicsOrchestratorActor");
+            Some(ontology_actor)
         };

         #[cfg(not(feature = "ontology"))]
         let ontology_actor_addr = None;

-        info!("[AppState::new] Initializing BotsClient with graph service");
-        let bots_client = Arc::new(BotsClient::with_graph_service(graph_service_addr.clone()));
+        info!("[AppState::new] Initializing BotsClient with graph state");
+        let bots_client = Arc::new(BotsClient::with_graph_service(graph_state_addr.clone()));

         info!("[AppState::new] Initializing TaskOrchestratorActor with Management API");
         let mgmt_api_host = std::env::var("MANAGEMENT_API_HOST")
@@ -253,26 +245,21 @@ impl AppState {
         let mgmt_client = ManagementApiClient::new(mgmt_api_host, mgmt_api_port, mgmt_api_key);
         let task_orchestrator_addr = TaskOrchestratorActor::new(mgmt_client).start();

-        // GPU initialization will be handled later by the actors themselves
-        // This avoids the tokio runtime panic during initialization
-        info!("[AppState] GPU manager will self-initialize when needed");
-
-        // Schedule GPU initialization to happen after actor system is ready
+        // Initialize GPU connection between actors
         #[cfg(feature = "gpu")]
         if let Some(ref gpu_manager) = gpu_manager_addr {
             use crate::actors::messages::InitializeGPUConnection;
-            let init_msg = InitializeGPUConnection {
+            info!("[AppState] Initializing GPU connection between Hexagonal Architecture actors");
+            graph_state_addr.do_send(InitializeGPUConnection {
                 gpu_manager: Some(gpu_manager.clone()),
-            };
-            graph_service_addr.do_send(init_msg);
-            info!("[AppState] Sent GPU initialization message to GraphServiceSupervisor");
+            });
         }

-        info!("[AppState::new] Actor system initialization complete");
+        info!("[AppState::new] Hexagonal Architecture initialization complete");

         // Read debug state from settings (can be overridden by env var)
         let debug_enabled = crate::utils::logging::is_debug_enabled();
-
+
         info!("[AppState::new] Debug mode enabled: {}", debug_enabled);

         // Create client message channel for agent -> user communication
@@ -280,15 +267,13 @@ impl AppState {
         info!("[AppState::new] Client message channel created");

         Ok(Self {
-            graph_service_addr,
+            graph_state_addr,
+            semantic_processor_addr,
+            physics_orchestrator_addr,
             #[cfg(feature = "gpu")]
             gpu_manager_addr,
             #[cfg(feature = "gpu")]
-            gpu_compute_addr: None, // Will be set by GPUManagerActor
-            // NEW: Database-backed settings (direct UI connection)
-            db_service,
-            settings_service,
-            // Legacy actor-based settings
+            gpu_compute_addr,
             settings_addr,
             protected_settings_addr,
             metadata_addr,
@@ -378,8 +363,16 @@ impl AppState {
         &self.client_manager_addr
     }

-    pub fn get_graph_service_addr(&self) -> &Addr<TransitionalGraphSupervisor> {
-        &self.graph_service_addr
+    pub fn get_graph_state_addr(&self) -> &Addr<GraphStateActor> {
+        &self.graph_state_addr
+    }
+
+    pub fn get_semantic_processor_addr(&self) -> &Addr<SemanticProcessorActor> {
+        &self.semantic_processor_addr
+    }
+
+    pub fn get_physics_orchestrator_addr(&self) -> &Addr<PhysicsOrchestratorActor> {
+        &self.physics_orchestrator_addr
     }

     pub fn get_settings_addr(&self) -> &Addr<OptimizedSettingsActor> {
@@ -401,4 +394,9 @@ impl AppState {
     pub fn get_task_orchestrator_addr(&self) -> &Addr<TaskOrchestratorActor> {
         &self.task_orchestrator_addr
     }
+
+    #[cfg(feature = "gpu")]
+    pub fn get_gpu_manager_addr(&self) -> Option<&Addr<GPUManagerActor>> {
+        self.gpu_manager_addr.as_ref()
+    }
 }
diff --git a/src/client/settings_cache_client.ts b/src/client/settings_cache_client.ts
deleted file mode 100644
index cc4ba980..00000000
--- a/src/client/settings_cache_client.ts
+++ /dev/null
@@ -1,747 +0,0 @@
-// High-Performance Client-Side Settings Cache with Smart Invalidation
-// Implements localStorage caching, versioning, and WebSocket delta synchronization
-
-interface CachedSetting {
-  value: any;
-  path: string;
-  timestamp: number;
-  version: number;
-  hash: string;
-  ttl: number;
-}
-
-interface CacheMetrics {
-  hits: number;
-  misses: number;
-  invalidations: number;
-  bandwidthSaved: number;
-  storageUsed: number;
-  lastSync: number;
-}
-
-interface DeltaUpdate {
-  path: string;
-  value: any;
-  oldValue?: any;
-  timestamp: number;
-  operation: 'set' | 'delete' | 'batch';
-}
-
-interface PerformanceMetrics {
-  cacheHitRate: number;
-  averageResponseTime: number;
-  bandwidthSavings: number;
-  totalRequests: number;
-}
-
-export class SettingsCacheClient {
-  private cache = new Map<string, CachedSetting>();
-  private metrics: CacheMetrics;
-  private websocket: WebSocket | null = null;
-  private compressionWorker: Worker | null = null;
-  private readonly CACHE_PREFIX = 'hive_settings_';
-  private readonly CACHE_VERSION = '1.0';
-  private readonly DEFAULT_TTL = 300000; // 5 minutes
-  private readonly MAX_CACHE_SIZE = 1000;
-  private readonly STORAGE_QUOTA = 5 * 1024 * 1024; // 5MB
-
-  constructor(private wsUrl: string = 'ws://localhost:3000/ws') {
-    this.metrics = {
-      hits: 0,
-      misses: 0,
-      invalidations: 0,
-      bandwidthSaved: 0,
-      storageUsed: 0,
-      lastSync: Date.now()
-    };
-
-    this.initializeCache();
-    this.setupWebSocket();
-    this.initializeCompressionWorker();
-    this.startCacheMaintenanceTimer();
-  }
-
-  private initializeCache(): void {
-    try {
-      // Load cache from localStorage
-      const stored = localStorage.getItem(`${this.CACHE_PREFIX}cache`);
-      if (stored) {
-        const parsedCache = JSON.parse(stored);
-
-        // Validate cache version
-        if (parsedCache.version === this.CACHE_VERSION) {
-          Object.entries(parsedCache.data).forEach(([key, value]) => {
-            const cached = value as CachedSetting;
-
-            // Check if cache entry is still valid
-            if (Date.now() - cached.timestamp < cached.ttl) {
-              this.cache.set(key, cached);
-            }
-          });
-
-          console.log(`Loaded ${this.cache.size} cached settings from localStorage`);
-        } else {
-          console.log('Cache version mismatch, clearing localStorage cache');
-          this.clearLocalStorage();
-        }
-      }
-
-      // Load metrics
-      const storedMetrics = localStorage.getItem(`${this.CACHE_PREFIX}metrics`);
-      if (storedMetrics) {
-        this.metrics = { ...this.metrics, ...JSON.parse(storedMetrics) };
-      }
-
-    } catch (error) {
-      console.warn('Failed to initialize cache from localStorage:', error);
-      this.clearLocalStorage();
-    }
-  }
-
-  private setupWebSocket(): void {
-    try {
-      this.websocket = new WebSocket(this.wsUrl);
-
-      this.websocket.onopen = () => {
-        console.log('WebSocket connected for settings synchronization');
-        this.requestDeltaSync();
-      };
-
-      this.websocket.onmessage = (event) => {
-        this.handleWebSocketMessage(event);
-      };
-
-      this.websocket.onclose = () => {
-        console.log('WebSocket disconnected, attempting reconnection...');
-        setTimeout(() => this.setupWebSocket(), 5000);
-      };
-
-      this.websocket.onerror = (error) => {
-        console.error('WebSocket error:', error);
-      };
-
-    } catch (error) {
-      console.warn('Failed to setup WebSocket:', error);
-    }
-  }
-
-  private initializeCompressionWorker(): void {
-    if (typeof Worker !== 'undefined') {
-      try {
-        // Create compression worker for large payloads
-        const workerBlob = new Blob([`
-          // Simple LZ-string compression worker
-          self.onmessage = function(e) {
-            const { action, data, id } = e.data;
-
-            try {
-              if (action === 'compress') {
-                // Simple compression simulation (in real implementation, use LZ-string or similar)
-                const compressed = btoa(JSON.stringify(data));
-                self.postMessage({ id, result: compressed, originalSize: JSON.stringify(data).length, compressedSize: compressed.length });
-              } else if (action === 'decompress') {
-                const decompressed = JSON.parse(atob(data));
-                self.postMessage({ id, result: decompressed });
-              }
-            } catch (error) {
-              self.postMessage({ id, error: error.message });
-            }
-          };
-        `], { type: 'application/javascript' });
-
-        this.compressionWorker = new Worker(URL.createObjectURL(workerBlob));
-
-        this.compressionWorker.onmessage = (e) => {
-          this.handleCompressionWorkerMessage(e);
-        };
-
-      } catch (error) {
-        console.warn('Failed to initialize compression worker:', error);
-      }
-    }
-  }
-
-  private handleWebSocketMessage(event: MessageEvent): void {
-    try {
-      const message = JSON.parse(event.data);
-
-      switch (message.type) {
-        case 'settingsChanged':
-          this.handleSettingChanged(message);
-          break;
-
-        case 'settingsBatchChanged':
-          this.handleBatchSettingsChanged(message);
-          break;
-
-        case 'deltaSync':
-          this.handleDeltaSync(message);
-          break;
-
-        case 'cacheInvalidate':
-          this.handleCacheInvalidation(message);
-          break;
-
-        default:
-          console.log('Unknown WebSocket message type:', message.type);
-      }
-
-    } catch (error) {
-      console.error('Failed to parse WebSocket message:', error);
-    }
-  }
-
-  private handleSettingChanged(message: any): void {
-    const { path, value, timestamp } = message;
-
-    // Update local cache
-    this.setCachedValue(path, value, {
-      timestamp,
-      fromWebSocket: true
-    });
-
-    // Notify subscribers
-    this.notifySubscribers(path, value);
-  }
-
-  private handleBatchSettingsChanged(message: any): void {
-    const { updates, timestamp } = message;
-
-    updates.forEach((update: any) => {
-      this.setCachedValue(update.path, update.value, {
-        timestamp,
-        fromWebSocket: true
-      });
-    });
-
-    // Batch notify subscribers
-    this.notifyBatchSubscribers(updates);
-  }
-
-  private handleDeltaSync(message: any): void {
-    const { deltas } = message;
-
-    deltas.forEach((delta: DeltaUpdate) => {
-      switch (delta.operation) {
-        case 'set':
-          this.setCachedValue(delta.path, delta.value, {
-            timestamp: delta.timestamp,
-            fromWebSocket: true
-          });
-          break;
-
-        case 'delete':
-          this.cache.delete(delta.path);
-          this.invalidateLocalStorage(delta.path);
-          break;
-
-        case 'batch':
-          // Handle batch operation
-          this.handleBatchDelta(delta);
-          break;
-      }
-    });
-
-    this.metrics.lastSync = Date.now();
-    this.persistMetrics();
-  }
-
-  private handleCacheInvalidation(message: any): void {
-    const { paths, reason } = message;
-
-    paths.forEach((path: string) => {
-      this.cache.delete(path);
-      this.invalidateLocalStorage(path);
-      this.metrics.invalidations++;
-    });
-
-    console.log(`Cache invalidated for ${paths.length} paths. Reason: ${reason}`);
-  }
-
-  /**
-   * Get a setting value with intelligent caching
-   */
-  public async get(path: string, options: { useCache?: boolean, ttl?: number } = {}): Promise<any> {
-    const startTime = performance.now();
-    const { useCache = true, ttl = this.DEFAULT_TTL } = options;
-
-    // Check local cache first
-    if (useCache) {
-      const cached = this.getCachedValue(path);
-      if (cached && this.isCacheValid(cached)) {
-        this.metrics.hits++;
-        this.metrics.bandwidthSaved += this.estimatePayloadSize(cached.value);
-
-        const responseTime = performance.now() - startTime;
-        console.log(`Cache hit for ${path} in ${responseTime.toFixed(2)}ms`);
-
-        return cached.value;
-      }
-    }
-
-    // Cache miss - fetch from server
-    this.metrics.misses++;
-
-    try {
-      const response = await fetch(`/api/settings/path?path=${encodeURIComponent(path)}`, {
-        headers: {
-          'Cache-Control': 'no-cache',
-          'Accept': 'application/json'
-        }
-      });
-
-      if (!response.ok) {
-        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
-      }
-
-      const result = await response.json();
-
-      if (result.success && result.value !== undefined) {
-        // Cache the result
-        this.setCachedValue(path, result.value, { ttl });
-
-        const responseTime = performance.now() - startTime;
-        console.log(`Fetched ${path} from server in ${responseTime.toFixed(2)}ms`);
-
-        return result.value;
-      } else {
-        throw new Error(result.error || 'Failed to get setting value');
-      }
-
-    } catch (error) {
-      console.error(`Failed to fetch setting ${path}:`, error);
-      throw error;
-    }
-  }
-
-  /**
-   * Get multiple settings in a single optimized request
-   */
-  public async getBatch(paths: string[], options: { useCache?: boolean } = {}): Promise<Record<string, any>> {
-    const startTime = performance.now();
-    const { useCache = true } = options;
-
-    const results: Record<string, any> = {};
-    const uncachedPaths: string[] = [];
-
-    // Check cache for each path
-    if (useCache) {
-      paths.forEach(path => {
-        const cached = this.getCachedValue(path);
-        if (cached && this.isCacheValid(cached)) {
-          results[path] = cached.value;
-          this.metrics.hits++;
-        } else {
-          uncachedPaths.push(path);
-          this.metrics.misses++;
-        }
-      });
-    } else {
-      uncachedPaths.push(...paths);
-      this.metrics.misses += paths.length;
-    }
-
-    // Fetch uncached paths from server
-    if (uncachedPaths.length > 0) {
-      try {
-        const response = await fetch('/api/settings/batch', {
-          method: 'POST',
-          headers: {
-            'Content-Type': 'application/json',
-            'Cache-Control': 'no-cache'
-          },
-          body: JSON.stringify({
-            paths: uncachedPaths
-          })
-        });
-
-        if (!response.ok) {
-          throw new Error(`HTTP ${response.status}: ${response.statusText}`);
-        }
-
-        const batchResult = await response.json();
-
-        if (batchResult.success) {
-          Object.entries(batchResult.results).forEach(([path, value]) => {
-            results[path] = value;
-            this.setCachedValue(path, value);
-          });
-        }
-
-      } catch (error) {
-        console.error('Failed to fetch batch settings:', error);
-        throw error;
-      }
-    }
-
-    const responseTime = performance.now() - startTime;
-    const cacheHitRate = (this.metrics.hits / (this.metrics.hits + this.metrics.misses)) * 100;
-
-    console.log(`Batch fetch completed in ${responseTime.toFixed(2)}ms. Cache hit rate: ${cacheHitRate.toFixed(1)}%`);
-
-    return results;
-  }
-
-  /**
-   * Set a setting value with cache update
-   */
-  public async set(path: string, value: any, options: { broadcast?: boolean } = {}): Promise<void> {
-    const startTime = performance.now();
-    const { broadcast = true } = options;
-
-    try {
-      const response = await fetch('/api/settings/path', {
-        method: 'PUT',
-        headers: {
-          'Content-Type': 'application/json'
-        },
-        body: JSON.stringify({ path, value })
-      });
-
-      if (!response.ok) {
-        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
-      }
-
-      const result = await response.json();
-
-      if (result.success) {
-        // Update local cache immediately
-        this.setCachedValue(path, value);
-
-        // Broadcast via WebSocket if enabled
-        if (broadcast && this.websocket && this.websocket.readyState === WebSocket.OPEN) {
-          this.websocket.send(JSON.stringify({
-            type: 'settingChanged',
-            path,
-            value,
-            timestamp: Date.now()
-          }));
-        }
-
-        const responseTime = performance.now() - startTime;
-        console.log(`Updated ${path} in ${responseTime.toFixed(2)}ms`);
-
-      } else {
-        throw new Error(result.error || 'Failed to update setting');
-      }
-
-    } catch (error) {
-      console.error(`Failed to set setting ${path}:`, error);
-      throw error;
-    }
-  }
-
-  /**
-   * Set multiple settings in an optimized batch operation
-   */
-  public async setBatch(updates: Array<{path: string, value: any}>, options: { broadcast?: boolean } = {}): Promise<void> {
-    const startTime = performance.now();
-    const { broadcast = true } = options;
-
-    try {
-      const response = await fetch('/api/settings/batch', {
-        method: 'PUT',
-        headers: {
-          'Content-Type': 'application/json'
-        },
-        body: JSON.stringify({ updates })
-      });
-
-      if (!response.ok) {
-        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
-      }
-
-      const result = await response.json();
-
-      if (result.success) {
-        // Update local cache for all successful updates
-        updates.forEach(({path, value}) => {
-          this.setCachedValue(path, value);
-        });
-
-        // Broadcast batch update via WebSocket
-        if (broadcast && this.websocket && this.websocket.readyState === WebSocket.OPEN) {
-          this.websocket.send(JSON.stringify({
-            type: 'settingsBatchChanged',
-            updates,
-            timestamp: Date.now()
-          }));
-        }
-
-        const responseTime = performance.now() - startTime;
-        console.log(`Batch updated ${updates.length} settings in ${responseTime.toFixed(2)}ms`);
-
-      } else {
-        throw new Error(result.error || 'Failed to update settings batch');
-      }
-
-    } catch (error) {
-      console.error('Failed to set batch settings:', error);
-      throw error;
-    }
-  }
-
-  /**
-   * Get comprehensive performance metrics
-   */
-  public getPerformanceMetrics(): PerformanceMetrics {
-    const totalRequests = this.metrics.hits + this.metrics.misses;
-
-    return {
-      cacheHitRate: totalRequests > 0 ? (this.metrics.hits / totalRequests) * 100 : 0,
-      averageResponseTime: 0, // Would need to track this
-      bandwidthSavings: this.metrics.bandwidthSaved,
-      totalRequests
-    };
-  }
-
-  /**
-   * Clear all caches and reset metrics
-   */
-  public clearCache(): void {
-    this.cache.clear();
-    this.clearLocalStorage();
-    this.metrics = {
-      hits: 0,
-      misses: 0,
-      invalidations: 0,
-      bandwidthSaved: 0,
-      storageUsed: 0,
-      lastSync: Date.now()
-    };
-
-    console.log('All caches cleared and metrics reset');
-  }
-
-  private getCachedValue(path: string): CachedSetting | null {
-    return this.cache.get(path) || null;
-  }
-
-  private setCachedValue(path: string, value: any, options: {
-    timestamp?: number,
-    ttl?: number,
-    fromWebSocket?: boolean
-  } = {}): void {
-    const {
-      timestamp = Date.now(),
-      ttl = this.DEFAULT_TTL,
-      fromWebSocket = false
-    } = options;
-
-    const hash = this.calculateHash(value);
-    const version = this.getNextVersion();
-
-    const cached: CachedSetting = {
-      value,
-      path,
-      timestamp,
-      version,
-      hash,
-      ttl
-    };
-
-    // Add to memory cache
-    this.cache.set(path, cached);
-
-    // Persist to localStorage (async)
-    this.persistToLocalStorage();
-
-    // Update storage metrics
-    this.updateStorageMetrics();
-
-    // Cleanup if cache is too large
-    this.enforCacheSizeLimit();
-  }
-
-  private isCacheValid(cached: CachedSetting): boolean {
-    return (Date.now() - cached.timestamp) < cached.ttl;
-  }
-
-  private calculateHash(value: any): string {
-    // Simple hash function for cache validation
-    return btoa(JSON.stringify(value)).slice(0, 16);
-  }
-
-  private getNextVersion(): number {
-    return Date.now();
-  }
-
-  private persistToLocalStorage(): void {
-    try {
-      const cacheData = {
-        version: this.CACHE_VERSION,
-        timestamp: Date.now(),
-        data: Object.fromEntries(this.cache)
-      };
-
-      const serialized = JSON.stringify(cacheData);
-
-      // Check storage quota
-      if (serialized.length > this.STORAGE_QUOTA) {
-        console.warn('Cache size exceeds storage quota, performing cleanup');
-        this.cleanupOldestEntries();
-        return;
-      }
-
-      localStorage.setItem(`${this.CACHE_PREFIX}cache`, serialized);
-      this.persistMetrics();
-
-    } catch (error) {
-      console.warn('Failed to persist cache to localStorage:', error);
-
-      // If storage is full, try cleanup
-      if (error.name === 'QuotaExceededError') {
-        this.cleanupOldestEntries();
-      }
-    }
-  }
-
-  private persistMetrics(): void {
-    try {
-      localStorage.setItem(`${this.CACHE_PREFIX}metrics`, JSON.stringify(this.metrics));
-    } catch (error) {
-      console.warn('Failed to persist metrics:', error);
-    }
-  }
-
-  private invalidateLocalStorage(path: string): void {
-    // Remove specific path from localStorage cache
-    try {
-      const stored = localStorage.getItem(`${this.CACHE_PREFIX}cache`);
-      if (stored) {
-        const parsed = JSON.parse(stored);
-        delete parsed.data[path];
-        localStorage.setItem(`${this.CACHE_PREFIX}cache`, JSON.stringify(parsed));
-      }
-    } catch (error) {
-      console.warn('Failed to invalidate localStorage cache:', error);
-    }
-  }
-
-  private clearLocalStorage(): void {
-    try {
-      Object.keys(localStorage).forEach(key => {
-        if (key.startsWith(this.CACHE_PREFIX)) {
-          localStorage.removeItem(key);
-        }
-      });
-    } catch (error) {
-      console.warn('Failed to clear localStorage:', error);
-    }
-  }
-
-  private updateStorageMetrics(): void {
-    this.metrics.storageUsed = this.cache.size;
-  }
-
-  private enforCacheSizeLimit(): void {
-    if (this.cache.size > this.MAX_CACHE_SIZE) {
-      // Remove oldest entries
-      const entries = Array.from(this.cache.entries());
-      entries.sort((a, b) => a[1].timestamp - b[1].timestamp);
-
-      const toRemove = entries.slice(0, Math.floor(this.MAX_CACHE_SIZE * 0.2));
-      toRemove.forEach(([key]) => {
-        this.cache.delete(key);
-      });
-
-      console.log(`Removed ${toRemove.length} old cache entries`);
-    }
-  }
-
-  private cleanupOldestEntries(): void {
-    const entries = Array.from(this.cache.entries());
-    entries.sort((a, b) => a[1].timestamp - b[1].timestamp);
-
-    // Remove oldest 30%
-    const toRemove = entries.slice(0, Math.floor(entries.length * 0.3));
-    toRemove.forEach(([key]) => {
-      this.cache.delete(key);
-    });
-
-    // Try to persist again
-    this.persistToLocalStorage();
-  }
-
-  private estimatePayloadSize(value: any): number {
-    try {
-      return JSON.stringify(value).length;
-    } catch {
-      return 0;
-    }
-  }
-
-  private requestDeltaSync(): void {
-    if (this.websocket && this.websocket.readyState === WebSocket.OPEN) {
-      this.websocket.send(JSON.stringify({
-        type: 'requestDeltaSync',
-        lastSync: this.metrics.lastSync
-      }));
-    }
-  }
-
-  private handleBatchDelta(delta: DeltaUpdate): void {
-    // Handle batch delta operations
-    if (Array.isArray(delta.value)) {
-      delta.value.forEach((update: any) => {
-        this.setCachedValue(update.path, update.value, {
-          timestamp: delta.timestamp,
-          fromWebSocket: true
-        });
-      });
-    }
-  }
-
-  private handleCompressionWorkerMessage(e: MessageEvent): void {
-    // Handle compression worker responses
-    const { id, result, error, originalSize, compressedSize } = e.data;
-
-    if (error) {
-      console.warn('Compression worker error:', error);
-      return;
-    }
-
-    if (originalSize && compressedSize) {
-      const ratio = (originalSize - compressedSize) / originalSize;
-      console.log(`Compression achieved ${(ratio * 100).toFixed(1)}% size reduction`);
-    }
-  }
-
-  private startCacheMaintenanceTimer(): void {
-    // Cleanup expired entries every 5 minutes
-    setInterval(() => {
-      const now = Date.now();
-      const expiredKeys: string[] = [];
-
-      this.cache.forEach((cached, key) => {
-        if (now - cached.timestamp > cached.ttl) {
-          expiredKeys.push(key);
-        }
-      });
-
-      expiredKeys.forEach(key => {
-        this.cache.delete(key);
-      });
-
-      if (expiredKeys.length > 0) {
-        console.log(`Cleaned up ${expiredKeys.length} expired cache entries`);
-        this.persistToLocalStorage();
-      }
-
-    }, 5 * 60 * 1000);
-  }
-
-  private notifySubscribers(path: string, value: any): void {
-    // Emit custom event for subscribers
-    window.dispatchEvent(new CustomEvent('settingChanged', {
-      detail: { path, value }
-    }));
-  }
-
-  private notifyBatchSubscribers(updates: any[]): void {
-    // Emit custom event for batch updates
-    window.dispatchEvent(new CustomEvent('settingsBatchChanged', {
-      detail: { updates }
-    }));
-  }
-}
\ No newline at end of file
diff --git a/src/config/dev_config.rs b/src/config/dev_config.rs
index 57db5b9f..da5b6960 100644
--- a/src/config/dev_config.rs
+++ b/src/config/dev_config.rs
@@ -3,9 +3,12 @@
 // These settings control internal behavior, performance tuning, and debug features

 use serde::{Deserialize, Serialize};
-use std::sync::OnceLock;
+use std::sync::{Arc, RwLock, OnceLock};
+use log::{info, warn, error};

-static DEV_CONFIG: OnceLock<DevConfig> = OnceLock::new();
+use crate::services::database_service::{DatabaseService, SettingValue};
+
+static DEV_CONFIG: OnceLock<Arc<RwLock<DevConfig>>> = OnceLock::new();

 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct DevConfig {
@@ -20,60 +23,60 @@ pub struct DevConfig {
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct PhysicsInternals {
     // Force calculation parameters
-    pub force_epsilon: f32,                    // Small value to prevent division by zero
-    pub spring_length_multiplier: f32,         // Natural spring length = separation_radius * this
-    pub spring_length_max: f32,                // Maximum natural spring length
-    pub spring_force_clamp_factor: f32,        // Clamp spring forces to max_force * this
+    pub force_epsilon: f32,
+    pub spring_length_multiplier: f32,
+    pub spring_length_max: f32,
+    pub spring_force_clamp_factor: f32,

     // CUDA kernel parameters
-    pub rest_length: f32,                      // Natural spring rest length
-    pub repulsion_cutoff: f32,                 // Maximum distance for repulsion calculations
-    pub repulsion_softening_epsilon: f32,      // Prevents division by zero in force calculations
-    pub center_gravity_k: f32,                 // Gravity toward center
-    pub grid_cell_size: f32,                   // Spatial grid resolution for neighbor searches
-    pub warmup_iterations: u32,                // Number of warmup simulation steps
-    pub cooling_rate: f32,                     // Rate of cooling during warmup
+    pub rest_length: f32,
+    pub repulsion_cutoff: f32,
+    pub repulsion_softening_epsilon: f32,
+    pub center_gravity_k: f32,
+    pub grid_cell_size: f32,
+    pub warmup_iterations: u32,
+    pub cooling_rate: f32,

     // GPU kernel-specific physics parameters
-    pub max_force: f32,                        // Maximum force magnitude for clamping
-    pub max_velocity: f32,                     // Maximum velocity magnitude for clamping
-    pub world_bounds_min: f32,                 // Minimum world coordinate
-    pub world_bounds_max: f32,                 // Maximum world coordinate
-    pub cell_size_lod: f32,                    // Level of detail cell size
-    pub k_neighbors_max: u32,                  // Maximum k-neighbors for LOF
-    pub anomaly_detection_radius: f32,         // Default radius for anomaly detection
-    pub learning_rate_default: f32,            // Default learning rate for GPU algorithms
-    pub min_velocity_threshold: f32,           // Minimum velocity threshold for stability gates
-    pub stability_threshold: f32,              // System stability threshold for early exit
+    pub max_force: f32,
+    pub max_velocity: f32,
+    pub world_bounds_min: f32,
+    pub world_bounds_max: f32,
+    pub cell_size_lod: f32,
+    pub k_neighbors_max: u32,
+    pub anomaly_detection_radius: f32,
+    pub learning_rate_default: f32,
+    pub min_velocity_threshold: f32,
+    pub stability_threshold: f32,

     // Additional kernel constants for fine-tuning
-    pub norm_delta_cap: f32,                   // Cap for SSSP delta normalization
-    pub position_constraint_attraction: f32,   // Gentle attraction factor for position constraints
-    pub lof_score_min: f32,                    // Minimum LOF score clamp
-    pub lof_score_max: f32,                    // Maximum LOF score clamp
-    pub weight_precision_multiplier: f32,      // Weight precision multiplier for integer operations
+    pub norm_delta_cap: f32,
+    pub position_constraint_attraction: f32,
+    pub lof_score_min: f32,
+    pub lof_score_max: f32,
+    pub weight_precision_multiplier: f32,

     // Boundary behavior
-    pub boundary_extreme_multiplier: f32,      // Position is extreme if > viewport_bounds * this
-    pub boundary_extreme_force_multiplier: f32,// Force multiplier for extreme positions
-    pub boundary_velocity_damping: f32,        // Velocity reduction on boundary hit
+    pub boundary_extreme_multiplier: f32,
+    pub boundary_extreme_force_multiplier: f32,
+    pub boundary_velocity_damping: f32,

     // Node distribution
-    pub golden_ratio: f32,                     // For initial node positioning
-    pub initial_radius_min: f32,               // Minimum initial node radius
-    pub initial_radius_range: f32,             // Range for initial radius variation
+    pub golden_ratio: f32,
+    pub initial_radius_min: f32,
+    pub initial_radius_range: f32,

     // Graph-based scaling
-    pub cross_graph_repulsion_scale: f32,      // Repulsion between different graphs
-    pub cross_graph_spring_scale: f32,         // Spring force between different graphs
+    pub cross_graph_repulsion_scale: f32,
+    pub cross_graph_spring_scale: f32,

     // Clustering
-    pub cluster_repulsion_scale: f32,          // Repulsion within same cluster
-    pub importance_scale_factor: f32,          // Scale based on node importance
+    pub cluster_repulsion_scale: f32,
+    pub importance_scale_factor: f32,

     // Distance thresholds
-    pub repulsion_distance_squared_min: f32,   // Minimum distance squared for repulsion
-    pub stress_majorization_epsilon: f32,      // Small value for stress calculations
+    pub repulsion_distance_squared_min: f32,
+    pub stress_majorization_epsilon: f32,
 }

 #[derive(Debug, Clone, Serialize, Deserialize)]
@@ -84,13 +87,13 @@ pub struct CudaInternals {
     pub warmup_damping_end: f32,
     pub warmup_temperature_scale: f32,
     pub warmup_cooling_iterations: u32,
-
+
     // GPU safety
     pub max_kernel_time_ms: u32,
     pub max_gpu_failures: u32,
     pub debug_output_throttle: u32,
-    pub debug_node_count: u32,                 // Number of nodes to debug output
-
+    pub debug_node_count: u32,
+
     // Memory limits
     pub max_nodes: u32,
     pub max_edges: u32,
@@ -102,24 +105,24 @@ pub struct NetworkInternals {
     pub pool_max_idle_per_host: usize,
     pub pool_idle_timeout_secs: u64,
     pub pool_connect_timeout_secs: u64,
-
+
     // Circuit breaker
     pub circuit_failure_threshold: u32,
     pub circuit_recovery_timeout_secs: u64,
     pub circuit_half_open_max_requests: u32,
-
-    // Retry logic
+
+    // Retry logic
     pub max_retry_attempts: u32,
     pub retry_base_delay_ms: u64,
     pub retry_max_delay_ms: u64,
     pub retry_exponential_base: f32,
-
+
     // WebSocket internals
     pub ws_ping_interval_secs: u64,
     pub ws_pong_timeout_secs: u64,
     pub ws_frame_size: usize,
     pub ws_max_pending_messages: usize,
-
+
     // Rate limiting internals
     pub rate_limit_burst_size: u32,
     pub rate_limit_refill_rate: f32,
@@ -129,19 +132,19 @@ pub struct NetworkInternals {
 pub struct RenderingInternals {
     // Agent colors (hex strings)
     pub agent_colors: AgentColors,
-
+
     // Size calculations
     pub agent_base_size: f32,
     pub agent_size_per_task: f32,
     pub agent_max_size: f32,
     pub node_base_radius: f32,
-
+
     // Animation speeds
     pub pulse_speed: f32,
     pub rotate_speed: f32,
     pub glow_speed: f32,
     pub wave_speed: f32,
-
+
     // Quality thresholds
     pub lod_distance_high: f32,
     pub lod_distance_medium: f32,
@@ -168,17 +171,17 @@ pub struct PerformanceInternals {
     pub batch_size_nodes: usize,
     pub batch_size_edges: usize,
     pub batch_timeout_ms: u64,
-
+
     // Caching
     pub cache_ttl_secs: u64,
     pub cache_max_entries: usize,
     pub cache_eviction_percentage: f32,
-
+
     // Threading
     pub worker_threads: usize,
     pub blocking_threads: usize,
     pub stack_size_mb: usize,
-
+
     // Memory management
     pub gc_interval_secs: u64,
     pub memory_warning_threshold_mb: usize,
@@ -192,7 +195,7 @@ pub struct DebugInternals {
     pub enable_network_debug: bool,
     pub enable_memory_tracking: bool,
     pub enable_performance_tracking: bool,
-
+
     pub log_slow_operations_ms: u64,
     pub log_memory_usage_interval_secs: u64,
     pub profile_sample_rate: f32,
@@ -333,65 +336,333 @@ impl Default for DevConfig {
 }

 impl DevConfig {
-    /// Load developer configuration from file or use defaults
-    pub fn load() -> &'static Self {
-        DEV_CONFIG.get_or_init(|| {
-            // Try to load from dev_config.toml
-            match std::fs::read_to_string("data/dev_config.toml") {
-                Ok(content) => {
-                    match toml::from_str::<DevConfig>(&content) {
-                        Ok(config) => {
-                            log::info!("Loaded developer configuration from data/dev_config.toml");
-                            config
-                        }
-                        Err(e) => {
-                            log::warn!("Failed to parse dev_config.toml: {}, using defaults", e);
-                            Self::default()
-                        }
-                    }
-                }
-                Err(_) => {
-                    log::info!("No dev_config.toml found, using default developer configuration");
-                    Self::default()
-                }
-            }
-        })
+    /// Initialize DevConfig from database
+    pub fn initialize(db_service: Arc<DatabaseService>) -> Result<(), Box<dyn std::error::Error>> {
+        let config = Self::from_database(db_service)?;
+
+        DEV_CONFIG.set(Arc::new(RwLock::new(config)))
+            .map_err(|_| "DevConfig already initialized")?;
+
+        info!("DevConfig initialized from database");
+        Ok(())
     }
-
+
+    /// Load configuration from database
+    pub fn from_database(db_service: Arc<DatabaseService>) -> Result<Self, Box<dyn std::error::Error>> {
+        // Check for deprecated file
+        if std::path::Path::new("data/dev_config.toml").exists() {
+            warn!("DEPRECATED: data/dev_config.toml file found. Configuration is now loaded from database.");
+            warn!("Please remove or rename this file to data/dev_config.toml.deprecated");
+        }
+
+        let mut config = Self::default();
+
+        // Load physics parameters
+        config.physics.force_epsilon = Self::get_f32(&db_service, "dev.physics.force_epsilon")
+            .unwrap_or(config.physics.force_epsilon);
+        config.physics.spring_length_multiplier = Self::get_f32(&db_service, "dev.physics.spring_length_multiplier")
+            .unwrap_or(config.physics.spring_length_multiplier);
+        config.physics.spring_length_max = Self::get_f32(&db_service, "dev.physics.spring_length_max")
+            .unwrap_or(config.physics.spring_length_max);
+        config.physics.spring_force_clamp_factor = Self::get_f32(&db_service, "dev.physics.spring_force_clamp_factor")
+            .unwrap_or(config.physics.spring_force_clamp_factor);
+        config.physics.rest_length = Self::get_f32(&db_service, "dev.physics.rest_length")
+            .unwrap_or(config.physics.rest_length);
+        config.physics.repulsion_cutoff = Self::get_f32(&db_service, "dev.physics.repulsion_cutoff")
+            .unwrap_or(config.physics.repulsion_cutoff);
+        config.physics.repulsion_softening_epsilon = Self::get_f32(&db_service, "dev.physics.repulsion_softening_epsilon")
+            .unwrap_or(config.physics.repulsion_softening_epsilon);
+        config.physics.center_gravity_k = Self::get_f32(&db_service, "dev.physics.center_gravity_k")
+            .unwrap_or(config.physics.center_gravity_k);
+        config.physics.grid_cell_size = Self::get_f32(&db_service, "dev.physics.grid_cell_size")
+            .unwrap_or(config.physics.grid_cell_size);
+        config.physics.warmup_iterations = Self::get_u32(&db_service, "dev.physics.warmup_iterations")
+            .unwrap_or(config.physics.warmup_iterations);
+        config.physics.cooling_rate = Self::get_f32(&db_service, "dev.physics.cooling_rate")
+            .unwrap_or(config.physics.cooling_rate);
+        config.physics.max_force = Self::get_f32(&db_service, "dev.physics.max_force")
+            .unwrap_or(config.physics.max_force);
+        config.physics.max_velocity = Self::get_f32(&db_service, "dev.physics.max_velocity")
+            .unwrap_or(config.physics.max_velocity);
+        config.physics.world_bounds_min = Self::get_f32(&db_service, "dev.physics.world_bounds_min")
+            .unwrap_or(config.physics.world_bounds_min);
+        config.physics.world_bounds_max = Self::get_f32(&db_service, "dev.physics.world_bounds_max")
+            .unwrap_or(config.physics.world_bounds_max);
+        config.physics.cell_size_lod = Self::get_f32(&db_service, "dev.physics.cell_size_lod")
+            .unwrap_or(config.physics.cell_size_lod);
+        config.physics.k_neighbors_max = Self::get_u32(&db_service, "dev.physics.k_neighbors_max")
+            .unwrap_or(config.physics.k_neighbors_max);
+        config.physics.anomaly_detection_radius = Self::get_f32(&db_service, "dev.physics.anomaly_detection_radius")
+            .unwrap_or(config.physics.anomaly_detection_radius);
+        config.physics.learning_rate_default = Self::get_f32(&db_service, "dev.physics.learning_rate_default")
+            .unwrap_or(config.physics.learning_rate_default);
+        config.physics.min_velocity_threshold = Self::get_f32(&db_service, "dev.physics.min_velocity_threshold")
+            .unwrap_or(config.physics.min_velocity_threshold);
+        config.physics.stability_threshold = Self::get_f32(&db_service, "dev.physics.stability_threshold")
+            .unwrap_or(config.physics.stability_threshold);
+        config.physics.norm_delta_cap = Self::get_f32(&db_service, "dev.physics.norm_delta_cap")
+            .unwrap_or(config.physics.norm_delta_cap);
+        config.physics.position_constraint_attraction = Self::get_f32(&db_service, "dev.physics.position_constraint_attraction")
+            .unwrap_or(config.physics.position_constraint_attraction);
+        config.physics.lof_score_min = Self::get_f32(&db_service, "dev.physics.lof_score_min")
+            .unwrap_or(config.physics.lof_score_min);
+        config.physics.lof_score_max = Self::get_f32(&db_service, "dev.physics.lof_score_max")
+            .unwrap_or(config.physics.lof_score_max);
+        config.physics.weight_precision_multiplier = Self::get_f32(&db_service, "dev.physics.weight_precision_multiplier")
+            .unwrap_or(config.physics.weight_precision_multiplier);
+        config.physics.boundary_extreme_multiplier = Self::get_f32(&db_service, "dev.physics.boundary_extreme_multiplier")
+            .unwrap_or(config.physics.boundary_extreme_multiplier);
+        config.physics.boundary_extreme_force_multiplier = Self::get_f32(&db_service, "dev.physics.boundary_extreme_force_multiplier")
+            .unwrap_or(config.physics.boundary_extreme_force_multiplier);
+        config.physics.boundary_velocity_damping = Self::get_f32(&db_service, "dev.physics.boundary_velocity_damping")
+            .unwrap_or(config.physics.boundary_velocity_damping);
+        config.physics.golden_ratio = Self::get_f32(&db_service, "dev.physics.golden_ratio")
+            .unwrap_or(config.physics.golden_ratio);
+        config.physics.initial_radius_min = Self::get_f32(&db_service, "dev.physics.initial_radius_min")
+            .unwrap_or(config.physics.initial_radius_min);
+        config.physics.initial_radius_range = Self::get_f32(&db_service, "dev.physics.initial_radius_range")
+            .unwrap_or(config.physics.initial_radius_range);
+        config.physics.cross_graph_repulsion_scale = Self::get_f32(&db_service, "dev.physics.cross_graph_repulsion_scale")
+            .unwrap_or(config.physics.cross_graph_repulsion_scale);
+        config.physics.cross_graph_spring_scale = Self::get_f32(&db_service, "dev.physics.cross_graph_spring_scale")
+            .unwrap_or(config.physics.cross_graph_spring_scale);
+        config.physics.cluster_repulsion_scale = Self::get_f32(&db_service, "dev.physics.cluster_repulsion_scale")
+            .unwrap_or(config.physics.cluster_repulsion_scale);
+        config.physics.importance_scale_factor = Self::get_f32(&db_service, "dev.physics.importance_scale_factor")
+            .unwrap_or(config.physics.importance_scale_factor);
+        config.physics.repulsion_distance_squared_min = Self::get_f32(&db_service, "dev.physics.repulsion_distance_squared_min")
+            .unwrap_or(config.physics.repulsion_distance_squared_min);
+        config.physics.stress_majorization_epsilon = Self::get_f32(&db_service, "dev.physics.stress_majorization_epsilon")
+            .unwrap_or(config.physics.stress_majorization_epsilon);
+
+        // Load CUDA parameters
+        config.cuda.warmup_iterations_default = Self::get_u32(&db_service, "dev.cuda.warmup_iterations_default")
+            .unwrap_or(config.cuda.warmup_iterations_default);
+        config.cuda.warmup_damping_start = Self::get_f32(&db_service, "dev.cuda.warmup_damping_start")
+            .unwrap_or(config.cuda.warmup_damping_start);
+        config.cuda.warmup_damping_end = Self::get_f32(&db_service, "dev.cuda.warmup_damping_end")
+            .unwrap_or(config.cuda.warmup_damping_end);
+        config.cuda.warmup_temperature_scale = Self::get_f32(&db_service, "dev.cuda.warmup_temperature_scale")
+            .unwrap_or(config.cuda.warmup_temperature_scale);
+        config.cuda.warmup_cooling_iterations = Self::get_u32(&db_service, "dev.cuda.warmup_cooling_iterations")
+            .unwrap_or(config.cuda.warmup_cooling_iterations);
+        config.cuda.max_kernel_time_ms = Self::get_u32(&db_service, "dev.cuda.max_kernel_time_ms")
+            .unwrap_or(config.cuda.max_kernel_time_ms);
+        config.cuda.max_gpu_failures = Self::get_u32(&db_service, "dev.cuda.max_gpu_failures")
+            .unwrap_or(config.cuda.max_gpu_failures);
+        config.cuda.debug_output_throttle = Self::get_u32(&db_service, "dev.cuda.debug_output_throttle")
+            .unwrap_or(config.cuda.debug_output_throttle);
+        config.cuda.debug_node_count = Self::get_u32(&db_service, "dev.cuda.debug_node_count")
+            .unwrap_or(config.cuda.debug_node_count);
+        config.cuda.max_nodes = Self::get_u32(&db_service, "dev.cuda.max_nodes")
+            .unwrap_or(config.cuda.max_nodes);
+        config.cuda.max_edges = Self::get_u32(&db_service, "dev.cuda.max_edges")
+            .unwrap_or(config.cuda.max_edges);
+
+        // Load network parameters
+        config.network.pool_max_idle_per_host = Self::get_usize(&db_service, "dev.network.pool_max_idle_per_host")
+            .unwrap_or(config.network.pool_max_idle_per_host);
+        config.network.pool_idle_timeout_secs = Self::get_u64(&db_service, "dev.network.pool_idle_timeout_secs")
+            .unwrap_or(config.network.pool_idle_timeout_secs);
+        config.network.pool_connect_timeout_secs = Self::get_u64(&db_service, "dev.network.pool_connect_timeout_secs")
+            .unwrap_or(config.network.pool_connect_timeout_secs);
+        config.network.circuit_failure_threshold = Self::get_u32(&db_service, "dev.network.circuit_failure_threshold")
+            .unwrap_or(config.network.circuit_failure_threshold);
+        config.network.circuit_recovery_timeout_secs = Self::get_u64(&db_service, "dev.network.circuit_recovery_timeout_secs")
+            .unwrap_or(config.network.circuit_recovery_timeout_secs);
+        config.network.circuit_half_open_max_requests = Self::get_u32(&db_service, "dev.network.circuit_half_open_max_requests")
+            .unwrap_or(config.network.circuit_half_open_max_requests);
+        config.network.max_retry_attempts = Self::get_u32(&db_service, "dev.network.max_retry_attempts")
+            .unwrap_or(config.network.max_retry_attempts);
+        config.network.retry_base_delay_ms = Self::get_u64(&db_service, "dev.network.retry_base_delay_ms")
+            .unwrap_or(config.network.retry_base_delay_ms);
+        config.network.retry_max_delay_ms = Self::get_u64(&db_service, "dev.network.retry_max_delay_ms")
+            .unwrap_or(config.network.retry_max_delay_ms);
+        config.network.retry_exponential_base = Self::get_f32(&db_service, "dev.network.retry_exponential_base")
+            .unwrap_or(config.network.retry_exponential_base);
+        config.network.ws_ping_interval_secs = Self::get_u64(&db_service, "dev.network.ws_ping_interval_secs")
+            .unwrap_or(config.network.ws_ping_interval_secs);
+        config.network.ws_pong_timeout_secs = Self::get_u64(&db_service, "dev.network.ws_pong_timeout_secs")
+            .unwrap_or(config.network.ws_pong_timeout_secs);
+        config.network.ws_frame_size = Self::get_usize(&db_service, "dev.network.ws_frame_size")
+            .unwrap_or(config.network.ws_frame_size);
+        config.network.ws_max_pending_messages = Self::get_usize(&db_service, "dev.network.ws_max_pending_messages")
+            .unwrap_or(config.network.ws_max_pending_messages);
+        config.network.rate_limit_burst_size = Self::get_u32(&db_service, "dev.network.rate_limit_burst_size")
+            .unwrap_or(config.network.rate_limit_burst_size);
+        config.network.rate_limit_refill_rate = Self::get_f32(&db_service, "dev.network.rate_limit_refill_rate")
+            .unwrap_or(config.network.rate_limit_refill_rate);
+
+        // Load rendering parameters
+        config.rendering.agent_base_size = Self::get_f32(&db_service, "dev.rendering.agent_base_size")
+            .unwrap_or(config.rendering.agent_base_size);
+        config.rendering.agent_size_per_task = Self::get_f32(&db_service, "dev.rendering.agent_size_per_task")
+            .unwrap_or(config.rendering.agent_size_per_task);
+        config.rendering.agent_max_size = Self::get_f32(&db_service, "dev.rendering.agent_max_size")
+            .unwrap_or(config.rendering.agent_max_size);
+        config.rendering.node_base_radius = Self::get_f32(&db_service, "dev.rendering.node_base_radius")
+            .unwrap_or(config.rendering.node_base_radius);
+        config.rendering.pulse_speed = Self::get_f32(&db_service, "dev.rendering.pulse_speed")
+            .unwrap_or(config.rendering.pulse_speed);
+        config.rendering.rotate_speed = Self::get_f32(&db_service, "dev.rendering.rotate_speed")
+            .unwrap_or(config.rendering.rotate_speed);
+        config.rendering.glow_speed = Self::get_f32(&db_service, "dev.rendering.glow_speed")
+            .unwrap_or(config.rendering.glow_speed);
+        config.rendering.wave_speed = Self::get_f32(&db_service, "dev.rendering.wave_speed")
+            .unwrap_or(config.rendering.wave_speed);
+        config.rendering.lod_distance_high = Self::get_f32(&db_service, "dev.rendering.lod_distance_high")
+            .unwrap_or(config.rendering.lod_distance_high);
+        config.rendering.lod_distance_medium = Self::get_f32(&db_service, "dev.rendering.lod_distance_medium")
+            .unwrap_or(config.rendering.lod_distance_medium);
+        config.rendering.lod_distance_low = Self::get_f32(&db_service, "dev.rendering.lod_distance_low")
+            .unwrap_or(config.rendering.lod_distance_low);
+
+        // Load agent colors
+        config.rendering.agent_colors.coordinator = Self::get_string(&db_service, "dev.rendering.agent_colors.coordinator")
+            .unwrap_or(config.rendering.agent_colors.coordinator);
+        config.rendering.agent_colors.coder = Self::get_string(&db_service, "dev.rendering.agent_colors.coder")
+            .unwrap_or(config.rendering.agent_colors.coder);
+        config.rendering.agent_colors.architect = Self::get_string(&db_service, "dev.rendering.agent_colors.architect")
+            .unwrap_or(config.rendering.agent_colors.architect);
+        config.rendering.agent_colors.analyst = Self::get_string(&db_service, "dev.rendering.agent_colors.analyst")
+            .unwrap_or(config.rendering.agent_colors.analyst);
+        config.rendering.agent_colors.tester = Self::get_string(&db_service, "dev.rendering.agent_colors.tester")
+            .unwrap_or(config.rendering.agent_colors.tester);
+        config.rendering.agent_colors.researcher = Self::get_string(&db_service, "dev.rendering.agent_colors.researcher")
+            .unwrap_or(config.rendering.agent_colors.researcher);
+        config.rendering.agent_colors.reviewer = Self::get_string(&db_service, "dev.rendering.agent_colors.reviewer")
+            .unwrap_or(config.rendering.agent_colors.reviewer);
+        config.rendering.agent_colors.optimizer = Self::get_string(&db_service, "dev.rendering.agent_colors.optimizer")
+            .unwrap_or(config.rendering.agent_colors.optimizer);
+        config.rendering.agent_colors.documenter = Self::get_string(&db_service, "dev.rendering.agent_colors.documenter")
+            .unwrap_or(config.rendering.agent_colors.documenter);
+        config.rendering.agent_colors.default = Self::get_string(&db_service, "dev.rendering.agent_colors.default")
+            .unwrap_or(config.rendering.agent_colors.default);
+
+        // Load performance parameters
+        config.performance.batch_size_nodes = Self::get_usize(&db_service, "dev.performance.batch_size_nodes")
+            .unwrap_or(config.performance.batch_size_nodes);
+        config.performance.batch_size_edges = Self::get_usize(&db_service, "dev.performance.batch_size_edges")
+            .unwrap_or(config.performance.batch_size_edges);
+        config.performance.batch_timeout_ms = Self::get_u64(&db_service, "dev.performance.batch_timeout_ms")
+            .unwrap_or(config.performance.batch_timeout_ms);
+        config.performance.cache_ttl_secs = Self::get_u64(&db_service, "dev.performance.cache_ttl_secs")
+            .unwrap_or(config.performance.cache_ttl_secs);
+        config.performance.cache_max_entries = Self::get_usize(&db_service, "dev.performance.cache_max_entries")
+            .unwrap_or(config.performance.cache_max_entries);
+        config.performance.cache_eviction_percentage = Self::get_f32(&db_service, "dev.performance.cache_eviction_percentage")
+            .unwrap_or(config.performance.cache_eviction_percentage);
+        config.performance.worker_threads = Self::get_usize(&db_service, "dev.performance.worker_threads")
+            .unwrap_or(config.performance.worker_threads);
+        config.performance.blocking_threads = Self::get_usize(&db_service, "dev.performance.blocking_threads")
+            .unwrap_or(config.performance.blocking_threads);
+        config.performance.stack_size_mb = Self::get_usize(&db_service, "dev.performance.stack_size_mb")
+            .unwrap_or(config.performance.stack_size_mb);
+        config.performance.gc_interval_secs = Self::get_u64(&db_service, "dev.performance.gc_interval_secs")
+            .unwrap_or(config.performance.gc_interval_secs);
+        config.performance.memory_warning_threshold_mb = Self::get_usize(&db_service, "dev.performance.memory_warning_threshold_mb")
+            .unwrap_or(config.performance.memory_warning_threshold_mb);
+        config.performance.memory_critical_threshold_mb = Self::get_usize(&db_service, "dev.performance.memory_critical_threshold_mb")
+            .unwrap_or(config.performance.memory_critical_threshold_mb);
+
+        // Load debug parameters
+        config.debug.enable_cuda_debug = Self::get_bool(&db_service, "dev.debug.enable_cuda_debug")
+            .unwrap_or(config.debug.enable_cuda_debug);
+        config.debug.enable_physics_debug = Self::get_bool(&db_service, "dev.debug.enable_physics_debug")
+            .unwrap_or(config.debug.enable_physics_debug);
+        config.debug.enable_network_debug = Self::get_bool(&db_service, "dev.debug.enable_network_debug")
+            .unwrap_or(config.debug.enable_network_debug);
+        config.debug.enable_memory_tracking = Self::get_bool(&db_service, "dev.debug.enable_memory_tracking")
+            .unwrap_or(config.debug.enable_memory_tracking);
+        config.debug.enable_performance_tracking = Self::get_bool(&db_service, "dev.debug.enable_performance_tracking")
+            .unwrap_or(config.debug.enable_performance_tracking);
+        config.debug.log_slow_operations_ms = Self::get_u64(&db_service, "dev.debug.log_slow_operations_ms")
+            .unwrap_or(config.debug.log_slow_operations_ms);
+        config.debug.log_memory_usage_interval_secs = Self::get_u64(&db_service, "dev.debug.log_memory_usage_interval_secs")
+            .unwrap_or(config.debug.log_memory_usage_interval_secs);
+        config.debug.profile_sample_rate = Self::get_f32(&db_service, "dev.debug.profile_sample_rate")
+            .unwrap_or(config.debug.profile_sample_rate);
+
+        Ok(config)
+    }
+
     /// Get the global developer configuration instance
-    pub fn get() -> &'static Self {
-        Self::load()
+    pub fn get() -> &'static Arc<RwLock<DevConfig>> {
+        DEV_CONFIG.get().expect("DevConfig not initialized. Call DevConfig::initialize() first.")
     }
-
-    /// Save current configuration to file
-    pub fn save_to_file(&self, path: &str) -> Result<(), Box<dyn std::error::Error>> {
-        let toml_string = toml::to_string_pretty(self)?;
-        std::fs::write(path, toml_string)?;
-        Ok(())
+
+    // Helper methods for database access
+    fn get_f32(db: &Arc<DatabaseService>, key: &str) -> Option<f32> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Float(f) => Some(f as f32),
+            SettingValue::Integer(i) => Some(i as f32),
+            _ => None,
+        }
+    }
+
+    fn get_u32(db: &Arc<DatabaseService>, key: &str) -> Option<u32> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Integer(i) => Some(i as u32),
+            _ => None,
+        }
+    }
+
+    fn get_u64(db: &Arc<DatabaseService>, key: &str) -> Option<u64> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Integer(i) => Some(i as u64),
+            _ => None,
+        }
+    }
+
+    fn get_usize(db: &Arc<DatabaseService>, key: &str) -> Option<usize> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Integer(i) => Some(i as usize),
+            _ => None,
+        }
+    }
+
+    fn get_bool(db: &Arc<DatabaseService>, key: &str) -> Option<bool> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::Boolean(b) => Some(b),
+            _ => None,
+        }
+    }
+
+    fn get_string(db: &Arc<DatabaseService>, key: &str) -> Option<String> {
+        let value = db.get_setting(key).ok()??;
+        match value {
+            SettingValue::String(s) => Some(s),
+            _ => None,
+        }
     }
 }

 // Convenience functions for common access patterns
-pub fn physics() -> &'static PhysicsInternals {
-    &DevConfig::get().physics
+pub fn physics() -> PhysicsInternals {
+    DevConfig::get().read().unwrap().physics.clone()
 }

-pub fn cuda() -> &'static CudaInternals {
-    &DevConfig::get().cuda
+pub fn cuda() -> CudaInternals {
+    DevConfig::get().read().unwrap().cuda.clone()
 }

-pub fn network() -> &'static NetworkInternals {
-    &DevConfig::get().network
+pub fn network() -> NetworkInternals {
+    DevConfig::get().read().unwrap().network.clone()
 }

-pub fn rendering() -> &'static RenderingInternals {
-    &DevConfig::get().rendering
+pub fn rendering() -> RenderingInternals {
+    DevConfig::get().read().unwrap().rendering.clone()
 }

-pub fn performance() -> &'static PerformanceInternals {
-    &DevConfig::get().performance
+pub fn performance() -> PerformanceInternals {
+    DevConfig::get().read().unwrap().performance.clone()
 }

-pub fn debug() -> &'static DebugInternals {
-    &DevConfig::get().debug
-}
\ No newline at end of file
+pub fn debug() -> DebugInternals {
+    DevConfig::get().read().unwrap().debug.clone()
+}
diff --git a/src/config/mod.rs b/src/config/mod.rs
index a1211702..1c65b471 100644
--- a/src/config/mod.rs
+++ b/src/config/mod.rs
@@ -839,6 +839,7 @@ pub struct RenderingSettings {
 #[derive(Debug, Serialize, Deserialize, Clone, Default, Type, Validate)]
 #[serde(rename_all = "camelCase")]
 pub struct AnimationSettings {
+    pub enabled: bool,
     #[serde(alias = "enable_motion_blur")]
     pub enable_motion_blur: bool,
     #[serde(alias = "enable_node_animations")]
@@ -1085,6 +1086,95 @@ pub struct Sensitivity {
     pub rotation: f32,
 }

+// Additional Settings Structs
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct SyncSettings {
+    pub enabled: bool,
+    pub camera: bool,
+    pub selection: bool,
+}
+
+impl Default for SyncSettings {
+    fn default() -> Self {
+        Self {
+            enabled: false,
+            camera: true,
+            selection: true,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct EffectsSettings {
+    pub bloom: bool,
+    pub glow: bool,
+}
+
+impl Default for EffectsSettings {
+    fn default() -> Self {
+        Self {
+            bloom: false,
+            glow: true,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct PerformanceSettings {
+    pub auto_optimize: bool,
+    pub simplify_edges: bool,
+    pub cull_distance: f32,
+}
+
+impl Default for PerformanceSettings {
+    fn default() -> Self {
+        Self {
+            auto_optimize: false,
+            simplify_edges: true,
+            cull_distance: 50.0,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct InteractionSettings {
+    pub enable_hover: bool,
+    pub enable_click: bool,
+    pub enable_drag: bool,
+    pub hover_delay: u32,
+}
+
+impl Default for InteractionSettings {
+    fn default() -> Self {
+        Self {
+            enable_hover: true,
+            enable_click: true,
+            enable_drag: true,
+            hover_delay: 300,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Type, Validate)]
+#[serde(rename_all = "camelCase")]
+pub struct ExportSettings {
+    pub format: String,
+    pub include_metadata: bool,
+}
+
+impl Default for ExportSettings {
+    fn default() -> Self {
+        Self {
+            format: "json".to_string(),
+            include_metadata: true,
+        }
+    }
+}
+
 // Graph-specific settings
 #[derive(Debug, Serialize, Deserialize, Clone, Default, Type, Validate)]
 #[serde(rename_all = "camelCase")]
@@ -1112,7 +1202,7 @@ pub struct GraphsSettings {
 #[derive(Debug, Serialize, Deserialize, Clone, Default, Type, Validate)]
 #[serde(rename_all = "camelCase")]
 pub struct VisualisationSettings {
-
+
     // Global settings
     #[validate(nested)]
     pub rendering: RenderingSettings,
@@ -1126,6 +1216,10 @@ pub struct VisualisationSettings {
     pub hologram: HologramSettings,
     #[validate(nested)]
     pub graphs: GraphsSettings,
+    #[validate(nested)]
+    pub sync: SyncSettings,
+    #[validate(nested)]
+    pub effects: EffectsSettings,
     #[serde(skip_serializing_if = "Option::is_none")]
     pub camera: Option<CameraSettings>,
     #[serde(skip_serializing_if = "Option::is_none")]
@@ -1667,6 +1761,12 @@ pub struct AppFullSettings {
     #[validate(nested)]
     #[serde(alias = "auth")]
     pub auth: AuthSettings,
+    #[validate(nested)]
+    pub performance: PerformanceSettings,
+    #[validate(nested)]
+    pub interaction: InteractionSettings,
+    #[validate(nested)]
+    pub export: ExportSettings,
     #[serde(skip_serializing_if = "Option::is_none", alias = "ragflow")]
     pub ragflow: Option<RagFlowSettings>,
     #[serde(skip_serializing_if = "Option::is_none", alias = "perplexity")]
@@ -1692,6 +1792,9 @@ impl Default for AppFullSettings {
             system: SystemSettings::default(),
             xr: XRSettings::default(),
             auth: AuthSettings::default(),
+            performance: PerformanceSettings::default(),
+            interaction: InteractionSettings::default(),
+            export: ExportSettings::default(),
             ragflow: None,
             perplexity: None,
             openai: None,
@@ -1703,106 +1806,37 @@ impl Default for AppFullSettings {
 }

 impl AppFullSettings {
-    /// Load AppFullSettings from a YAML file with proper snake_case to camelCase conversion
-    pub fn from_yaml_file(path: &PathBuf) -> Result<Self, ConfigError> {
-        let yaml_content = std::fs::read_to_string(path)
-            .map_err(|e| ConfigError::Message(format!("Failed to read YAML file {:?}: {}", path, e)))?;
-
-        // Try direct YAML deserialization first
-        match serde_yaml::from_str::<AppFullSettings>(&yaml_content) {
-            Ok(settings) => {
-                debug!("Successfully loaded settings using direct YAML deserialization");
-                return Ok(settings);
-            }
-            Err(yaml_err) => {
-                debug!("Direct YAML deserialization failed: {}, trying YAML->JSON conversion", yaml_err);
-
-                // Parse as raw Value first
-                let raw_value = serde_yaml::from_str::<Value>(&yaml_content)
-                    .map_err(|e| ConfigError::Message(format!("Failed to parse YAML as Value: {}", e)))?;
-
-                // Convert to JSON string (this preserves the structure)
-                let json_str = serde_json::to_string(&raw_value)
-                    .map_err(|e| ConfigError::Message(format!("Failed to convert YAML to JSON: {}", e)))?;
-
-                // Deserialize from JSON (this will respect serde rename attributes)
-                serde_json::from_str::<AppFullSettings>(&json_str)
-                    .map_err(|e| ConfigError::Message(format!("Failed to deserialize JSON: {}", e)))
-            }
-        }
+    /// DEPRECATED: Use SettingsService for database-backed settings
+    /// Returns default settings - all settings now loaded from SQLite database
+    #[deprecated(note = "Settings are now stored in SQLite. Use SettingsService for runtime access.")]
+    pub fn from_yaml_file(_path: &PathBuf) -> Result<Self, ConfigError> {
+        log::warn!("from_yaml_file() is deprecated - settings now loaded from SQLite database");
+        log::warn!("Returning default AppFullSettings structure");
+        Ok(Self::default())
     }

+    /// Load AppFullSettings - returns defaults as all settings are now in SQLite database
+    ///
+    /// NOTE: This method exists for backward compatibility only. All settings are now
+    /// stored in and loaded from the SQLite database via SettingsService.
+    ///
+    /// The migration from YAML/TOML to SQLite happens automatically on first startup
+    /// via the SettingsMigration service in ontology_init.rs
     pub fn new() -> Result<Self, ConfigError> {
-        debug!("Initializing AppFullSettings from YAML");
+        log::info!("AppFullSettings::new() - returning default structure");
+        log::info!("All settings are now managed via SQLite database (see SettingsService)");
         dotenvy::dotenv().ok();
-
-        let settings_path = std::env::var("SETTINGS_FILE_PATH")
-            .map(PathBuf::from)
-            .unwrap_or_else(|_| PathBuf::from("data/settings.yaml"));
-        debug!("Loading AppFullSettings from YAML file: {:?}", settings_path);
-
-        // Use our improved YAML loading method
-        match Self::from_yaml_file(&settings_path) {
-            Ok(settings) => {
-                info!("Successfully loaded settings from YAML file");
-                return Ok(settings);
-            }
-            Err(yaml_err) => {
-                error!("YAML loading failed: {}", yaml_err);
-                debug!("Trying config crate fallback (this may fail due to case conversion issues)");
-            }
-        }
-
-        // Fallback to config crate approach (with environment variable support)
-        // NOTE: This fallback doesn't respect serde rename attributes, so it will likely fail
-        // with "missing field ambientLightIntensity" type errors
-        let builder = ConfigBuilder::<config::builder::DefaultState>::default()
-            .add_source(config::File::from(settings_path.clone()).required(true))
-            .add_source(
-                Environment::default()
-                    .separator("_")
-                    .list_separator(",")
-            );
-        let config = builder.build()?;
-        debug!("Configuration built successfully. Attempting deserialization...");
-
-        let settings: AppFullSettings = config.clone().try_deserialize()
-            .map_err(|e| {
-                error!("Config crate deserialization failed as expected: {}", e);
-                error!("This is because config crate doesn't respect #[serde(rename_all = \"camelCase\")] attributes");
-                e
-            })?;
-
-        debug!("Unexpectedly succeeded with config crate fallback");
-        Ok(settings)
+        Ok(Self::default())
     }


+    /// DEPRECATED: Settings persistence now handled by SQLite database
+    ///
+    /// This method no longer writes to YAML files. All settings changes should be
+    /// made through the SettingsService API which persists to SQLite automatically.
+    #[deprecated(note = "Use SettingsService to persist settings to SQLite database")]
     pub fn save(&self) -> Result<(), String> {
-        // Check if persist_settings is enabled
-        if !self.system.persist_settings {
-            debug!("Settings persistence is disabled (persist_settings: false), skipping save");
-            return Ok(());
-        }
-
-        let settings_path = std::env::var("SETTINGS_FILE_PATH")
-            .map(PathBuf::from)
-            .unwrap_or_else(|_| PathBuf::from("data/settings.yaml"));
-        info!("Saving AppFullSettings to YAML file: {:?}", settings_path);
-
-        // Create parent directory if it doesn't exist
-        if let Some(parent) = settings_path.parent() {
-            std::fs::create_dir_all(parent)
-                .map_err(|e| format!("Failed to create directory {:?}: {}", parent, e))?;
-        }
-
-        let yaml = serde_yaml::to_string(&self)
-            .map_err(|e| format!("Failed to serialize AppFullSettings to YAML: {}", e))?;
-
-        std::fs::write(&settings_path, yaml)
-            .map_err(|e| format!("Failed to write settings file {:?}: {}", settings_path, e))?;
-        info!("Successfully saved AppFullSettings to {:?}", settings_path);
-        Ok(())
+        Err("Settings persistence has been moved to SQLite database. Use SettingsService API instead.".to_string())
     }

     /// Get physics settings for a specific graph
@@ -1967,6 +2001,27 @@ impl PathAccessible for AppFullSettings {
                     Err("Auth fields are not deeply accessible".to_string())
                 }
             }
+            "performance" => {
+                if segments.len() == 1 {
+                    Ok(Box::new(self.performance.clone()))
+                } else {
+                    Err("Performance fields are not deeply accessible".to_string())
+                }
+            }
+            "interaction" => {
+                if segments.len() == 1 {
+                    Ok(Box::new(self.interaction.clone()))
+                } else {
+                    Err("Interaction fields are not deeply accessible".to_string())
+                }
+            }
+            "export" => {
+                if segments.len() == 1 {
+                    Ok(Box::new(self.export.clone()))
+                } else {
+                    Err("Export fields are not deeply accessible".to_string())
+                }
+            }
             _ => Err(format!("Unknown top-level field: {}", segments[0]))
         }
     }
@@ -2030,6 +2085,45 @@ impl PathAccessible for AppFullSettings {
                     Err("Auth nested fields are not modifiable".to_string())
                 }
             }
+            "performance" => {
+                if segments.len() == 1 {
+                    match value.downcast::<PerformanceSettings>() {
+                        Ok(v) => {
+                            self.performance = *v;
+                            Ok(())
+                        }
+                        Err(_) => Err("Type mismatch for performance field".to_string())
+                    }
+                } else {
+                    Err("Performance fields are not deeply settable".to_string())
+                }
+            }
+            "interaction" => {
+                if segments.len() == 1 {
+                    match value.downcast::<InteractionSettings>() {
+                        Ok(v) => {
+                            self.interaction = *v;
+                            Ok(())
+                        }
+                        Err(_) => Err("Type mismatch for interaction field".to_string())
+                    }
+                } else {
+                    Err("Interaction fields are not deeply settable".to_string())
+                }
+            }
+            "export" => {
+                if segments.len() == 1 {
+                    match value.downcast::<ExportSettings>() {
+                        Ok(v) => {
+                            self.export = *v;
+                            Ok(())
+                        }
+                        Err(_) => Err("Type mismatch for export field".to_string())
+                    }
+                } else {
+                    Err("Export fields are not deeply settable".to_string())
+                }
+            }
             _ => Err(format!("Unknown top-level field: {}", segments[0]))
         }
     }
diff --git a/src/handlers/admin_handler.rs b/src/handlers/admin_handler.rs
new file mode 100644
index 00000000..d2518ce0
--- /dev/null
+++ b/src/handlers/admin_handler.rs
@@ -0,0 +1,262 @@
+use actix_web::{web, Error, HttpResponse, HttpRequest};
+use serde::{Deserialize, Serialize};
+use log::{error, info};
+
+use crate::services::user_service::{UserService, UserServiceError, User, AuditLogEntry};
+use crate::middleware::permissions::extract_auth_context;
+
+#[derive(Debug, Serialize)]
+struct UsersListResponse {
+    users: Vec<User>,
+    count: usize,
+}
+
+#[derive(Debug, Serialize)]
+struct UserResponse {
+    user: User,
+}
+
+#[derive(Debug, Serialize)]
+struct MessageResponse {
+    message: String,
+}
+
+#[derive(Debug, Serialize)]
+struct ErrorResponse {
+    error: String,
+}
+
+#[derive(Debug, Serialize)]
+struct AuditLogResponse {
+    entries: Vec<AuditLogEntry>,
+    count: usize,
+}
+
+#[derive(Debug, Deserialize)]
+struct AuditLogQuery {
+    key: Option<String>,
+    user_id: Option<i64>,
+    #[serde(default = "default_limit")]
+    limit: i64,
+}
+
+fn default_limit() -> i64 {
+    100
+}
+
+pub async fn list_users(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    match user_service.list_all_users().await {
+        Ok(users) => {
+            let count = users.len();
+            info!("Listed {} users for admin {}", count, auth_context.nostr_pubkey);
+            Ok(HttpResponse::Ok().json(UsersListResponse { users, count }))
+        }
+        Err(e) => {
+            error!("Failed to list users: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch users".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn get_user_by_pubkey(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    path: web::Path<String>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    let pubkey = path.into_inner();
+
+    match user_service.get_user_by_nostr_pubkey(&pubkey).await {
+        Ok(user) => {
+            info!("Fetched user {} for admin {}", pubkey, auth_context.nostr_pubkey);
+            Ok(HttpResponse::Ok().json(UserResponse { user }))
+        }
+        Err(UserServiceError::UserNotFound) => {
+            Ok(HttpResponse::NotFound().json(ErrorResponse {
+                error: "User not found".to_string(),
+            }))
+        }
+        Err(e) => {
+            error!("Failed to fetch user {}: {:?}", pubkey, e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch user".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn grant_power_user(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    path: web::Path<String>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    let pubkey = path.into_inner();
+
+    let target_user = match user_service.get_user_by_nostr_pubkey(&pubkey).await {
+        Ok(user) => user,
+        Err(UserServiceError::UserNotFound) => {
+            return Ok(HttpResponse::NotFound().json(ErrorResponse {
+                error: "User not found".to_string(),
+            }));
+        }
+        Err(e) => {
+            error!("Failed to fetch user {}: {:?}", pubkey, e);
+            return Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch user".to_string(),
+            }));
+        }
+    };
+
+    match user_service.grant_power_user(target_user.id).await {
+        Ok(()) => {
+            info!(
+                "Admin {} granted power user to user {}",
+                auth_context.nostr_pubkey, pubkey
+            );
+            Ok(HttpResponse::Ok().json(MessageResponse {
+                message: format!("Power user granted to {}", pubkey),
+            }))
+        }
+        Err(e) => {
+            error!("Failed to grant power user to {}: {:?}", pubkey, e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to grant power user".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn revoke_power_user(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    path: web::Path<String>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    let pubkey = path.into_inner();
+
+    let target_user = match user_service.get_user_by_nostr_pubkey(&pubkey).await {
+        Ok(user) => user,
+        Err(UserServiceError::UserNotFound) => {
+            return Ok(HttpResponse::NotFound().json(ErrorResponse {
+                error: "User not found".to_string(),
+            }));
+        }
+        Err(e) => {
+            error!("Failed to fetch user {}: {:?}", pubkey, e);
+            return Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch user".to_string(),
+            }));
+        }
+    };
+
+    match user_service.revoke_power_user(target_user.id).await {
+        Ok(()) => {
+            info!(
+                "Admin {} revoked power user from user {}",
+                auth_context.nostr_pubkey, pubkey
+            );
+            Ok(HttpResponse::Ok().json(MessageResponse {
+                message: format!("Power user revoked from {}", pubkey),
+            }))
+        }
+        Err(e) => {
+            error!("Failed to revoke power user from {}: {:?}", pubkey, e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to revoke power user".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn get_audit_log(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    query: web::Query<AuditLogQuery>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required".to_string(),
+        }));
+    }
+
+    let limit = if query.limit > 1000 { 1000 } else { query.limit };
+
+    match user_service
+        .get_audit_log(query.key.clone(), query.user_id, limit)
+        .await
+    {
+        Ok(entries) => {
+            let count = entries.len();
+            info!(
+                "Fetched {} audit log entries for admin {}",
+                count, auth_context.nostr_pubkey
+            );
+            Ok(HttpResponse::Ok().json(AuditLogResponse { entries, count }))
+        }
+        Err(e) => {
+            error!("Failed to fetch audit log: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch audit log".to_string(),
+            }))
+        }
+    }
+}
+
+pub fn configure_routes(cfg: &mut web::ServiceConfig) {
+    cfg.service(
+        web::scope("/api/admin")
+            .route("/users", web::get().to(list_users))
+            .route("/users/{pubkey}", web::get().to(get_user_by_pubkey))
+            .route(
+                "/users/{pubkey}/power-user",
+                web::put().to(grant_power_user),
+            )
+            .route(
+                "/users/{pubkey}/power-user",
+                web::delete().to(revoke_power_user),
+            )
+            .route("/settings/audit", web::get().to(get_audit_log)),
+    );
+}
diff --git a/src/handlers/api_handler/analytics/clustering.rs b/src/handlers/api_handler/analytics/clustering.rs
index 524ec993..edea3518 100644
--- a/src/handlers/api_handler/analytics/clustering.rs
+++ b/src/handlers/api_handler/analytics/clustering.rs
@@ -65,7 +65,7 @@ pub async fn perform_clustering(

     // Get current graph data
     let graph_data = {
-        let graph_addr = app_state.get_graph_service_addr();
+        let graph_addr = app_state.get_graph_state_addr();
         match graph_addr.send(GetGraphData).await {
             Ok(Ok(data)) => data,
             Ok(Err(e)) => {
diff --git a/src/handlers/api_handler/analytics/mod.rs b/src/handlers/api_handler/analytics/mod.rs
index fd5d1f37..880f7d0a 100644
--- a/src/handlers/api_handler/analytics/mod.rs
+++ b/src/handlers/api_handler/analytics/mod.rs
@@ -1058,7 +1058,7 @@ async fn perform_clustering(

     // Get graph data for clustering
     let graph_data = {
-        match app_state.graph_service_addr.send(GetGraphData).await {
+        match app_state.graph_state_addr.send(GetGraphData).await {
             Ok(Ok(data)) => data,
             _ => return Err("Failed to get graph data".to_string()),
         }
@@ -1431,7 +1431,7 @@ pub async fn get_ai_insights(
     info!("Generating AI insights for graph analysis");

     // Get current graph data
-    let graph_data = match app_state.graph_service_addr.send(GetGraphData).await {
+    let graph_data = match app_state.graph_state_addr.send(GetGraphData).await {
         Ok(Ok(data)) => Some(data),
         _ => None,
     };
@@ -1933,7 +1933,7 @@ pub async fn get_realtime_insights(
     info!("Client requesting real-time AI insights");

     // Get current state for real-time analysis
-    let graph_data = app_state.graph_service_addr.send(GetGraphData).await
+    let graph_data = app_state.graph_state_addr.send(GetGraphData).await
         .map_err(|e| {
             error!("Failed to get graph data: {}", e);
             actix_web::error::ErrorInternalServerError("Failed to get graph data")
@@ -2094,14 +2094,14 @@ impl Default for FeatureFlags {
     fn default() -> Self {
         Self {
             gpu_clustering: true,
+            ontology_validation: false,
             gpu_anomaly_detection: true,
             real_time_insights: true,
             advanced_visualizations: true,
             performance_monitoring: true,
-            stress_majorization: false, // Disabled by default as per task.md
-            semantic_constraints: false, // Disabled by default
+            stress_majorization: false,
+            semantic_constraints: false,
             sssp_integration: true,
-            ontology_validation: false, // Disabled by default
         }
     }
 }
@@ -2448,7 +2448,7 @@ pub async fn compute_sssp(

     // Send SSSP computation request to graph service
     use crate::actors::messages::ComputeShortestPaths;
-    match app_state.graph_service_addr.send(ComputeShortestPaths {
+    match app_state.graph_state_addr.send(ComputeShortestPaths {
         source_node_id: source_node,
     }).await {
         Ok(Ok(_)) => {
diff --git a/src/handlers/api_handler/files/mod.rs b/src/handlers/api_handler/files/mod.rs
index efded917..186ce6f1 100644
--- a/src/handlers/api_handler/files/mod.rs
+++ b/src/handlers/api_handler/files/mod.rs
@@ -62,7 +62,7 @@ pub async fn fetch_and_process_files(state: web::Data<AppState>) -> HttpResponse
             }

             // Send AddNodesFromMetadata for incremental updates instead of full rebuild
-            match state.graph_service_addr.send(AddNodesFromMetadata { metadata: metadata_store.clone() }).await {
+            match state.graph_state_addr.send(AddNodesFromMetadata { metadata: metadata_store.clone() }).await {
                 Ok(Ok(())) => {
                     info!("Graph data structure updated successfully via GraphServiceActor");

@@ -134,7 +134,7 @@ pub async fn refresh_graph(state: web::Data<AppState>) -> HttpResponse {
     info!("Manually triggering graph refresh - returning current state");

     // Instead of rebuilding, just return the current graph data
-    match state.graph_service_addr.send(crate::actors::messages::GetGraphData).await {
+    match state.graph_state_addr.send(crate::actors::messages::GetGraphData).await {
         Ok(Ok(graph_data)) => {
             debug!("Retrieved current graph state with {} nodes and {} edges",
                 graph_data.nodes.len(),
@@ -177,7 +177,7 @@ pub async fn update_graph(state: web::Data<AppState>) -> Result<HttpResponse, Ac
         }
     };

-    match state.graph_service_addr.send(AddNodesFromMetadata { metadata: metadata_store.clone() }).await {
+    match state.graph_state_addr.send(AddNodesFromMetadata { metadata: metadata_store.clone() }).await {
         Ok(Ok(())) => {
             info!("Graph data structure updated successfully via GraphServiceActor in update_graph");

diff --git a/src/handlers/api_handler/graph/mod.rs b/src/handlers/api_handler/graph/mod.rs
index 8868a548..eb5ce487 100644
--- a/src/handlers/api_handler/graph/mod.rs
+++ b/src/handlers/api_handler/graph/mod.rs
@@ -96,9 +96,9 @@ pub async fn get_graph_data(state: web::Data<AppState>, req: HttpRequest) -> imp
     info!("Received request for graph data with positions");

     // Fetch graph structure AND physics state in parallel
-    let graph_data_future = state.graph_service_addr.send(GetGraphData);
-    let node_map_future = state.graph_service_addr.send(GetNodeMap);
-    let physics_state_future = state.graph_service_addr.send(GetPhysicsState);
+    let graph_data_future = state.graph_state_addr.send(GetGraphData);
+    let node_map_future = state.graph_state_addr.send(GetNodeMap);
+    let physics_state_future = state.graph_state_addr.send(GetPhysicsState);

     let (graph_result, node_map_result, physics_result) = tokio::join!(
         graph_data_future,
@@ -185,7 +185,7 @@ pub async fn get_paginated_graph_data(
     // This part is complex due to mutable access.
     // For now, let's assume get_graph_data_mut was for reading and we use GetGraphData.
     // If mutable access is truly needed, specific messages for modifications are required.
-    let graph_result = state.graph_service_addr.send(GetGraphData).await;
+    let graph_result = state.graph_state_addr.send(GetGraphData).await;
     let graph_data_owned = match graph_result { // graph_data_owned is GraphData
         Ok(Ok(g_owned)) => g_owned,
         _ => {
@@ -254,7 +254,7 @@ pub async fn refresh_graph(state: web::Data<AppState>) -> impl Responder {
     info!("Received request to refresh graph - returning current state");

     // Instead of rebuilding, just return the current graph data
-    let graph_data_result = state.graph_service_addr.send(GetGraphData).await;
+    let graph_data_result = state.graph_state_addr.send(GetGraphData).await;

     match graph_data_result {
         Ok(Ok(graph_data_owned)) => {
@@ -340,7 +340,7 @@ pub async fn update_graph(state: web::Data<AppState>) -> impl Responder {
             }

             // Send AddNodesFromMetadata for incremental updates instead of full rebuild
-            match state.graph_service_addr.send(AddNodesFromMetadata { metadata }).await {
+            match state.graph_state_addr.send(AddNodesFromMetadata { metadata }).await {
                 Ok(Ok(())) => {
                     // Position preservation logic would need to be handled by the actor or subsequent messages.
                     debug!("Graph updated successfully via GraphServiceActor after file processing");
@@ -384,8 +384,8 @@ pub async fn get_auto_balance_notifications(
         .and_then(|v| v.as_i64());

     let msg = GetAutoBalanceNotifications { since_timestamp };
-
-    match state.graph_service_addr.send(msg).await {
+
+    match state.graph_state_addr.send(msg).await {
         Ok(Ok(notifications)) => {
             HttpResponse::Ok().json(serde_json::json!({
                 "success": true,
diff --git a/src/handlers/api_handler/mod.rs b/src/handlers/api_handler/mod.rs
index 5adc51c7..215f19e2 100644
--- a/src/handlers/api_handler/mod.rs
+++ b/src/handlers/api_handler/mod.rs
@@ -6,7 +6,10 @@ pub mod analytics;
 pub mod quest3;
 #[cfg(feature = "ontology")]
 pub mod ontology;
+#[cfg(feature = "ontology")]
+pub mod ontology_data;
 pub mod sessions;
+// pub mod settings; // Disabled - incompatible with current SettingsService API

 // Re-export specific types and functions
 // Re-export specific types and functions
@@ -39,16 +42,19 @@ pub fn config(cfg: &mut web::ServiceConfig) {
             .configure(analytics::config)
             .configure(quest3::config)
             .configure(crate::handlers::nostr_handler::config)
-            .configure(crate::handlers::settings_handler::config)
-            .configure(crate::handlers::settings_paths::configure_settings_paths)
+            .configure(crate::handlers::settings_handler::config) // Includes settings_paths via internal configure
+            .configure(settings::configure_routes) // REST API for settings management
             .configure(crate::handlers::ragflow_handler::config)
             .configure(crate::handlers::clustering_handler::config)
             .configure(crate::handlers::constraints_handler::config)
     );

     #[cfg(feature = "ontology")]
-    cfg.service(
-        web::scope("")
-            .configure(ontology::config)
-    );
+    {
+        cfg.service(
+            web::scope("")
+                .configure(ontology::config)
+                .configure(ontology_data::config)
+        );
+    }
 }
diff --git a/src/handlers/api_handler/ontology_data/cache.rs b/src/handlers/api_handler/ontology_data/cache.rs
new file mode 100644
index 00000000..3b62eb7b
--- /dev/null
+++ b/src/handlers/api_handler/ontology_data/cache.rs
@@ -0,0 +1,416 @@
+//! Cache Layer for Ontology Data
+//!
+//! Provides in-memory caching with TTL, LRU eviction, and cache invalidation
+//! for ontology queries, entities, and graph visualization data.
+
+use chrono::{DateTime, Utc};
+use log::{debug, info, warn};
+use lru::LruCache;
+use std::collections::HashMap;
+use std::num::NonZeroUsize;
+use std::sync::{Arc, Mutex};
+use std::time::{Duration, Instant};
+
+use super::QueryResult;
+
+/// Cache entry with TTL
+#[derive(Clone, Debug)]
+struct CacheEntry<T> {
+    data: T,
+    inserted_at: Instant,
+    ttl: Duration,
+    access_count: u64,
+}
+
+impl<T: Clone> CacheEntry<T> {
+    fn new(data: T, ttl: Duration) -> Self {
+        Self {
+            data,
+            inserted_at: Instant::now(),
+            ttl,
+            access_count: 0,
+        }
+    }
+
+    fn is_expired(&self) -> bool {
+        self.inserted_at.elapsed() > self.ttl
+    }
+
+    fn get(&mut self) -> Option<T> {
+        if self.is_expired() {
+            None
+        } else {
+            self.access_count += 1;
+            Some(self.data.clone())
+        }
+    }
+}
+
+/// Cache statistics
+#[derive(Clone, Debug)]
+pub struct CacheStats {
+    pub hits: u64,
+    pub misses: u64,
+    pub evictions: u64,
+    pub size: usize,
+    pub capacity: usize,
+    pub hit_rate: f32,
+}
+
+/// Ontology cache with LRU eviction and TTL
+pub struct OntologyCache {
+    query_cache: Arc<Mutex<LruCache<String, CacheEntry<QueryResult>>>>,
+    entity_cache: Arc<Mutex<LruCache<String, CacheEntry<String>>>>, // Stores JSON strings
+    stats: Arc<Mutex<CacheStats>>,
+    default_ttl: Duration,
+}
+
+impl OntologyCache {
+    /// Create new cache with default configuration
+    pub fn new() -> Self {
+        Self::with_capacity(1000, Duration::from_secs(3600))
+    }
+
+    /// Create cache with specified capacity and TTL
+    pub fn with_capacity(capacity: usize, default_ttl: Duration) -> Self {
+        let cache_size = NonZeroUsize::new(capacity).unwrap();
+
+        Self {
+            query_cache: Arc::new(Mutex::new(LruCache::new(cache_size))),
+            entity_cache: Arc::new(Mutex::new(LruCache::new(cache_size))),
+            stats: Arc::new(Mutex::new(CacheStats {
+                hits: 0,
+                misses: 0,
+                evictions: 0,
+                size: 0,
+                capacity,
+                hit_rate: 0.0,
+            })),
+            default_ttl,
+        }
+    }
+
+    /// Get query result from cache
+    pub fn get_query_result(&self, key: &str) -> Option<QueryResult> {
+        let mut cache = self.query_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        if let Some(entry) = cache.get_mut(key) {
+            if let Some(result) = entry.get() {
+                stats.hits += 1;
+                stats.hit_rate = stats.hits as f32 / (stats.hits + stats.misses) as f32;
+                debug!("Cache hit for query: {}", key);
+                return Some(result);
+            } else {
+                // Entry expired, remove it
+                cache.pop(key);
+                stats.evictions += 1;
+            }
+        }
+
+        stats.misses += 1;
+        stats.hit_rate = stats.hits as f32 / (stats.hits + stats.misses) as f32;
+        debug!("Cache miss for query: {}", key);
+        None
+    }
+
+    /// Set query result in cache
+    pub fn set_query_result(&self, key: &str, result: &QueryResult) {
+        let mut cache = self.query_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        let entry = CacheEntry::new(result.clone(), self.default_ttl);
+
+        // Check if we're evicting an entry
+        if cache.len() >= cache.cap().get() {
+            stats.evictions += 1;
+        }
+
+        cache.put(key.to_string(), entry);
+        stats.size = cache.len();
+
+        debug!("Cached query result: {}", key);
+    }
+
+    /// Get entity from cache
+    pub fn get_entity(&self, entity_id: &str) -> Option<String> {
+        let mut cache = self.entity_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        if let Some(entry) = cache.get_mut(entity_id) {
+            if let Some(data) = entry.get() {
+                stats.hits += 1;
+                stats.hit_rate = stats.hits as f32 / (stats.hits + stats.misses) as f32;
+                debug!("Cache hit for entity: {}", entity_id);
+                return Some(data);
+            } else {
+                // Entry expired, remove it
+                cache.pop(entity_id);
+                stats.evictions += 1;
+            }
+        }
+
+        stats.misses += 1;
+        stats.hit_rate = stats.hits as f32 / (stats.hits + stats.misses) as f32;
+        debug!("Cache miss for entity: {}", entity_id);
+        None
+    }
+
+    /// Set entity in cache
+    pub fn set_entity(&self, entity_id: &str, data: &str) {
+        let mut cache = self.entity_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        let entry = CacheEntry::new(data.to_string(), self.default_ttl);
+
+        // Check if we're evicting an entry
+        if cache.len() >= cache.cap().get() {
+            stats.evictions += 1;
+        }
+
+        cache.put(entity_id.to_string(), entry);
+        stats.size = cache.len();
+
+        debug!("Cached entity: {}", entity_id);
+    }
+
+    /// Invalidate specific cache key
+    pub fn invalidate(&self, key: &str) {
+        let mut query_cache = self.query_cache.lock().unwrap();
+        let mut entity_cache = self.entity_cache.lock().unwrap();
+
+        query_cache.pop(key);
+        entity_cache.pop(key);
+
+        info!("Invalidated cache key: {}", key);
+    }
+
+    /// Invalidate all entries matching a pattern
+    pub fn invalidate_pattern(&self, pattern: &str) {
+        let mut query_cache = self.query_cache.lock().unwrap();
+        let mut entity_cache = self.entity_cache.lock().unwrap();
+
+        let query_keys: Vec<String> = query_cache
+            .iter()
+            .filter(|(k, _)| k.contains(pattern))
+            .map(|(k, _)| k.clone())
+            .collect();
+
+        let entity_keys: Vec<String> = entity_cache
+            .iter()
+            .filter(|(k, _)| k.contains(pattern))
+            .map(|(k, _)| k.clone())
+            .collect();
+
+        let total_invalidated = query_keys.len() + entity_keys.len();
+
+        for key in query_keys {
+            query_cache.pop(&key);
+        }
+
+        for key in entity_keys {
+            entity_cache.pop(&key);
+        }
+
+        info!("Invalidated {} cache entries matching pattern: {}",
+            total_invalidated, pattern);
+    }
+
+    /// Clear all cache entries
+    pub fn clear(&self) {
+        let mut query_cache = self.query_cache.lock().unwrap();
+        let mut entity_cache = self.entity_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        let total_cleared = query_cache.len() + entity_cache.len();
+
+        query_cache.clear();
+        entity_cache.clear();
+
+        stats.size = 0;
+        stats.evictions += total_cleared as u64;
+
+        info!("Cleared {} cache entries", total_cleared);
+    }
+
+    /// Remove expired entries
+    pub fn evict_expired(&self) {
+        let mut query_cache = self.query_cache.lock().unwrap();
+        let mut entity_cache = self.entity_cache.lock().unwrap();
+        let mut stats = self.stats.lock().unwrap();
+
+        let mut evicted = 0;
+
+        // Collect expired query keys
+        let expired_query_keys: Vec<String> = query_cache
+            .iter()
+            .filter(|(_, entry)| entry.is_expired())
+            .map(|(k, _)| k.clone())
+            .collect();
+
+        // Collect expired entity keys
+        let expired_entity_keys: Vec<String> = entity_cache
+            .iter()
+            .filter(|(_, entry)| entry.is_expired())
+            .map(|(k, _)| k.clone())
+            .collect();
+
+        // Remove expired entries
+        for key in expired_query_keys {
+            query_cache.pop(&key);
+            evicted += 1;
+        }
+
+        for key in expired_entity_keys {
+            entity_cache.pop(&key);
+            evicted += 1;
+        }
+
+        stats.evictions += evicted;
+        stats.size = query_cache.len() + entity_cache.len();
+
+        if evicted > 0 {
+            debug!("Evicted {} expired cache entries", evicted);
+        }
+    }
+
+    /// Get cache statistics
+    pub fn get_stats(&self) -> CacheStats {
+        let stats = self.stats.lock().unwrap();
+        stats.clone()
+    }
+
+    /// Reset statistics
+    pub fn reset_stats(&self) {
+        let mut stats = self.stats.lock().unwrap();
+        stats.hits = 0;
+        stats.misses = 0;
+        stats.evictions = 0;
+        stats.hit_rate = 0.0;
+        info!("Reset cache statistics");
+    }
+}
+
+impl Default for OntologyCache {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+
+/// Background task to periodically evict expired entries
+pub fn start_cache_eviction_task(cache: Arc<OntologyCache>) {
+    use std::thread;
+
+    thread::spawn(move || {
+        loop {
+            thread::sleep(Duration::from_secs(300)); // Every 5 minutes
+            cache.evict_expired();
+        }
+    });
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use chrono::Utc;
+    use uuid::Uuid;
+
+    #[test]
+    fn test_cache_basic_operations() {
+        let cache = OntologyCache::new();
+
+        let query_result = QueryResult {
+            query_id: Uuid::new_v4().to_string(),
+            results: vec![],
+            total_count: 0,
+            execution_time_ms: 100,
+            execution_plan: None,
+            from_cache: false,
+            timestamp: Utc::now(),
+        };
+
+        // Set and get
+        cache.set_query_result("test_query", &query_result);
+        let retrieved = cache.get_query_result("test_query");
+        assert!(retrieved.is_some());
+
+        // Cache miss
+        let missing = cache.get_query_result("nonexistent");
+        assert!(missing.is_none());
+    }
+
+    #[test]
+    fn test_cache_expiration() {
+        let cache = OntologyCache::with_capacity(100, Duration::from_millis(100));
+
+        let query_result = QueryResult {
+            query_id: Uuid::new_v4().to_string(),
+            results: vec![],
+            total_count: 0,
+            execution_time_ms: 100,
+            execution_plan: None,
+            from_cache: false,
+            timestamp: Utc::now(),
+        };
+
+        cache.set_query_result("expiring_query", &query_result);
+
+        // Should exist immediately
+        assert!(cache.get_query_result("expiring_query").is_some());
+
+        // Wait for expiration
+        std::thread::sleep(Duration::from_millis(150));
+
+        // Should be expired
+        assert!(cache.get_query_result("expiring_query").is_none());
+    }
+
+    #[test]
+    fn test_cache_stats() {
+        let cache = OntologyCache::new();
+
+        let query_result = QueryResult {
+            query_id: Uuid::new_v4().to_string(),
+            results: vec![],
+            total_count: 0,
+            execution_time_ms: 100,
+            execution_plan: None,
+            from_cache: false,
+            timestamp: Utc::now(),
+        };
+
+        cache.set_query_result("test", &query_result);
+
+        // Hit
+        cache.get_query_result("test");
+
+        // Miss
+        cache.get_query_result("nonexistent");
+
+        let stats = cache.get_stats();
+        assert_eq!(stats.hits, 1);
+        assert_eq!(stats.misses, 1);
+        assert_eq!(stats.hit_rate, 0.5);
+    }
+
+    #[test]
+    fn test_cache_invalidation() {
+        let cache = OntologyCache::new();
+
+        let query_result = QueryResult {
+            query_id: Uuid::new_v4().to_string(),
+            results: vec![],
+            total_count: 0,
+            execution_time_ms: 100,
+            execution_plan: None,
+            from_cache: false,
+            timestamp: Utc::now(),
+        };
+
+        cache.set_query_result("test", &query_result);
+        assert!(cache.get_query_result("test").is_some());
+
+        cache.invalidate("test");
+        assert!(cache.get_query_result("test").is_none());
+    }
+}
diff --git a/src/handlers/api_handler/ontology_data/db.rs b/src/handlers/api_handler/ontology_data/db.rs
new file mode 100644
index 00000000..caee55bd
--- /dev/null
+++ b/src/handlers/api_handler/ontology_data/db.rs
@@ -0,0 +1,528 @@
+//! SQLite Database Layer for Ontology Data
+//!
+//! Provides persistent storage and querying for ontology metadata, domains, classes,
+//! properties, entities, and relationships. Uses SQLite for lightweight, embedded storage
+//! with full-text search and indexing capabilities.
+
+use chrono::{DateTime, Utc};
+use log::{debug, error, info, warn};
+use serde_json::Value as JsonValue;
+use std::collections::HashMap;
+use std::path::PathBuf;
+use std::sync::{Arc, Mutex};
+
+use super::{
+    CardinalityConstraint, ClassInfo, DomainInfo, EntityInfo, GraphEdge, GraphNode,
+    GraphVisualizationData, PropertyInfo, RelationshipInfo,
+};
+
+/// Ontology database connection
+pub struct OntologyDatabase {
+    db_path: PathBuf,
+    conn: Arc<Mutex<Option<()>>>, // Placeholder for actual SQLite connection
+}
+
+impl OntologyDatabase {
+    /// Create new database instance
+    pub fn new() -> Result<Self, String> {
+        let db_path = Self::get_db_path();
+
+        info!("Initializing ontology database at: {:?}", db_path);
+
+        // Ensure directory exists
+        if let Some(parent) = db_path.parent() {
+            std::fs::create_dir_all(parent)
+                .map_err(|e| format!("Failed to create database directory: {}", e))?;
+        }
+
+        let db = Self {
+            db_path,
+            conn: Arc::new(Mutex::new(None)),
+        };
+
+        db.initialize_schema()?;
+
+        Ok(db)
+    }
+
+    /// Get database file path
+    fn get_db_path() -> PathBuf {
+        let mut path = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
+        path.push(".data");
+        path.push("ontology.db");
+        path
+    }
+
+    /// Initialize database schema
+    fn initialize_schema(&self) -> Result<(), String> {
+        debug!("Initializing database schema");
+
+        // In a real implementation, this would execute SQL to create tables:
+        // - domains (id, name, description, namespace, class_count, property_count, updated_at)
+        // - classes (id, name, description, domain_id, namespace, instance_count, created_at)
+        // - class_hierarchy (parent_id, child_id)
+        // - properties (id, name, description, property_type, domain_id, is_functional, is_inverse_functional, is_transitive, is_symmetric)
+        // - property_domain_constraints (property_id, class_id)
+        // - property_range_constraints (property_id, class_id)
+        // - property_cardinality (property_id, min_cardinality, max_cardinality, exact_cardinality)
+        // - entities (id, label, entity_type, domain_id, properties_json, created_at, updated_at)
+        // - relationships (id, source_id, target_id, relationship_type, properties_json, is_inferred, confidence)
+        // - entity_fts (for full-text search on entity labels)
+
+        // Placeholder: In production, use rusqlite or another SQLite library
+        info!("Database schema initialized (mock implementation)");
+
+        Ok(())
+    }
+
+    /// List all domains
+    pub fn list_domains(
+        &self,
+        filter: Option<&str>,
+        include_stats: bool,
+    ) -> Result<Vec<DomainInfo>, String> {
+        debug!("Listing domains: filter={:?}, include_stats={}", filter, include_stats);
+
+        // Mock data for demonstration
+        let domains = vec![
+            DomainInfo {
+                id: "etsi-nfv".to_string(),
+                name: "ETSI NFV".to_string(),
+                description: "Network Functions Virtualization domain".to_string(),
+                class_count: 125,
+                property_count: 250,
+                namespace: "http://etsi.org/nfv#".to_string(),
+                updated_at: Utc::now(),
+            },
+            DomainInfo {
+                id: "etsi-mec".to_string(),
+                name: "ETSI MEC".to_string(),
+                description: "Multi-access Edge Computing domain".to_string(),
+                class_count: 85,
+                property_count: 175,
+                namespace: "http://etsi.org/mec#".to_string(),
+                updated_at: Utc::now(),
+            },
+            DomainInfo {
+                id: "etsi-core".to_string(),
+                name: "ETSI Core".to_string(),
+                description: "Core ETSI ontology concepts".to_string(),
+                class_count: 50,
+                property_count: 100,
+                namespace: "http://etsi.org/core#".to_string(),
+                updated_at: Utc::now(),
+            },
+        ];
+
+        // Apply filter if provided
+        let filtered = if let Some(filter_str) = filter {
+            domains
+                .into_iter()
+                .filter(|d| {
+                    d.name.to_lowercase().contains(&filter_str.to_lowercase())
+                        || d.id.to_lowercase().contains(&filter_str.to_lowercase())
+                })
+                .collect()
+        } else {
+            domains
+        };
+
+        Ok(filtered)
+    }
+
+    /// List ontology classes
+    pub fn list_classes(
+        &self,
+        domain: Option<&str>,
+        filter: Option<&str>,
+        include_subclasses: bool,
+        include_properties: bool,
+        offset: u32,
+        limit: u32,
+    ) -> Result<(Vec<ClassInfo>, usize), String> {
+        debug!(
+            "Listing classes: domain={:?}, filter={:?}, offset={}, limit={}",
+            domain, filter, offset, limit
+        );
+
+        // Mock data for demonstration
+        let mut classes = vec![
+            ClassInfo {
+                id: "vnf".to_string(),
+                name: "VirtualNetworkFunction".to_string(),
+                description: Some("A virtualized network function".to_string()),
+                parent_classes: vec!["network-function".to_string()],
+                child_classes: if include_subclasses {
+                    vec!["vnfc".to_string(), "vnf-instance".to_string()]
+                } else {
+                    vec![]
+                },
+                domain: "etsi-nfv".to_string(),
+                properties: if include_properties {
+                    vec![PropertyInfo {
+                        id: "has-vnfc".to_string(),
+                        name: "hasVNFC".to_string(),
+                        description: Some("VNF has VNFC components".to_string()),
+                        property_type: "object_property".to_string(),
+                        domain_classes: vec!["vnf".to_string()],
+                        range_classes: vec!["vnfc".to_string()],
+                        is_functional: false,
+                        is_inverse_functional: false,
+                        is_transitive: false,
+                        is_symmetric: false,
+                        cardinality: Some(CardinalityConstraint {
+                            min: Some(1),
+                            max: None,
+                            exact: None,
+                        }),
+                        domain: "etsi-nfv".to_string(),
+                    }]
+                } else {
+                    vec![]
+                },
+                instance_count: 42,
+                namespace: "http://etsi.org/nfv#".to_string(),
+            },
+            ClassInfo {
+                id: "mec-application".to_string(),
+                name: "MECApplication".to_string(),
+                description: Some("An application running at the edge".to_string()),
+                parent_classes: vec!["application".to_string()],
+                child_classes: if include_subclasses {
+                    vec!["mec-service".to_string()]
+                } else {
+                    vec![]
+                },
+                domain: "etsi-mec".to_string(),
+                properties: if include_properties {
+                    vec![PropertyInfo {
+                        id: "has-endpoint".to_string(),
+                        name: "hasEndpoint".to_string(),
+                        description: Some("Application service endpoint".to_string()),
+                        property_type: "data_property".to_string(),
+                        domain_classes: vec!["mec-application".to_string()],
+                        range_classes: vec!["xsd:string".to_string()],
+                        is_functional: false,
+                        is_inverse_functional: false,
+                        is_transitive: false,
+                        is_symmetric: false,
+                        cardinality: None,
+                        domain: "etsi-mec".to_string(),
+                    }]
+                } else {
+                    vec![]
+                },
+                instance_count: 28,
+                namespace: "http://etsi.org/mec#".to_string(),
+            },
+        ];
+
+        // Apply domain filter
+        if let Some(domain_filter) = domain {
+            classes.retain(|c| c.domain == domain_filter);
+        }
+
+        // Apply name filter
+        if let Some(filter_str) = filter {
+            classes.retain(|c| {
+                c.name.to_lowercase().contains(&filter_str.to_lowercase())
+                    || c.id.to_lowercase().contains(&filter_str.to_lowercase())
+            });
+        }
+
+        let total_count = classes.len();
+
+        // Apply pagination
+        let start = offset as usize;
+        let end = (start + limit as usize).min(total_count);
+        let paginated = if start < total_count {
+            classes[start..end].to_vec()
+        } else {
+            vec![]
+        };
+
+        Ok((paginated, total_count))
+    }
+
+    /// List ontology properties
+    pub fn list_properties(
+        &self,
+        domain: Option<&str>,
+        filter: Option<&str>,
+        property_type: Option<&str>,
+        include_constraints: bool,
+        offset: u32,
+        limit: u32,
+    ) -> Result<(Vec<PropertyInfo>, usize), String> {
+        debug!(
+            "Listing properties: domain={:?}, type={:?}, offset={}, limit={}",
+            domain, property_type, offset, limit
+        );
+
+        // Mock data for demonstration
+        let mut properties = vec![
+            PropertyInfo {
+                id: "has-vnfc".to_string(),
+                name: "hasVNFC".to_string(),
+                description: Some("VNF has VNFC components".to_string()),
+                property_type: "object_property".to_string(),
+                domain_classes: vec!["vnf".to_string()],
+                range_classes: vec!["vnfc".to_string()],
+                is_functional: false,
+                is_inverse_functional: false,
+                is_transitive: false,
+                is_symmetric: false,
+                cardinality: if include_constraints {
+                    Some(CardinalityConstraint {
+                        min: Some(1),
+                        max: None,
+                        exact: None,
+                    })
+                } else {
+                    None
+                },
+                domain: "etsi-nfv".to_string(),
+            },
+            PropertyInfo {
+                id: "deployment-status".to_string(),
+                name: "deploymentStatus".to_string(),
+                description: Some("Current deployment status".to_string()),
+                property_type: "data_property".to_string(),
+                domain_classes: vec!["vnf".to_string(), "mec-application".to_string()],
+                range_classes: vec!["xsd:string".to_string()],
+                is_functional: true,
+                is_inverse_functional: false,
+                is_transitive: false,
+                is_symmetric: false,
+                cardinality: if include_constraints {
+                    Some(CardinalityConstraint {
+                        min: None,
+                        max: Some(1),
+                        exact: None,
+                    })
+                } else {
+                    None
+                },
+                domain: "etsi-core".to_string(),
+            },
+            PropertyInfo {
+                id: "connected-to".to_string(),
+                name: "connectedTo".to_string(),
+                description: Some("Network connectivity relationship".to_string()),
+                property_type: "object_property".to_string(),
+                domain_classes: vec!["network-function".to_string()],
+                range_classes: vec!["network-function".to_string()],
+                is_functional: false,
+                is_inverse_functional: false,
+                is_transitive: false,
+                is_symmetric: true,
+                cardinality: None,
+                domain: "etsi-nfv".to_string(),
+            },
+        ];
+
+        // Apply filters
+        if let Some(domain_filter) = domain {
+            properties.retain(|p| p.domain == domain_filter);
+        }
+
+        if let Some(type_filter) = property_type {
+            properties.retain(|p| p.property_type == type_filter);
+        }
+
+        if let Some(filter_str) = filter {
+            properties.retain(|p| {
+                p.name.to_lowercase().contains(&filter_str.to_lowercase())
+                    || p.id.to_lowercase().contains(&filter_str.to_lowercase())
+            });
+        }
+
+        let total_count = properties.len();
+
+        // Apply pagination
+        let start = offset as usize;
+        let end = (start + limit as usize).min(total_count);
+        let paginated = if start < total_count {
+            properties[start..end].to_vec()
+        } else {
+            vec![]
+        };
+
+        Ok((paginated, total_count))
+    }
+
+    /// Get entity by ID with relationships
+    pub fn get_entity(
+        &self,
+        entity_id: &str,
+        include_incoming: bool,
+        include_outgoing: bool,
+        include_inferred: bool,
+        max_depth: u32,
+    ) -> Result<Option<EntityInfo>, String> {
+        debug!("Getting entity: {}, depth={}", entity_id, max_depth);
+
+        // Mock data for demonstration
+        if entity_id == "vnf-123" {
+            let mut properties = HashMap::new();
+            properties.insert("label".to_string(), serde_json::json!("Example VNF Instance"));
+            properties.insert("deploymentStatus".to_string(), serde_json::json!("deployed"));
+            properties.insert("version".to_string(), serde_json::json!("1.2.3"));
+
+            Ok(Some(EntityInfo {
+                id: entity_id.to_string(),
+                label: "Example VNF Instance".to_string(),
+                entity_type: "vnf".to_string(),
+                properties,
+                incoming_relationships: if include_incoming {
+                    vec![RelationshipInfo {
+                        id: "rel-1".to_string(),
+                        source_id: "vnf-manager-1".to_string(),
+                        target_id: entity_id.to_string(),
+                        relationship_type: "manages".to_string(),
+                        properties: HashMap::new(),
+                        is_inferred: false,
+                        confidence: None,
+                    }]
+                } else {
+                    vec![]
+                },
+                outgoing_relationships: if include_outgoing {
+                    vec![
+                        RelationshipInfo {
+                            id: "rel-2".to_string(),
+                            source_id: entity_id.to_string(),
+                            target_id: "vnfc-456".to_string(),
+                            relationship_type: "hasVNFC".to_string(),
+                            properties: HashMap::new(),
+                            is_inferred: false,
+                            confidence: None,
+                        },
+                        RelationshipInfo {
+                            id: "rel-3".to_string(),
+                            source_id: entity_id.to_string(),
+                            target_id: "vnf-789".to_string(),
+                            relationship_type: "connectedTo".to_string(),
+                            properties: HashMap::new(),
+                            is_inferred: false,
+                            confidence: None,
+                        },
+                    ]
+                } else {
+                    vec![]
+                },
+                inferred_relationships: if include_inferred {
+                    vec![RelationshipInfo {
+                        id: "rel-inferred-1".to_string(),
+                        source_id: entity_id.to_string(),
+                        target_id: "network-segment-1".to_string(),
+                        relationship_type: "deployedOn".to_string(),
+                        properties: HashMap::new(),
+                        is_inferred: true,
+                        confidence: Some(0.85),
+                    }]
+                } else {
+                    vec![]
+                },
+                related_entities: vec![
+                    "vnf-manager-1".to_string(),
+                    "vnfc-456".to_string(),
+                    "vnf-789".to_string(),
+                ],
+                domain: "etsi-nfv".to_string(),
+                created_at: Utc::now() - chrono::Duration::days(30),
+                updated_at: Utc::now() - chrono::Duration::hours(2),
+            }))
+        } else {
+            Ok(None)
+        }
+    }
+
+    /// Get graph visualization data
+    pub fn get_graph_visualization(
+        &self,
+        domain_filter: Option<&str>,
+        max_nodes: u32,
+    ) -> Result<GraphVisualizationData, String> {
+        debug!("Getting graph visualization: domain={:?}, max_nodes={}", domain_filter, max_nodes);
+
+        // Mock data for demonstration
+        let nodes = vec![
+            GraphNode {
+                id: "vnf-123".to_string(),
+                label: "Example VNF".to_string(),
+                node_type: "vnf".to_string(),
+                domain: "etsi-nfv".to_string(),
+                size: 1.5,
+                color: "#4a90e2".to_string(),
+                metadata: {
+                    let mut m = HashMap::new();
+                    m.insert("status".to_string(), serde_json::json!("deployed"));
+                    m
+                },
+            },
+            GraphNode {
+                id: "vnfc-456".to_string(),
+                label: "VNF Component".to_string(),
+                node_type: "vnfc".to_string(),
+                domain: "etsi-nfv".to_string(),
+                size: 1.0,
+                color: "#7cb342".to_string(),
+                metadata: HashMap::new(),
+            },
+            GraphNode {
+                id: "mec-app-789".to_string(),
+                label: "Edge Application".to_string(),
+                node_type: "mec-application".to_string(),
+                domain: "etsi-mec".to_string(),
+                size: 1.2,
+                color: "#fb8c00".to_string(),
+                metadata: HashMap::new(),
+            },
+        ];
+
+        let edges = vec![
+            GraphEdge {
+                id: "edge-1".to_string(),
+                source: "vnf-123".to_string(),
+                target: "vnfc-456".to_string(),
+                edge_type: "hasVNFC".to_string(),
+                label: "has component".to_string(),
+                is_inferred: false,
+                weight: 1.0,
+                metadata: HashMap::new(),
+            },
+            GraphEdge {
+                id: "edge-2".to_string(),
+                source: "vnf-123".to_string(),
+                target: "mec-app-789".to_string(),
+                edge_type: "connectedTo".to_string(),
+                label: "connected".to_string(),
+                is_inferred: true,
+                weight: 0.7,
+                metadata: {
+                    let mut m = HashMap::new();
+                    m.insert("confidence".to_string(), serde_json::json!(0.85));
+                    m
+                },
+            },
+        ];
+
+        let mut metadata = HashMap::new();
+        metadata.insert("generated_at".to_string(), serde_json::json!(Utc::now()));
+        metadata.insert("node_count".to_string(), serde_json::json!(nodes.len()));
+        metadata.insert("edge_count".to_string(), serde_json::json!(edges.len()));
+
+        Ok(GraphVisualizationData {
+            nodes,
+            edges,
+            metadata,
+        })
+    }
+}
+
+impl Default for OntologyDatabase {
+    fn default() -> Self {
+        Self::new().expect("Failed to create default OntologyDatabase")
+    }
+}
diff --git a/src/handlers/api_handler/ontology_data/mod.rs b/src/handlers/api_handler/ontology_data/mod.rs
new file mode 100644
index 00000000..d911b54d
--- /dev/null
+++ b/src/handlers/api_handler/ontology_data/mod.rs
@@ -0,0 +1,933 @@
+//! Ontology Data Exposure API
+//!
+//! REST endpoints and WebSocket handlers for exposing ontology data to clients.
+//! Provides domain listings, class hierarchies, property schemas, entity relationships,
+//! advanced query interface, real-time updates, and graph visualization integration.
+//!
+//! ## Endpoints
+//! - GET /api/ontology/domains - List all ETSI domains
+//! - GET /api/ontology/classes - List ontology classes with filters
+//! - GET /api/ontology/properties - List properties with schemas
+//! - GET /api/ontology/entities/:id - Get specific entity with relationships
+//! - POST /api/ontology/query - Advanced query interface
+//! - WebSocket /api/ontology/stream - Real-time ontology updates
+//!
+//! ## Features
+//! - SQLite-based caching for performance
+//! - Integration with graph visualization
+//! - Real-time WebSocket updates
+//! - Comprehensive error handling and validation
+
+use actix::prelude::*;
+use actix_web::{web, HttpRequest, HttpResponse, Responder, Error as ActixError};
+use actix_web_actors::ws;
+use chrono::{DateTime, Utc};
+use log::{info, debug, error, warn};
+use serde::{Deserialize, Serialize};
+use std::collections::HashMap;
+use std::time::Duration as StdDuration;
+use uuid::Uuid;
+
+mod db;
+mod cache;
+mod query;
+
+use crate::AppState;
+use crate::actors::messages::{
+    GetOntologyHealth, OntologyHealth, GetCachedOntologies, CachedOntologyInfo,
+    ValidateOntology, ValidationMode, GetOntologyReport
+};
+use crate::services::owl_validator::PropertyGraph;
+
+// Re-export submodules
+pub use db::OntologyDatabase;
+pub use cache::OntologyCache;
+pub use query::QueryEngine;
+
+// ============================================================================
+// REQUEST/RESPONSE DTOs
+// ============================================================================
+
+/// Request to list ETSI domains
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ListDomainsRequest {
+    /// Filter by domain name pattern
+    pub filter: Option<String>,
+    /// Include domain statistics
+    pub include_stats: Option<bool>,
+}
+
+/// ETSI domain information
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct DomainInfo {
+    /// Domain identifier (e.g., "etsi-nfv", "etsi-mec")
+    pub id: String,
+    /// Human-readable domain name
+    pub name: String,
+    /// Domain description
+    pub description: String,
+    /// Number of classes in domain
+    pub class_count: u32,
+    /// Number of properties in domain
+    pub property_count: u32,
+    /// Domain namespace URI
+    pub namespace: String,
+    /// Last updated timestamp
+    pub updated_at: DateTime<Utc>,
+}
+
+/// Response for domain listing
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct DomainsResponse {
+    pub domains: Vec<DomainInfo>,
+    pub total_count: usize,
+    pub timestamp: DateTime<Utc>,
+}
+
+/// Request to list ontology classes
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ListClassesRequest {
+    /// Filter by domain
+    pub domain: Option<String>,
+    /// Filter by class name pattern
+    pub filter: Option<String>,
+    /// Include subclasses
+    pub include_subclasses: Option<bool>,
+    /// Include properties
+    pub include_properties: Option<bool>,
+    /// Pagination offset
+    pub offset: Option<u32>,
+    /// Pagination limit
+    pub limit: Option<u32>,
+}
+
+/// Ontology class information
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ClassInfo {
+    /// Class identifier
+    pub id: String,
+    /// Class name
+    pub name: String,
+    /// Class description
+    pub description: Option<String>,
+    /// Parent class IDs
+    pub parent_classes: Vec<String>,
+    /// Child class IDs
+    pub child_classes: Vec<String>,
+    /// Domain this class belongs to
+    pub domain: String,
+    /// Class properties
+    pub properties: Vec<PropertyInfo>,
+    /// Number of instances in graph
+    pub instance_count: u32,
+    /// Class namespace URI
+    pub namespace: String,
+}
+
+/// Response for class listing
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ClassesResponse {
+    pub classes: Vec<ClassInfo>,
+    pub total_count: usize,
+    pub offset: u32,
+    pub limit: u32,
+    pub timestamp: DateTime<Utc>,
+}
+
+/// Request to list ontology properties
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ListPropertiesRequest {
+    /// Filter by domain
+    pub domain: Option<String>,
+    /// Filter by property name pattern
+    pub filter: Option<String>,
+    /// Filter by property type (object_property, data_property, annotation_property)
+    pub property_type: Option<String>,
+    /// Include domain/range constraints
+    pub include_constraints: Option<bool>,
+    /// Pagination offset
+    pub offset: Option<u32>,
+    /// Pagination limit
+    pub limit: Option<u32>,
+}
+
+/// Ontology property information
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct PropertyInfo {
+    /// Property identifier
+    pub id: String,
+    /// Property name
+    pub name: String,
+    /// Property description
+    pub description: Option<String>,
+    /// Property type (object_property, data_property, annotation_property)
+    pub property_type: String,
+    /// Domain constraint (classes this property applies to)
+    pub domain_classes: Vec<String>,
+    /// Range constraint (target classes or data types)
+    pub range_classes: Vec<String>,
+    /// Whether property is functional
+    pub is_functional: bool,
+    /// Whether property is inverse functional
+    pub is_inverse_functional: bool,
+    /// Whether property is transitive
+    pub is_transitive: bool,
+    /// Whether property is symmetric
+    pub is_symmetric: bool,
+    /// Cardinality constraints
+    pub cardinality: Option<CardinalityConstraint>,
+    /// Domain this property belongs to
+    pub domain: String,
+}
+
+/// Cardinality constraint for properties
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct CardinalityConstraint {
+    /// Minimum cardinality
+    pub min: Option<u32>,
+    /// Maximum cardinality
+    pub max: Option<u32>,
+    /// Exact cardinality
+    pub exact: Option<u32>,
+}
+
+/// Response for property listing
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct PropertiesResponse {
+    pub properties: Vec<PropertyInfo>,
+    pub total_count: usize,
+    pub offset: u32,
+    pub limit: u32,
+    pub timestamp: DateTime<Utc>,
+}
+
+/// Request to get entity details
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct GetEntityRequest {
+    /// Include incoming relationships
+    pub include_incoming: Option<bool>,
+    /// Include outgoing relationships
+    pub include_outgoing: Option<bool>,
+    /// Include inferred relationships
+    pub include_inferred: Option<bool>,
+    /// Maximum relationship depth
+    pub max_depth: Option<u32>,
+}
+
+/// Entity information with relationships
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct EntityInfo {
+    /// Entity identifier
+    pub id: String,
+    /// Entity label
+    pub label: String,
+    /// Entity type (class)
+    pub entity_type: String,
+    /// Entity properties
+    pub properties: HashMap<String, serde_json::Value>,
+    /// Incoming relationships
+    pub incoming_relationships: Vec<RelationshipInfo>,
+    /// Outgoing relationships
+    pub outgoing_relationships: Vec<RelationshipInfo>,
+    /// Inferred relationships
+    pub inferred_relationships: Vec<RelationshipInfo>,
+    /// Related entities (neighbors)
+    pub related_entities: Vec<String>,
+    /// Domain this entity belongs to
+    pub domain: String,
+    /// Creation timestamp
+    pub created_at: DateTime<Utc>,
+    /// Last updated timestamp
+    pub updated_at: DateTime<Utc>,
+}
+
+/// Relationship information
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct RelationshipInfo {
+    /// Relationship identifier
+    pub id: String,
+    /// Source entity ID
+    pub source_id: String,
+    /// Target entity ID
+    pub target_id: String,
+    /// Relationship type (property)
+    pub relationship_type: String,
+    /// Relationship properties
+    pub properties: HashMap<String, serde_json::Value>,
+    /// Whether this is an inferred relationship
+    pub is_inferred: bool,
+    /// Inference confidence (0.0 - 1.0)
+    pub confidence: Option<f32>,
+}
+
+/// Advanced query request
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct QueryRequest {
+    /// SPARQL-like query string
+    pub query: String,
+    /// Query parameters
+    pub parameters: Option<HashMap<String, serde_json::Value>>,
+    /// Maximum results
+    pub limit: Option<u32>,
+    /// Result offset
+    pub offset: Option<u32>,
+    /// Include query execution plan
+    pub explain: Option<bool>,
+    /// Query timeout in seconds
+    pub timeout_seconds: Option<u32>,
+}
+
+/// Query execution result
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct QueryResult {
+    /// Query execution ID
+    pub query_id: String,
+    /// Result rows
+    pub results: Vec<HashMap<String, serde_json::Value>>,
+    /// Total result count
+    pub total_count: usize,
+    /// Execution time in milliseconds
+    pub execution_time_ms: u64,
+    /// Query execution plan (if requested)
+    pub execution_plan: Option<String>,
+    /// Whether query was served from cache
+    pub from_cache: bool,
+    /// Timestamp
+    pub timestamp: DateTime<Utc>,
+}
+
+/// Graph visualization nodes and edges
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct GraphVisualizationData {
+    /// Graph nodes
+    pub nodes: Vec<GraphNode>,
+    /// Graph edges
+    pub edges: Vec<GraphEdge>,
+    /// Metadata
+    pub metadata: HashMap<String, serde_json::Value>,
+}
+
+/// Graph node for visualization
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct GraphNode {
+    pub id: String,
+    pub label: String,
+    pub node_type: String,
+    pub domain: String,
+    pub size: f32,
+    pub color: String,
+    pub metadata: HashMap<String, serde_json::Value>,
+}
+
+/// Graph edge for visualization
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct GraphEdge {
+    pub id: String,
+    pub source: String,
+    pub target: String,
+    pub edge_type: String,
+    pub label: String,
+    pub is_inferred: bool,
+    pub weight: f32,
+    pub metadata: HashMap<String, serde_json::Value>,
+}
+
+/// WebSocket update message types
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase", tag = "type")]
+pub enum OntologyUpdate {
+    /// Ontology loaded
+    OntologyLoaded {
+        ontology_id: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Validation started
+    ValidationStarted {
+        job_id: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Validation progress
+    ValidationProgress {
+        job_id: String,
+        progress: f32,
+        current_step: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Validation completed
+    ValidationCompleted {
+        job_id: String,
+        report_id: String,
+        violations_count: u32,
+        timestamp: DateTime<Utc>,
+    },
+    /// Validation failed
+    ValidationFailed {
+        job_id: String,
+        error: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Entity added
+    EntityAdded {
+        entity_id: String,
+        entity_type: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Entity updated
+    EntityUpdated {
+        entity_id: String,
+        changes: HashMap<String, serde_json::Value>,
+        timestamp: DateTime<Utc>,
+    },
+    /// Entity removed
+    EntityRemoved {
+        entity_id: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Relationship added
+    RelationshipAdded {
+        relationship_id: String,
+        source_id: String,
+        target_id: String,
+        relationship_type: String,
+        timestamp: DateTime<Utc>,
+    },
+    /// Cache cleared
+    CacheCleared {
+        timestamp: DateTime<Utc>,
+    },
+    /// Health status update
+    HealthUpdate {
+        health: OntologyHealthDto,
+        timestamp: DateTime<Utc>,
+    },
+}
+
+/// DTO for ontology health
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct OntologyHealthDto {
+    pub loaded_ontologies: u32,
+    pub cached_reports: u32,
+    pub validation_queue_size: u32,
+    pub last_validation: Option<DateTime<Utc>>,
+    pub cache_hit_rate: f32,
+    pub avg_validation_time_ms: f32,
+    pub active_jobs: u32,
+    pub memory_usage_mb: f32,
+}
+
+impl From<OntologyHealth> for OntologyHealthDto {
+    fn from(health: OntologyHealth) -> Self {
+        OntologyHealthDto {
+            loaded_ontologies: health.loaded_ontologies,
+            cached_reports: health.cached_reports,
+            validation_queue_size: health.validation_queue_size,
+            last_validation: health.last_validation,
+            cache_hit_rate: health.cache_hit_rate,
+            avg_validation_time_ms: health.avg_validation_time_ms,
+            active_jobs: health.active_jobs,
+            memory_usage_mb: health.memory_usage_mb,
+        }
+    }
+}
+
+/// Error response structure
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ErrorResponse {
+    pub error: String,
+    pub code: String,
+    pub details: Option<HashMap<String, serde_json::Value>>,
+    pub timestamp: DateTime<Utc>,
+    pub trace_id: String,
+}
+
+impl ErrorResponse {
+    pub fn new(error: &str, code: &str) -> Self {
+        Self {
+            error: error.to_string(),
+            code: code.to_string(),
+            details: None,
+            timestamp: Utc::now(),
+            trace_id: Uuid::new_v4().to_string(),
+        }
+    }
+
+    pub fn with_details(mut self, details: HashMap<String, serde_json::Value>) -> Self {
+        self.details = Some(details);
+        self
+    }
+}
+
+// ============================================================================
+// REST ENDPOINTS
+// ============================================================================
+
+/// GET /api/ontology/domains - List all ETSI domains
+pub async fn list_domains(
+    state: web::Data<AppState>,
+    req: web::Query<ListDomainsRequest>,
+) -> impl Responder {
+    info!("Listing ontology domains with filter: {:?}", req.filter);
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    match db.list_domains(req.filter.as_deref(), req.include_stats.unwrap_or(false)) {
+        Ok(domains) => {
+            let response = DomainsResponse {
+                total_count: domains.len(),
+                domains,
+                timestamp: Utc::now(),
+            };
+            HttpResponse::Ok().json(response)
+        }
+        Err(e) => {
+            error!("Failed to list domains: {}", e);
+            let error_response = ErrorResponse::new(&e, "DOMAIN_LIST_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+/// GET /api/ontology/classes - List ontology classes with filters
+pub async fn list_classes(
+    state: web::Data<AppState>,
+    req: web::Query<ListClassesRequest>,
+) -> impl Responder {
+    info!("Listing ontology classes: domain={:?}, filter={:?}", req.domain, req.filter);
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    let offset = req.offset.unwrap_or(0);
+    let limit = req.limit.unwrap_or(50).min(500); // Cap at 500
+
+    match db.list_classes(
+        req.domain.as_deref(),
+        req.filter.as_deref(),
+        req.include_subclasses.unwrap_or(false),
+        req.include_properties.unwrap_or(false),
+        offset,
+        limit,
+    ) {
+        Ok((classes, total_count)) => {
+            let response = ClassesResponse {
+                classes,
+                total_count,
+                offset,
+                limit,
+                timestamp: Utc::now(),
+            };
+            HttpResponse::Ok().json(response)
+        }
+        Err(e) => {
+            error!("Failed to list classes: {}", e);
+            let error_response = ErrorResponse::new(&e, "CLASS_LIST_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+/// GET /api/ontology/properties - List properties with schemas
+pub async fn list_properties(
+    state: web::Data<AppState>,
+    req: web::Query<ListPropertiesRequest>,
+) -> impl Responder {
+    info!("Listing ontology properties: domain={:?}, type={:?}", req.domain, req.property_type);
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    let offset = req.offset.unwrap_or(0);
+    let limit = req.limit.unwrap_or(50).min(500); // Cap at 500
+
+    match db.list_properties(
+        req.domain.as_deref(),
+        req.filter.as_deref(),
+        req.property_type.as_deref(),
+        req.include_constraints.unwrap_or(false),
+        offset,
+        limit,
+    ) {
+        Ok((properties, total_count)) => {
+            let response = PropertiesResponse {
+                properties,
+                total_count,
+                offset,
+                limit,
+                timestamp: Utc::now(),
+            };
+            HttpResponse::Ok().json(response)
+        }
+        Err(e) => {
+            error!("Failed to list properties: {}", e);
+            let error_response = ErrorResponse::new(&e, "PROPERTY_LIST_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+/// GET /api/ontology/entities/:id - Get specific entity with relationships
+pub async fn get_entity(
+    state: web::Data<AppState>,
+    path: web::Path<String>,
+    req: web::Query<GetEntityRequest>,
+) -> impl Responder {
+    let entity_id = path.into_inner();
+    info!("Getting entity: {}", entity_id);
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    match db.get_entity(
+        &entity_id,
+        req.include_incoming.unwrap_or(true),
+        req.include_outgoing.unwrap_or(true),
+        req.include_inferred.unwrap_or(false),
+        req.max_depth.unwrap_or(1),
+    ) {
+        Ok(Some(entity)) => HttpResponse::Ok().json(entity),
+        Ok(None) => {
+            warn!("Entity not found: {}", entity_id);
+            let error_response = ErrorResponse::new("Entity not found", "ENTITY_NOT_FOUND");
+            HttpResponse::NotFound().json(error_response)
+        }
+        Err(e) => {
+            error!("Failed to get entity: {}", e);
+            let error_response = ErrorResponse::new(&e, "ENTITY_RETRIEVAL_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+/// POST /api/ontology/query - Advanced query interface
+pub async fn query_ontology(
+    state: web::Data<AppState>,
+    req: web::Json<QueryRequest>,
+) -> impl Responder {
+    info!("Executing ontology query");
+    debug!("Query: {}", req.query);
+
+    let start_time = std::time::Instant::now();
+    let query_id = Uuid::new_v4().to_string();
+
+    // Check cache first
+    let cache = OntologyCache::new();
+    let cache_key = format!("query:{}:{:?}", req.query, req.parameters);
+
+    if let Some(cached_result) = cache.get_query_result(&cache_key) {
+        info!("Serving query from cache");
+        return HttpResponse::Ok().json(cached_result);
+    }
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    let query_engine = QueryEngine::new(db);
+
+    match query_engine.execute_query(
+        &req.query,
+        req.parameters.clone(),
+        req.limit.unwrap_or(100),
+        req.offset.unwrap_or(0),
+        req.timeout_seconds.unwrap_or(30),
+    ) {
+        Ok(results) => {
+            let execution_time_ms = start_time.elapsed().as_millis() as u64;
+
+            let result = QueryResult {
+                query_id,
+                results: results.clone(),
+                total_count: results.len(),
+                execution_time_ms,
+                execution_plan: if req.explain.unwrap_or(false) {
+                    Some("Query execution plan not yet implemented".to_string())
+                } else {
+                    None
+                },
+                from_cache: false,
+                timestamp: Utc::now(),
+            };
+
+            // Cache the result
+            cache.set_query_result(&cache_key, &result);
+
+            HttpResponse::Ok().json(result)
+        }
+        Err(e) => {
+            error!("Query execution failed: {}", e);
+            let error_response = ErrorResponse::new(&e, "QUERY_FAILED");
+            HttpResponse::BadRequest().json(error_response)
+        }
+    }
+}
+
+/// GET /api/ontology/graph - Get graph visualization data
+pub async fn get_graph_visualization(
+    state: web::Data<AppState>,
+    query: web::Query<HashMap<String, String>>,
+) -> impl Responder {
+    info!("Getting graph visualization data");
+
+    let domain_filter = query.get("domain").map(|s| s.as_str());
+    let max_nodes = query.get("max_nodes")
+        .and_then(|s| s.parse::<u32>().ok())
+        .unwrap_or(500)
+        .min(2000); // Cap at 2000 nodes
+
+    let db = match OntologyDatabase::new() {
+        Ok(db) => db,
+        Err(e) => {
+            error!("Failed to initialize database: {}", e);
+            let error_response = ErrorResponse::new("Database initialization failed", "DB_ERROR");
+            return HttpResponse::InternalServerError().json(error_response);
+        }
+    };
+
+    match db.get_graph_visualization(domain_filter, max_nodes) {
+        Ok(graph_data) => HttpResponse::Ok().json(graph_data),
+        Err(e) => {
+            error!("Failed to get graph visualization: {}", e);
+            let error_response = ErrorResponse::new(&e, "GRAPH_VIZ_FAILED");
+            HttpResponse::InternalServerError().json(error_response)
+        }
+    }
+}
+
+// ============================================================================
+// WEBSOCKET IMPLEMENTATION
+// ============================================================================
+
+/// WebSocket actor for real-time ontology updates
+pub struct OntologyStreamSocket {
+    /// Client ID for tracking
+    client_id: String,
+    /// Subscription filters
+    filters: HashMap<String, String>,
+    /// Last heartbeat
+    last_heartbeat: std::time::Instant,
+}
+
+impl OntologyStreamSocket {
+    pub fn new(client_id: String) -> Self {
+        Self {
+            client_id,
+            filters: HashMap::new(),
+            last_heartbeat: std::time::Instant::now(),
+        }
+    }
+
+    fn send_update(&self, ctx: &mut ws::WebsocketContext<Self>, update: OntologyUpdate) {
+        match serde_json::to_string(&update) {
+            Ok(json) => ctx.text(json),
+            Err(e) => error!("Failed to serialize update: {}", e),
+        }
+    }
+
+    fn heartbeat(&self, ctx: &mut ws::WebsocketContext<Self>) {
+        ctx.run_interval(StdDuration::from_secs(30), |act, ctx| {
+            if std::time::Instant::now().duration_since(act.last_heartbeat) > StdDuration::from_secs(90) {
+                warn!("Client {} heartbeat timeout", act.client_id);
+                ctx.stop();
+            } else {
+                ctx.ping(b"");
+            }
+        });
+    }
+}
+
+impl Actor for OntologyStreamSocket {
+    type Context = ws::WebsocketContext<Self>;
+
+    fn started(&mut self, ctx: &mut Self::Context) {
+        info!("WebSocket connection started for client: {}", self.client_id);
+
+        // Start heartbeat
+        self.heartbeat(ctx);
+
+        // Send initial connection confirmation
+        let msg = serde_json::json!({
+            "type": "connected",
+            "clientId": self.client_id,
+            "timestamp": Utc::now()
+        });
+        ctx.text(msg.to_string());
+    }
+
+    fn stopped(&mut self, _ctx: &mut Self::Context) {
+        info!("WebSocket connection stopped for client: {}", self.client_id);
+    }
+}
+
+impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for OntologyStreamSocket {
+    fn handle(&mut self, msg: Result<ws::Message, ws::ProtocolError>, ctx: &mut Self::Context) {
+        match msg {
+            Ok(ws::Message::Text(text)) => {
+                debug!("Received WebSocket message from {}: {}", self.client_id, text);
+
+                // Parse client commands
+                if let Ok(command) = serde_json::from_str::<serde_json::Value>(&text) {
+                    if let Some(cmd_type) = command.get("type").and_then(|v| v.as_str()) {
+                        match cmd_type {
+                            "subscribe" => {
+                                if let Some(filter) = command.get("filter") {
+                                    info!("Client {} subscribing with filter: {:?}", self.client_id, filter);
+                                }
+                            }
+                            "unsubscribe" => {
+                                info!("Client {} unsubscribing", self.client_id);
+                                self.filters.clear();
+                            }
+                            _ => {
+                                warn!("Unknown command type: {}", cmd_type);
+                            }
+                        }
+                    }
+                }
+            }
+            Ok(ws::Message::Ping(msg)) => {
+                self.last_heartbeat = std::time::Instant::now();
+                ctx.pong(&msg);
+            }
+            Ok(ws::Message::Pong(_)) => {
+                self.last_heartbeat = std::time::Instant::now();
+            }
+            Ok(ws::Message::Close(reason)) => {
+                info!("WebSocket close received from {}: {:?}", self.client_id, reason);
+                ctx.close(reason);
+            }
+            _ => {}
+        }
+    }
+}
+
+/// WebSocket upgrade handler
+pub async fn ontology_stream(
+    req: HttpRequest,
+    stream: web::Payload,
+    query: web::Query<HashMap<String, String>>,
+) -> Result<HttpResponse, ActixError> {
+    info!("WebSocket upgrade request for ontology stream");
+
+    let client_id = query.get("client_id")
+        .cloned()
+        .unwrap_or_else(|| Uuid::new_v4().to_string());
+
+    let websocket = OntologyStreamSocket::new(client_id);
+
+    ws::start(websocket, &req, stream)
+}
+
+// ============================================================================
+// ROUTE CONFIGURATION
+// ============================================================================
+
+/// Configure ontology data API routes
+pub fn config(cfg: &mut web::ServiceConfig) {
+    cfg.service(
+        web::scope("/ontology")
+            // Domain endpoints
+            .route("/domains", web::get().to(list_domains))
+            // Class endpoints
+            .route("/classes", web::get().to(list_classes))
+            // Property endpoints
+            .route("/properties", web::get().to(list_properties))
+            // Entity endpoints
+            .route("/entities/{id}", web::get().to(get_entity))
+            // Query endpoint
+            .route("/query", web::post().to(query_ontology))
+            // Graph visualization endpoint
+            .route("/graph", web::get().to(get_graph_visualization))
+            // WebSocket stream
+            .route("/stream", web::get().to(ontology_stream))
+    );
+}
+
+// ============================================================================
+// TESTS
+// ============================================================================
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_domain_info_serialization() {
+        let domain = DomainInfo {
+            id: "etsi-nfv".to_string(),
+            name: "ETSI NFV".to_string(),
+            description: "Network Functions Virtualization".to_string(),
+            class_count: 150,
+            property_count: 300,
+            namespace: "http://example.org/etsi-nfv#".to_string(),
+            updated_at: Utc::now(),
+        };
+
+        let json = serde_json::to_value(&domain).unwrap();
+        assert!(json.get("id").is_some());
+        assert!(json.get("classCount").is_some());
+    }
+
+    #[test]
+    fn test_query_request_deserialization() {
+        let json = r#"{
+            "query": "SELECT ?s WHERE { ?s a ?type }",
+            "limit": 100,
+            "explain": true
+        }"#;
+
+        let req: QueryRequest = serde_json::from_str(json).unwrap();
+        assert_eq!(req.query, "SELECT ?s WHERE { ?s a ?type }");
+        assert_eq!(req.limit, Some(100));
+        assert_eq!(req.explain, Some(true));
+    }
+}
diff --git a/src/handlers/api_handler/ontology_data/query.rs b/src/handlers/api_handler/ontology_data/query.rs
new file mode 100644
index 00000000..447a6d2a
--- /dev/null
+++ b/src/handlers/api_handler/ontology_data/query.rs
@@ -0,0 +1,284 @@
+//! Query Engine for Ontology Data
+//!
+//! Provides a SPARQL-like query interface for ontology data with support for:
+//! - Entity filtering and selection
+//! - Relationship traversal
+//! - Property-based filtering
+//! - Aggregations
+//! - Sorting and pagination
+
+use log::{debug, error, info, warn};
+use serde_json::Value as JsonValue;
+use std::collections::HashMap;
+
+use super::db::OntologyDatabase;
+
+/// Query execution engine
+pub struct QueryEngine {
+    db: OntologyDatabase,
+}
+
+impl QueryEngine {
+    /// Create new query engine
+    pub fn new(db: OntologyDatabase) -> Self {
+        Self { db }
+    }
+
+    /// Execute a query with parameters
+    pub fn execute_query(
+        &self,
+        query: &str,
+        parameters: Option<HashMap<String, JsonValue>>,
+        limit: u32,
+        offset: u32,
+        timeout_seconds: u32,
+    ) -> Result<Vec<HashMap<String, JsonValue>>, String> {
+        debug!("Executing query: {}", query);
+        debug!("Parameters: {:?}", parameters);
+
+        // Parse query
+        let parsed = self.parse_query(query)?;
+
+        // Execute based on query type
+        match parsed.query_type.as_str() {
+            "SELECT" => self.execute_select(&parsed, parameters, limit, offset),
+            "COUNT" => self.execute_count(&parsed, parameters),
+            "DESCRIBE" => self.execute_describe(&parsed, parameters),
+            _ => Err(format!("Unsupported query type: {}", parsed.query_type)),
+        }
+    }
+
+    /// Parse query string into structured format
+    fn parse_query(&self, query: &str) -> Result<ParsedQuery, String> {
+        let query_lower = query.to_lowercase();
+
+        let query_type = if query_lower.starts_with("select") {
+            "SELECT"
+        } else if query_lower.starts_with("count") {
+            "COUNT"
+        } else if query_lower.starts_with("describe") {
+            "DESCRIBE"
+        } else {
+            return Err("Query must start with SELECT, COUNT, or DESCRIBE".to_string());
+        };
+
+        // Extract WHERE clause
+        let where_clause = if let Some(where_idx) = query_lower.find("where") {
+            Some(&query[where_idx + 5..])
+        } else {
+            None
+        };
+
+        Ok(ParsedQuery {
+            query_type: query_type.to_string(),
+            select_fields: self.extract_select_fields(query),
+            where_clause: where_clause.map(|s| s.trim().to_string()),
+            filters: self.extract_filters(where_clause),
+        })
+    }
+
+    /// Extract SELECT fields from query
+    fn extract_select_fields(&self, query: &str) -> Vec<String> {
+        // Simplified extraction - in production, use a proper SPARQL parser
+        let mut fields = Vec::new();
+
+        if let Some(select_idx) = query.to_lowercase().find("select") {
+            let after_select = &query[select_idx + 6..];
+
+            if let Some(where_idx) = after_select.to_lowercase().find("where") {
+                let fields_str = &after_select[..where_idx];
+                for field in fields_str.split_whitespace() {
+                    if !field.is_empty() && field.starts_with('?') {
+                        fields.push(field[1..].to_string());
+                    }
+                }
+            }
+        }
+
+        if fields.is_empty() {
+            fields.push("*".to_string());
+        }
+
+        fields
+    }
+
+    /// Extract filters from WHERE clause
+    fn extract_filters(&self, where_clause: Option<&str>) -> Vec<QueryFilter> {
+        let mut filters = Vec::new();
+
+        if let Some(clause) = where_clause {
+            // Simplified filter extraction
+            // In production, use a proper SPARQL parser
+            let parts: Vec<&str> = clause.split('.').collect();
+
+            for part in parts {
+                let trimmed = part.trim();
+                if trimmed.is_empty() {
+                    continue;
+                }
+
+                // Look for patterns like "?s a ?type" or "?s rdf:type vnf"
+                let tokens: Vec<&str> = trimmed.split_whitespace().collect();
+
+                if tokens.len() >= 3 {
+                    filters.push(QueryFilter {
+                        subject: tokens[0].to_string(),
+                        predicate: tokens[1].to_string(),
+                        object: tokens[2].to_string(),
+                    });
+                }
+            }
+        }
+
+        filters
+    }
+
+    /// Execute SELECT query
+    fn execute_select(
+        &self,
+        parsed: &ParsedQuery,
+        parameters: Option<HashMap<String, JsonValue>>,
+        limit: u32,
+        offset: u32,
+    ) -> Result<Vec<HashMap<String, JsonValue>>, String> {
+        debug!("Executing SELECT query");
+
+        // Mock implementation - return sample data
+        let mut results = Vec::new();
+
+        // Create sample result rows
+        for i in 0..limit.min(10) {
+            let mut row = HashMap::new();
+
+            for field in &parsed.select_fields {
+                let value = match field.as_str() {
+                    "*" | "entity" => JsonValue::String(format!("entity-{}", i)),
+                    "type" => JsonValue::String("vnf".to_string()),
+                    "label" => JsonValue::String(format!("VNF Instance {}", i)),
+                    "domain" => JsonValue::String("etsi-nfv".to_string()),
+                    "status" => JsonValue::String("deployed".to_string()),
+                    _ => JsonValue::Null,
+                };
+
+                row.insert(field.clone(), value);
+            }
+
+            results.push(row);
+        }
+
+        Ok(results)
+    }
+
+    /// Execute COUNT query
+    fn execute_count(
+        &self,
+        parsed: &ParsedQuery,
+        parameters: Option<HashMap<String, JsonValue>>,
+    ) -> Result<Vec<HashMap<String, JsonValue>>, String> {
+        debug!("Executing COUNT query");
+
+        let mut result = HashMap::new();
+        result.insert("count".to_string(), JsonValue::Number(serde_json::Number::from(42)));
+
+        Ok(vec![result])
+    }
+
+    /// Execute DESCRIBE query
+    fn execute_describe(
+        &self,
+        parsed: &ParsedQuery,
+        parameters: Option<HashMap<String, JsonValue>>,
+    ) -> Result<Vec<HashMap<String, JsonValue>>, String> {
+        debug!("Executing DESCRIBE query");
+
+        // Return entity description
+        let mut result = HashMap::new();
+        result.insert("id".to_string(), JsonValue::String("vnf-123".to_string()));
+        result.insert("type".to_string(), JsonValue::String("vnf".to_string()));
+        result.insert("label".to_string(), JsonValue::String("Example VNF".to_string()));
+        result.insert("properties".to_string(), JsonValue::Object({
+            let mut props = serde_json::Map::new();
+            props.insert("deploymentStatus".to_string(), JsonValue::String("deployed".to_string()));
+            props.insert("version".to_string(), JsonValue::String("1.2.3".to_string()));
+            props
+        }));
+
+        Ok(vec![result])
+    }
+}
+
+/// Parsed query structure
+#[derive(Debug, Clone)]
+struct ParsedQuery {
+    query_type: String,
+    select_fields: Vec<String>,
+    where_clause: Option<String>,
+    filters: Vec<QueryFilter>,
+}
+
+/// Query filter
+#[derive(Debug, Clone)]
+struct QueryFilter {
+    subject: String,
+    predicate: String,
+    object: String,
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_query_parsing() {
+        let db = OntologyDatabase::new().unwrap();
+        let engine = QueryEngine::new(db);
+
+        let query = "SELECT ?entity ?type WHERE { ?entity a ?type }";
+        let parsed = engine.parse_query(query).unwrap();
+
+        assert_eq!(parsed.query_type, "SELECT");
+        assert!(parsed.select_fields.contains(&"entity".to_string()));
+        assert!(parsed.select_fields.contains(&"type".to_string()));
+    }
+
+    #[test]
+    fn test_select_query_execution() {
+        let db = OntologyDatabase::new().unwrap();
+        let engine = QueryEngine::new(db);
+
+        let query = "SELECT ?entity WHERE { ?entity a vnf }";
+        let result = engine.execute_query(query, None, 10, 0, 30);
+
+        assert!(result.is_ok());
+        let rows = result.unwrap();
+        assert!(!rows.is_empty());
+    }
+
+    #[test]
+    fn test_count_query_execution() {
+        let db = OntologyDatabase::new().unwrap();
+        let engine = QueryEngine::new(db);
+
+        let query = "COUNT WHERE { ?entity a vnf }";
+        let result = engine.execute_query(query, None, 10, 0, 30);
+
+        assert!(result.is_ok());
+        let rows = result.unwrap();
+        assert_eq!(rows.len(), 1);
+        assert!(rows[0].contains_key("count"));
+    }
+
+    #[test]
+    fn test_describe_query_execution() {
+        let db = OntologyDatabase::new().unwrap();
+        let engine = QueryEngine::new(db);
+
+        let query = "DESCRIBE vnf-123";
+        let result = engine.execute_query(query, None, 10, 0, 30);
+
+        assert!(result.is_ok());
+        let rows = result.unwrap();
+        assert!(!rows.is_empty());
+        assert!(rows[0].contains_key("id"));
+    }
+}
diff --git a/src/handlers/api_handler/settings/mod.rs b/src/handlers/api_handler/settings/mod.rs
new file mode 100644
index 00000000..27183d8f
--- /dev/null
+++ b/src/handlers/api_handler/settings/mod.rs
@@ -0,0 +1,458 @@
+// REST API for Settings Management
+// Provides complete CRUD operations for settings with validation and permission checks
+
+use actix_web::{web, HttpRequest, HttpResponse, Error as ActixError};
+use serde::{Deserialize, Serialize};
+use serde_json::{json, Value as JsonValue};
+use std::sync::Arc;
+use log::{info, error, debug, warn};
+
+use crate::app_state::AppState;
+use crate::services::settings_service::SettingsService;
+use crate::services::database_service::SettingValue;
+use crate::config::PhysicsSettings;
+
+/// Response DTO for settings list
+#[derive(Debug, Serialize)]
+#[serde(rename_all = "camelCase")]
+pub struct SettingsListResponse {
+    pub settings: Vec<SettingItem>,
+    pub total: usize,
+}
+
+#[derive(Debug, Serialize)]
+#[serde(rename_all = "camelCase")]
+pub struct SettingItem {
+    pub key: String,
+    pub value: JsonValue,
+    pub value_type: String,
+}
+
+/// Request DTO for setting update
+#[derive(Debug, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct UpdateSettingRequest {
+    pub value: JsonValue,
+}
+
+/// Request DTO for validation only
+#[derive(Debug, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ValidateSettingRequest {
+    pub key: String,
+    pub value: JsonValue,
+}
+
+/// Response DTO for validation
+#[derive(Debug, Serialize)]
+#[serde(rename_all = "camelCase")]
+pub struct ValidationResponse {
+    pub is_valid: bool,
+    pub errors: Vec<String>,
+    pub warnings: Vec<String>,
+}
+
+/// Extract user ID from request (from Nostr pubkey or auth token)
+fn extract_user_id(req: &HttpRequest) -> Option<String> {
+    req.headers()
+        .get("x-nostr-pubkey")
+        .and_then(|h| h.to_str().ok())
+        .map(|s| s.to_string())
+}
+
+/// Check if user has power user permissions
+fn check_power_user(app_state: &AppState, user_id: Option<&str>) -> bool {
+    match user_id {
+        Some(uid) => app_state.is_power_user(uid),
+        None => false,
+    }
+}
+
+/// GET /api/settings - List all settings with permission filtering
+pub async fn list_settings(
+    req: HttpRequest,
+    app_state: web::Data<AppState>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let user_id = extract_user_id(&req);
+    let is_power_user = check_power_user(&app_state, user_id.as_deref());
+
+    debug!("Listing settings for user: {:?} (power: {})", user_id, is_power_user);
+
+    match settings_service.list_all_settings().await {
+        Ok(settings) => {
+            let items: Vec<SettingItem> = settings
+                .into_iter()
+                .map(|(key, value)| {
+                    let (json_value, value_type) = convert_setting_value_to_json(&value);
+                    SettingItem {
+                        key,
+                        value: json_value,
+                        value_type,
+                    }
+                })
+                .collect();
+
+            let total = items.len();
+            Ok(HttpResponse::Ok().json(SettingsListResponse {
+                settings: items,
+                total,
+            }))
+        }
+        Err(e) => {
+            error!("Failed to list settings: {}", e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to list settings",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// GET /api/settings/{key} - Get specific setting (supports dots in key)
+pub async fn get_setting(
+    path: web::Path<String>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let key = path.into_inner();
+
+    debug!("Getting setting: {}", key);
+
+    match settings_service.get_setting(&key).await {
+        Ok(Some(value)) => {
+            let (json_value, value_type) = convert_setting_value_to_json(&value);
+            Ok(HttpResponse::Ok().json(json!({
+                "key": key,
+                "value": json_value,
+                "valueType": value_type
+            })))
+        }
+        Ok(None) => Ok(HttpResponse::NotFound().json(json!({
+            "error": "Setting not found",
+            "key": key
+        }))),
+        Err(e) => {
+            error!("Failed to get setting {}: {}", key, e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get setting",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// PUT /api/settings/{key} - Update setting (requires power user)
+pub async fn update_setting(
+    req: HttpRequest,
+    path: web::Path<String>,
+    body: web::Json<UpdateSettingRequest>,
+    app_state: web::Data<AppState>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let key = path.into_inner();
+    let user_id = extract_user_id(&req);
+
+    // Check power user permission
+    if !check_power_user(&app_state, user_id.as_deref()) {
+        warn!("Unauthorized settings update attempt by user: {:?}", user_id);
+        return Ok(HttpResponse::Forbidden().json(json!({
+            "error": "Power user permission required"
+        })));
+    }
+
+    debug!("Updating setting: {} by user: {:?}", key, user_id);
+
+    // Convert JSON value to SettingValue
+    let setting_value = match json_value_to_setting_value(&body.value) {
+        Ok(v) => v,
+        Err(e) => {
+            return Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Invalid value type",
+                "details": e
+            })));
+        }
+    };
+
+    match settings_service
+        .set_setting(&key, setting_value, user_id.as_deref())
+        .await
+    {
+        Ok(()) => {
+            info!("Setting {} updated by user {:?}", key, user_id);
+            Ok(HttpResponse::Ok().json(json!({
+                "success": true,
+                "key": key,
+                "message": "Setting updated successfully"
+            })))
+        }
+        Err(e) => {
+            error!("Failed to update setting {}: {}", key, e);
+            Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Failed to update setting",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// GET /api/settings/tree/{prefix} - Get hierarchical tree
+pub async fn get_settings_tree(
+    path: web::Path<String>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let prefix = path.into_inner();
+
+    debug!("Getting settings tree for prefix: {}", prefix);
+
+    match settings_service.get_settings_tree(&prefix).await {
+        Ok(tree) => Ok(HttpResponse::Ok().json(tree)),
+        Err(e) => {
+            error!("Failed to get settings tree for {}: {}", prefix, e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get settings tree",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// GET /api/settings/physics/{profile} - Get physics profile
+pub async fn get_physics_profile(
+    path: web::Path<String>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let profile_name = path.into_inner();
+
+    debug!("Getting physics profile: {}", profile_name);
+
+    match settings_service.get_physics_profile(&profile_name).await {
+        Ok(profile) => Ok(HttpResponse::Ok().json(profile)),
+        Err(e) => {
+            error!("Failed to get physics profile {}: {}", profile_name, e);
+            Ok(HttpResponse::NotFound().json(json!({
+                "error": "Physics profile not found",
+                "profile": profile_name,
+                "details": e
+            })))
+        }
+    }
+}
+
+/// PUT /api/settings/physics/{profile} - Update physics profile (requires power user)
+pub async fn update_physics_profile(
+    req: HttpRequest,
+    path: web::Path<String>,
+    body: web::Json<PhysicsSettings>,
+    app_state: web::Data<AppState>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let profile_name = path.into_inner();
+    let user_id = extract_user_id(&req);
+
+    // Check power user permission
+    if !check_power_user(&app_state, user_id.as_deref()) {
+        warn!(
+            "Unauthorized physics profile update attempt by user: {:?}",
+            user_id
+        );
+        return Ok(HttpResponse::Forbidden().json(json!({
+            "error": "Power user permission required"
+        })));
+    }
+
+    debug!(
+        "Updating physics profile: {} by user: {:?}",
+        profile_name, user_id
+    );
+
+    match settings_service
+        .update_physics_profile(&profile_name, body.into_inner(), user_id.as_deref())
+        .await
+    {
+        Ok(()) => {
+            info!(
+                "Physics profile {} updated by user {:?}",
+                profile_name, user_id
+            );
+            Ok(HttpResponse::Ok().json(json!({
+                "success": true,
+                "profile": profile_name,
+                "message": "Physics profile updated successfully"
+            })))
+        }
+        Err(e) => {
+            error!("Failed to update physics profile {}: {}", profile_name, e);
+            Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Failed to update physics profile",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// POST /api/settings/validate - Validate settings without saving
+pub async fn validate_setting(
+    body: web::Json<ValidateSettingRequest>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    debug!("Validating setting: {}", body.key);
+
+    // Convert JSON value to SettingValue
+    let setting_value = match json_value_to_setting_value(&body.value) {
+        Ok(v) => v,
+        Err(e) => {
+            return Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Invalid value type",
+                "details": e
+            })));
+        }
+    };
+
+    // Perform validation
+    let validator = &settings_service.validator;
+    match validator.validate_setting(&body.key, &setting_value) {
+        Ok(result) => Ok(HttpResponse::Ok().json(ValidationResponse {
+            is_valid: result.is_valid,
+            errors: result.errors,
+            warnings: result.warnings,
+        })),
+        Err(e) => {
+            error!("Validation error for {}: {}", body.key, e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Validation failed",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// GET /api/settings/search?q=pattern - Search settings
+pub async fn search_settings(
+    query: web::Query<SearchQuery>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let pattern = &query.q;
+
+    debug!("Searching settings with pattern: {}", pattern);
+
+    match settings_service.search_settings(pattern).await {
+        Ok(results) => {
+            let items: Vec<SettingItem> = results
+                .into_iter()
+                .map(|(key, value)| {
+                    let (json_value, value_type) = convert_setting_value_to_json(&value);
+                    SettingItem {
+                        key,
+                        value: json_value,
+                        value_type,
+                    }
+                })
+                .collect();
+
+            Ok(HttpResponse::Ok().json(json!({
+                "results": items,
+                "count": items.len()
+            })))
+        }
+        Err(e) => {
+            error!("Failed to search settings: {}", e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to search settings",
+                "details": e
+            })))
+        }
+    }
+}
+
+#[derive(Debug, Deserialize)]
+pub struct SearchQuery {
+    pub q: String,
+}
+
+/// DELETE /api/settings/{key} - Reset to default (requires power user)
+pub async fn reset_setting(
+    req: HttpRequest,
+    path: web::Path<String>,
+    app_state: web::Data<AppState>,
+    settings_service: web::Data<Arc<SettingsService>>,
+) -> Result<HttpResponse, ActixError> {
+    let key = path.into_inner();
+    let user_id = extract_user_id(&req);
+
+    // Check power user permission
+    if !check_power_user(&app_state, user_id.as_deref()) {
+        warn!("Unauthorized reset attempt by user: {:?}", user_id);
+        return Ok(HttpResponse::Forbidden().json(json!({
+            "error": "Power user permission required"
+        })));
+    }
+
+    debug!("Resetting setting: {} by user: {:?}", key, user_id);
+
+    match settings_service
+        .reset_to_default(&key, user_id.as_deref())
+        .await
+    {
+        Ok(()) => {
+            info!("Setting {} reset to default by user {:?}", key, user_id);
+            Ok(HttpResponse::Ok().json(json!({
+                "success": true,
+                "key": key,
+                "message": "Setting reset to default"
+            })))
+        }
+        Err(e) => {
+            error!("Failed to reset setting {}: {}", key, e);
+            Ok(HttpResponse::BadRequest().json(json!({
+                "error": "Failed to reset setting",
+                "details": e
+            })))
+        }
+    }
+}
+
+/// Helper: Convert SettingValue to JSON
+fn convert_setting_value_to_json(value: &SettingValue) -> (JsonValue, String) {
+    match value {
+        SettingValue::String(s) => (json!(s), "string".to_string()),
+        SettingValue::Integer(i) => (json!(i), "integer".to_string()),
+        SettingValue::Float(f) => (json!(f), "float".to_string()),
+        SettingValue::Boolean(b) => (json!(b), "boolean".to_string()),
+        SettingValue::Json(j) => (j.clone(), "json".to_string()),
+    }
+}
+
+/// Helper: Convert JSON value to SettingValue
+fn json_value_to_setting_value(value: &JsonValue) -> Result<SettingValue, String> {
+    match value {
+        JsonValue::String(s) => Ok(SettingValue::String(s.clone())),
+        JsonValue::Number(n) => {
+            if let Some(i) = n.as_i64() {
+                Ok(SettingValue::Integer(i))
+            } else if let Some(f) = n.as_f64() {
+                Ok(SettingValue::Float(f))
+            } else {
+                Err("Invalid number type".to_string())
+            }
+        }
+        JsonValue::Bool(b) => Ok(SettingValue::Boolean(*b)),
+        JsonValue::Object(_) | JsonValue::Array(_) => Ok(SettingValue::Json(value.clone())),
+        JsonValue::Null => Err("Cannot store null values".to_string()),
+    }
+}
+
+/// Configure settings API routes
+pub fn configure_routes(cfg: &mut web::ServiceConfig) {
+    cfg.service(
+        web::scope("/api/settings")
+            .route("", web::get().to(list_settings))
+            .route("/search", web::get().to(search_settings))
+            .route("/validate", web::post().to(validate_setting))
+            .route("/tree/{prefix:.*}", web::get().to(get_settings_tree))
+            .route("/physics/{profile}", web::get().to(get_physics_profile))
+            .route("/physics/{profile}", web::put().to(update_physics_profile))
+            .route("/{key:.*}", web::get().to(get_setting))
+            .route("/{key:.*}", web::put().to(update_setting))
+            .route("/{key:.*}", web::delete().to(reset_setting)),
+    );
+}
diff --git a/src/handlers/api_handler/settings/websocket.rs b/src/handlers/api_handler/settings/websocket.rs
new file mode 100644
index 00000000..030e262a
--- /dev/null
+++ b/src/handlers/api_handler/settings/websocket.rs
@@ -0,0 +1,218 @@
+// WebSocket Broadcast Integration for Settings Changes
+// Notifies all connected clients when settings are modified
+
+use actix::prelude::*;
+use serde::{Deserialize, Serialize};
+use serde_json::Value as JsonValue;
+use std::collections::HashMap;
+use std::sync::Arc;
+use tokio::sync::RwLock;
+use log::{info, debug, warn};
+
+use crate::services::database_service::SettingValue;
+
+/// Manager for broadcasting settings changes to WebSocket clients
+#[derive(Clone)]
+pub struct SettingsBroadcastManager {
+    clients: Arc<RwLock<HashMap<String, SettingsClientInfo>>>,
+    subscriptions: Arc<RwLock<HashMap<String, Vec<String>>>>, // key prefix -> client_ids
+}
+
+#[derive(Clone)]
+struct SettingsClientInfo {
+    client_id: String,
+    subscriptions: Vec<String>, // Prefixes the client is subscribed to
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(rename_all = "camelCase")]
+pub struct SettingsChangeNotification {
+    pub event_type: String,
+    pub key: String,
+    pub value: JsonValue,
+    pub changed_by: Option<String>,
+    pub timestamp: u64,
+}
+
+impl SettingsBroadcastManager {
+    pub fn new() -> Self {
+        Self {
+            clients: Arc::new(RwLock::new(HashMap::new())),
+            subscriptions: Arc::new(RwLock::new(HashMap::new())),
+        }
+    }
+
+    /// Register a new WebSocket client
+    pub async fn register_client(&self, client_id: String) {
+        let mut clients = self.clients.write().await;
+        clients.insert(
+            client_id.clone(),
+            SettingsClientInfo {
+                client_id: client_id.clone(),
+                subscriptions: vec!["*".to_string()], // Default: subscribe to all
+            },
+        );
+        info!("Settings WebSocket client registered: {}", client_id);
+    }
+
+    /// Unregister a WebSocket client
+    pub async fn unregister_client(&self, client_id: &str) {
+        let mut clients = self.clients.write().await;
+        if clients.remove(client_id).is_some() {
+            info!("Settings WebSocket client unregistered: {}", client_id);
+        }
+
+        // Clean up subscriptions
+        let mut subscriptions = self.subscriptions.write().await;
+        for (_, client_ids) in subscriptions.iter_mut() {
+            client_ids.retain(|id| id != client_id);
+        }
+    }
+
+    /// Subscribe client to specific setting prefix
+    pub async fn subscribe(&self, client_id: &str, prefix: String) {
+        let mut clients = self.clients.write().await;
+        if let Some(client) = clients.get_mut(client_id) {
+            if !client.subscriptions.contains(&prefix) {
+                client.subscriptions.push(prefix.clone());
+            }
+        }
+
+        let mut subscriptions = self.subscriptions.write().await;
+        subscriptions
+            .entry(prefix.clone())
+            .or_insert_with(Vec::new)
+            .push(client_id.to_string());
+
+        debug!(
+            "Client {} subscribed to settings prefix: {}",
+            client_id, prefix
+        );
+    }
+
+    /// Unsubscribe client from specific setting prefix
+    pub async fn unsubscribe(&self, client_id: &str, prefix: &str) {
+        let mut clients = self.clients.write().await;
+        if let Some(client) = clients.get_mut(client_id) {
+            client.subscriptions.retain(|p| p != prefix);
+        }
+
+        let mut subscriptions = self.subscriptions.write().await;
+        if let Some(client_ids) = subscriptions.get_mut(prefix) {
+            client_ids.retain(|id| id != client_id);
+        }
+
+        debug!(
+            "Client {} unsubscribed from settings prefix: {}",
+            client_id, prefix
+        );
+    }
+
+    /// Broadcast setting change to all interested clients
+    pub async fn broadcast_change(&self, key: &str, value: &SettingValue, user_id: Option<&str>) {
+        let clients = self.clients.read().await;
+        let interested_clients: Vec<String> = clients
+            .values()
+            .filter(|client| self.is_subscribed_to(client, key))
+            .map(|client| client.client_id.clone())
+            .collect();
+
+        if interested_clients.is_empty() {
+            return;
+        }
+
+        let notification = SettingsChangeNotification {
+            event_type: "settingChanged".to_string(),
+            key: key.to_string(),
+            value: convert_setting_value_to_json(value),
+            changed_by: user_id.map(|s| s.to_string()),
+            timestamp: current_timestamp(),
+        };
+
+        info!(
+            "Broadcasting setting change for '{}' to {} clients",
+            key,
+            interested_clients.len()
+        );
+
+        // In a real implementation, this would send the notification via WebSocket
+        // For now, we just log it
+        debug!("Broadcast notification: {:?}", notification);
+    }
+
+    /// Check if client is subscribed to a specific key
+    fn is_subscribed_to(&self, client: &SettingsClientInfo, key: &str) -> bool {
+        for subscription in &client.subscriptions {
+            if subscription == "*" || key.starts_with(subscription) {
+                return true;
+            }
+        }
+        false
+    }
+
+    /// Get all registered clients
+    pub async fn get_client_count(&self) -> usize {
+        self.clients.read().await.len()
+    }
+
+    /// Get subscription stats
+    pub async fn get_subscription_stats(&self) -> HashMap<String, usize> {
+        let subscriptions = self.subscriptions.read().await;
+        subscriptions
+            .iter()
+            .map(|(prefix, clients)| (prefix.clone(), clients.len()))
+            .collect()
+    }
+}
+
+fn convert_setting_value_to_json(value: &SettingValue) -> JsonValue {
+    match value {
+        SettingValue::String(s) => JsonValue::String(s.clone()),
+        SettingValue::Integer(i) => JsonValue::Number((*i).into()),
+        SettingValue::Float(f) => JsonValue::Number(
+            serde_json::Number::from_f64(*f).unwrap_or_else(|| serde_json::Number::from(0)),
+        ),
+        SettingValue::Boolean(b) => JsonValue::Bool(*b),
+        SettingValue::Json(j) => j.clone(),
+    }
+}
+
+fn current_timestamp() -> u64 {
+    std::time::SystemTime::now()
+        .duration_since(std::time::UNIX_EPOCH)
+        .unwrap_or_default()
+        .as_millis() as u64
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[tokio::test]
+    async fn test_register_unregister_client() {
+        let manager = SettingsBroadcastManager::new();
+
+        manager.register_client("client1".to_string()).await;
+        assert_eq!(manager.get_client_count().await, 1);
+
+        manager.unregister_client("client1").await;
+        assert_eq!(manager.get_client_count().await, 0);
+    }
+
+    #[tokio::test]
+    async fn test_subscribe_unsubscribe() {
+        let manager = SettingsBroadcastManager::new();
+
+        manager.register_client("client1".to_string()).await;
+        manager
+            .subscribe("client1", "visualisation.physics".to_string())
+            .await;
+
+        let stats = manager.get_subscription_stats().await;
+        assert_eq!(stats.get("visualisation.physics"), Some(&1));
+
+        manager.unsubscribe("client1", "visualisation.physics").await;
+        let stats = manager.get_subscription_stats().await;
+        assert_eq!(stats.get("visualisation.physics"), Some(&0));
+    }
+}
diff --git a/src/handlers/bots_handler.rs b/src/handlers/bots_handler.rs
index 63033f14..cc24a5be 100644
--- a/src/handlers/bots_handler.rs
+++ b/src/handlers/bots_handler.rs
@@ -195,7 +195,7 @@ pub async fn update_bots_graph(request: web::Json<BotsDataRequest>, _state: web:

 pub async fn get_bots_data(state: web::Data<AppState>) -> Result<impl Responder> {
     // First try to get data from graph actor if available
-    if let Ok(graph_data) = state.graph_service_addr.send(GetBotsGraphData).await {
+    if let Ok(graph_data) = state.graph_state_addr.send(GetBotsGraphData).await {
         if let Ok(graph) = graph_data {
             let nodes = &graph.nodes;
             let edges = &graph.edges;
diff --git a/src/handlers/client_logs.rs b/src/handlers/client_logs.rs
new file mode 100644
index 00000000..5ca06d25
--- /dev/null
+++ b/src/handlers/client_logs.rs
@@ -0,0 +1,32 @@
+use actix_web::{web, HttpResponse, Error};
+use serde::Deserialize;
+use log::{info, warn, error, debug};
+
+#[derive(Deserialize)]
+pub struct ClientLogEntry {
+    level: String,
+    message: String,
+    timestamp: Option<String>,
+    context: Option<String>,
+}
+
+/// Simple handler to receive client-side logs and forward to server logger
+pub async fn post_client_logs(
+    web::Json(payload): web::Json<ClientLogEntry>
+) -> Result<HttpResponse, Error> {
+    let prefix = "[CLIENT]";
+    let msg = if let Some(ctx) = payload.context {
+        format!("{} {}", payload.message, ctx)
+    } else {
+        payload.message
+    };
+
+    match payload.level.to_lowercase().as_str() {
+        "error" => error!("{} {}", prefix, msg),
+        "warn" => warn!("{} {}", prefix, msg),
+        "debug" => debug!("{} {}", prefix, msg),
+        _ => info!("{} {}", prefix, msg),
+    }
+
+    Ok(HttpResponse::Ok().json(serde_json::json!({"success": true})))
+}
diff --git a/src/handlers/clustering_handler.rs b/src/handlers/clustering_handler.rs
index b627a934..3a8e66a2 100644
--- a/src/handlers/clustering_handler.rs
+++ b/src/handlers/clustering_handler.rs
@@ -300,7 +300,7 @@ async fn get_clustering_results(
         use crate::actors::messages::{GetClusteringResults, GetGraphData};

         // Get graph data for node count
-        let graph_data = match state.graph_service_addr.send(GetGraphData).await {
+        let graph_data = match state.graph_state_addr.send(GetGraphData).await {
             Ok(Ok(data)) => data,
             Ok(Err(e)) => {
                 error!("Failed to get graph data: {}", e);
@@ -510,7 +510,7 @@ async fn export_cluster_assignments(
     }

     // Fallback: try to get data from graph service
-    match state.graph_service_addr.send(crate::actors::messages::GetGraphData).await {
+    match state.graph_state_addr.send(crate::actors::messages::GetGraphData).await {
         Ok(Ok(graph_data)) => {
             if !graph_data.nodes.is_empty() {
                 info!("Using graph data for clustering export with {} nodes", graph_data.nodes.len());
diff --git a/src/handlers/consolidated_health_handler.rs b/src/handlers/consolidated_health_handler.rs
index 357809f7..e314df48 100644
--- a/src/handlers/consolidated_health_handler.rs
+++ b/src/handlers/consolidated_health_handler.rs
@@ -143,7 +143,7 @@ async fn check_service_metrics(
     // Check graph service
     let (nodes_count, edges_count) = match tokio::time::timeout(
         Duration::from_secs(5),
-        app_state.graph_service_addr.send(GetGraphData)
+        app_state.graph_state_addr.send(GetGraphData)
     ).await {
         Ok(Ok(Ok(graph_data))) => (graph_data.nodes.len(), graph_data.edges.len()),
         Ok(Ok(Err(_))) => {
@@ -275,7 +275,7 @@ async fn get_physics_diagnostics(app_state: &web::Data<AppState>) -> Result<(Str
     // Check graph service
     match tokio::time::timeout(
         Duration::from_secs(3),
-        app_state.graph_service_addr.send(GetGraphData)
+        app_state.graph_state_addr.send(GetGraphData)
     ).await {
         Ok(Ok(Ok(graph_data))) => {
             diagnostics.push(format!("Graph: {} nodes, {} edges", graph_data.nodes.len(), graph_data.edges.len()));
diff --git a/src/handlers/graph_state_handler.rs b/src/handlers/graph_state_handler.rs
index c026bb72..685e9f5f 100644
--- a/src/handlers/graph_state_handler.rs
+++ b/src/handlers/graph_state_handler.rs
@@ -28,8 +28,8 @@ pub struct NodePosition {
 pub async fn get_graph_state(state: web::Data<AppState>) -> impl Responder {
     info!("Received request for complete graph state");

-    // Get graph data from GraphServiceActor
-    let graph_data_result = state.graph_service_addr.send(GetGraphData).await;
+    // Get graph data from GraphStateActor
+    let graph_data_result = state.graph_state_addr.send(GetGraphData).await;

     match graph_data_result {
         Ok(Ok(graph_data)) => {
diff --git a/src/handlers/mod.rs b/src/handlers/mod.rs
index 0f26f745..131a910e 100755
--- a/src/handlers/mod.rs
+++ b/src/handlers/mod.rs
@@ -23,7 +23,12 @@ pub mod graph_export_handler;
 pub mod realtime_websocket_handler;
 pub mod websocket_settings_handler;
 pub mod client_log_handler;
+pub mod client_logs; // Simplified client logs handler
 pub mod client_messages_handler;
+#[cfg(feature = "ontology")]
+pub mod admin_handler;
+#[cfg(feature = "ontology")]
+pub mod user_settings_handler;

 #[cfg(test)]
 pub mod tests;
diff --git a/src/handlers/settings_handler.rs b/src/handlers/settings_handler.rs
index 2c43d3a4..f423123d 100644
--- a/src/handlers/settings_handler.rs
+++ b/src/handlers/settings_handler.rs
@@ -1,409 +1,3510 @@
-// NEW: Database-Backed Settings Handler
-// Direct connection: UI → Handler → SettingsService → Database
-//
-// This replaces the old actor-based approach with direct database access
-// All settings operations go through SettingsService which handles caching and persistence
-
+// Unified Settings Handler - Single source of truth: AppFullSettings
 use actix_web::{web, Error, HttpResponse, HttpRequest};
 use crate::app_state::AppState;
 use crate::config::AppFullSettings;
+use crate::config::path_access::JsonPathAccessible;
+use crate::actors::messages::{GetSettings, UpdateSettings, UpdateSimulationParams};
+use crate::handlers::validation_handler::ValidationService;
+use crate::utils::validation::rate_limit::{RateLimiter, RateLimitConfig, EndpointRateLimits, extract_client_id};
+use crate::utils::validation::MAX_REQUEST_SIZE;
+use log::{info, warn, error, debug};
+use tracing::{info as trace_info, debug as trace_debug};
+use uuid::Uuid;
+
+// Import comprehensive validation for GPU parameters
+use crate::handlers::settings_validation_fix::{validate_physics_settings_complete, validate_constraint, convert_to_snake_case_recursive, get_complete_field_mappings, apply_field_mappings};
+
+/// Get a human-readable name for a JSON value type
+fn value_type_name(value: &Value) -> &'static str {
+    match value {
+        Value::Null => "null",
+        Value::Bool(_) => "boolean",
+        Value::Number(_) => "number",
+        Value::String(_) => "string",
+        Value::Array(_) => "array",
+        Value::Object(_) => "object",
+    }
+}
 use serde_json::{json, Value};
-use log::{info, warn, error};
+use serde::{Serialize, Deserialize};
+use std::sync::Arc;
+use std::borrow::Cow;
+
+/// DTO for settings responses with camelCase serialization
+#[derive(Debug, Serialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct SettingsResponseDTO {
+    pub visualisation: VisualisationSettingsDTO,
+    pub system: SystemSettingsDTO,
+    pub xr: XRSettingsDTO,
+    pub auth: AuthSettingsDTO,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub ragflow: Option<RagFlowSettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub perplexity: Option<PerplexitySettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub openai: Option<OpenAISettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub kokoro: Option<KokoroSettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub whisper: Option<WhisperSettingsDTO>,
+}
+
+/// DTO for settings updates with camelCase deserialization
+#[derive(Debug, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct SettingsUpdateDTO {
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub visualisation: Option<VisualisationSettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub system: Option<SystemSettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub xr: Option<XRSettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub auth: Option<AuthSettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub ragflow: Option<RagFlowSettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub perplexity: Option<PerplexitySettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub openai: Option<OpenAISettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub kokoro: Option<KokoroSettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub whisper: Option<WhisperSettingsDTO>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct VisualisationSettingsDTO {
+    pub rendering: RenderingSettingsDTO,
+    pub animations: AnimationSettingsDTO,
+    // Use "glow" consistently across all layers
+    pub glow: GlowSettingsDTO,
+    pub hologram: HologramSettingsDTO,
+    pub graphs: GraphsSettingsDTO,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub camera: Option<CameraSettingsDTO>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub space_pilot: Option<SpacePilotSettingsDTO>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct RenderingSettingsDTO {
+    pub ambient_light_intensity: f32,
+    pub background_color: String,
+    pub directional_light_intensity: f32,
+    pub enable_ambient_occlusion: bool,
+    pub enable_antialiasing: bool,
+    pub enable_shadows: bool,
+    pub environment_intensity: f32,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub shadow_map_size: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub shadow_bias: Option<f32>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub context: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub agent_colors: Option<AgentColorsDTO>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct AgentColorsDTO {
+    pub coordinator: String,
+    pub coder: String,
+    pub architect: String,
+    pub analyst: String,
+    pub tester: String,
+    pub researcher: String,
+    pub reviewer: String,
+    pub optimizer: String,
+    pub documenter: String,
+    pub queen: String,
+    pub default: String,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct AnimationSettingsDTO {
+    pub enable_motion_blur: bool,
+    pub enable_node_animations: bool,
+    pub motion_blur_strength: f32,
+    pub selection_wave_enabled: bool,
+    pub pulse_enabled: bool,
+    pub pulse_speed: f32,
+    pub pulse_strength: f32,
+    pub wave_speed: f32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct GlowSettingsDTO {
+    pub enabled: bool,
+    pub intensity: f32,
+    pub radius: f32,
+    pub threshold: f32,
+    pub diffuse_strength: f32,
+    pub atmospheric_density: f32,
+    pub volumetric_intensity: f32,
+    pub base_color: String,
+    pub emission_color: String,
+    pub opacity: f32,
+    pub pulse_speed: f32,
+    pub flow_speed: f32,
+    pub node_glow_strength: f32,
+    pub edge_glow_strength: f32,
+    pub environment_glow_strength: f32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct HologramSettingsDTO {
+    pub ring_count: u32,
+    pub ring_color: String,
+    pub ring_opacity: f32,
+    pub sphere_sizes: Vec<f32>,
+    pub ring_rotation_speed: f32,
+    pub enable_buckminster: bool,
+    pub buckminster_size: f32,
+    pub buckminster_opacity: f32,
+    pub enable_geodesic: bool,
+    pub geodesic_size: f32,
+    pub geodesic_opacity: f32,
+    pub enable_triangle_sphere: bool,
+    pub triangle_sphere_size: f32,
+    pub triangle_sphere_opacity: f32,
+    pub global_rotation_speed: f32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct GraphsSettingsDTO {
+    pub logseq: GraphSettingsDTO,
+    pub visionflow: GraphSettingsDTO,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct GraphSettingsDTO {
+    pub nodes: NodeSettingsDTO,
+    pub edges: EdgeSettingsDTO,
+    pub labels: LabelSettingsDTO,
+    pub physics: PhysicsSettingsDTO,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct NodeSettingsDTO {
+    pub base_color: String,
+    pub metalness: f32,
+    pub opacity: f32,
+    pub roughness: f32,
+    pub node_size: f32,
+    pub quality: String,
+    pub enable_instancing: bool,
+    pub enable_hologram: bool,
+    pub enable_metadata_shape: bool,
+    pub enable_metadata_visualisation: bool,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct EdgeSettingsDTO {
+    pub arrow_size: f32,
+    pub base_width: f32,
+    pub color: String,
+    pub enable_arrows: bool,
+    pub opacity: f32,
+    pub width_range: Vec<f32>,
+    pub quality: String,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct LabelSettingsDTO {
+    pub desktop_font_size: f32,
+    pub enable_labels: bool,
+    pub text_color: String,
+    pub text_outline_color: String,
+    pub text_outline_width: f32,
+    pub text_resolution: u32,
+    pub text_padding: f32,
+    pub billboard_mode: String,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub show_metadata: Option<bool>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub max_label_width: Option<f32>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct PhysicsSettingsDTO {
+    pub auto_balance: bool,
+    pub auto_balance_interval_ms: u32,
+    pub auto_balance_config: AutoBalanceConfigDTO,
+    pub spring_k: f32,
+    pub bounds_size: f32,
+    pub separation_radius: f32,
+    pub damping: f32,
+    pub enable_bounds: bool,
+    pub enabled: bool,
+    pub iterations: u32,
+    pub max_velocity: f32,
+    pub max_force: f32,
+    pub repel_k: f32,
+    pub mass_scale: f32,
+    pub boundary_damping: f32,
+    pub update_threshold: f32,
+    pub dt: f32,
+    pub temperature: f32,
+    pub gravity: f32,
+    pub stress_weight: f32,
+    pub stress_alpha: f32,
+    pub boundary_limit: f32,
+    pub alignment_strength: f32,
+    pub cluster_strength: f32,
+    pub compute_mode: i32,
+    pub rest_length: f32,
+    pub repulsion_cutoff: f32,
+    pub repulsion_softening_epsilon: f32,
+    pub center_gravity_k: f32,
+    pub grid_cell_size: f32,
+    pub warmup_iterations: u32,
+    pub cooling_rate: f32,
+    pub boundary_extreme_multiplier: f32,
+    pub boundary_extreme_force_multiplier: f32,
+    pub boundary_velocity_damping: f32,
+    pub min_distance: f32,
+    pub max_repulsion_dist: f32,
+    pub boundary_margin: f32,
+    pub boundary_force_strength: f32,
+    pub warmup_curve: String,
+    pub zero_velocity_iterations: u32,
+    pub clustering_algorithm: String,
+    pub cluster_count: u32,
+    pub clustering_resolution: f32,
+    pub clustering_iterations: u32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct AutoBalanceConfigDTO {
+    pub stability_variance_threshold: f32,
+    pub stability_frame_count: u32,
+    pub clustering_distance_threshold: f32,
+    pub bouncing_node_percentage: f32,
+    pub boundary_min_distance: f32,
+    pub boundary_max_distance: f32,
+    pub extreme_distance_threshold: f32,
+    pub explosion_distance_threshold: f32,
+    pub spreading_distance_threshold: f32,
+    pub oscillation_detection_frames: usize,
+    pub oscillation_change_threshold: f32,
+    pub min_oscillation_changes: usize,
+    pub grid_cell_size_min: f32,
+    pub grid_cell_size_max: f32,
+    pub repulsion_cutoff_min: f32,
+    pub repulsion_cutoff_max: f32,
+    pub repulsion_softening_min: f32,
+    pub repulsion_softening_max: f32,
+    pub center_gravity_min: f32,
+    pub center_gravity_max: f32,
+    pub spatial_hash_efficiency_threshold: f32,
+    pub cluster_density_threshold: f32,
+    pub numerical_instability_threshold: f32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct CameraSettingsDTO {
+    pub fov: f32,
+    pub near: f32,
+    pub far: f32,
+    pub position: PositionDTO,
+    pub look_at: PositionDTO,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct PositionDTO {
+    pub x: f32,
+    pub y: f32,
+    pub z: f32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct SpacePilotSettingsDTO {
+    pub enabled: bool,
+    pub mode: String,
+    pub sensitivity: SensitivityDTO,
+    pub smoothing: f32,
+    pub deadzone: f32,
+    pub button_functions: std::collections::HashMap<String, String>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct SensitivityDTO {
+    pub translation: f32,
+    pub rotation: f32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct SystemSettingsDTO {
+    pub network: NetworkSettingsDTO,
+    pub websocket: WebSocketSettingsDTO,
+    pub security: SecuritySettingsDTO,
+    pub debug: DebugSettingsDTO,
+    pub persist_settings: bool,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub custom_backend_url: Option<String>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct NetworkSettingsDTO {
+    pub bind_address: String,
+    pub domain: String,
+    pub enable_http2: bool,
+    pub enable_rate_limiting: bool,
+    pub enable_tls: bool,
+    pub max_request_size: usize,
+    pub min_tls_version: String,
+    pub port: u16,
+    pub rate_limit_requests: u32,
+    pub rate_limit_window: u32,
+    pub tunnel_id: String,
+    pub api_client_timeout: u64,
+    pub enable_metrics: bool,
+    pub max_concurrent_requests: u32,
+    pub max_retries: u32,
+    pub metrics_port: u16,
+    pub retry_delay: u32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct WebSocketSettingsDTO {
+    pub binary_chunk_size: usize,
+    pub binary_update_rate: u32,
+    pub min_update_rate: u32,
+    pub max_update_rate: u32,
+    pub motion_threshold: f32,
+    pub motion_damping: f32,
+    pub binary_message_version: u32,
+    pub compression_enabled: bool,
+    pub compression_threshold: usize,
+    pub heartbeat_interval: u64,
+    pub heartbeat_timeout: u64,
+    pub max_connections: usize,
+    pub max_message_size: usize,
+    pub reconnect_attempts: u32,
+    pub reconnect_delay: u64,
+    pub update_rate: u32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct SecuritySettingsDTO {
+    pub allowed_origins: Vec<String>,
+    pub audit_log_path: String,
+    pub cookie_httponly: bool,
+    pub cookie_samesite: String,
+    pub cookie_secure: bool,
+    pub csrf_token_timeout: u32,
+    pub enable_audit_logging: bool,
+    pub enable_request_validation: bool,
+    pub session_timeout: u32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct DebugSettingsDTO {
+    pub enabled: bool,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct XRSettingsDTO {
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub enabled: Option<bool>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub client_side_enable_xr: Option<bool>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub mode: Option<String>,
+    pub room_scale: f32,
+    pub space_type: String,
+    pub quality: String,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub render_scale: Option<f32>,
+    pub interaction_distance: f32,
+    pub locomotion_method: String,
+    pub teleport_ray_color: String,
+    pub controller_ray_color: String,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub controller_model: Option<String>,
+    pub enable_hand_tracking: bool,
+    pub hand_mesh_enabled: bool,
+    pub hand_mesh_color: String,
+    pub hand_mesh_opacity: f32,
+    pub hand_point_size: f32,
+    pub hand_ray_enabled: bool,
+    pub hand_ray_color: String,
+    pub hand_ray_width: f32,
+    pub gesture_smoothing: f32,
+    pub enable_haptics: bool,
+    pub haptic_intensity: f32,
+    pub drag_threshold: f32,
+    pub pinch_threshold: f32,
+    pub rotation_threshold: f32,
+    pub interaction_radius: f32,
+    pub movement_speed: f32,
+    pub dead_zone: f32,
+    pub movement_axes: MovementAxesDTO,
+    pub enable_light_estimation: bool,
+    pub enable_plane_detection: bool,
+    pub enable_scene_understanding: bool,
+    pub plane_color: String,
+    pub plane_opacity: f32,
+    pub plane_detection_distance: f32,
+    pub show_plane_overlay: bool,
+    pub snap_to_floor: bool,
+    pub enable_passthrough_portal: bool,
+    pub passthrough_opacity: f32,
+    pub passthrough_brightness: f32,
+    pub passthrough_contrast: f32,
+    pub portal_size: f32,
+    pub portal_edge_color: String,
+    pub portal_edge_width: f32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct MovementAxesDTO {
+    pub horizontal: i32,
+    pub vertical: i32,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct AuthSettingsDTO {
+    pub enabled: bool,
+    pub provider: String,
+    pub required: bool,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct RagFlowSettingsDTO {
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub api_key: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub agent_id: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub api_base_url: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub timeout: Option<u64>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub max_retries: Option<u32>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub chat_id: Option<String>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct PerplexitySettingsDTO {
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub api_key: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub model: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub api_url: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub max_tokens: Option<u32>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub temperature: Option<f32>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub top_p: Option<f32>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub presence_penalty: Option<f32>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub frequency_penalty: Option<f32>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub timeout: Option<u64>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub rate_limit: Option<u32>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct OpenAISettingsDTO {
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub api_key: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub base_url: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub timeout: Option<u64>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub rate_limit: Option<u32>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct KokoroSettingsDTO {
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub api_url: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub default_voice: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub default_format: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub default_speed: Option<f32>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub timeout: Option<u64>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub stream: Option<bool>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub return_timestamps: Option<bool>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub sample_rate: Option<u32>,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
+#[serde(rename_all = "camelCase")]
+pub struct WhisperSettingsDTO {
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub api_url: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub default_model: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub default_language: Option<String>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub timeout: Option<u64>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub temperature: Option<f32>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub return_timestamps: Option<bool>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub vad_filter: Option<bool>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub word_timestamps: Option<bool>,
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub initial_prompt: Option<String>,
+}
+
+// Conversion functions between AppFullSettings and DTOs
+impl From<&AppFullSettings> for SettingsResponseDTO {
+    fn from(settings: &AppFullSettings) -> Self {
+        Self {
+            visualisation: (&settings.visualisation).into(),
+            system: (&settings.system).into(),
+            xr: (&settings.xr).into(),
+            auth: (&settings.auth).into(),
+            ragflow: settings.ragflow.as_ref().map(|r| r.into()),
+            perplexity: settings.perplexity.as_ref().map(|p| p.into()),
+            openai: settings.openai.as_ref().map(|o| o.into()),
+            kokoro: settings.kokoro.as_ref().map(|k| k.into()),
+            whisper: settings.whisper.as_ref().map(|w| w.into()),
+        }
+    }
+}
+
+// Implement all the necessary From conversions for nested structures
+impl From<&crate::config::VisualisationSettings> for VisualisationSettingsDTO {
+    fn from(settings: &crate::config::VisualisationSettings) -> Self {
+        Self {
+            rendering: (&settings.rendering).into(),
+            animations: (&settings.animations).into(),
+            glow: (&settings.glow).into(),
+            hologram: (&settings.hologram).into(),
+            graphs: (&settings.graphs).into(),
+            camera: settings.camera.as_ref().map(|c| c.into()),
+            space_pilot: settings.space_pilot.as_ref().map(|sp| sp.into()),
+        }
+    }
+}
+
+impl From<&crate::config::RenderingSettings> for RenderingSettingsDTO {
+    fn from(settings: &crate::config::RenderingSettings) -> Self {
+        // Load agent colors from dev_config
+        let dev_config = crate::config::dev_config::rendering();
+        let agent_colors = Some(AgentColorsDTO {
+            coordinator: dev_config.agent_colors.coordinator.clone(),
+            coder: dev_config.agent_colors.coder.clone(),
+            architect: dev_config.agent_colors.architect.clone(),
+            analyst: dev_config.agent_colors.analyst.clone(),
+            tester: dev_config.agent_colors.tester.clone(),
+            researcher: dev_config.agent_colors.researcher.clone(),
+            reviewer: dev_config.agent_colors.reviewer.clone(),
+            optimizer: dev_config.agent_colors.optimizer.clone(),
+            documenter: dev_config.agent_colors.documenter.clone(),
+            queen: "#FFD700".to_string(), // Gold color for queen
+            default: dev_config.agent_colors.default.clone(),
+        });
+
+        Self {
+            ambient_light_intensity: settings.ambient_light_intensity,
+            background_color: settings.background_color.clone(),
+            directional_light_intensity: settings.directional_light_intensity,
+            enable_ambient_occlusion: settings.enable_ambient_occlusion,
+            enable_antialiasing: settings.enable_antialiasing,
+            enable_shadows: settings.enable_shadows,
+            environment_intensity: settings.environment_intensity,
+            shadow_map_size: settings.shadow_map_size.clone(),
+            shadow_bias: settings.shadow_bias,
+            context: settings.context.clone(),
+            agent_colors,
+        }
+    }
+}
+
+impl From<&crate::config::AnimationSettings> for AnimationSettingsDTO {
+    fn from(settings: &crate::config::AnimationSettings) -> Self {
+        Self {
+            enable_motion_blur: settings.enable_motion_blur,
+            enable_node_animations: settings.enable_node_animations,
+            motion_blur_strength: settings.motion_blur_strength,
+            selection_wave_enabled: settings.selection_wave_enabled,
+            pulse_enabled: settings.pulse_enabled,
+            pulse_speed: settings.pulse_speed,
+            pulse_strength: settings.pulse_strength,
+            wave_speed: settings.wave_speed,
+        }
+    }
+}
+
+impl From<&crate::config::GlowSettings> for GlowSettingsDTO {
+    fn from(settings: &crate::config::GlowSettings) -> Self {
+        Self {
+            enabled: settings.enabled,
+            intensity: settings.intensity,
+            radius: settings.radius,
+            threshold: settings.threshold,
+            diffuse_strength: settings.diffuse_strength,
+            atmospheric_density: settings.atmospheric_density,
+            volumetric_intensity: settings.volumetric_intensity,
+            base_color: settings.base_color.clone(),
+            emission_color: settings.emission_color.clone(),
+            opacity: settings.opacity,
+            pulse_speed: settings.pulse_speed,
+            flow_speed: settings.flow_speed,
+            node_glow_strength: settings.node_glow_strength,
+            edge_glow_strength: settings.edge_glow_strength,
+            environment_glow_strength: settings.environment_glow_strength,
+        }
+    }
+}
+
+impl From<&crate::config::HologramSettings> for HologramSettingsDTO {
+    fn from(settings: &crate::config::HologramSettings) -> Self {
+        Self {
+            ring_count: settings.ring_count,
+            ring_color: settings.ring_color.clone(),
+            ring_opacity: settings.ring_opacity,
+            sphere_sizes: settings.sphere_sizes.clone(),
+            ring_rotation_speed: settings.ring_rotation_speed,
+            enable_buckminster: settings.enable_buckminster,
+            buckminster_size: settings.buckminster_size,
+            buckminster_opacity: settings.buckminster_opacity,
+            enable_geodesic: settings.enable_geodesic,
+            geodesic_size: settings.geodesic_size,
+            geodesic_opacity: settings.geodesic_opacity,
+            enable_triangle_sphere: settings.enable_triangle_sphere,
+            triangle_sphere_size: settings.triangle_sphere_size,
+            triangle_sphere_opacity: settings.triangle_sphere_opacity,
+            global_rotation_speed: settings.global_rotation_speed,
+        }
+    }
+}
+
+impl From<&crate::config::GraphsSettings> for GraphsSettingsDTO {
+    fn from(settings: &crate::config::GraphsSettings) -> Self {
+        Self {
+            logseq: (&settings.logseq).into(),
+            visionflow: (&settings.visionflow).into(),
+        }
+    }
+}
+
+impl From<&crate::config::GraphSettings> for GraphSettingsDTO {
+    fn from(settings: &crate::config::GraphSettings) -> Self {
+        Self {
+            nodes: (&settings.nodes).into(),
+            edges: (&settings.edges).into(),
+            labels: (&settings.labels).into(),
+            physics: (&settings.physics).into(),
+        }
+    }
+}
+
+impl From<&crate::config::NodeSettings> for NodeSettingsDTO {
+    fn from(settings: &crate::config::NodeSettings) -> Self {
+        Self {
+            base_color: settings.base_color.clone(),
+            metalness: settings.metalness,
+            opacity: settings.opacity,
+            roughness: settings.roughness,
+            node_size: settings.node_size,
+            quality: settings.quality.clone(),
+            enable_instancing: settings.enable_instancing,
+            enable_hologram: settings.enable_hologram,
+            enable_metadata_shape: settings.enable_metadata_shape,
+            enable_metadata_visualisation: settings.enable_metadata_visualisation,
+        }
+    }
+}
+
+impl From<&crate::config::EdgeSettings> for EdgeSettingsDTO {
+    fn from(settings: &crate::config::EdgeSettings) -> Self {
+        Self {
+            arrow_size: settings.arrow_size,
+            base_width: settings.base_width,
+            color: settings.color.clone(),
+            enable_arrows: settings.enable_arrows,
+            opacity: settings.opacity,
+            width_range: settings.width_range.clone(),
+            quality: settings.quality.clone(),
+        }
+    }
+}
+
+impl From<&crate::config::LabelSettings> for LabelSettingsDTO {
+    fn from(settings: &crate::config::LabelSettings) -> Self {
+        Self {
+            desktop_font_size: settings.desktop_font_size,
+            enable_labels: settings.enable_labels,
+            text_color: settings.text_color.clone(),
+            text_outline_color: settings.text_outline_color.clone(),
+            text_outline_width: settings.text_outline_width,
+            text_resolution: settings.text_resolution,
+            text_padding: settings.text_padding,
+            billboard_mode: settings.billboard_mode.clone(),
+            show_metadata: settings.show_metadata,
+            max_label_width: settings.max_label_width,
+        }
+    }
+}
+
+impl From<&crate::config::PhysicsSettings> for PhysicsSettingsDTO {
+    fn from(settings: &crate::config::PhysicsSettings) -> Self {
+        Self {
+            auto_balance: settings.auto_balance,
+            auto_balance_interval_ms: settings.auto_balance_interval_ms,
+            auto_balance_config: (&settings.auto_balance_config).into(),
+            spring_k: settings.spring_k,
+            bounds_size: settings.bounds_size,
+            separation_radius: settings.separation_radius,
+            damping: settings.damping,
+            enable_bounds: settings.enable_bounds,
+            enabled: settings.enabled,
+            iterations: settings.iterations,
+            max_velocity: settings.max_velocity,
+            max_force: settings.max_force,
+            repel_k: settings.repel_k,
+            mass_scale: settings.mass_scale,
+            boundary_damping: settings.boundary_damping,
+            update_threshold: settings.update_threshold,
+            dt: settings.dt,
+            temperature: settings.temperature,
+            gravity: settings.gravity,
+            stress_weight: settings.stress_weight,
+            stress_alpha: settings.stress_alpha,
+            boundary_limit: settings.boundary_limit,
+            alignment_strength: settings.alignment_strength,
+            cluster_strength: settings.cluster_strength,
+            compute_mode: settings.compute_mode,
+            rest_length: settings.rest_length,
+            repulsion_cutoff: settings.repulsion_cutoff,
+            repulsion_softening_epsilon: settings.repulsion_softening_epsilon,
+            center_gravity_k: settings.center_gravity_k,
+            grid_cell_size: settings.grid_cell_size,
+            warmup_iterations: settings.warmup_iterations,
+            cooling_rate: settings.cooling_rate,
+            boundary_extreme_multiplier: settings.boundary_extreme_multiplier,
+            boundary_extreme_force_multiplier: settings.boundary_extreme_force_multiplier,
+            boundary_velocity_damping: settings.boundary_velocity_damping,
+            min_distance: settings.min_distance,
+            max_repulsion_dist: settings.max_repulsion_dist,
+            boundary_margin: settings.boundary_margin,
+            boundary_force_strength: settings.boundary_force_strength,
+            warmup_curve: settings.warmup_curve.clone(),
+            zero_velocity_iterations: settings.zero_velocity_iterations,
+            clustering_algorithm: settings.clustering_algorithm.clone(),
+            cluster_count: settings.cluster_count,
+            clustering_resolution: settings.clustering_resolution,
+            clustering_iterations: settings.clustering_iterations,
+        }
+    }
+}
+
+impl From<&crate::config::AutoBalanceConfig> for AutoBalanceConfigDTO {
+    fn from(settings: &crate::config::AutoBalanceConfig) -> Self {
+        Self {
+            stability_variance_threshold: settings.stability_variance_threshold,
+            stability_frame_count: settings.stability_frame_count,
+            clustering_distance_threshold: settings.clustering_distance_threshold,
+            bouncing_node_percentage: settings.bouncing_node_percentage,
+            boundary_min_distance: settings.boundary_min_distance,
+            boundary_max_distance: settings.boundary_max_distance,
+            extreme_distance_threshold: settings.extreme_distance_threshold,
+            explosion_distance_threshold: settings.explosion_distance_threshold,
+            spreading_distance_threshold: settings.spreading_distance_threshold,
+            oscillation_detection_frames: settings.oscillation_detection_frames,
+            oscillation_change_threshold: settings.oscillation_change_threshold,
+            min_oscillation_changes: settings.min_oscillation_changes,
+            grid_cell_size_min: settings.grid_cell_size_min,
+            grid_cell_size_max: settings.grid_cell_size_max,
+            repulsion_cutoff_min: settings.repulsion_cutoff_min,
+            repulsion_cutoff_max: settings.repulsion_cutoff_max,
+            repulsion_softening_min: settings.repulsion_softening_min,
+            repulsion_softening_max: settings.repulsion_softening_max,
+            center_gravity_min: settings.center_gravity_min,
+            center_gravity_max: settings.center_gravity_max,
+            spatial_hash_efficiency_threshold: settings.spatial_hash_efficiency_threshold,
+            cluster_density_threshold: settings.cluster_density_threshold,
+            numerical_instability_threshold: settings.numerical_instability_threshold,
+        }
+    }
+}
+
+impl From<&crate::config::CameraSettings> for CameraSettingsDTO {
+    fn from(settings: &crate::config::CameraSettings) -> Self {
+        Self {
+            fov: settings.fov,
+            near: settings.near,
+            far: settings.far,
+            position: (&settings.position).into(),
+            look_at: (&settings.look_at).into(),
+        }
+    }
+}
+
+impl From<&crate::config::Position> for PositionDTO {
+    fn from(pos: &crate::config::Position) -> Self {
+        Self {
+            x: pos.x,
+            y: pos.y,
+            z: pos.z,
+        }
+    }
+}
+
+impl From<&crate::config::SpacePilotSettings> for SpacePilotSettingsDTO {
+    fn from(settings: &crate::config::SpacePilotSettings) -> Self {
+        Self {
+            enabled: settings.enabled,
+            mode: settings.mode.clone(),
+            sensitivity: (&settings.sensitivity).into(),
+            smoothing: settings.smoothing,
+            deadzone: settings.deadzone,
+            button_functions: settings.button_functions.clone(),
+        }
+    }
+}
+
+impl From<&crate::config::Sensitivity> for SensitivityDTO {
+    fn from(sens: &crate::config::Sensitivity) -> Self {
+        Self {
+            translation: sens.translation,
+            rotation: sens.rotation,
+        }
+    }
+}
+
+impl From<&crate::config::SystemSettings> for SystemSettingsDTO {
+    fn from(settings: &crate::config::SystemSettings) -> Self {
+        Self {
+            network: (&settings.network).into(),
+            websocket: (&settings.websocket).into(),
+            security: (&settings.security).into(),
+            debug: (&settings.debug).into(),
+            persist_settings: settings.persist_settings,
+            custom_backend_url: settings.custom_backend_url.clone(),
+        }
+    }
+}
+
+impl From<&crate::config::NetworkSettings> for NetworkSettingsDTO {
+    fn from(settings: &crate::config::NetworkSettings) -> Self {
+        Self {
+            bind_address: settings.bind_address.clone(),
+            domain: settings.domain.clone(),
+            enable_http2: settings.enable_http2,
+            enable_rate_limiting: settings.enable_rate_limiting,
+            enable_tls: settings.enable_tls,
+            max_request_size: settings.max_request_size,
+            min_tls_version: settings.min_tls_version.clone(),
+            port: settings.port,
+            rate_limit_requests: settings.rate_limit_requests,
+            rate_limit_window: settings.rate_limit_window,
+            tunnel_id: settings.tunnel_id.clone(),
+            api_client_timeout: settings.api_client_timeout,
+            enable_metrics: settings.enable_metrics,
+            max_concurrent_requests: settings.max_concurrent_requests,
+            max_retries: settings.max_retries,
+            metrics_port: settings.metrics_port,
+            retry_delay: settings.retry_delay,
+        }
+    }
+}
+
+impl From<&crate::config::WebSocketSettings> for WebSocketSettingsDTO {
+    fn from(settings: &crate::config::WebSocketSettings) -> Self {
+        Self {
+            binary_chunk_size: settings.binary_chunk_size,
+            binary_update_rate: settings.binary_update_rate,
+            min_update_rate: settings.min_update_rate,
+            max_update_rate: settings.max_update_rate,
+            motion_threshold: settings.motion_threshold,
+            motion_damping: settings.motion_damping,
+            binary_message_version: settings.binary_message_version,
+            compression_enabled: settings.compression_enabled,
+            compression_threshold: settings.compression_threshold,
+            heartbeat_interval: settings.heartbeat_interval,
+            heartbeat_timeout: settings.heartbeat_timeout,
+            max_connections: settings.max_connections,
+            max_message_size: settings.max_message_size,
+            reconnect_attempts: settings.reconnect_attempts,
+            reconnect_delay: settings.reconnect_delay,
+            update_rate: settings.update_rate,
+        }
+    }
+}
+
+impl From<&crate::config::SecuritySettings> for SecuritySettingsDTO {
+    fn from(settings: &crate::config::SecuritySettings) -> Self {
+        Self {
+            allowed_origins: settings.allowed_origins.clone(),
+            audit_log_path: settings.audit_log_path.clone(),
+            cookie_httponly: settings.cookie_httponly,
+            cookie_samesite: settings.cookie_samesite.clone(),
+            cookie_secure: settings.cookie_secure,
+            csrf_token_timeout: settings.csrf_token_timeout,
+            enable_audit_logging: settings.enable_audit_logging,
+            enable_request_validation: settings.enable_request_validation,
+            session_timeout: settings.session_timeout,
+        }
+    }
+}
+
+impl From<&crate::config::DebugSettings> for DebugSettingsDTO {
+    fn from(settings: &crate::config::DebugSettings) -> Self {
+        Self {
+            enabled: settings.enabled,
+        }
+    }
+}
+
+impl From<&crate::config::XRSettings> for XRSettingsDTO {
+    fn from(settings: &crate::config::XRSettings) -> Self {
+        Self {
+            enabled: settings.enabled,
+            client_side_enable_xr: settings.client_side_enable_xr,
+            mode: settings.mode.clone(),
+            room_scale: settings.room_scale,
+            space_type: settings.space_type.clone(),
+            quality: settings.quality.clone(),
+            render_scale: settings.render_scale,
+            interaction_distance: settings.interaction_distance,
+            locomotion_method: settings.locomotion_method.clone(),
+            teleport_ray_color: settings.teleport_ray_color.clone(),
+            controller_ray_color: settings.controller_ray_color.clone(),
+            controller_model: settings.controller_model.clone(),
+            enable_hand_tracking: settings.enable_hand_tracking,
+            hand_mesh_enabled: settings.hand_mesh_enabled,
+            hand_mesh_color: settings.hand_mesh_color.clone(),
+            hand_mesh_opacity: settings.hand_mesh_opacity,
+            hand_point_size: settings.hand_point_size,
+            hand_ray_enabled: settings.hand_ray_enabled,
+            hand_ray_color: settings.hand_ray_color.clone(),
+            hand_ray_width: settings.hand_ray_width,
+            gesture_smoothing: settings.gesture_smoothing,
+            enable_haptics: settings.enable_haptics,
+            haptic_intensity: settings.haptic_intensity,
+            drag_threshold: settings.drag_threshold,
+            pinch_threshold: settings.pinch_threshold,
+            rotation_threshold: settings.rotation_threshold,
+            interaction_radius: settings.interaction_radius,
+            movement_speed: settings.movement_speed,
+            dead_zone: settings.dead_zone,
+            movement_axes: (&settings.movement_axes).into(),
+            enable_light_estimation: settings.enable_light_estimation,
+            enable_plane_detection: settings.enable_plane_detection,
+            enable_scene_understanding: settings.enable_scene_understanding,
+            plane_color: settings.plane_color.clone(),
+            plane_opacity: settings.plane_opacity,
+            plane_detection_distance: settings.plane_detection_distance,
+            show_plane_overlay: settings.show_plane_overlay,
+            snap_to_floor: settings.snap_to_floor,
+            enable_passthrough_portal: settings.enable_passthrough_portal,
+            passthrough_opacity: settings.passthrough_opacity,
+            passthrough_brightness: settings.passthrough_brightness,
+            passthrough_contrast: settings.passthrough_contrast,
+            portal_size: settings.portal_size,
+            portal_edge_color: settings.portal_edge_color.clone(),
+            portal_edge_width: settings.portal_edge_width,
+        }
+    }
+}
+
+impl From<&crate::config::MovementAxes> for MovementAxesDTO {
+    fn from(axes: &crate::config::MovementAxes) -> Self {
+        Self {
+            horizontal: axes.horizontal,
+            vertical: axes.vertical,
+        }
+    }
+}
+
+impl From<&crate::config::AuthSettings> for AuthSettingsDTO {
+    fn from(settings: &crate::config::AuthSettings) -> Self {
+        Self {
+            enabled: settings.enabled,
+            provider: settings.provider.clone(),
+            required: settings.required,
+        }
+    }
+}
+
+impl From<&crate::config::RagFlowSettings> for RagFlowSettingsDTO {
+    fn from(settings: &crate::config::RagFlowSettings) -> Self {
+        Self {
+            api_key: settings.api_key.clone(),
+            agent_id: settings.agent_id.clone(),
+            api_base_url: settings.api_base_url.clone(),
+            timeout: settings.timeout,
+            max_retries: settings.max_retries,
+            chat_id: settings.chat_id.clone(),
+        }
+    }
+}
+
+impl From<&crate::config::PerplexitySettings> for PerplexitySettingsDTO {
+    fn from(settings: &crate::config::PerplexitySettings) -> Self {
+        Self {
+            api_key: settings.api_key.clone(),
+            model: settings.model.clone(),
+            api_url: settings.api_url.clone(),
+            max_tokens: settings.max_tokens,
+            temperature: settings.temperature,
+            top_p: settings.top_p,
+            presence_penalty: settings.presence_penalty,
+            frequency_penalty: settings.frequency_penalty,
+            timeout: settings.timeout,
+            rate_limit: settings.rate_limit,
+        }
+    }
+}
+
+impl From<&crate::config::OpenAISettings> for OpenAISettingsDTO {
+    fn from(settings: &crate::config::OpenAISettings) -> Self {
+        Self {
+            api_key: settings.api_key.clone(),
+            base_url: settings.base_url.clone(),
+            timeout: settings.timeout,
+            rate_limit: settings.rate_limit,
+        }
+    }
+}
+
+impl From<&crate::config::KokoroSettings> for KokoroSettingsDTO {
+    fn from(settings: &crate::config::KokoroSettings) -> Self {
+        Self {
+            api_url: settings.api_url.clone(),
+            default_voice: settings.default_voice.clone(),
+            default_format: settings.default_format.clone(),
+            default_speed: settings.default_speed,
+            timeout: settings.timeout,
+            stream: settings.stream,
+            return_timestamps: settings.return_timestamps,
+            sample_rate: settings.sample_rate,
+        }
+    }
+}
+
+impl From<&crate::config::WhisperSettings> for WhisperSettingsDTO {
+    fn from(settings: &crate::config::WhisperSettings) -> Self {
+        Self {
+            api_url: settings.api_url.clone(),
+            default_model: settings.default_model.clone(),
+            default_language: settings.default_language.clone(),
+            timeout: settings.timeout,
+            temperature: settings.temperature,
+            return_timestamps: settings.return_timestamps,
+            vad_filter: settings.vad_filter,
+            word_timestamps: settings.word_timestamps,
+            initial_prompt: settings.initial_prompt.clone(),
+        }
+    }
+}
+
+/// Enhanced settings handler with comprehensive validation
+pub struct EnhancedSettingsHandler {
+    validation_service: ValidationService,
+    rate_limiter: Arc<RateLimiter>,
+}
+
+impl EnhancedSettingsHandler {
+    pub fn new() -> Self {
+        let config = EndpointRateLimits::settings_update();
+        let rate_limiter = Arc::new(RateLimiter::new(config));
+
+        Self {
+            validation_service: ValidationService::new(),
+            rate_limiter,
+        }
+    }
+
+    /// Enhanced settings update with full validation
+    pub async fn update_settings_enhanced(
+        &self,
+        req: HttpRequest,
+        state: web::Data<AppState>,
+        payload: web::Json<Value>,
+    ) -> Result<HttpResponse, Error> {
+        // Extract request ID for tracing
+        let request_id = req.headers()
+            .get("X-Request-ID")
+            .and_then(|v| v.to_str().ok())
+            .unwrap_or(&Uuid::new_v4().to_string())
+            .to_string();
+
+        // Extract authentication info
+        let pubkey = req.headers()
+            .get("X-Nostr-Pubkey")
+            .and_then(|v| v.to_str().ok());
+        let has_token = req.headers().get("X-Nostr-Token").is_some();
+
+        trace_info!(
+            request_id = %request_id,
+            user_pubkey = ?pubkey,
+            authenticated = pubkey.is_some() && has_token,
+            "Settings update request received"
+        );
+
+        let client_id = extract_client_id(&req);
+
+        // Rate limiting check
+        if !self.rate_limiter.is_allowed(&client_id) {
+            warn!("Rate limit exceeded for settings update from client: {}", client_id);
+            return Ok(HttpResponse::TooManyRequests().json(json!({
+                "error": "rate_limit_exceeded",
+                "message": "Too many settings update requests. Please wait before retrying.",
+                "client_id": client_id,
+                "retry_after": self.rate_limiter.reset_time(&client_id).as_secs()
+            })));
+        }
+
+        // Request size check
+        let payload_size = serde_json::to_vec(&*payload).unwrap_or_default().len();
+        if payload_size > MAX_REQUEST_SIZE {
+            error!("Settings update payload too large: {} bytes", payload_size);
+            return Ok(HttpResponse::PayloadTooLarge().json(json!({
+                "error": "payload_too_large",
+                "message": format!("Payload size {} bytes exceeds limit of {} bytes", payload_size, MAX_REQUEST_SIZE),
+                "max_size": MAX_REQUEST_SIZE
+            })));
+        }
+
+        // Processing settings update
+
+        // Comprehensive validation
+        let validated_payload = match self.validation_service.validate_settings_update(&payload) {
+            Ok(sanitized) => sanitized,
+            Err(validation_error) => {
+                warn!("Settings validation failed for client {}: {}", client_id, validation_error);
+                return Ok(validation_error.to_http_response());
+            }
+        };
+
+        // Settings validation passed
+
+        // Continue with existing update logic using validated payload
+        let update = validated_payload;
+
+        // Settings update received
+
+        // Get current settings
+        let mut app_settings = match state.settings_addr.send(GetSettings).await {
+            Ok(Ok(s)) => s,
+            Ok(Err(e)) => {
+                error!("Failed to get current settings: {}", e);
+                return Ok(HttpResponse::InternalServerError().json(json!({
+                    "error": "Failed to get current settings"
+                })));
+            }
+            Err(e) => {
+                error!("Settings actor error: {}", e);
+                return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                    "error": "Settings service unavailable"
+                })));
+            }
+        };
+
+        // Continue with existing auto-balance logic...
+        let mut modified_update = update.clone();
+        let auto_balance_update = update.get("visualisation")
+            .and_then(|v| v.get("graphs"))
+            .and_then(|g| {
+                if let Some(logseq) = g.get("logseq") {
+                    if let Some(physics) = logseq.get("physics") {
+                        if let Some(auto_balance) = physics.get("autoBalance") {
+                            return Some(auto_balance.clone());
+                        }
+                    }
+                }
+                if let Some(visionflow) = g.get("visionflow") {
+                    if let Some(physics) = visionflow.get("physics") {
+                        if let Some(auto_balance) = physics.get("autoBalance") {
+                            return Some(auto_balance.clone());
+                        }
+                    }
+                }
+                None
+            });
+
+        // If auto_balance is being updated, apply to both graphs
+        if let Some(ref auto_balance_value) = auto_balance_update {
+            // Synchronizing auto_balance setting across both graphs
+
+            let vis_obj = modified_update.as_object_mut()
+                .and_then(|o| o.entry("visualisation").or_insert_with(|| json!({})).as_object_mut())
+                .and_then(|v| v.entry("graphs").or_insert_with(|| json!({})).as_object_mut());
+
+            if let Some(graphs) = vis_obj {
+                let logseq_physics = graphs
+                    .entry("logseq").or_insert_with(|| json!({})).as_object_mut()
+                    .and_then(|l| l.entry("physics").or_insert_with(|| json!({})).as_object_mut());
+                if let Some(physics) = logseq_physics {
+                    physics.insert("autoBalance".to_string(), auto_balance_value.clone());
+                }
+
+                let visionflow_physics = graphs
+                    .entry("visionflow").or_insert_with(|| json!({})).as_object_mut()
+                    .and_then(|v| v.entry("physics").or_insert_with(|| json!({})).as_object_mut());
+                if let Some(physics) = visionflow_physics {
+                    physics.insert("autoBalance".to_string(), auto_balance_value.clone());
+                }
+            }
+        }
+
+        // Merge the (possibly modified) update
+        if let Err(e) = app_settings.merge_update(modified_update.clone()) {
+            error!("Failed to merge settings: {}", e);
+            if crate::utils::logging::is_debug_enabled() {
+                error!("Update payload that caused error: {}", serde_json::to_string_pretty(&modified_update).unwrap_or_else(|_| "Could not serialize".to_string()));
+            }
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": format!("Failed to merge settings: {}", e)
+            })));
+        }
+
+        // Continue with existing update logic...
+        let _updated_graphs = if auto_balance_update.is_some() {
+            vec!["logseq", "visionflow"]
+        } else {
+            // Use the extract_physics_updates helper function
+            let _physics_updates = extract_physics_updates(&modified_update);
+            modified_update.get("visualisation")
+                .and_then(|v| v.get("graphs"))
+                .and_then(|g| g.as_object())
+                .map(|graphs| {
+                    let mut updated = Vec::new();
+                    if graphs.contains_key("logseq") {
+                        updated.push("logseq");
+                    }
+                    if graphs.contains_key("visionflow") {
+                        updated.push("visionflow");
+                    }
+                    updated
+                })
+                .unwrap_or_default()
+        };
+
+        let auto_balance_active = app_settings.visualisation.graphs.logseq.physics.auto_balance
+            || app_settings.visualisation.graphs.visionflow.physics.auto_balance;
+
+        // Save updated settings
+        match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+            Ok(Ok(())) => {
+                // Settings updated successfully
+
+                let is_auto_balance_change = auto_balance_update.is_some();
+
+                if is_auto_balance_change || !auto_balance_active {
+                    // Only use logseq (knowledge graph) physics for now
+                    // TODO: Add graph type selection when agent graph is implemented
+                    propagate_physics_to_gpu(&state, &app_settings, "logseq").await;
+                    if is_auto_balance_change {
+                        // Propagating auto_balance setting change to GPU
+                    }
+                } else {
+                    // Skipping physics propagation - auto-balance is active
+                }
+
+                let response_dto: SettingsResponseDTO = (&app_settings).into();
+
+                Ok(HttpResponse::Ok().json(json!({
+                    "status": "success",
+                    "message": "Settings updated successfully",
+                    "settings": response_dto,
+                    "client_id": client_id,
+                    "timestamp": chrono::Utc::now().to_rfc3339()
+                })))
+            }
+            Ok(Err(e)) => {
+                error!("Failed to save settings: {}", e);
+                Ok(HttpResponse::InternalServerError().json(json!({
+                    "error": format!("Failed to save settings: {}", e)
+                })))
+            }
+            Err(e) => {
+                error!("Settings actor error: {}", e);
+                Ok(HttpResponse::ServiceUnavailable().json(json!({
+                    "error": "Settings service unavailable"
+                })))
+            }
+        }
+    }
+
+    /// Enhanced get settings with validation metadata
+    pub async fn get_settings_enhanced(
+        &self,
+        req: HttpRequest,
+        state: web::Data<AppState>,
+    ) -> Result<HttpResponse, Error> {
+        // Extract request ID for tracing
+        let request_id = req.headers()
+            .get("X-Request-ID")
+            .and_then(|v| v.to_str().ok())
+            .unwrap_or(&Uuid::new_v4().to_string())
+            .to_string();
+
+        // Extract authentication info
+        let pubkey = req.headers()
+            .get("X-Nostr-Pubkey")
+            .and_then(|v| v.to_str().ok());
+        let has_token = req.headers().get("X-Nostr-Token").is_some();
+
+        trace_info!(
+            request_id = %request_id,
+            user_pubkey = ?pubkey,
+            authenticated = pubkey.is_some() && has_token,
+            "Settings GET request received"
+        );
+
+        let client_id = extract_client_id(&req);
+
+        // Rate limiting (more permissive for GET requests)
+        let get_rate_limiter = Arc::new(RateLimiter::new(RateLimitConfig {
+            requests_per_minute: 120,
+            burst_size: 20,
+            ..Default::default()
+        }));
+
+        if !get_rate_limiter.is_allowed(&client_id) {
+            return Ok(HttpResponse::TooManyRequests().json(json!({
+                "error": "rate_limit_exceeded",
+                "message": "Too many get settings requests"
+            })));
+        }
+
+        // Processing get settings request
+
+        let app_settings = match state.settings_addr.send(GetSettings).await {
+            Ok(Ok(settings)) => settings,
+            Ok(Err(e)) => {
+                error!("Failed to get settings: {}", e);
+                return Ok(HttpResponse::InternalServerError().json(json!({
+                    "error": "Failed to retrieve settings"
+                })));
+            }
+            Err(e) => {
+                error!("Settings actor error: {}", e);
+                return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                    "error": "Settings service unavailable"
+                })));
+            }
+        };
+
+        let response_dto: SettingsResponseDTO = (&app_settings).into();
+
+        Ok(HttpResponse::Ok().json(json!({
+            "status": "success",
+            "settings": response_dto,
+            "validation_info": {
+                "input_sanitization": "enabled",
+                "rate_limiting": "active",
+                "schema_validation": "enforced"
+            },
+            "client_id": client_id,
+            "timestamp": chrono::Utc::now().to_rfc3339()
+        })))
+    }
+
+    /// Reset settings with validation
+    pub async fn reset_settings_enhanced(
+        &self,
+        req: HttpRequest,
+        state: web::Data<AppState>,
+    ) -> Result<HttpResponse, Error> {
+        let client_id = extract_client_id(&req);
+
+        // Stricter rate limiting for reset operations
+        let reset_rate_limiter = Arc::new(RateLimiter::new(RateLimitConfig {
+            requests_per_minute: 10,
+            burst_size: 2,
+            ..Default::default()
+        }));
+
+        if !reset_rate_limiter.is_allowed(&client_id) {
+            warn!("Rate limit exceeded for settings reset from client: {}", client_id);
+            return Ok(HttpResponse::TooManyRequests().json(json!({
+                "error": "rate_limit_exceeded",
+                "message": "Too many reset requests. This is a destructive operation with strict limits."
+            })));
+        }
+
+        // Processing settings reset request
+
+        // Load default settings
+        let default_settings = match AppFullSettings::new() {
+            Ok(settings) => settings,
+            Err(e) => {
+                error!("Failed to load default settings: {}", e);
+                return Ok(HttpResponse::InternalServerError().json(json!({
+                    "error": "Failed to load default settings"
+                })));
+            }
+        };
+
+        // Save as current settings
+        match state.settings_addr.send(UpdateSettings { settings: default_settings.clone() }).await {
+            Ok(Ok(())) => {
+                info!("Settings reset to defaults for client: {}", client_id);
+
+                let response_dto: SettingsResponseDTO = (&default_settings).into();
+
+                Ok(HttpResponse::Ok().json(json!({
+                    "status": "success",
+                    "message": "Settings reset to defaults successfully",
+                    "settings": response_dto,
+                    "client_id": client_id,
+                    "timestamp": chrono::Utc::now().to_rfc3339()
+                })))
+            }
+            Ok(Err(e)) => {
+                error!("Failed to reset settings: {}", e);
+                Ok(HttpResponse::InternalServerError().json(json!({
+                    "error": format!("Failed to reset settings: {}", e)
+                })))
+            }
+            Err(e) => {
+                error!("Settings actor error during reset: {}", e);
+                Ok(HttpResponse::ServiceUnavailable().json(json!({
+                    "error": "Settings service unavailable during reset"
+                })))
+            }
+        }
+    }
+
+    /// Settings health check endpoint
+    pub async fn settings_health(
+        &self,
+        req: HttpRequest,
+        state: web::Data<AppState>,
+    ) -> Result<HttpResponse, Error> {
+        let request_id = req.headers()
+            .get("X-Request-ID")
+            .and_then(|v| v.to_str().ok())
+            .unwrap_or(&Uuid::new_v4().to_string())
+            .to_string();
+
+        trace_info!(
+            request_id = %request_id,
+            "Settings health check requested"
+        );
+
+        // Get cache statistics
+        // TODO: Integrate with new user_service-based caching
+        let cache_entries = 0;
+        let cache_ages: Vec<(String, std::time::Duration)> = Vec::new();
+
+        // Calculate cache metrics
+        let cache_hit_rate = if cache_entries > 0 {
+            // This is an estimate - in production, track actual hits/misses
+            0.85 // Placeholder
+        } else {
+            0.0
+        };
+
+        let oldest_cache_entry = cache_ages.iter()
+            .map(|(_, age)| age.as_secs())
+            .max()
+            .unwrap_or(0);
+
+        let avg_cache_age = if !cache_ages.is_empty() {
+            cache_ages.iter()
+                .map(|(_, age)| age.as_secs())
+                .sum::<u64>() / cache_ages.len() as u64
+        } else {
+            0
+        };
+
+        // Check settings actor health
+        let settings_healthy = match state.settings_addr.send(GetSettings).await {
+            Ok(Ok(_)) => true,
+            _ => false,
+        };
+
+        Ok(HttpResponse::Ok().json(json!({
+            "status": if settings_healthy { "healthy" } else { "degraded" },
+            "request_id": request_id,
+            "cache": {
+                "entries": cache_entries,
+                "hit_rate": cache_hit_rate,
+                "oldest_entry_secs": oldest_cache_entry,
+                "avg_age_secs": avg_cache_age,
+                "ttl_secs": 600, // 10 minutes
+            },
+            "settings_actor": {
+                "responsive": settings_healthy,
+            },
+            "rate_limiting": {
+                "stats": self.rate_limiter.get_stats(),
+            },
+            "timestamp": chrono::Utc::now().to_rfc3339()
+        })))
+    }
+
+    /// Get validation statistics for settings
+    pub async fn get_validation_stats(
+        &self,
+        req: HttpRequest,
+    ) -> Result<HttpResponse, Error> {
+        let client_id = extract_client_id(&req);
+        debug!("Validation stats request from client: {}", client_id);
+
+        let stats = self.rate_limiter.get_stats();
+
+        Ok(HttpResponse::Ok().json(json!({
+            "validation_service": "active",
+            "rate_limiting": {
+                "total_clients": stats.total_clients,
+                "banned_clients": stats.banned_clients,
+                "active_clients": stats.active_clients,
+                "config": stats.config
+            },
+            "security_features": [
+                "comprehensive_input_validation",
+                "xss_prevention",
+                "sql_injection_prevention",
+                "path_traversal_prevention",
+                "malicious_content_detection",
+                "rate_limiting",
+                "request_size_validation"
+            ],
+            "endpoints_protected": [
+                "/settings",
+                "/settings/reset",
+                "/physics/update",
+                "/physics/compute-mode",
+                "/clustering/algorithm",
+                "/constraints/update",
+                "/stress/optimization"
+            ],
+            "timestamp": chrono::Utc::now().to_rfc3339()
+        })))
+    }
+
+    /// Propagate physics updates to GPU actors
+    async fn propagate_physics_updates(
+        &self,
+        state: &web::Data<AppState>,
+        settings: &AppFullSettings,
+        update: &Value,
+    ) {
+        // Check if physics was updated
+        let has_physics_update = update.get("visualisation")
+            .and_then(|v| v.get("graphs"))
+            .map(|g| {
+                g.as_object()
+                    .map(|obj| obj.values().any(|graph| graph.get("physics").is_some()))
+                    .unwrap_or(false)
+            })
+            .unwrap_or(false);
+
+        if has_physics_update {
+            info!("Propagating physics updates to GPU actors");
+
+            // Only use logseq (knowledge graph) physics for now
+            // TODO: Add graph type selection when agent graph is implemented
+            let graph_name = "logseq";
+            let physics = settings.get_physics(graph_name);
+            let sim_params = crate::models::simulation_params::SimulationParams::from(physics);
+
+            if let Some(gpu_addr) = &state.gpu_compute_addr {
+                if let Err(e) = gpu_addr.send(UpdateSimulationParams { params: sim_params }).await {
+                    error!("Failed to update GPU simulation params for {}: {}", graph_name, e);
+                } else {
+                    info!("GPU simulation params updated for {} (knowledge graph)", graph_name);
+                }
+            }
+        }
+    }
+}
+
+impl Default for EnhancedSettingsHandler {
+    fn default() -> Self {
+        Self::new()
+    }
+}

-/// Configure settings routes
+/// Configure routes for settings endpoints
 pub fn config(cfg: &mut web::ServiceConfig) {
-    cfg.service(
-        web::scope("/settings")
-            .route("", web::get().to(get_settings))
-            .route("", web::post().to(update_settings))
-            .route("/health", web::get().to(settings_health))
-            .route("/reset", web::post().to(reset_settings))
-            .route("/export", web::get().to(export_settings))
-            .route("/import", web::post().to(import_settings))
-            .route("/cache/clear", web::post().to(clear_cache))
-            .route("/path/{path:.*}", web::get().to(get_setting_by_path))
-            .route("/path/{path:.*}", web::put().to(update_setting_by_path))
-            .route("/batch", web::post().to(get_settings_batch))
-            .route("/physics/{graph_name}", web::get().to(get_physics_settings))
-            .route("/physics/{graph_name}", web::put().to(update_physics_settings))
-    );
+    let handler = web::Data::new(EnhancedSettingsHandler::new());
+
+    cfg.app_data(handler.clone())
+        .service(
+            web::scope("/settings")
+                // Path-based batch endpoints from settings_paths module (includes /path, /batch, /schema)
+                .configure(crate::handlers::settings_paths::configure_settings_paths)
+                // Additional endpoints specific to this handler
+                .route("/current", web::get().to(get_current_settings))
+                // Legacy endpoints (kept for compatibility but deprecated)
+                .route("", web::get().to(get_settings))
+                .route("", web::post().to(update_settings))
+                .route("/reset", web::post().to(reset_settings))
+                .route("/save", web::post().to(save_settings))
+                .route("/validation/stats", web::get().to(|req, handler: web::Data<EnhancedSettingsHandler>| async move {
+                    handler.get_validation_stats(req).await
+                }))
+        )
+        .service(
+            web::scope("/api/physics")
+                .route("/compute-mode", web::post().to(update_compute_mode))
+        )
+        .service(
+            web::scope("/api/clustering")
+                .route("/algorithm", web::post().to(update_clustering_algorithm))
+        )
+        .service(
+            web::scope("/api/constraints")
+                .route("/update", web::post().to(update_constraints))
+        )
+        .service(
+            web::scope("/api/analytics")
+                .route("/clusters", web::get().to(get_cluster_analytics))
+        )
+        .service(
+            web::scope("/api/stress")
+                .route("/optimization", web::post().to(update_stress_optimization))
+        );
+}
+
+/// Get single setting by path
+async fn get_setting_by_path(
+    req: HttpRequest,
+    state: web::Data<AppState>,
+) -> Result<HttpResponse, Error> {
+    let path = req.query_string()
+        .split('&')
+        .find(|param| param.starts_with("path="))
+        .and_then(|p| p.strip_prefix("path="))
+        .map(|p| urlencoding::decode(p).unwrap_or(Cow::Borrowed(p)).to_string())
+        .ok_or_else(|| actix_web::error::ErrorBadRequest("Missing 'path' query parameter"))?;
+
+    let app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(settings)) => settings,
+        Ok(Err(e)) => {
+            error!("Failed to get settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to retrieve settings",
+                "path": path
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable",
+                "path": path
+            })));
+        }
+    };
+
+    match app_settings.get_json_by_path(&path) {
+        Ok(value_json) => {
+            Ok(HttpResponse::Ok().json(json!({
+                "success": true,
+                "path": path,
+                "value": value_json
+            })))
+        }
+        Err(e) => {
+            warn!("Path not found '{}': {}", path, e);
+            Ok(HttpResponse::NotFound().json(json!({
+                "success": false,
+                "error": "Path not found",
+                "path": path,
+                "message": e
+            })))
+        }
+    }
+}
+
+/// Update single setting by path
+async fn update_setting_by_path(
+    _req: HttpRequest,
+    state: web::Data<AppState>,
+    payload: web::Json<Value>,
+) -> Result<HttpResponse, Error> {
+    let update = payload.into_inner();
+    let path = update.get("path")
+        .and_then(|p| p.as_str())
+        .ok_or_else(|| actix_web::error::ErrorBadRequest("Missing 'path' in request body"))?
+        .to_string();
+    let value = update.get("value")
+        .ok_or_else(|| actix_web::error::ErrorBadRequest("Missing 'value' in request body"))?
+        .clone();
+
+    let mut app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(settings)) => settings,
+        Ok(Err(e)) => {
+            error!("Failed to get settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to retrieve settings",
+                "path": path
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable",
+                "path": path
+            })));
+        }
+    };
+
+    let previous_value = app_settings.get_json_by_path(&path).ok();
+
+    match app_settings.set_json_by_path(&path, value.clone()) {
+        Ok(()) => {
+            match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+                Ok(Ok(())) => {
+                    info!("Updated setting at path: {}", path);
+
+                    // Check if this is a physics setting and propagate to GPU if so
+                    if path.contains(".physics.") || path.contains(".graphs.logseq.") || path.contains(".graphs.visionflow.") {
+                        info!("Physics setting changed, propagating to GPU actors");
+
+                        // Determine which graph was updated
+                        let graph_name = if path.contains(".graphs.logseq.") {
+                            "logseq"
+                        } else if path.contains(".graphs.visionflow.") {
+                            "visionflow"
+                        } else {
+                            // Default to logseq for general physics settings
+                            "logseq"
+                        };
+
+                        // Propagate physics to GPU
+                        propagate_physics_to_gpu(&state, &app_settings, graph_name).await;
+                    }
+
+                    Ok(HttpResponse::Ok().json(json!({
+                        "success": true,
+                        "path": path,
+                        "value": update.get("value").unwrap(),
+                        "previousValue": previous_value
+                    })))
+                }
+                Ok(Err(e)) => {
+                    error!("Failed to save settings: {}", e);
+                    Ok(HttpResponse::InternalServerError().json(json!({
+                        "error": format!("Failed to save settings: {}", e),
+                        "path": path
+                    })))
+                }
+                Err(e) => {
+                    error!("Settings actor error: {}", e);
+                    Ok(HttpResponse::ServiceUnavailable().json(json!({
+                        "error": "Settings service unavailable",
+                        "path": path
+                    })))
+                }
+            }
+        }
+        Err(e) => {
+            warn!("Failed to update path '{}': {}", path, e);
+            Ok(HttpResponse::BadRequest().json(json!({
+                "success": false,
+                "error": "Invalid path or value",
+                "path": path,
+                "message": e
+            })))
+        }
+    }
+}
+
+/// Batch get multiple settings
+async fn batch_get_settings(
+    state: web::Data<AppState>,
+    payload: web::Json<Value>,
+) -> Result<HttpResponse, Error> {
+    let paths = payload.get("paths")
+        .and_then(|p| p.as_array())
+        .ok_or_else(|| actix_web::error::ErrorBadRequest("Missing 'paths' array"))?
+        .iter()
+        .map(|p| p.as_str().unwrap_or("").to_string())
+        .collect::<Vec<String>>();
+
+    if paths.is_empty() {
+        return Ok(HttpResponse::BadRequest().json(json!({
+            "success": false,
+            "error": "Paths array cannot be empty"
+        })));
+    }
+
+    let app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(settings)) => settings,
+        Ok(Err(e)) => {
+            error!("Failed to get settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to retrieve settings"
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    let results: Vec<Value> = paths.iter().map(|path| {
+        match app_settings.get_json_by_path(path) {
+            Ok(value_json) => {
+                json!({
+                    "path": path,
+                    "value": value_json,
+                    "success": true
+                })
+            }
+            Err(e) => {
+                warn!("Path not found '{}': {}", path, e);
+                json!({
+                    "path": path,
+                    "success": false,
+                    "error": "Path not found",
+                    "message": e
+                })
+            }
+        }
+    }).collect();
+
+    Ok(HttpResponse::Ok().json(json!({
+        "success": true,
+        "message": format!("Successfully processed {} paths", results.len()),
+        "values": results
+    })))
+}
+
+/// Batch update multiple settings
+async fn batch_update_settings(
+    state: web::Data<AppState>,
+    payload: web::Json<Value>,
+) -> Result<HttpResponse, Error> {
+    // Log the incoming request for debugging
+    info!("Batch update request received: {:?}", payload);
+
+    let updates = payload.get("updates")
+        .and_then(|u| u.as_array())
+        .ok_or_else(|| {
+            error!("Batch update failed: Missing 'updates' array in payload: {:?}", payload);
+            actix_web::error::ErrorBadRequest("Missing 'updates' array")
+        })?;
+
+    if updates.is_empty() {
+        error!("Batch update failed: Empty updates array");
+        return Ok(HttpResponse::BadRequest().json(json!({
+            "success": false,
+            "error": "Updates array cannot be empty"
+        })));
+    }
+
+    info!("Processing {} batch updates", updates.len());
+
+    let mut app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(settings)) => settings,
+        Ok(Err(e)) => {
+            error!("Failed to get settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to retrieve settings"
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    let mut results = Vec::new();
+    let mut success_count = 0;
+
+    for update in updates {
+        let path = update.get("path").and_then(|p| p.as_str()).unwrap_or("");
+        let value = update.get("value").unwrap_or(&Value::Null).clone();
+
+        info!("Processing batch update: path='{}', value={:?}", path, value);
+
+        let previous_value = app_settings.get_json_by_path(path).ok();
+
+        match app_settings.set_json_by_path(path, value.clone()) {
+            Ok(()) => {
+                success_count += 1;
+                info!("Successfully updated path '{}' with value {:?}", path, value);
+                results.push(json!({
+                    "path": path,
+                    "success": true,
+                    "value": update.get("value").unwrap(),
+                    "previousValue": previous_value
+                }));
+            }
+            Err(e) => {
+                // Enhanced error logging with more context
+                error!("Failed to update path '{}' with value {:?}: {}", path, value, e);
+
+                // Try to provide more specific error information
+                let error_detail = if e.contains("does not exist") {
+                    format!("Path '{}' does not exist in settings structure", path)
+                } else if e.contains("Type mismatch") {
+                    format!("Type mismatch: {}", e)
+                } else if e.contains("not found") {
+                    format!("Field not found: {}", e)
+                } else {
+                    e.clone()
+                };
+
+                results.push(json!({
+                    "path": path,
+                    "success": false,
+                    "error": error_detail,
+                    "message": e,
+                    "providedValue": value,
+                    "expectedType": previous_value.as_ref().map(|v| value_type_name(v))
+                }));
+            }
+        }
+    }
+
+    // Save only if at least one update succeeded
+    if success_count > 0 {
+        match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+            Ok(Ok(())) => {
+                info!("Batch updated {} settings successfully", success_count);
+
+                // Check if any physics settings were updated and propagate if so
+                let mut physics_updated = false;
+                for update in updates {
+                    let path = update.get("path").and_then(|p| p.as_str()).unwrap_or("");
+                    if path.contains(".physics.") || path.contains(".graphs.logseq.") || path.contains(".graphs.visionflow.") {
+                        physics_updated = true;
+                        break;
+                    }
+                }
+
+                if physics_updated {
+                    info!("Physics settings changed in batch update, propagating to GPU actors");
+                    // Propagate to both graphs since batch update might affect both
+                    propagate_physics_to_gpu(&state, &app_settings, "logseq").await;
+                    // Optionally propagate to visionflow if needed
+                    // propagate_physics_to_gpu(&state, &app_settings, "visionflow").await;
+                }
+            }
+            Ok(Err(e)) => {
+                error!("Failed to save batch settings: {}", e);
+                return Ok(HttpResponse::InternalServerError().json(json!({
+                    "success": false,
+                    "error": format!("Failed to save settings: {}", e),
+                    "results": results
+                })));
+            }
+            Err(e) => {
+                error!("Settings actor error: {}", e);
+                return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                    "success": false,
+                    "error": "Settings service unavailable",
+                    "results": results
+                })));
+            }
+        }
+    }
+
+    Ok(HttpResponse::Ok().json(json!({
+        "success": true,
+        "message": format!("Successfully updated {} out of {} settings", success_count, updates.len()),
+        "results": results
+    })))
+}
+
+/// Get settings schema for introspection
+async fn get_settings_schema(
+    req: HttpRequest,
+    _state: web::Data<AppState>,
+) -> Result<HttpResponse, Error> {
+    let path = req.query_string()
+        .split('&')
+        .find(|param| param.starts_with("path="))
+        .and_then(|p| p.strip_prefix("path="))
+        .map(|p| urlencoding::decode(p).unwrap_or(Cow::Borrowed(p)).to_string())
+        .unwrap_or_default();
+
+    // For now, return a simple schema based on the path
+    // In a full implementation, this would reflect the actual structure
+    let schema = json!({
+        "type": "object",
+        "properties": {
+            "damping": { "type": "number", "description": "Physics damping factor (0.0-1.0)" },
+            "gravity": { "type": "number", "description": "Physics gravity strength" },
+            // Add more fields based on path
+        },
+        "path": path
+    });
+
+    Ok(HttpResponse::Ok().json(json!({
+        "success": true,
+        "path": path,
+        "schema": schema
+    })))
+}
+
+/// Get current settings - returns camelCase JSON (legacy, kept for compatibility)
+async fn get_settings(
+    _req: HttpRequest,
+    state: web::Data<AppState>,
+) -> Result<HttpResponse, Error> {
+    let app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(settings)) => settings,
+        Ok(Err(e)) => {
+            error!("Failed to get settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to retrieve settings"
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    // Convert to DTO with camelCase serialization for client
+    let response_dto: SettingsResponseDTO = (&app_settings).into();
+
+    Ok(HttpResponse::Ok().json(response_dto))
+}
+
+/// Get current settings with version information
+async fn get_current_settings(
+    _req: HttpRequest,
+    state: web::Data<AppState>,
+) -> Result<HttpResponse, Error> {
+    let app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(settings)) => settings,
+        Ok(Err(e)) => {
+            error!("Failed to get settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to retrieve settings"
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    // Convert to DTO with camelCase serialization for client
+    let response_dto: SettingsResponseDTO = (&app_settings).into();
+
+    // Wrap with version info
+    Ok(HttpResponse::Ok().json(json!({
+        "settings": response_dto,
+        "version": app_settings.version,
+        "timestamp": std::time::SystemTime::now()
+            .duration_since(std::time::UNIX_EPOCH)
+            .unwrap_or_default()
+            .as_secs()
+    })))
+}
+
+/// Update settings with validation - accepts camelCase JSON
+async fn update_settings(
+    _req: HttpRequest,
+    state: web::Data<AppState>,
+    payload: web::Json<Value>,
+) -> Result<HttpResponse, Error> {
+    let mut update = payload.into_inner();
+
+    // Apply comprehensive case conversion from camelCase to snake_case
+    convert_to_snake_case_recursive(&mut update);
+
+    debug!("Settings update received: {:?}", update);
+
+    // Validate the update
+    if let Err(e) = validate_settings_update(&update) {
+        error!("Settings validation failed: {}", e);
+        error!("Failed update payload: {}", serde_json::to_string_pretty(&update).unwrap_or_default());
+        return Ok(HttpResponse::BadRequest().json(json!({
+            "error": format!("Invalid settings: {}", e)
+        })));
+    }
+
+    // Get current settings
+    let mut app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(s)) => s,
+        Ok(Err(e)) => {
+            error!("Failed to get current settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get current settings"
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    // Debug: Log the update payload before merging
+    if crate::utils::logging::is_debug_enabled() {
+        debug!("Settings update payload (before merge): {}", serde_json::to_string_pretty(&update).unwrap_or_else(|_| "Could not serialize".to_string()));
+    }
+
+    // Check if auto_balance is being updated in either graph
+    // If so, apply it to both graphs for consistency
+    let mut modified_update = update.clone();
+    let auto_balance_update = update.get("visualisation")
+        .and_then(|v| v.get("graphs"))
+        .and_then(|g| {
+            // Check if logseq graph has auto_balance update
+            if let Some(logseq) = g.get("logseq") {
+                if let Some(physics) = logseq.get("physics") {
+                    if let Some(auto_balance) = physics.get("autoBalance") {
+                        return Some(auto_balance.clone());
+                    }
+                }
+            }
+            // Check if visionflow graph has auto_balance update
+            if let Some(visionflow) = g.get("visionflow") {
+                if let Some(physics) = visionflow.get("physics") {
+                    if let Some(auto_balance) = physics.get("autoBalance") {
+                        return Some(auto_balance.clone());
+                    }
+                }
+            }
+            None
+        });
+
+    // If auto_balance is being updated, apply to both graphs
+    if let Some(ref auto_balance_value) = auto_balance_update {
+        info!("Synchronizing auto_balance setting across both graphs: {}", auto_balance_value);
+
+        // Ensure the update structure exists for both graphs
+        let vis_obj = modified_update.as_object_mut()
+            .and_then(|o| o.entry("visualisation").or_insert_with(|| json!({})).as_object_mut())
+            .and_then(|v| v.entry("graphs").or_insert_with(|| json!({})).as_object_mut());
+
+        if let Some(graphs) = vis_obj {
+            // Update logseq graph
+            let logseq_physics = graphs
+                .entry("logseq").or_insert_with(|| json!({})).as_object_mut()
+                .and_then(|l| l.entry("physics").or_insert_with(|| json!({})).as_object_mut());
+            if let Some(physics) = logseq_physics {
+                physics.insert("autoBalance".to_string(), auto_balance_value.clone());
+            }
+
+            // Update visionflow graph
+            let visionflow_physics = graphs
+                .entry("visionflow").or_insert_with(|| json!({})).as_object_mut()
+                .and_then(|v| v.entry("physics").or_insert_with(|| json!({})).as_object_mut());
+            if let Some(physics) = visionflow_physics {
+                physics.insert("autoBalance".to_string(), auto_balance_value.clone());
+            }
+        }
+    }
+
+    // Merge the (possibly modified) update
+    if let Err(e) = app_settings.merge_update(modified_update.clone()) {
+        error!("Failed to merge settings: {}", e);
+        if crate::utils::logging::is_debug_enabled() {
+            error!("Update payload that caused error: {}", serde_json::to_string_pretty(&modified_update).unwrap_or_else(|_| "Could not serialize".to_string()));
+        }
+        return Ok(HttpResponse::InternalServerError().json(json!({
+            "error": format!("Failed to merge settings: {}", e)
+        })));
+    }
+
+    // Check which graphs had physics updated
+    // If auto_balance was synchronized, both graphs are considered updated
+    let _updated_graphs = if auto_balance_update.is_some() {
+        // Also extract physics updates for analysis
+        let _physics_updates = extract_physics_updates(&update);
+        vec!["logseq", "visionflow"]
+    } else {
+        modified_update.get("visualisation")
+            .and_then(|v| v.get("graphs"))
+            .and_then(|g| g.as_object())
+            .map(|graphs| {
+                let mut updated = Vec::new();
+                if graphs.contains_key("logseq") {
+                    updated.push("logseq");
+                }
+                if graphs.contains_key("visionflow") {
+                    updated.push("visionflow");
+                }
+                updated
+            })
+            .unwrap_or_default()
+    };
+
+    // Check if auto-balance is enabled in the current settings
+    // If auto-balance is active, don't propagate physics back to avoid feedback loop
+    let auto_balance_active = app_settings.visualisation.graphs.logseq.physics.auto_balance
+        || app_settings.visualisation.graphs.visionflow.physics.auto_balance;
+
+    // Save updated settings
+    match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+        Ok(Ok(())) => {
+            info!("Settings updated successfully");
+
+            // Check if this update is changing the auto_balance setting itself
+            // If so, we MUST propagate it regardless of current auto_balance state
+            let is_auto_balance_change = auto_balance_update.is_some();
+
+            // Propagate physics updates to GPU
+            // - Always propagate if auto_balance setting is being changed
+            // - Skip only if auto_balance is already active AND this isn't an auto_balance change
+            //   (to prevent feedback loops from auto-tuning adjustments)
+            if is_auto_balance_change || !auto_balance_active {
+                // Only use logseq (knowledge graph) physics for now
+                // TODO: Add graph type selection when agent graph is implemented
+                propagate_physics_to_gpu(&state, &app_settings, "logseq").await;
+                if is_auto_balance_change {
+                    info!("[AUTO-BALANCE] Propagating auto_balance setting change to GPU (logseq only)");
+                }
+            } else {
+                info!("[AUTO-BALANCE] Skipping physics propagation to GPU - auto-balance is active and not changing");
+            }
+
+            // Return updated settings using DTO with camelCase serialization
+            let response_dto: SettingsResponseDTO = (&app_settings).into();
+
+            Ok(HttpResponse::Ok().json(response_dto))
+        }
+        Ok(Err(e)) => {
+            error!("Failed to save settings: {}", e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": format!("Failed to save settings: {}", e)
+            })))
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })))
+        }
+    }
+}
+
+/// Reset settings to defaults from settings.yaml
+async fn reset_settings(
+    _req: HttpRequest,
+    state: web::Data<AppState>,
+) -> Result<HttpResponse, Error> {
+    // Load default settings from YAML
+    let default_settings = match AppFullSettings::new() {
+        Ok(settings) => settings,
+        Err(e) => {
+            error!("Failed to load default settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to load default settings"
+            })));
+        }
+    };
+
+    // Save as current settings
+    match state.settings_addr.send(UpdateSettings { settings: default_settings.clone() }).await {
+        Ok(Ok(())) => {
+            info!("Settings reset to defaults");
+
+            // Return default settings using DTO with camelCase serialization
+            let response_dto: SettingsResponseDTO = (&default_settings).into();
+
+            Ok(HttpResponse::Ok().json(response_dto))
+        }
+        Ok(Err(e)) => {
+            error!("Failed to reset settings: {}", e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": format!("Failed to reset settings: {}", e)
+            })))
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })))
+        }
+    }
+}
+
+/// Explicitly save current settings to database
+async fn save_settings(
+    _req: HttpRequest,
+    state: web::Data<AppState>,
+    payload: Option<web::Json<Value>>,
+) -> Result<HttpResponse, Error> {
+    // Get current settings
+    let mut app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(s)) => s,
+        Ok(Err(e)) => {
+            error!("Failed to get current settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get current settings"
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    // If payload is provided, merge it with current settings first
+    if let Some(update) = payload {
+        let update_value = update.into_inner();
+
+        // Validate the update
+        if let Err(e) = validate_settings_update(&update_value) {
+            error!("Settings validation failed: {}", e);
+            return Ok(HttpResponse::BadRequest().json(json!({
+                "error": format!("Invalid settings: {}", e)
+            })));
+        }
+
+        // Merge the update
+        if let Err(e) = app_settings.merge_update(update_value) {
+            error!("Failed to merge settings update: {}", e);
+            return Ok(HttpResponse::BadRequest().json(json!({
+                "error": format!("Failed to merge settings: {}", e)
+            })));
+        }
+    }
+
+    // Update the settings in the actor (which automatically saves to database)
+    match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+        Ok(Ok(())) => {
+            info!("Settings successfully saved to database");
+            let response_dto: SettingsResponseDTO = (&app_settings).into();
+            Ok(HttpResponse::Ok().json(json!({
+                "message": "Settings saved successfully to database",
+                "settings": response_dto
+            })))
+        }
+        Ok(Err(e)) => {
+            error!("Failed to save settings to database: {}", e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to save settings to database",
+                "details": e.to_string()
+            })))
+        }
+        Err(e) => {
+            error!("Settings actor communication error: {}", e);
+            Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })))
+        }
+    }
+}
+
+/// Validate settings update payload
+fn validate_settings_update(update: &Value) -> Result<(), String> {
+    // Validate visualisation settings
+    if let Some(vis) = update.get("visualisation") {
+        if let Some(graphs) = vis.get("graphs") {
+            // Validate graph settings
+            for (graph_name, graph_settings) in graphs.as_object().ok_or("graphs must be an object")?.iter() {
+                if graph_name != "logseq" && graph_name != "visionflow" {
+                    return Err(format!("Invalid graph name: {}", graph_name));
+                }
+
+                // Validate physics settings
+                if let Some(physics) = graph_settings.get("physics") {
+                    validate_physics_settings(physics)?;
+                }
+
+                // Validate node settings
+                if let Some(nodes) = graph_settings.get("nodes") {
+                    validate_node_settings(nodes)?;
+                }
+            }
+        }
+
+        // Validate rendering settings
+        if let Some(rendering) = vis.get("rendering") {
+            validate_rendering_settings(rendering)?;
+        }
+
+        // Validate hologram settings
+        if let Some(hologram) = vis.get("hologram") {
+            validate_hologram_settings(hologram)?;
+        }
+    }
+
+    // Validate XR settings
+    if let Some(xr) = update.get("xr") {
+        validate_xr_settings(xr)?;
+    }
+
+    // Validate system settings
+    if let Some(system) = update.get("system") {
+        validate_system_settings(system)?;
+    }
+
+    Ok(())
 }

-/// Get all settings from database
-pub async fn get_settings(
-    state: web::Data<AppState>,
-) -> Result<HttpResponse, Error> {
-    info!("[Settings Handler] GET /settings - Loading from database");
+fn validate_physics_settings(physics: &Value) -> Result<(), String> {
+    // Use comprehensive validation with proper GPU bounds to prevent NaN and explosions
+    validate_physics_settings_complete(physics)?;
+
+    // Log what fields are actually being sent for debugging
+    if let Some(obj) = physics.as_object() {
+        debug!("Physics settings fields received: {:?}", obj.keys().collect::<Vec<_>>());
+    }

-    match state.settings_service.load_all_settings() {
-        Ok(Some(settings)) => {
-            info!("[Settings Handler] Settings loaded successfully from database");
-            Ok(HttpResponse::Ok().json(settings))
+    // Additional validations for iterations (accept both int and float from JS)
+    if let Some(iterations) = physics.get("iterations") {
+        let val = iterations.as_f64()
+            .map(|f| f.round() as u64)
+            .or_else(|| iterations.as_u64())
+            .ok_or("iterations must be a positive number")?;
+        if val == 0 || val > 1000 {
+            return Err("iterations must be between 1 and 1000".to_string());
         }
-        Ok(None) => {
-            warn!("[Settings Handler] No settings found in database, using defaults");
-            let default_settings = AppFullSettings::default();
+    }

-            // Save defaults to database for next time
-            if let Err(e) = state.settings_service.save_all_settings(&default_settings) {
-                error!("[Settings Handler] Failed to save default settings: {}", e);
-            }
+    // Auto-balance interval validation
+    if let Some(auto_balance_interval) = physics.get("autoBalanceIntervalMs") {
+        let val = auto_balance_interval.as_u64()
+            .or_else(|| auto_balance_interval.as_f64().map(|f| f.round() as u64))
+            .ok_or("autoBalanceIntervalMs must be a positive integer")?;
+        if val < 10 || val > 60000 {
+            return Err("autoBalanceIntervalMs must be between 10 and 60000 ms".to_string());
+        }
+    }

-            Ok(HttpResponse::Ok().json(default_settings))
+    // Boundary limit validation (should be ~98% of boundsSize)
+    if let Some(boundary_limit) = physics.get("boundaryLimit") {
+        let val = boundary_limit.as_f64().ok_or("boundaryLimit must be a number")?;
+        if val < 0.1 || val > 100000.0 {
+            return Err("boundaryLimit must be between 0.1 and 100000.0".to_string());
         }
-        Err(e) => {
-            error!("[Settings Handler] Database error loading settings: {}", e);
-            Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to load settings: {}", e)
-            })))
+
+        // If boundsSize is also present, validate the relationship
+        if let Some(bounds_size) = physics.get("boundsSize").and_then(|b| b.as_f64()) {
+            let max_boundary = bounds_size * 0.99;
+            if val > max_boundary {
+                return Err(format!("boundaryLimit ({:.1}) must be less than 99% of boundsSize ({:.1})", val, bounds_size));
+            }
         }
     }
-}

-/// Update settings in database
-pub async fn update_settings(
-    state: web::Data<AppState>,
-    payload: web::Json<AppFullSettings>,
-) -> Result<HttpResponse, Error> {
-    info!("[Settings Handler] POST /settings - Updating database");
+    Ok(())
+}

-    let settings = payload.into_inner();
+fn validate_node_settings(nodes: &Value) -> Result<(), String> {
+    // UNIFIED FORMAT: Only accept camelCase "baseColor"
+    if let Some(color) = nodes.get("baseColor") {
+        let color_str = color.as_str().ok_or("baseColor must be a string")?;
+        if !color_str.starts_with('#') || (color_str.len() != 7 && color_str.len() != 4) {
+            return Err("baseColor must be a valid hex color (e.g., #ffffff or #fff)".to_string());
+        }
+    }
+
+    if let Some(opacity) = nodes.get("opacity") {
+        let val = opacity.as_f64().ok_or("opacity must be a number")?;
+        if !(0.0..=1.0).contains(&val) {
+            return Err("opacity must be between 0.0 and 1.0".to_string());
+        }
+    }
+
+    if let Some(metalness) = nodes.get("metalness") {
+        let val = metalness.as_f64().ok_or("metalness must be a number")?;
+        if !(0.0..=1.0).contains(&val) {
+            return Err("metalness must be between 0.0 and 1.0".to_string());
+        }
+    }
+
+    if let Some(roughness) = nodes.get("roughness") {
+        let val = roughness.as_f64().ok_or("roughness must be a number")?;
+        if !(0.0..=1.0).contains(&val) {
+            return Err("roughness must be between 0.0 and 1.0".to_string());
+        }
+    }
+
+    // UNIFIED FORMAT: Only accept "nodeSize"
+    if let Some(node_size) = nodes.get("nodeSize") {
+        let val = node_size.as_f64().ok_or("nodeSize must be a number")?;
+        if val <= 0.0 || val > 1000.0 {
+            return Err("nodeSize must be between 0.0 and 1000.0".to_string());
+        }
+    }
+
+    if let Some(quality) = nodes.get("quality") {
+        let q = quality.as_str().ok_or("quality must be a string")?;
+        if !["low", "medium", "high"].contains(&q) {
+            return Err("quality must be 'low', 'medium', or 'high'".to_string());
+        }
+    }
+
+    Ok(())
+}

-    // CRITICAL: Validate that graph settings are separate (logseq vs visionflow)
-    // This prevents the conflation bug mentioned by user
-    if settings.visualisation.graphs.logseq.nodes == settings.visualisation.graphs.visionflow.nodes {
-        warn!("[Settings Handler] WARNING: Logseq and Visionflow graphs have identical node settings - possible conflation!");
+fn validate_rendering_settings(rendering: &Value) -> Result<(), String> {
+    // UNIFIED FORMAT: Only accept "ambientLightIntensity"
+    if let Some(ambient) = rendering.get("ambientLightIntensity") {
+        let val = ambient.as_f64().ok_or("ambientLightIntensity must be a number")?;
+        if val < 0.0 || val > 100.0 {
+            return Err("ambientLightIntensity must be between 0.0 and 100.0".to_string());
+        }
     }
+
+    // Validate glow settings - use "glow" field consistently
+    if let Some(glow) = rendering.get("glow") {
+        validate_glow_settings(glow)?;
+    }
+
+    Ok(())
+}

-    match state.settings_service.save_all_settings(&settings) {
-        Ok(()) => {
-            info!("[Settings Handler] Settings saved successfully to database");
-            Ok(HttpResponse::Ok().json(json!({
-                "success": true,
-                "message": "Settings saved to database"
-            })))
+/// Validate glow effect settings
+fn validate_glow_settings(glow: &Value) -> Result<(), String> {
+    // Validate enabled flag
+    if let Some(enabled) = glow.get("enabled") {
+        if !enabled.is_boolean() {
+            return Err("glow enabled must be a boolean".to_string());
         }
-        Err(e) => {
-            error!("[Settings Handler] Database error saving settings: {}", e);
-            Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to save settings: {}", e)
-            })))
+    }
+
+    // Validate intensity/strength fields
+    for field_name in ["intensity", "strength"] {
+        if let Some(intensity) = glow.get(field_name) {
+            let val = intensity.as_f64().ok_or(format!("glow {} must be a number", field_name))?;
+            if val < 0.0 || val > 10.0 {
+                return Err(format!("glow {} must be between 0.0 and 10.0", field_name));
+            }
+        }
+    }
+
+    // Validate radius field
+    if let Some(radius) = glow.get("radius") {
+        let val = radius.as_f64().ok_or("glow radius must be a number")?;
+        if val < 0.0 || val > 5.0 {
+            return Err("glow radius must be between 0.0 and 5.0".to_string());
+        }
+    }
+
+    // Validate threshold field
+    if let Some(threshold) = glow.get("threshold") {
+        let val = threshold.as_f64().ok_or("glow threshold must be a number")?;
+        if val < 0.0 || val > 2.0 {
+            return Err("glow threshold must be between 0.0 and 2.0".to_string());
+        }
+    }
+
+    // Validate specific glow strength fields
+    for field_name in ["edgeGlowStrength", "environmentGlowStrength", "nodeGlowStrength"] {
+        if let Some(strength) = glow.get(field_name) {
+            let val = strength.as_f64().ok_or(format!("glow {} must be a number", field_name))?;
+            if val < 0.0 || val > 1.0 {
+                return Err(format!("glow {} must be between 0.0 and 1.0", field_name));
+            }
         }
     }
+
+    Ok(())
 }

-/// Get a single setting by path (supports camelCase and snake_case)
-pub async fn get_setting_by_path(
-    state: web::Data<AppState>,
-    path: web::Path<String>,
-) -> Result<HttpResponse, Error> {
-    let setting_path = path.into_inner();
-    info!("[Settings Handler] GET /settings/path/{} - Direct database lookup", setting_path);
-
-    match state.settings_service.get_setting(&setting_path).await {
-        Ok(Some(value)) => {
-            info!("[Settings Handler] Setting '{}' found in database", setting_path);
-            // Convert SettingValue to JSON
-            let json_value = match value {
-                crate::services::database_service::SettingValue::String(s) => json!(s),
-                crate::services::database_service::SettingValue::Integer(i) => json!(i),
-                crate::services::database_service::SettingValue::Float(f) => json!(f),
-                crate::services::database_service::SettingValue::Boolean(b) => json!(b),
-                crate::services::database_service::SettingValue::Json(j) => j,
-            };
-            Ok(HttpResponse::Ok().json(json!({
-                "path": setting_path,
-                "value": json_value
-            })))
+fn validate_hologram_settings(hologram: &Value) -> Result<(), String> {
+    // Validate ringCount - MUST be an integer
+    if let Some(ring_count) = hologram.get("ringCount") {
+        // Accept both integer and float values (JavaScript might send 5.0)
+        let val = ring_count.as_f64()
+            .map(|f| f.round() as u64)  // Round float to u64
+            .or_else(|| ring_count.as_u64())  // Also accept direct integer
+            .ok_or("ringCount must be a positive integer")?;
+
+        if val > 20 {
+            return Err("ringCount must be between 0 and 20".to_string());
         }
-        Ok(None) => {
-            info!("[Settings Handler] Setting '{}' not found", setting_path);
-            Ok(HttpResponse::NotFound().json(json!({
-                "error": "not_found",
-                "path": setting_path
-            })))
+    }
+
+    // Validate ringColor (hex color)
+    if let Some(color) = hologram.get("ringColor") {
+        let color_str = color.as_str().ok_or("ringColor must be a string")?;
+        if !color_str.starts_with('#') || (color_str.len() != 7 && color_str.len() != 4) {
+            return Err("ringColor must be a valid hex color (e.g., #ffffff or #fff)".to_string());
         }
-        Err(e) => {
-            error!("[Settings Handler] Database error getting setting '{}': {}", setting_path, e);
-            Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to get setting: {}", e)
-            })))
+    }
+
+    // Validate ringOpacity
+    if let Some(opacity) = hologram.get("ringOpacity") {
+        let val = opacity.as_f64().ok_or("ringOpacity must be a number")?;
+        if !(0.0..=1.0).contains(&val) {
+            return Err("ringOpacity must be between 0.0 and 1.0".to_string());
+        }
+    }
+
+    // Validate ringRotationSpeed
+    if let Some(speed) = hologram.get("ringRotationSpeed") {
+        let val = speed.as_f64().ok_or("ringRotationSpeed must be a number")?;
+        if val < 0.0 || val > 1000.0 {
+            return Err("ringRotationSpeed must be between 0.0 and 1000.0".to_string());
         }
     }
+
+    Ok(())
 }

-/// Update a single setting by path
-pub async fn update_setting_by_path(
-    state: web::Data<AppState>,
-    path: web::Path<String>,
-    payload: web::Json<Value>,
-) -> Result<HttpResponse, Error> {
-    let setting_path = path.into_inner();
-    let value = payload.into_inner();
-
-    info!("[Settings Handler] PUT /settings/path/{} - Direct database update", setting_path);
-
-    // Convert JSON value to SettingValue
-    use crate::services::database_service::SettingValue;
-    let setting_value = match &value {
-        Value::String(s) => SettingValue::String(s.clone()),
-        Value::Number(n) if n.is_f64() => SettingValue::Float(n.as_f64().unwrap()),
-        Value::Number(n) if n.is_i64() => SettingValue::Integer(n.as_i64().unwrap()),
-        Value::Bool(b) => SettingValue::Boolean(*b),
-        _ => SettingValue::Json(value.clone()),
-    };
-
-    match state.settings_service.set_setting(&setting_path, setting_value).await {
-        Ok(()) => {
-            info!("[Settings Handler] Setting '{}' updated in database", setting_path);
-            Ok(HttpResponse::Ok().json(json!({
-                "success": true,
-                "path": setting_path,
-                "value": value
-            })))
+fn validate_system_settings(system: &Value) -> Result<(), String> {
+    // Handle debug settings
+    if let Some(debug) = system.get("debug") {
+        if let Some(debug_obj) = debug.as_object() {
+            // All debug flags should be booleans - UNIFIED FORMAT ONLY
+            let boolean_fields = [
+                "enabled",  // NOT "enableClientDebugMode" - unified format only!
+                "showFPS",
+                "showMemory",
+                "enablePerformanceDebug",
+                "enableTelemetry",
+                "enableDataDebug",
+                "enableWebSocketDebug",
+                "enablePhysicsDebug",
+                "enableNodeDebug",
+                "enableShaderDebug",
+                "enableMatrixDebug"
+            ];
+
+            for field in &boolean_fields {
+                if let Some(val) = debug_obj.get(*field) {
+                    if !val.is_boolean() {
+                        return Err(format!("debug.{} must be a boolean", field));
+                    }
+                }
+            }
+
+            // logLevel can be a number or string
+            if let Some(log_level) = debug_obj.get("logLevel") {
+                if let Some(val) = log_level.as_f64() {
+                    if val < 0.0 || val > 3.0 {
+                        return Err("debug.logLevel must be between 0 and 3".to_string());
+                    }
+                } else if let Some(val) = log_level.as_u64() {
+                    if val > 3 {
+                        return Err("debug.logLevel must be between 0 and 3".to_string());
+                    }
+                } else if let Some(val) = log_level.as_str() {
+                    // Accept string log levels from client
+                    match val {
+                        "error" | "warn" | "info" | "debug" => {
+                            // Valid string log level
+                        }
+                        _ => {
+                            return Err("debug.logLevel must be 'error', 'warn', 'info', or 'debug'".to_string());
+                        }
+                    }
+                } else {
+                    return Err("debug.logLevel must be a number or string".to_string());
+                }
+            }
         }
-        Err(e) => {
-            error!("[Settings Handler] Database error updating setting '{}': {}", setting_path, e);
-            Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to update setting: {}", e)
-            })))
+    }
+
+    // Handle persistSettingsOnServer
+    if let Some(persist) = system.get("persistSettingsOnServer") {
+        if !persist.is_boolean() {
+            return Err("system.persistSettingsOnServer must be a boolean".to_string());
+        }
+    }
+
+    // Handle customBackendUrl
+    if let Some(url) = system.get("customBackendUrl") {
+        if !url.is_string() && !url.is_null() {
+            return Err("system.customBackendUrl must be a string or null".to_string());
         }
     }
+
+    Ok(())
 }

-/// Get batch of settings by paths
-pub async fn get_settings_batch(
-    state: web::Data<AppState>,
-    payload: web::Json<Vec<String>>,
-) -> Result<HttpResponse, Error> {
-    let paths = payload.into_inner();
-    info!("[Settings Handler] POST /settings/batch - Getting {} settings from database", paths.len());
-
-    match state.settings_service.get_settings_batch(&paths).await {
-        Ok(results) => {
-            info!("[Settings Handler] Batch get successful: {}/{} settings found", results.len(), paths.len());
-            // Convert SettingValue map to JSON map
-            let json_results: std::collections::HashMap<String, Value> = results.into_iter().map(|(k, v)| {
-                let json_value = match v {
-                    crate::services::database_service::SettingValue::String(s) => json!(s),
-                    crate::services::database_service::SettingValue::Integer(i) => json!(i),
-                    crate::services::database_service::SettingValue::Float(f) => json!(f),
-                    crate::services::database_service::SettingValue::Boolean(b) => json!(b),
-                    crate::services::database_service::SettingValue::Json(j) => j,
-                };
-                (k, json_value)
-            }).collect();
-            Ok(HttpResponse::Ok().json(json_results))
+fn validate_xr_settings(xr: &Value) -> Result<(), String> {
+    // UNIFIED FORMAT: Only accept "enabled", not "enableXrMode"
+    if let Some(enabled) = xr.get("enabled") {
+        if !enabled.is_boolean() {
+            return Err("XR enabled must be a boolean".to_string());
         }
-        Err(e) => {
-            error!("[Settings Handler] Database error in batch get: {}", e);
-            Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to get settings batch: {}", e)
-            })))
+    }
+
+    // Handle quality setting
+    if let Some(quality) = xr.get("quality") {
+        if let Some(q) = quality.as_str() {
+            if !["Low", "Medium", "High", "low", "medium", "high"].contains(&q) {
+                return Err("XR quality must be Low, Medium, or High".to_string());
+            }
+        } else {
+            return Err("XR quality must be a string".to_string());
+        }
+    }
+
+    // UNIFIED FORMAT: Only accept "renderScale"
+    if let Some(render_scale) = xr.get("renderScale") {
+        let val = render_scale.as_f64().ok_or("renderScale must be a number")?;
+        if val < 0.1 || val > 10.0 {
+            return Err("renderScale must be between 0.1 and 10.0".to_string());
+        }
+    }
+
+    // UNIFIED FORMAT: Only accept "roomScale"
+    if let Some(room_scale) = xr.get("roomScale") {
+        let val = room_scale.as_f64().ok_or("roomScale must be a number")?;
+        if val <= 0.0 || val > 100.0 {
+            return Err("roomScale must be between 0.0 and 100.0".to_string());
         }
     }
+
+    // Handle nested handTracking object
+    if let Some(hand_tracking) = xr.get("handTracking") {
+        if let Some(ht_obj) = hand_tracking.as_object() {
+            if let Some(enabled) = ht_obj.get("enabled") {
+                if !enabled.is_boolean() {
+                    return Err("handTracking.enabled must be a boolean".to_string());
+                }
+            }
+        }
+    }
+
+    // Handle nested interactions object
+    if let Some(interactions) = xr.get("interactions") {
+        if let Some(int_obj) = interactions.as_object() {
+            if let Some(haptics) = int_obj.get("enableHaptics") {
+                if !haptics.is_boolean() {
+                    return Err("interactions.enableHaptics must be a boolean".to_string());
+                }
+            }
+        }
+    }
+
+    Ok(())
 }

-/// Reset settings to defaults
-pub async fn reset_settings(
-    state: web::Data<AppState>,
-) -> Result<HttpResponse, Error> {
-    info!("[Settings Handler] POST /settings/reset - Resetting to defaults in database");
-
-    let default_settings = AppFullSettings::default();
-
-    match state.settings_service.save_all_settings(&default_settings) {
-        Ok(()) => {
-            info!("[Settings Handler] Settings reset to defaults in database");
-            Ok(HttpResponse::Ok().json(json!({
-                "success": true,
-                "message": "Settings reset to defaults"
-            })))
+/// Propagate physics settings to GPU compute actor
+async fn propagate_physics_to_gpu(
+    state: &web::Data<AppState>,
+    settings: &AppFullSettings,
+    graph: &str,
+) {
+    let physics = settings.get_physics(graph);
+
+    // Always log critical physics values with new parameter names
+    info!(
+        "[PHYSICS UPDATE] Propagating {} physics to actors:", graph
+    );
+    info!(
+        "  - repulsion_k: {:.3} (affects node spreading)",
+        physics.repel_k
+    );
+    info!(
+        "  - spring_k: {:.3} (affects edge tension)",
+        physics.spring_k
+    );
+    info!(
+        "  - spring_k: {:.3} (affects clustering)",
+        physics.spring_k
+    );
+    info!(
+        "  - damping: {:.3} (affects settling, 1.0 = no movement)",
+        physics.damping
+    );
+    info!(
+        "  - time_step: {:.3} (simulation speed)",
+        physics.dt
+    );
+    info!(
+        "  - max_velocity: {:.3} (prevents explosions)",
+        physics.max_velocity
+    );
+    info!(
+        "  - temperature: {:.3} (random motion)",
+        physics.temperature
+    );
+    info!(
+        "  - gravity: {:.3} (directional force)",
+        physics.gravity
+    );
+
+    if crate::utils::logging::is_debug_enabled() {
+        debug!("  - bounds_size: {:.1}", physics.bounds_size);
+        debug!("  - separation_radius: {:.3}", physics.separation_radius);  // Updated name
+        debug!("  - mass_scale: {:.3}", physics.mass_scale);
+        debug!("  - boundary_damping: {:.3}", physics.boundary_damping);
+        debug!("  - update_threshold: {:.3}", physics.update_threshold);
+        debug!("  - iterations: {}", physics.iterations);
+        debug!("  - enabled: {}", physics.enabled);
+
+        // Log new GPU-aligned parameters
+        debug!("  - min_distance: {:.3}", physics.min_distance);
+        debug!("  - max_repulsion_dist: {:.1}", physics.max_repulsion_dist);
+        debug!("  - boundary_margin: {:.3}", physics.boundary_margin);
+        debug!("  - boundary_force_strength: {:.1}", physics.boundary_force_strength);
+        debug!("  - warmup_iterations: {}", physics.warmup_iterations);
+        debug!("  - warmup_curve: {}", physics.warmup_curve);
+        debug!("  - zero_velocity_iterations: {}", physics.zero_velocity_iterations);
+        debug!("  - cooling_rate: {:.6}", physics.cooling_rate);
+        debug!("  - clustering_algorithm: {}", physics.clustering_algorithm);
+        debug!("  - cluster_count: {}", physics.cluster_count);
+        debug!("  - clustering_resolution: {:.3}", physics.clustering_resolution);
+        debug!("  - clustering_iterations: {}", physics.clustering_iterations);
+        debug!("[GPU Parameters] All new parameters available for GPU processing");
+    }
+
+    let sim_params: crate::models::simulation_params::SimulationParams = physics.into();
+
+    info!(
+        "[PHYSICS UPDATE] Converted to SimulationParams - repulsion: {}, damping: {:.3}, time_step: {:.3}",
+        sim_params.repel_k, sim_params.damping, sim_params.dt
+    );
+
+    let update_msg = UpdateSimulationParams { params: sim_params.clone() };
+
+    // Send to GPU compute actor
+    if let Some(gpu_addr) = &state.gpu_compute_addr {
+        info!("[PHYSICS UPDATE] Sending to GPUComputeActor...");
+        if let Err(e) = gpu_addr.send(update_msg.clone()).await {
+            error!("[PHYSICS UPDATE] FAILED to update GPUComputeActor: {}", e);
+        } else {
+            info!("[PHYSICS UPDATE] GPUComputeActor updated successfully");
         }
-        Err(e) => {
-            error!("[Settings Handler] Database error resetting settings: {}", e);
-            Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to reset settings: {}", e)
-            })))
+    } else {
+        warn!("[PHYSICS UPDATE] No GPUComputeActor available");
+    }
+
+    // Send to graph service actor
+    info!("[PHYSICS UPDATE] Sending to GraphStateActor...");
+    if let Err(e) = state.graph_state_addr.send(update_msg).await {
+        error!("[PHYSICS UPDATE] FAILED to update GraphStateActor: {}", e);
+    } else {
+        info!("[PHYSICS UPDATE] GraphStateActor updated successfully");
+    }
+}
+
+/// Helper function to get field variants (camelCase or snake_case)
+fn get_field_variant<'a>(obj: &'a Value, variants: &[&str]) -> Option<&'a Value> {
+    for variant in variants {
+        if let Some(val) = obj.get(*variant) {
+            return Some(val);
         }
     }
+    None
 }

-/// Health check for settings service
-pub async fn settings_health(
-    state: web::Data<AppState>,
-) -> Result<HttpResponse, Error> {
-    // Try to load settings from database
-    let healthy = state.settings_service.load_all_settings().is_ok();
+/// Count the number of fields in a JSON object recursively
+fn count_fields(value: &Value) -> usize {
+    match value {
+        Value::Object(map) => {
+            map.len() + map.values().map(count_fields).sum::<usize>()
+        }
+        Value::Array(arr) => arr.iter().map(count_fields).sum(),
+        _ => 0,
+    }
+}

-    // Get cache stats
-    let cache_stats = state.settings_service.get_cache_stats().await;
+/// Extract which graphs have physics updates
+fn extract_physics_updates(update: &Value) -> Vec<&str> {
+    update.get("visualisation")
+        .and_then(|v| v.get("graphs"))
+        .and_then(|g| g.as_object())
+        .map(|graphs| {
+            let mut updated = Vec::new();
+            if graphs.contains_key("logseq") &&
+               graphs.get("logseq").and_then(|g| g.get("physics")).is_some() {
+                updated.push("logseq");
+            }
+            if graphs.contains_key("visionflow") &&
+               graphs.get("visionflow").and_then(|g| g.get("physics")).is_some() {
+                updated.push("visionflow");
+            }
+            updated
+        })
+        .unwrap_or_default()
+}

-    if healthy {
-        Ok(HttpResponse::Ok().json(json!({
-            "status": "healthy",
-            "database": "connected",
-            "cache_entries": cache_stats.entries,
-            "cache_age_seconds": cache_stats.last_updated
-        })))
+/// Extract the field name that failed validation
+fn extract_failed_field(physics: &Value) -> String {
+    if let Some(obj) = physics.as_object() {
+        obj.keys().next().unwrap_or(&"unknown".to_string()).clone()
     } else {
-        Ok(HttpResponse::ServiceUnavailable().json(json!({
-            "status": "unhealthy",
-            "database": "error"
-        })))
+        "unknown".to_string()
+    }
+}
+
+/// Create a proper settings update structure for physics parameters
+/// Maps old parameter names to new ones for backward compatibility
+fn create_physics_settings_update(physics_update: Value) -> Value {
+    let mut normalized_physics = physics_update.clone();
+
+    // Map old parameter names to new ones if old names are present
+    if let Some(obj) = normalized_physics.as_object_mut() {
+        // Map springStrength -> springK
+        if let Some(spring_strength) = obj.remove("springStrength") {
+            if !obj.contains_key("springK") {
+                obj.insert("springK".to_string(), spring_strength);
+            }
+        }
+
+        // Map repulsionStrength -> repelK (GPU-aligned name)
+        if let Some(repulsion_strength) = obj.remove("repulsionStrength") {
+            if !obj.contains_key("repelK") {
+                obj.insert("repelK".to_string(), repulsion_strength);
+            }
+        }
+
+        // Map attractionStrength -> attractionK
+        if let Some(attraction_strength) = obj.remove("attractionStrength") {
+            if !obj.contains_key("attractionK") {
+                obj.insert("attractionK".to_string(), attraction_strength);
+            }
+        }
+
+        // Map collisionRadius -> separationRadius
+        if let Some(collision_radius) = obj.remove("collisionRadius") {
+            if !obj.contains_key("separationRadius") {
+                obj.insert("separationRadius".to_string(), collision_radius);
+            }
+        }
     }
+
+    json!({
+        "visualisation": {
+            "graphs": {
+                "logseq": {
+                    "physics": normalized_physics
+                },
+                "visionflow": {
+                    "physics": normalized_physics.clone()
+                }
+            }
+        }
+    })
 }

-/// Get physics settings for a specific graph
-pub async fn get_physics_settings(
+/// Update compute mode endpoint
+async fn update_compute_mode(
+    _req: HttpRequest,
     state: web::Data<AppState>,
-    graph_name: web::Path<String>,
+    payload: web::Json<Value>,
 ) -> Result<HttpResponse, Error> {
-    let graph = graph_name.into_inner();
-    info!("[Settings Handler] GET /settings/physics/{} - Loading from database", graph);
-
-    // CRITICAL: Ensure graph separation (logseq vs visionflow)
-    if graph != "logseq" && graph != "visionflow" && graph != "default" {
+    let update = payload.into_inner();
+
+    info!("Compute mode update request received");
+    debug!("Compute mode payload: {}", serde_json::to_string_pretty(&update).unwrap_or_default());
+
+    // Validate compute mode
+    let compute_mode = update.get("computeMode")
+        .and_then(|v| v.as_u64())
+        .ok_or_else(|| {
+            actix_web::error::ErrorBadRequest("computeMode must be an integer between 0 and 3")
+        })?;
+
+    if compute_mode > 3 {
         return Ok(HttpResponse::BadRequest().json(json!({
-            "error": "invalid_graph",
-            "message": format!("Invalid graph name: {}. Must be 'logseq', 'visionflow', or 'default'", graph)
+            "error": "computeMode must be between 0 and 3"
         })));
     }
-
-    match state.settings_service.get_physics_settings(&graph) {
-        Ok(settings) => {
-            info!("[Settings Handler] Physics settings for '{}' loaded from database", graph);
-            Ok(HttpResponse::Ok().json(settings))
+
+    // Create physics update with compute mode
+    let physics_update = json!({
+        "computeMode": compute_mode
+    });
+
+    let settings_update = create_physics_settings_update(physics_update);
+
+    // Get and update settings
+    let mut app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(s)) => s,
+        Ok(Err(e)) => {
+            error!("Failed to get current settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get current settings"
+            })));
         }
         Err(e) => {
-            error!("[Settings Handler] Database error getting physics for '{}': {}", graph, e);
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    if let Err(e) = app_settings.merge_update(settings_update) {
+        error!("Failed to merge compute mode settings: {}", e);
+        return Ok(HttpResponse::InternalServerError().json(json!({
+            "error": format!("Failed to update compute mode: {}", e)
+        })));
+    }
+
+    // Save updated settings
+    match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+        Ok(Ok(())) => {
+            info!("Compute mode updated successfully to: {}", compute_mode);
+
+            // Propagate to GPU
+            propagate_physics_to_gpu(&state, &app_settings, "logseq").await;
+            propagate_physics_to_gpu(&state, &app_settings, "visionflow").await;
+
+            Ok(HttpResponse::Ok().json(json!({
+                "status": "Compute mode updated successfully",
+                "computeMode": compute_mode
+            })))
+        }
+        Ok(Err(e)) => {
+            error!("Failed to save compute mode settings: {}", e);
             Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to get physics settings: {}", e)
+                "error": format!("Failed to save compute mode settings: {}", e)
+            })))
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
             })))
         }
     }
 }

-/// Update physics settings for a specific graph
-pub async fn update_physics_settings(
+/// Update clustering algorithm endpoint
+async fn update_clustering_algorithm(
+    _req: HttpRequest,
     state: web::Data<AppState>,
-    graph_name: web::Path<String>,
-    payload: web::Json<crate::config::PhysicsSettings>,
+    payload: web::Json<Value>,
 ) -> Result<HttpResponse, Error> {
-    let graph = graph_name.into_inner();
-    let physics_settings = payload.into_inner();
-
-    info!("[Settings Handler] PUT /settings/physics/{} - Updating database", graph);
-
-    // CRITICAL: Validate graph name to prevent conflation
-    if graph != "logseq" && graph != "visionflow" && graph != "default" {
+    let update = payload.into_inner();
+
+    info!("Clustering algorithm update request received");
+    debug!("Clustering payload: {}", serde_json::to_string_pretty(&update).unwrap_or_default());
+
+    // Validate clustering algorithm
+    let algorithm = update.get("algorithm")
+        .and_then(|v| v.as_str())
+        .ok_or_else(|| {
+            actix_web::error::ErrorBadRequest("algorithm must be a string")
+        })?;
+
+    if !["none", "kmeans", "spectral", "louvain"].contains(&algorithm) {
         return Ok(HttpResponse::BadRequest().json(json!({
-            "error": "invalid_graph",
-            "message": format!("Invalid graph name: {}. Must be 'logseq', 'visionflow', or 'default'", graph)
+            "error": "algorithm must be 'none', 'kmeans', 'spectral', or 'louvain'"
         })));
     }
-
-    match state.settings_service.save_physics_settings(&graph, &physics_settings) {
-        Ok(()) => {
-            info!("[Settings Handler] Physics settings for '{}' saved to database", graph);
+
+    // Extract optional parameters
+    let cluster_count = update.get("clusterCount").and_then(|v| v.as_u64()).unwrap_or(5);
+    let resolution = update.get("resolution").and_then(|v| v.as_f64()).unwrap_or(1.0) as f32;
+    let iterations = update.get("iterations").and_then(|v| v.as_u64()).unwrap_or(30);
+
+    // Create physics update with clustering parameters
+    let physics_update = json!({
+        "clusteringAlgorithm": algorithm,
+        "clusterCount": cluster_count,
+        "clusteringResolution": resolution,
+        "clusteringIterations": iterations
+    });
+
+    let settings_update = create_physics_settings_update(physics_update);
+
+    // Get and update settings
+    let mut app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(s)) => s,
+        Ok(Err(e)) => {
+            error!("Failed to get current settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get current settings"
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    if let Err(e) = app_settings.merge_update(settings_update) {
+        error!("Failed to merge clustering settings: {}", e);
+        return Ok(HttpResponse::InternalServerError().json(json!({
+            "error": format!("Failed to update clustering algorithm: {}", e)
+        })));
+    }
+
+    // Save updated settings
+    match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+        Ok(Ok(())) => {
+            info!("Clustering algorithm updated successfully to: {}", algorithm);
+
+            // Propagate to GPU
+            propagate_physics_to_gpu(&state, &app_settings, "logseq").await;
+            propagate_physics_to_gpu(&state, &app_settings, "visionflow").await;
+
             Ok(HttpResponse::Ok().json(json!({
-                "success": true,
-                "graph": graph,
-                "message": "Physics settings saved"
+                "status": "Clustering algorithm updated successfully",
+                "algorithm": algorithm,
+                "clusterCount": cluster_count,
+                "resolution": resolution,
+                "iterations": iterations
             })))
         }
-        Err(e) => {
-            error!("[Settings Handler] Database error saving physics for '{}': {}", graph, e);
+        Ok(Err(e)) => {
+            error!("Failed to save clustering settings: {}", e);
             Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to save physics settings: {}", e)
+                "error": format!("Failed to save clustering settings: {}", e)
+            })))
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
             })))
         }
     }
 }

-/// Export settings as JSON
-pub async fn export_settings(
+/// Update constraints endpoint
+async fn update_constraints(
+    _req: HttpRequest,
     state: web::Data<AppState>,
+    payload: web::Json<Value>,
 ) -> Result<HttpResponse, Error> {
-    info!("[Settings Handler] GET /settings/export - Exporting from database");
-
-    match state.settings_service.load_all_settings() {
-        Ok(Some(settings)) => {
-            match serde_json::to_string_pretty(&settings) {
-                Ok(json_string) => {
-                    Ok(HttpResponse::Ok()
-                        .content_type("application/json")
-                        .insert_header(("Content-Disposition", "attachment; filename=\"settings.json\""))
-                        .body(json_string))
-                }
-                Err(e) => {
-                    error!("[Settings Handler] Failed to serialize settings: {}", e);
-                    Ok(HttpResponse::InternalServerError().json(json!({
-                        "error": "serialization_error"
-                    })))
+    let update = payload.into_inner();
+
+    info!("Constraints update request received");
+    debug!("Constraints payload: {}", serde_json::to_string_pretty(&update).unwrap_or_default());
+
+    // Validate constraint data structure
+    if let Err(e) = validate_constraints(&update) {
+        return Ok(HttpResponse::BadRequest().json(json!({
+            "error": format!("Invalid constraints: {}", e)
+        })));
+    }
+
+    // For now, store constraints in physics settings
+    // In a real implementation, you'd have a dedicated constraints store
+    let settings_update = json!({
+        "visualisation": {
+            "graphs": {
+                "logseq": {
+                    "physics": {
+                        "computeMode": 2  // Enable constraints mode
+                    }
+                },
+                "visionflow": {
+                    "physics": {
+                        "computeMode": 2
+                    }
                 }
             }
         }
-        Ok(None) => {
-            Ok(HttpResponse::NotFound().json(json!({
-                "error": "no_settings"
-            })))
+    });
+
+    // Get and update settings
+    let mut app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(s)) => s,
+        Ok(Err(e)) => {
+            error!("Failed to get current settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get current settings"
+            })));
         }
         Err(e) => {
-            error!("[Settings Handler] Database error exporting settings: {}", e);
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    if let Err(e) = app_settings.merge_update(settings_update) {
+        error!("Failed to merge constraints settings: {}", e);
+        return Ok(HttpResponse::InternalServerError().json(json!({
+            "error": format!("Failed to update constraints: {}", e)
+        })));
+    }
+
+    // Save updated settings
+    match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+        Ok(Ok(())) => {
+            info!("Constraints updated successfully");
+
+            // Propagate to GPU
+            propagate_physics_to_gpu(&state, &app_settings, "logseq").await;
+            propagate_physics_to_gpu(&state, &app_settings, "visionflow").await;
+
+            Ok(HttpResponse::Ok().json(json!({
+                "status": "Constraints updated successfully"
+            })))
+        }
+        Ok(Err(e)) => {
+            error!("Failed to save constraints settings: {}", e);
             Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to export settings: {}", e)
+                "error": format!("Failed to save constraints settings: {}", e)
+            })))
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
             })))
         }
     }
 }

-/// Import settings from JSON
-pub async fn import_settings(
+/// Get cluster analytics endpoint
+async fn get_cluster_analytics(
+    _req: HttpRequest,
     state: web::Data<AppState>,
-    payload: web::Json<AppFullSettings>,
 ) -> Result<HttpResponse, Error> {
-    info!("[Settings Handler] POST /settings/import - Importing to database");
+    info!("Cluster analytics request received");
+
+    // Check if GPU clustering is available
+    if let Some(gpu_addr) = &state.gpu_compute_addr {
+        // Get real cluster data from GPU
+        use crate::actors::messages::{GetClusteringResults, GetGraphData};
+
+        // First get graph data
+        let graph_data = match state.graph_state_addr.send(GetGraphData).await {
+            Ok(Ok(data)) => data,
+            Ok(Err(e)) => {
+                error!("Failed to get graph data for clustering analytics: {}", e);
+                return Ok(HttpResponse::InternalServerError().json(json!({
+                    "error": "Failed to get graph data for analytics"
+                })));
+            }
+            Err(e) => {
+                error!("Graph service communication error: {}", e);
+                return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                    "error": "Graph service unavailable"
+                })));
+            }
+        };

-    let settings = payload.into_inner();
+        // Use CPU fallback analytics since GPU clustering requires different actor
+        info!("GPU compute actor available but clustering not handled by force compute actor");
+        get_cpu_fallback_analytics(&graph_data).await
+    } else {
+        // Get graph data for CPU fallback
+        use crate::actors::messages::GetGraphData;
+        match state.graph_state_addr.send(GetGraphData).await {
+            Ok(Ok(graph_data)) => get_cpu_fallback_analytics(&graph_data).await,
+            Ok(Err(e)) => {
+                error!("Failed to get graph data: {}", e);
+                Ok(HttpResponse::InternalServerError().json(json!({
+                    "error": "Failed to get graph data for analytics"
+                })))
+            }
+            Err(e) => {
+                error!("Graph service unavailable: {}", e);
+                Ok(HttpResponse::ServiceUnavailable().json(json!({
+                    "error": "Graph service unavailable"
+                })))
+            }
+        }
+    }
+}

-    match state.settings_service.save_all_settings(&settings) {
-        Ok(()) => {
-            info!("[Settings Handler] Settings imported successfully to database");
+/// CPU fallback analytics when GPU clustering is unavailable
+async fn get_cpu_fallback_analytics(graph_data: &crate::models::graph::GraphData) -> Result<HttpResponse, Error> {
+    use std::collections::HashMap;
+
+    // Basic CPU-based clustering analysis
+    let node_count = graph_data.nodes.len();
+    let edge_count = graph_data.edges.len();
+
+    // Group nodes by type for basic clustering
+    let mut type_clusters: HashMap<String, Vec<&crate::models::node::Node>> = HashMap::new();
+
+    for node in &graph_data.nodes {
+        let node_type = node.node_type.as_ref().unwrap_or(&"unknown".to_string()).clone();
+        type_clusters.entry(node_type).or_insert_with(Vec::new).push(node);
+    }
+
+    // Generate basic cluster statistics
+    let clusters: Vec<_> = type_clusters.into_iter().enumerate().map(|(i, (type_name, nodes))| {
+        // Calculate centroid
+        let centroid = if !nodes.is_empty() {
+            let sum_x: f32 = nodes.iter().map(|n| n.data.x).sum();
+            let sum_y: f32 = nodes.iter().map(|n| n.data.y).sum();
+            let sum_z: f32 = nodes.iter().map(|n| n.data.z).sum();
+            let count = nodes.len() as f32;
+            [sum_x / count, sum_y / count, sum_z / count]
+        } else {
+            [0.0, 0.0, 0.0]
+        };
+
+        json!({
+            "id": format!("cpu_cluster_{}", i),
+            "nodeCount": nodes.len(),
+            "coherence": 0.6, // Basic heuristic for CPU clustering
+            "centroid": centroid,
+            "keywords": [type_name.clone(), "cpu_cluster"],
+            "type": type_name
+        })
+    }).collect();
+
+    let fallback_analytics = json!({
+        "clusters": clusters,
+        "totalNodes": node_count,
+        "algorithmUsed": "cpu_heuristic",
+        "modularity": 0.4, // Estimated modularity for type-based clustering
+        "lastUpdated": chrono::Utc::now().to_rfc3339(),
+        "gpu_accelerated": false,
+        "note": "CPU fallback clustering based on node types",
+        "computation_time_ms": 0
+    });
+
+    Ok(HttpResponse::Ok().json(fallback_analytics))
+}
+
+/// Update stress optimization endpoint
+async fn update_stress_optimization(
+    _req: HttpRequest,
+    state: web::Data<AppState>,
+    payload: web::Json<Value>,
+) -> Result<HttpResponse, Error> {
+    let update = payload.into_inner();
+
+    info!("Stress optimization update request received");
+    debug!("Stress optimization payload: {}", serde_json::to_string_pretty(&update).unwrap_or_default());
+
+    // Validate stress parameters
+    let stress_weight = update.get("stressWeight")
+        .and_then(|v| v.as_f64())
+        .unwrap_or(0.1) as f32;
+
+    let stress_alpha = update.get("stressAlpha")
+        .and_then(|v| v.as_f64())
+        .unwrap_or(0.1) as f32;
+
+    if !(0.0..=1.0).contains(&stress_weight) || !(0.0..=1.0).contains(&stress_alpha) {
+        return Ok(HttpResponse::BadRequest().json(json!({
+            "error": "stressWeight and stressAlpha must be between 0.0 and 1.0"
+        })));
+    }
+
+    // Create physics update with stress optimization parameters
+    let physics_update = json!({
+        "stressWeight": stress_weight,
+        "stressAlpha": stress_alpha
+    });
+
+    let settings_update = create_physics_settings_update(physics_update);
+
+    // Get and update settings
+    let mut app_settings = match state.settings_addr.send(GetSettings).await {
+        Ok(Ok(s)) => s,
+        Ok(Err(e)) => {
+            error!("Failed to get current settings: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "error": "Failed to get current settings"
+            })));
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            return Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
+            })));
+        }
+    };
+
+    if let Err(e) = app_settings.merge_update(settings_update) {
+        error!("Failed to merge stress optimization settings: {}", e);
+        return Ok(HttpResponse::InternalServerError().json(json!({
+            "error": format!("Failed to update stress optimization: {}", e)
+        })));
+    }
+
+    // Save updated settings
+    match state.settings_addr.send(UpdateSettings { settings: app_settings.clone() }).await {
+        Ok(Ok(())) => {
+            info!("Stress optimization updated successfully");
+
+            // Propagate to GPU
+            propagate_physics_to_gpu(&state, &app_settings, "logseq").await;
+            propagate_physics_to_gpu(&state, &app_settings, "visionflow").await;
+
             Ok(HttpResponse::Ok().json(json!({
-                "success": true,
-                "message": "Settings imported"
+                "status": "Stress optimization updated successfully",
+                "stressWeight": stress_weight,
+                "stressAlpha": stress_alpha
             })))
         }
-        Err(e) => {
-            error!("[Settings Handler] Database error importing settings: {}", e);
+        Ok(Err(e)) => {
+            error!("Failed to save stress optimization settings: {}", e);
             Ok(HttpResponse::InternalServerError().json(json!({
-                "error": "database_error",
-                "message": format!("Failed to import settings: {}", e)
+                "error": format!("Failed to save stress optimization settings: {}", e)
+            })))
+        }
+        Err(e) => {
+            error!("Settings actor error: {}", e);
+            Ok(HttpResponse::ServiceUnavailable().json(json!({
+                "error": "Settings service unavailable"
             })))
         }
     }
 }

-/// Clear settings cache
-pub async fn clear_cache(
-    state: web::Data<AppState>,
-) -> Result<HttpResponse, Error> {
-    info!("[Settings Handler] POST /settings/cache/clear - Clearing cache");
-
-    state.settings_service.clear_cache().await;
-
-    Ok(HttpResponse::Ok().json(json!({
-        "success": true,
-        "message": "Cache cleared"
-    })))
+/// Validate constraint data structure
+fn validate_constraints(constraints: &Value) -> Result<(), String> {
+    // Basic validation for constraint structure
+    if let Some(obj) = constraints.as_object() {
+        for (constraint_type, constraint_data) in obj {
+            if !["separation", "boundary", "alignment", "cluster"].contains(&constraint_type.as_str()) {
+                return Err(format!("Unknown constraint type: {}", constraint_type));
+            }
+
+            if let Some(data) = constraint_data.as_object() {
+                if let Some(strength) = data.get("strength") {
+                    let val = strength.as_f64().ok_or("strength must be a number")?;
+                    if val < 0.0 || val > 100.0 {
+                        return Err("strength must be between 0.0 and 100.0".to_string());
+                    }
+                }
+
+                if let Some(enabled) = data.get("enabled") {
+                    if !enabled.is_boolean() {
+                        return Err("enabled must be a boolean".to_string());
+                    }
+                }
+            }
+        }
+    }
+
+    Ok(())
 }
}
diff --git a/src/handlers/settings_paths.rs b/src/handlers/settings_paths.rs
index 9987ff3f..3a01191d 100644
--- a/src/handlers/settings_paths.rs
+++ b/src/handlers/settings_paths.rs
@@ -617,13 +617,12 @@ fn generate_value_schema(value: &Value, path: &str) -> Value {
 }

 // Configuration for path-based settings routes
+// These are direct routes without a scope wrapper, meant to be included within /settings scope
 pub fn configure_settings_paths(cfg: &mut web::ServiceConfig) {
-    cfg.service(
-        web::scope("/settings")
-            .route("/path", web::get().to(get_settings_by_path))
-            .route("/path", web::put().to(update_settings_by_path))
-            .route("/batch", web::post().to(batch_read_settings_by_path))
-            .route("/batch", web::put().to(batch_update_settings_by_path))
-            .route("/schema", web::get().to(get_settings_schema))
-    );
+    cfg
+        .route("/path", web::get().to(get_settings_by_path))
+        .route("/path", web::put().to(update_settings_by_path))
+        .route("/batch", web::post().to(batch_read_settings_by_path))
+        .route("/batch", web::put().to(batch_update_settings_by_path))
+        .route("/schema", web::get().to(get_settings_schema));
 }
\ No newline at end of file
diff --git a/src/handlers/socket_flow_handler.rs b/src/handlers/socket_flow_handler.rs
index 38bafba9..2149e2d9 100644
--- a/src/handlers/socket_flow_handler.rs
+++ b/src/handlers/socket_flow_handler.rs
@@ -201,17 +201,51 @@ impl SocketFlowServer {
         // Spawn async task to fetch and send state
         actix::spawn(async move {
             // Get graph state
-            if let Ok(Ok(graph_data)) = app_state.graph_service_addr.send(crate::actors::messages::GetGraphData).await {
+            if let Ok(Ok(graph_data)) = app_state.graph_state_addr.send(crate::actors::messages::GetGraphData).await {
                 // Get settings with version
                 if let Ok(Ok(settings)) = app_state.settings_addr.send(crate::actors::messages::GetSettings).await {
-                    // Prepare state sync message
+                    // Prepare nodes with labels for JSON
+                    let nodes_json: Vec<serde_json::Value> = graph_data.nodes.iter().map(|node| {
+                        serde_json::json!({
+                            "id": node.id,
+                            "metadata_id": node.metadata_id,
+                            "label": node.label,
+                            "position": {
+                                "x": node.data.x,
+                                "y": node.data.y,
+                                "z": node.data.z,
+                            },
+                            "velocity": {
+                                "x": node.data.vx,
+                                "y": node.data.vy,
+                                "z": node.data.vz,
+                            },
+                            "type": node.node_type,
+                            "size": node.size,
+                            "color": node.color,
+                            "weight": node.weight,
+                            "group": node.group,
+                        })
+                    }).collect();
+
+                    // Prepare edges for JSON
+                    let edges_json: Vec<serde_json::Value> = graph_data.edges.iter().map(|edge| {
+                        serde_json::json!({
+                            "id": edge.id,
+                            "source": edge.source,
+                            "target": edge.target,
+                            "weight": edge.weight,
+                        })
+                    }).collect();
+
+                    // Prepare state sync message with FULL graph data
                     let state_sync = serde_json::json!({
                         "type": "state_sync",
                         "data": {
                             "graph": {
-                                "nodes_count": graph_data.nodes.len(),
-                                "edges_count": graph_data.edges.len(),
-                                "metadata_count": graph_data.metadata.len(),
+                                "nodes": nodes_json,
+                                "edges": edges_json,
+                                "metadata": graph_data.metadata,
                             },
                             "settings": {
                                 "version": settings.version,
@@ -222,21 +256,31 @@ impl SocketFlowServer {
                                 .as_secs(),
                         }
                     });
-
+
                     // Send state sync as text message through the actor
                     if let Ok(msg_str) = serde_json::to_string(&state_sync) {
                         addr.do_send(SendToClientText(msg_str));
-                        info!("Sent state sync: {} nodes, {} edges, version: {}",
-                            graph_data.nodes.len(),
+                        info!("Sent full state sync: {} nodes, {} edges (with labels), version: {}",
+                            graph_data.nodes.len(),
                             graph_data.edges.len(),
                             settings.version
                         );
                     }
-
-                    // Send initial positions as binary data
+
+                    // Send initial positions as binary data with type classification
                     if !graph_data.nodes.is_empty() {
-                        let node_data: Vec<(u32, BinaryNodeData)> = graph_data.nodes.iter()
-                            .map(|node| (node.id, BinaryNodeData {
+                        let mut node_data: Vec<(u32, BinaryNodeData)> = Vec::new();
+                        let mut agent_ids: Vec<u32> = Vec::new();
+                        let mut knowledge_ids: Vec<u32> = Vec::new();
+                        let mut ontology_class_ids: Vec<u32> = Vec::new();
+                        let mut ontology_individual_ids: Vec<u32> = Vec::new();
+                        let mut ontology_property_ids: Vec<u32> = Vec::new();
+
+                        // Debug: Sample node types
+                        let mut type_samples: std::collections::HashMap<String, u32> = std::collections::HashMap::new();
+
+                        for node in &graph_data.nodes {
+                            node_data.push((node.id, BinaryNodeData {
                                 node_id: node.id,
                                 x: node.data.x,
                                 y: node.data.y,
@@ -244,12 +288,43 @@ impl SocketFlowServer {
                                 vx: node.data.vx,
                                 vy: node.data.vy,
                                 vz: node.data.vz,
-                            }))
-                            .collect();
-
-                        // Send position update through the actor
-                        addr.do_send(BroadcastPositionUpdate(node_data));
-                        debug!("Sent initial node positions for state sync");
+                            }));
+
+                            // Classify node by its type field
+                            let node_type_key = node.node_type.as_ref().map(|s| s.as_str()).unwrap_or("None");
+                            *type_samples.entry(node_type_key.to_string()).or_insert(0) += 1;
+
+                            match node.node_type.as_deref() {
+                                Some("agent") | Some("bot") => agent_ids.push(node.id),
+                                Some("ontology_class") | Some("owl_class") => ontology_class_ids.push(node.id),
+                                Some("ontology_individual") | Some("owl_individual") => ontology_individual_ids.push(node.id),
+                                Some("ontology_property") | Some("owl_property") => ontology_property_ids.push(node.id),
+                                // Default to knowledge graph for backward compatibility
+                                _ => knowledge_ids.push(node.id),
+                            }
+                        }
+
+                        info!("STATE SYNC - Node type distribution: {:?}", type_samples);
+                        info!("STATE SYNC - Total nodes: {}, Knowledge: {}, Agents: {}, Ontology: {}",
+                              graph_data.nodes.len(), knowledge_ids.len(), agent_ids.len(),
+                              ontology_class_ids.len() + ontology_individual_ids.len() + ontology_property_ids.len());
+
+                        // Encode with proper type flags
+                        let binary_data = crate::utils::binary_protocol::encode_node_data_extended(
+                            &node_data,
+                            &agent_ids,
+                            &knowledge_ids,
+                            &ontology_class_ids,
+                            &ontology_individual_ids,
+                            &ontology_property_ids
+                        );
+
+                        // Send binary update to client
+                        addr.do_send(SendToClientBinary(binary_data));
+
+                        info!("Sent initial node positions for state sync: {} knowledge, {} agents, {} ontology",
+                               knowledge_ids.len(), agent_ids.len(),
+                               ontology_class_ids.len() + ontology_individual_ids.len() + ontology_property_ids.len());
                     }
                 }
             }
@@ -475,9 +550,9 @@ async fn fetch_nodes(
     app_state: Arc<AppState>,
     _settings_addr: actix::Addr<crate::actors::optimized_settings_actor::OptimizedSettingsActor>
 ) -> Option<(Vec<(u32, BinaryNodeData)>, bool)> {
-    // Fetch raw nodes asynchronously from GraphServiceActor
+    // Fetch raw nodes asynchronously from GraphStateActor
     use crate::actors::messages::GetGraphData;
-    let graph_data = match app_state.graph_service_addr.send(GetGraphData).await {
+    let graph_data = match app_state.graph_state_addr.send(GetGraphData).await {
         Ok(Ok(data)) => data,
         Ok(Err(e)) => {
             error!("[WebSocket] Failed to get graph data: {}", e);
@@ -576,7 +651,7 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for SocketFlowServer
                                     .map_or(true, |arr| arr.iter().any(|v| v.as_str() == Some("agent")));

                                 // Request snapshot from graph actor
-                                let graph_addr = self.app_state.graph_service_addr.clone();
+                                let graph_addr = self.app_state.graph_state_addr.clone();
                                 let fut = async move {
                                     use crate::actors::messages::RequestPositionSnapshot;
                                     graph_addr.send(RequestPositionSnapshot {
@@ -651,7 +726,7 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for SocketFlowServer
                                 info!("Client requested bots graph - returning optimized position data only");

                                 // Send position-only graph structure + REST API references
-                                let graph_addr = self.app_state.graph_service_addr.clone();
+                                let graph_addr = self.app_state.graph_state_addr.clone();

                                 ctx.spawn(actix::fut::wrap_future::<_, Self>(async move {
                                     // Get minimal graph data from GraphServiceActor
@@ -1084,9 +1159,9 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for SocketFlowServer
                                     debug!("Updated position for node ID {} to [{:.3}, {:.3}, {:.3}]",
                                          node_id, node_data.x, node_data.y, node_data.z);

-                                    // Send update message to GraphServiceActor (now uses u32 directly)
+                                    // Send update message to GraphStateActor (now uses u32 directly)
                                     use crate::actors::messages::UpdateNodePosition;
-                                    if let Err(e) = app_state.graph_service_addr.send(UpdateNodePosition {
+                                    if let Err(e) = app_state.graph_state_addr.send(UpdateNodePosition {
                                         node_id: node_id,
                                         position: node_data.position().into(),
                                         velocity: node_data.velocity().into(),
@@ -1112,9 +1187,9 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for SocketFlowServer
                                 if let Ok(Ok(_iterations_val)) = settings_addr.send(GetSettingByPath { path: "visualisation.graphs.logseq.physics.iterations".to_string() }).await {
                                     if let Ok(Ok(_spring_val)) = settings_addr.send(GetSettingByPath { path: "visualisation.graphs.logseq.physics.spring_k".to_string() }).await {
                                         if let Ok(Ok(_repulsion_val)) = settings_addr.send(GetSettingByPath { path: "visualisation.graphs.logseq.physics.repel_k".to_string() }).await {
-                                            // Send simulation step message to GraphServiceActor
+                                            // Send simulation step message to GraphStateActor
                                             use crate::actors::messages::SimulationStep;
-                                            if let Err(e) = app_state.graph_service_addr.send(SimulationStep).await {
+                                            if let Err(e) = app_state.graph_state_addr.send(SimulationStep).await {
                                                 error!("Failed to trigger simulation step: {}", e);
                                             } else {
                                                 info!("Successfully triggered layout recalculation");
diff --git a/src/handlers/user_settings_handler.rs b/src/handlers/user_settings_handler.rs
new file mode 100644
index 00000000..1d32ea93
--- /dev/null
+++ b/src/handlers/user_settings_handler.rs
@@ -0,0 +1,190 @@
+use actix_web::{web, Error, HttpResponse, HttpRequest};
+use serde::{Deserialize, Serialize};
+use serde_json::Value as JsonValue;
+use log::{error, info};
+
+use crate::services::user_service::{UserService, UserServiceError, SettingValue, UserSetting};
+use crate::middleware::permissions::extract_auth_context;
+
+#[derive(Debug, Serialize)]
+struct UserSettingsResponse {
+    settings: Vec<UserSettingDTO>,
+}
+
+#[derive(Debug, Serialize)]
+struct UserSettingDTO {
+    key: String,
+    value: JsonValue,
+    created_at: i64,
+    updated_at: i64,
+}
+
+#[derive(Debug, Deserialize)]
+struct SetUserSettingRequest {
+    key: String,
+    value: JsonValue,
+}
+
+#[derive(Debug, Serialize)]
+struct MessageResponse {
+    message: String,
+}
+
+#[derive(Debug, Serialize)]
+struct ErrorResponse {
+    error: String,
+}
+
+fn setting_value_to_json(value: &SettingValue) -> JsonValue {
+    match value {
+        SettingValue::String(s) => JsonValue::String(s.clone()),
+        SettingValue::Integer(i) => JsonValue::Number(serde_json::Number::from(*i)),
+        SettingValue::Float(f) => {
+            serde_json::Number::from_f64(*f)
+                .map(JsonValue::Number)
+                .unwrap_or(JsonValue::Null)
+        }
+        SettingValue::Boolean(b) => JsonValue::Bool(*b),
+        SettingValue::Json(j) => j.clone(),
+    }
+}
+
+fn json_to_setting_value(value: JsonValue) -> SettingValue {
+    match value {
+        JsonValue::String(s) => SettingValue::String(s),
+        JsonValue::Number(n) => {
+            if let Some(i) = n.as_i64() {
+                SettingValue::Integer(i)
+            } else if let Some(f) = n.as_f64() {
+                SettingValue::Float(f)
+            } else {
+                SettingValue::String(n.to_string())
+            }
+        }
+        JsonValue::Bool(b) => SettingValue::Boolean(b),
+        _ => SettingValue::Json(value),
+    }
+}
+
+pub async fn get_user_settings(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    match user_service.get_user_settings(auth_context.user_id).await {
+        Ok(settings) => {
+            let dto: Vec<UserSettingDTO> = settings
+                .into_iter()
+                .map(|s| UserSettingDTO {
+                    key: s.key,
+                    value: setting_value_to_json(&s.value),
+                    created_at: s.created_at,
+                    updated_at: s.updated_at,
+                })
+                .collect();
+
+            info!(
+                "Retrieved {} settings for user_id={}",
+                dto.len(),
+                auth_context.user_id
+            );
+            Ok(HttpResponse::Ok().json(UserSettingsResponse { settings: dto }))
+        }
+        Err(e) => {
+            error!("Failed to get user settings: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to fetch user settings".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn set_user_setting(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    payload: web::Json<SetUserSettingRequest>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required to modify settings".to_string(),
+        }));
+    }
+
+    let setting_value = json_to_setting_value(payload.value.clone());
+
+    match user_service
+        .set_user_setting(auth_context.user_id, &payload.key, setting_value)
+        .await
+    {
+        Ok(()) => {
+            info!(
+                "User {} set setting {}",
+                auth_context.nostr_pubkey, payload.key
+            );
+            Ok(HttpResponse::Ok().json(MessageResponse {
+                message: format!("Setting {} updated successfully", payload.key),
+            }))
+        }
+        Err(e) => {
+            error!("Failed to set user setting: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to update setting".to_string(),
+            }))
+        }
+    }
+}
+
+pub async fn delete_user_setting(
+    req: HttpRequest,
+    user_service: web::Data<UserService>,
+    path: web::Path<String>,
+) -> Result<HttpResponse, Error> {
+    let auth_context = extract_auth_context(&req)
+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Authentication required"))?;
+
+    if !auth_context.is_power_user {
+        return Ok(HttpResponse::Forbidden().json(ErrorResponse {
+            error: "Power user access required to delete settings".to_string(),
+        }));
+    }
+
+    let key = path.into_inner();
+
+    match user_service
+        .delete_user_setting(auth_context.user_id, &key)
+        .await
+    {
+        Ok(()) => {
+            info!(
+                "User {} deleted setting {}",
+                auth_context.nostr_pubkey, key
+            );
+            Ok(HttpResponse::Ok().json(MessageResponse {
+                message: format!("Setting {} deleted successfully", key),
+            }))
+        }
+        Err(UserServiceError::UserNotFound) => Ok(HttpResponse::NotFound().json(ErrorResponse {
+            error: "Setting not found".to_string(),
+        })),
+        Err(e) => {
+            error!("Failed to delete user setting: {:?}", e);
+            Ok(HttpResponse::InternalServerError().json(ErrorResponse {
+                error: "Failed to delete setting".to_string(),
+            }))
+        }
+    }
+}
+
+pub fn configure_routes(cfg: &mut web::ServiceConfig) {
+    cfg.service(
+        web::scope("/api/user-settings")
+            .route("", web::get().to(get_user_settings))
+            .route("", web::post().to(set_user_setting))
+            .route("/{key}", web::delete().to(delete_user_setting)),
+    );
+}
diff --git a/src/lib.rs b/src/lib.rs
index 44fb08d3..42004ba5 100755
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -7,6 +7,7 @@ pub mod config;
 pub mod errors;
 pub mod gpu;
 pub mod handlers;
+pub mod middleware;
 pub mod models;
 pub mod physics;
 pub mod ports;
@@ -24,4 +25,3 @@ pub use models::metadata::MetadataStore;
 pub use models::protected_settings::ProtectedSettings;
 pub use models::simulation_params::SimulationParams;
 // pub use models::ui_settings::UISettings; // Removed - consolidated into AppFullSettings"
-pub use models::user_settings::UserSettings;
diff --git a/src/main.rs b/src/main.rs
index a8ed724f..99f4cc6f 100755
--- a/src/main.rs
+++ b/src/main.rs
@@ -15,6 +15,7 @@ use webxr::{
         workspace_handler,
         graph_export_handler,
         client_log_handler,
+        client_logs,
         client_messages_handler,
     },
     services::{
@@ -161,11 +162,10 @@ async fn main() -> std::io::Result<()> {
         info!("Telemetry logger initialized with directory: {}", log_dir);
     }

-    // Load settings
+    // Load settings (returns default structure - actual settings managed via SQLite)
     let settings = match AppFullSettings::new() {
         Ok(s) => {
-            info!("✅ AppFullSettings loaded successfully from: {}",
-                std::env::var("SETTINGS_FILE_PATH").unwrap_or_else(|_| "/app/settings.yaml".to_string()));
+            info!("✅ AppFullSettings structure initialized (settings managed via SQLite database)");

             // Test JSON serialization to verify camelCase output works
             match serde_json::to_string(&s.visualisation.rendering) {
@@ -250,6 +250,70 @@ async fn main() -> std::io::Result<()> {
         error!("[main] ragflow_service_option is None after RAGFlowService::new attempt. Chat functionality will be unavailable.");
     }

+    // Initialize database and DevConfig before creating AppState
+    info!("Initializing database service...");
+    let db_path = std::env::var("DATA_ROOT")
+        .unwrap_or_else(|_| "/app/data".to_string());
+    let db_file = std::path::PathBuf::from(&db_path).join("settings.db");
+
+    let db_service = match webxr::services::database_service::DatabaseService::new(&db_file) {
+        Ok(service) => {
+            info!("✅ Database initialized successfully");
+            std::sync::Arc::new(service)
+        }
+        Err(e) => {
+            error!("❌ Failed to initialize database: {}", e);
+            return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Database initialization failed: {}", e)));
+        }
+    };
+
+    // Initialize schema
+    if let Err(e) = db_service.initialize_schema() {
+        error!("❌ Failed to initialize database schema: {}", e);
+        return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Schema initialization failed: {}", e)));
+    }
+
+    // Seed default settings if database is empty
+    info!("Checking if database needs default settings...");
+    match db_service.get_setting("app_full_settings") {
+        Ok(None) => {
+            info!("Database is empty, seeding default settings...");
+            let default_settings = AppFullSettings::default();
+            let settings_json = match serde_json::to_value(&default_settings) {
+                Ok(json) => json,
+                Err(e) => {
+                    error!("❌ Failed to serialize default settings: {}", e);
+                    return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Failed to serialize default settings: {}", e)));
+                }
+            };
+
+            if let Err(e) = db_service.set_setting(
+                "app_full_settings",
+                webxr::services::database_service::SettingValue::Json(settings_json),
+                Some("Default application settings in camelCase format")
+            ) {
+                error!("❌ Failed to seed default settings: {}", e);
+                return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Failed to seed default settings: {}", e)));
+            }
+            info!("✅ Default settings seeded successfully");
+        }
+        Ok(Some(_)) => {
+            info!("✅ Settings already exist in database");
+        }
+        Err(e) => {
+            error!("❌ Failed to check database settings: {}", e);
+            return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Failed to check database settings: {}", e)));
+        }
+    }
+
+    // Initialize DevConfig from database
+    info!("Initializing DevConfig from database...");
+    if let Err(e) = webxr::config::dev_config::DevConfig::initialize(std::sync::Arc::clone(&db_service)) {
+        warn!("DevConfig initialization failed, using defaults: {}", e);
+    } else {
+        info!("✅ DevConfig initialized successfully");
+    }
+
     // Initialize app state asynchronously
     // AppState::new now receives AppFullSettings directly (not Arc<RwLock<>>)
     let settings_value = {
@@ -259,6 +323,7 @@ async fn main() -> std::io::Result<()> {

     let mut app_state = match AppState::new(
             settings_value,
+            db_service.clone(), // Pass database service for settings actor
             github_client.clone(),
             content_api.clone(),
             None, // Perplexity placeholder
@@ -317,7 +382,7 @@ async fn main() -> std::io::Result<()> {
     let content_api_clone = content_api.clone();
     let settings_clone = settings.clone();
     let metadata_addr_clone = app_state.metadata_addr.clone();
-    let graph_service_addr_clone = app_state.graph_service_addr.clone();
+    let graph_state_addr_clone = app_state.graph_state_addr.clone();

     tokio::spawn(async move {
         // Wait a bit for the server to fully initialize
@@ -375,7 +440,7 @@ async fn main() -> std::io::Result<()> {
                 // Update graph with new data
                 use webxr::actors::messages::AddNodesFromMetadata;
                 info!("Background GitHub sync: Updating graph with new data...");
-                match graph_service_addr_clone.send(AddNodesFromMetadata { metadata: metadata_store_copy }).await {
+                match graph_state_addr_clone.send(AddNodesFromMetadata { metadata: metadata_store_copy }).await {
                     Ok(Ok(())) => {
                         info!("Background GitHub sync: Graph updated successfully with new GitHub data");
                     }
@@ -431,14 +496,14 @@ async fn main() -> std::io::Result<()> {

     if let Some(graph_data) = graph_data_option {
         // If we have pre-computed graph data, send it directly to the GraphServiceSupervisor
-        match app_state.graph_service_addr.send(UpdateGraphData { graph_data: StdArc::new(graph_data) }).await {
+        match app_state.graph_state_addr.send(UpdateGraphData { graph_data: StdArc::new(graph_data) }).await {
             Ok(Ok(())) => {
                 info!("Pre-computed graph data loaded successfully into GraphServiceSupervisor");
             },
             Ok(Err(e)) => {
                 error!("Failed to load pre-computed graph data into actor: {}", e);
                 // Fall back to building from metadata
-                match app_state.graph_service_addr.send(BuildGraphFromMetadata { metadata: metadata_store.clone() }).await {
+                match app_state.graph_state_addr.send(BuildGraphFromMetadata { metadata: metadata_store.clone() }).await {
                     Ok(Ok(())) => {
                         info!("Fallback: Graph built from metadata successfully");
                     },
@@ -459,7 +524,7 @@ async fn main() -> std::io::Result<()> {
         }
     } else {
         // No pre-computed graph data, build from metadata
-        match app_state.graph_service_addr.send(BuildGraphFromMetadata { metadata: metadata_store.clone() }).await {
+        match app_state.graph_state_addr.send(BuildGraphFromMetadata { metadata: metadata_store.clone() }).await {
             Ok(Ok(())) => {
                 info!("Graph built successfully using GraphServiceSupervisor - GPU initialization is handled automatically by the supervisor");
             },
@@ -476,7 +541,32 @@ async fn main() -> std::io::Result<()> {

     info!("Waiting for initial physics layout calculation to complete...");
     tokio::time::sleep(Duration::from_millis(500)).await;
-    info!("Initial delay complete. Starting HTTP server...");
+    info!("Initial delay complete.");
+
+    // Initialize ontology system
+    #[cfg(feature = "ontology")]
+    {
+        info!("🔮 Initializing Ontology System...");
+        use webxr::services::ontology_init::initialize_ontology_system;
+
+        match initialize_ontology_system().await {
+            Ok(_db_service) => {
+                info!("✅ Ontology system initialized successfully");
+                info!("📥 Background GitHub sync task spawned - will begin in 10 seconds");
+            }
+            Err(e) => {
+                error!("❌ Ontology system initialization failed: {}", e);
+                info!("⚠️  Continuing without ontology features");
+            }
+        }
+    }
+
+    #[cfg(not(feature = "ontology"))]
+    {
+        info!("Ontology feature not enabled");
+    }
+
+    info!("Starting HTTP server...");

     // Start simulation in GraphServiceSupervisor (Second start attempt commented out for debugging stack overflow)
     // use webxr::actors::messages::StartSimulation;
@@ -494,8 +584,18 @@ async fn main() -> std::io::Result<()> {
     // Start the server
     let bind_address = {
         let settings_read = settings.read().await; // Reads AppFullSettings
-        // Access network settings correctly
-        format!("{}:{}", settings_read.system.network.bind_address, settings_read.system.network.port)
+        // Access network settings correctly, with fallback to env vars
+        let addr = if settings_read.system.network.bind_address.is_empty() {
+            std::env::var("BIND_ADDRESS").unwrap_or_else(|_| "0.0.0.0".to_string())
+        } else {
+            settings_read.system.network.bind_address.clone()
+        };
+        let port = if settings_read.system.network.port == 0 {
+            std::env::var("PORT").unwrap_or_else(|_| "4000".to_string()).parse().unwrap_or(4000)
+        } else {
+            settings_read.system.network.port
+        };
+        format!("{}:{}", addr, port)
     };

     // Pre-read WebSocket settings for SocketFlowServer
@@ -535,7 +635,7 @@ async fn main() -> std::io::Result<()> {
             .app_data(app_state_data.clone()) // Add the complete AppState
             .app_data(pre_read_ws_settings_data.clone()) // Add pre-read WebSocket settings
             // Register actor addresses for handler access
-            .app_data(web::Data::new(app_state_data.graph_service_addr.clone()))
+            .app_data(web::Data::new(app_state_data.graph_state_addr.clone()))
             .app_data(web::Data::new(app_state_data.settings_addr.clone()))
             .app_data(web::Data::new(app_state_data.metadata_addr.clone()))
             .app_data(web::Data::new(app_state_data.client_manager_addr.clone()))
@@ -556,7 +656,8 @@ async fn main() -> std::io::Result<()> {
                     .service(web::scope("/bots").configure(api_handler::bots::config)) // This will now serve /api/bots/data and /api/bots/update
                     .configure(bots_visualization_handler::configure_routes) // Agent visualization endpoints
                     .configure(graph_export_handler::configure_routes) // Graph export and sharing endpoints
-                    .route("/client-logs", web::post().to(client_log_handler::handle_client_logs)) // Client browser logs endpoint
+                    .route("/client-logs", web::post().to(client_log_handler::handle_client_logs)) // Client browser logs endpoint (full telemetry)
+                    .route("/client-logs-simple", web::post().to(client_logs::post_client_logs)) // Simplified client logs endpoint (server logger only)
                     // DEPRECATED: hybrid health routes removed
             );

diff --git a/src/middleware/mod.rs b/src/middleware/mod.rs
new file mode 100644
index 00000000..a527b636
--- /dev/null
+++ b/src/middleware/mod.rs
@@ -0,0 +1 @@
+pub mod permissions;
diff --git a/src/middleware/permissions.rs b/src/middleware/permissions.rs
new file mode 100644
index 00000000..bb8cdca3
--- /dev/null
+++ b/src/middleware/permissions.rs
@@ -0,0 +1,190 @@
+use actix_web::{
+    dev::{forward_ready, Service, ServiceRequest, ServiceResponse, Transform},
+    Error, HttpMessage, HttpResponse,
+};
+use futures_util::future::LocalBoxFuture;
+use std::future::{ready, Ready};
+use std::rc::Rc;
+use log::{debug, warn};
+
+use crate::services::nostr_service::NostrService;
+use crate::services::user_service::{UserService, UserServiceError};
+
+pub struct AuthContext {
+    pub user_id: i64,
+    pub nostr_pubkey: String,
+    pub is_power_user: bool,
+}
+
+pub enum PermissionLevel {
+    Authenticated,
+    PowerUser,
+}
+
+pub struct PermissionMiddleware {
+    nostr_service: Rc<NostrService>,
+    user_service: Rc<UserService>,
+    required_level: PermissionLevel,
+}
+
+impl PermissionMiddleware {
+    pub fn new(
+        nostr_service: Rc<NostrService>,
+        user_service: Rc<UserService>,
+        required_level: PermissionLevel,
+    ) -> Self {
+        Self {
+            nostr_service,
+            user_service,
+            required_level,
+        }
+    }
+
+    pub fn authenticated(nostr_service: Rc<NostrService>, user_service: Rc<UserService>) -> Self {
+        Self::new(nostr_service, user_service, PermissionLevel::Authenticated)
+    }
+
+    pub fn power_user(nostr_service: Rc<NostrService>, user_service: Rc<UserService>) -> Self {
+        Self::new(nostr_service, user_service, PermissionLevel::PowerUser)
+    }
+}
+
+impl<S, B> Transform<S, ServiceRequest> for PermissionMiddleware
+where
+    S: Service<ServiceRequest, Response = ServiceResponse<B>, Error = Error> + 'static,
+    S::Future: 'static,
+    B: 'static,
+{
+    type Response = ServiceResponse<B>;
+    type Error = Error;
+    type InitError = ();
+    type Transform = PermissionMiddlewareService<S>;
+    type Future = Ready<Result<Self::Transform, Self::InitError>>;
+
+    fn new_transform(&self, service: S) -> Self::Future {
+        ready(Ok(PermissionMiddlewareService {
+            service: Rc::new(service),
+            nostr_service: self.nostr_service.clone(),
+            user_service: self.user_service.clone(),
+            required_level: match self.required_level {
+                PermissionLevel::Authenticated => PermissionLevel::Authenticated,
+                PermissionLevel::PowerUser => PermissionLevel::PowerUser,
+            },
+        }))
+    }
+}
+
+pub struct PermissionMiddlewareService<S> {
+    service: Rc<S>,
+    nostr_service: Rc<NostrService>,
+    user_service: Rc<UserService>,
+    required_level: PermissionLevel,
+}
+
+impl<S, B> Service<ServiceRequest> for PermissionMiddlewareService<S>
+where
+    S: Service<ServiceRequest, Response = ServiceResponse<B>, Error = Error> + 'static,
+    S::Future: 'static,
+    B: 'static,
+{
+    type Response = ServiceResponse<B>;
+    type Error = Error;
+    type Future = LocalBoxFuture<'static, Result<Self::Response, Self::Error>>;
+
+    forward_ready!(service);
+
+    fn call(&self, req: ServiceRequest) -> Self::Future {
+        let service = self.service.clone();
+        let nostr_service = self.nostr_service.clone();
+        let user_service = self.user_service.clone();
+        let required_level = match self.required_level {
+            PermissionLevel::Authenticated => PermissionLevel::Authenticated,
+            PermissionLevel::PowerUser => PermissionLevel::PowerUser,
+        };
+
+        Box::pin(async move {
+            let pubkey = match req.headers().get("X-Nostr-Pubkey") {
+                Some(value) => value.to_str().unwrap_or("").to_string(),
+                None => {
+                    warn!("Missing Nostr pubkey in request headers");
+                    return Err(actix_web::error::ErrorForbidden("Authentication required"));
+                }
+            };
+
+            let token = match req.headers().get("X-Nostr-Token") {
+                Some(value) => value.to_str().unwrap_or("").to_string(),
+                None => {
+                    warn!("Missing Nostr token in request headers");
+                    return Err(actix_web::error::ErrorForbidden("Authentication required"));
+                }
+            };
+
+            if !nostr_service.validate_session(&pubkey, &token).await {
+                warn!("Invalid or expired session for user {}", pubkey);
+                return Err(actix_web::error::ErrorUnauthorized(
+                    "Invalid or expired session",
+                ));
+            }
+
+            let user = match user_service.get_user_by_nostr_pubkey(&pubkey).await {
+                Ok(u) => u,
+                Err(UserServiceError::UserNotFound) => {
+                    match user_service
+                        .create_or_update_user(&pubkey, None)
+                        .await
+                    {
+                        Ok(u) => u,
+                        Err(e) => {
+                            warn!("Failed to create user for pubkey {}: {:?}", pubkey, e);
+                            return Err(actix_web::error::ErrorInternalServerError(
+                                "Failed to create user",
+                            ));
+                        }
+                    }
+                }
+                Err(e) => {
+                    warn!("Database error fetching user {}: {:?}", pubkey, e);
+                    return Err(actix_web::error::ErrorInternalServerError(
+                        "Database error",
+                    ));
+                }
+            };
+
+            match required_level {
+                PermissionLevel::Authenticated => {
+                    debug!("Authenticated access granted for user_id={}", user.id);
+                }
+                PermissionLevel::PowerUser => {
+                    if !user.is_power_user {
+                        warn!(
+                            "Non-power user {} attempted restricted operation",
+                            pubkey
+                        );
+                        return Err(actix_web::error::ErrorForbidden(
+                            "This operation requires power user access",
+                        ));
+                    }
+                    debug!("Power user access granted for user_id={}", user.id);
+                }
+            }
+
+            let auth_context = AuthContext {
+                user_id: user.id,
+                nostr_pubkey: pubkey,
+                is_power_user: user.is_power_user,
+            };
+
+            req.extensions_mut().insert(auth_context);
+
+            service.call(req).await
+        })
+    }
+}
+
+pub fn extract_auth_context(req: &actix_web::HttpRequest) -> Option<AuthContext> {
+    req.extensions().get::<AuthContext>().map(|ctx| AuthContext {
+        user_id: ctx.user_id,
+        nostr_pubkey: ctx.nostr_pubkey.clone(),
+        is_power_user: ctx.is_power_user,
+    })
+}
diff --git a/src/models/mod.rs b/src/models/mod.rs
index 4557460a..7c696cda 100755
--- a/src/models/mod.rs
+++ b/src/models/mod.rs
@@ -6,7 +6,6 @@ pub mod node;
 pub mod pagination;
 pub mod protected_settings;
 pub mod simulation_params;
-pub mod user_settings;
 pub mod ragflow_chat;
 pub mod constraints;
 pub mod workspace;
@@ -16,5 +15,4 @@ pub use metadata::MetadataStore;
 pub use pagination::PaginationParams;
 pub use protected_settings::ProtectedSettings;
 pub use simulation_params::SimulationParams;
-pub use user_settings::UserSettings;
 pub use workspace::{Workspace, CreateWorkspaceRequest, UpdateWorkspaceRequest, WorkspaceResponse, WorkspaceListResponse};
diff --git a/src/models/user_settings.rs b/src/models/user_settings.rs
deleted file mode 100644
index f0a4d70d..00000000
-
diff --git a/src/ports/graph_repository.rs b/src/ports/graph_repository.rs
index 218f8dda..b621c02e 100644
--- a/src/ports/graph_repository.rs
+++ b/src/ports/graph_repository.rs
@@ -1,35 +1,17 @@
-// src/ports/graph_repository.rs
-//! Graph Repository Port
-//!
-//! Defines the interface for graph data access and manipulation.
-//! This port abstracts away the concrete implementation (actor-based, direct access, etc.)
+// Port: GraphRepository
+// Defines the interface for graph state management
+// Future: Add #[derive(HexPort)] when Hexser is available

 use async_trait::async_trait;
-use std::collections::HashSet;
 use std::sync::Arc;
-
+use std::collections::HashSet;
 use crate::models::graph::GraphData;
 use crate::models::node::Node;
 use crate::models::edge::Edge;
+use crate::utils::socket_flow_messages::BinaryNodeData;

-// Placeholder for BinaryNodeData - will use actual type from GPU module
-pub type BinaryNodeData = (f32, f32, f32);
-
-pub type Result<T> = std::result::Result<T, GraphRepositoryError>;
-
-#[derive(Debug, thiserror::Error)]
-pub enum GraphRepositoryError {
-    #[error("Graph not found")]
-    NotFound,
-
-    #[error("Graph access error: {0}")]
-    AccessError(String),
+pub type Result<T> = std::result::Result<T, String>;

-    #[error("Invalid data: {0}")]
-    InvalidData(String),
-}
-
-/// Port for graph data repository operations
 #[async_trait]
 pub trait GraphRepository: Send + Sync {
     /// Get the current graph state
@@ -47,6 +29,18 @@ pub trait GraphRepository: Send + Sync {
     /// Get nodes that have changed since last sync
     async fn get_dirty_nodes(&self) -> Result<HashSet<u32>>;

-    /// Clear dirty node tracking
+    /// Clear the dirty nodes list
     async fn clear_dirty_nodes(&self) -> Result<()>;
+
+    /// Get current graph version for optimistic locking
+    async fn get_version(&self) -> Result<u64>;
+
+    /// Get specific nodes by ID
+    async fn get_nodes(&self, node_ids: Vec<u32>) -> Result<Vec<Node>>;
+
+    /// Remove nodes from the graph
+    async fn remove_nodes(&self, node_ids: Vec<u32>) -> Result<()>;
+
+    /// Clear the entire graph
+    async fn clear(&self) -> Result<()>;
 }
diff --git a/src/ports/mod.rs b/src/ports/mod.rs
index a5dd1b23..369a7801 100644
--- a/src/ports/mod.rs
+++ b/src/ports/mod.rs
@@ -1,9 +1,5 @@
-// src/ports/mod.rs
-//! Hexagonal Architecture Ports
-//!
-//! This module defines the port interfaces (traits) that represent
-//! the core application boundaries. These are technology-agnostic
-//! interfaces that the domain logic depends on.
+// Ports (Interfaces) - Hexagonal Architecture
+// These traits define the boundaries between application and infrastructure

 pub mod graph_repository;
 pub mod physics_simulator;
@@ -11,4 +7,4 @@ pub mod semantic_analyzer;

 pub use graph_repository::GraphRepository;
 pub use physics_simulator::PhysicsSimulator;
-pub use semantic_analyzer::SemanticAnalyzer;
+pub use semantic_analyzer::{SemanticAnalyzer, SSSPResult, ClusteringResult, CommunityResult, ClusterAlgorithm};
diff --git a/src/ports/physics_simulator.rs b/src/ports/physics_simulator.rs
index 5b7f9e8c..f28592ec 100644
--- a/src/ports/physics_simulator.rs
+++ b/src/ports/physics_simulator.rs
@@ -1,53 +1,15 @@
-// src/ports/physics_simulator.rs
-//! Physics Simulator Port
-//!
-//! Defines the interface for physics simulation operations.
-//! Abstracts GPU compute, CPU fallback, or any other physics engine.
+// Port: PhysicsSimulator
+// Defines the interface for physics simulation
+// Future: Add #[derive(HexPort)] when Hexser is available

 use async_trait::async_trait;
-
 use crate::models::graph::GraphData;
+use crate::models::simulation_params::SimulationParams;
+use crate::models::constraints::Constraint;
+use crate::utils::socket_flow_messages::BinaryNodeData;

-// Placeholder for BinaryNodeData - will use actual type from GPU module
-pub type BinaryNodeData = (f32, f32, f32);
-use crate::config::PhysicsSettings;
-
-pub type Result<T> = std::result::Result<T, PhysicsSimulatorError>;
-
-#[derive(Debug, thiserror::Error)]
-pub enum PhysicsSimulatorError {
-    #[error("Simulation error: {0}")]
-    SimulationError(String),
-
-    #[error("Invalid parameters: {0}")]
-    InvalidParameters(String),
+pub type Result<T> = std::result::Result<T, String>;

-    #[error("GPU error: {0}")]
-    GpuError(String),
-}
-
-#[derive(Debug, Clone)]
-pub struct SimulationParams {
-    pub settings: PhysicsSettings,
-    pub graph_name: String,
-}
-
-#[derive(Debug, Clone)]
-pub struct Constraint {
-    pub node_id: u32,
-    pub constraint_type: ConstraintType,
-    pub target_position: Option<(f32, f32, f32)>,
-    pub strength: f32,
-}
-
-#[derive(Debug, Clone)]
-pub enum ConstraintType {
-    Fixed,
-    Spring,
-    Boundary,
-}
-
-/// Port for physics simulation operations
 #[async_trait]
 pub trait PhysicsSimulator: Send + Sync {
     /// Run a single simulation step and return position updates
@@ -67,4 +29,7 @@ pub trait PhysicsSimulator: Send + Sync {

     /// Check if simulation is running
     async fn is_running(&self) -> Result<bool>;
+
+    /// Set SSSP source node for pathfinding visualization
+    async fn set_sssp_source(&self, source: Option<u32>) -> Result<()>;
 }
diff --git a/src/ports/semantic_analyzer.rs b/src/ports/semantic_analyzer.rs
index 26f9f54c..5ab31f93 100644
--- a/src/ports/semantic_analyzer.rs
+++ b/src/ports/semantic_analyzer.rs
@@ -1,57 +1,40 @@
-// src/ports/semantic_analyzer.rs
-//! Semantic Analyzer Port
-//!
-//! Defines the interface for semantic graph analysis operations.
-//! Abstracts GPU-accelerated algorithms, CPU fallbacks, or external services.
+// Port: SemanticAnalyzer
+// Defines the interface for graph algorithms and semantic analysis
+// Future: Add #[derive(HexPort)] when Hexser is available

 use async_trait::async_trait;
 use std::collections::HashMap;
-
 use crate::models::graph::GraphData;

-pub type Result<T> = std::result::Result<T, SemanticAnalyzerError>;
-
-#[derive(Debug, thiserror::Error)]
-pub enum SemanticAnalyzerError {
-    #[error("Analysis error: {0}")]
-    AnalysisError(String),
-
-    #[error("Invalid graph: {0}")]
-    InvalidGraph(String),
-
-    #[error("Algorithm not supported: {0}")]
-    UnsupportedAlgorithm(String),
-}
+pub type Result<T> = std::result::Result<T, String>;

 #[derive(Debug, Clone)]
 pub struct SSSPResult {
-    pub source: u32,
     pub distances: HashMap<u32, f32>,
-    pub predecessors: HashMap<u32, u32>,
+    pub parents: HashMap<u32, i32>,
+    pub source: u32,
 }

 #[derive(Debug, Clone)]
 pub struct ClusteringResult {
-    pub clusters: HashMap<u32, usize>,
-    pub cluster_count: usize,
-    pub modularity: f32,
+    pub clusters: HashMap<u32, u32>,  // node_id -> cluster_id
+    pub cluster_count: u32,
+    pub algorithm: ClusterAlgorithm,
+}
+
+#[derive(Debug, Clone, Copy)]
+pub enum ClusterAlgorithm {
+    KMeans { k: u32 },
+    DBSCAN { eps: f32, min_samples: u32 },
+    Hierarchical { num_clusters: u32 },
 }

 #[derive(Debug, Clone)]
 pub struct CommunityResult {
-    pub communities: HashMap<u32, usize>,
-    pub community_count: usize,
+    pub communities: HashMap<u32, u32>,  // node_id -> community_id
     pub modularity: f32,
 }

-#[derive(Debug, Clone, Copy)]
-pub enum ClusterAlgorithm {
-    Louvain,
-    LabelPropagation,
-    ConnectedComponents,
-}
-
-/// Port for semantic analysis operations
 #[async_trait]
 pub trait SemanticAnalyzer: Send + Sync {
     /// Run Single-Source Shortest Path from a source node
diff --git a/src/services/bots_client.rs b/src/services/bots_client.rs
index 221f525b..04243a69 100644
--- a/src/services/bots_client.rs
+++ b/src/services/bots_client.rs
@@ -4,7 +4,7 @@ use std::sync::Arc;
 use tokio::sync::RwLock;
 use log::{info, error, debug, warn};
 use actix::Addr;
-use crate::actors::graph_service_supervisor::TransitionalGraphSupervisor;
+use crate::actors::graph_state_actor::GraphStateActor;
 use crate::actors::messages::UpdateBotsGraph;
 use crate::utils::mcp_tcp_client::{McpTcpClient, create_mcp_client};
 use crate::services::agent_visualization_protocol::{McpServerType, MultiMcpAgentStatus};
@@ -67,7 +67,7 @@ impl From<MultiMcpAgentStatus> for Agent {
 #[derive(Clone)]
 pub struct BotsClient {
     mcp_client: McpTcpClient,
-    graph_service_addr: Option<Addr<TransitionalGraphSupervisor>>,
+    graph_state_addr: Option<Addr<GraphStateActor>>,
     agents: Arc<RwLock<Vec<Agent>>>,
 }

@@ -83,17 +83,17 @@ impl BotsClient {
             .unwrap_or(9500);

         let mcp_client = create_mcp_client(&McpServerType::ClaudeFlow, &host, port);
-
+
         Self {
             mcp_client,
-            graph_service_addr: None,
+            graph_state_addr: None,
             agents: Arc::new(RwLock::new(Vec::new())),
         }
     }
-
-    pub fn with_graph_service(graph_addr: Addr<TransitionalGraphSupervisor>) -> Self {
+
+    pub fn with_graph_service(graph_addr: Addr<GraphStateActor>) -> Self {
         let mut client = Self::new();
-        client.graph_service_addr = Some(graph_addr);
+        client.graph_state_addr = Some(graph_addr);
         client
     }

@@ -104,7 +104,7 @@ impl BotsClient {
         match self.mcp_client.test_connection().await {
             Ok(true) => {
                 info!("✓ MCP server is reachable");
-
+
                 // Initialize MCP session
                 match self.mcp_client.initialize_session().await {
                     Ok(_) => {
@@ -134,7 +134,7 @@ impl BotsClient {

     async fn start_polling(&self) {
         let mcp_client = self.mcp_client.clone();
-        let graph_service_addr = self.graph_service_addr.clone();
+        let graph_state_addr = self.graph_state_addr.clone();
         let agents = self.agents.clone();

         tokio::spawn(async move {
@@ -161,8 +161,8 @@ impl BotsClient {
                             }

                             // Send to graph if connected
-                            if let Some(ref graph_addr) = graph_service_addr {
-                                info!("📨 BotsClient sending {} agents to graph", converted_agents.len());
+                            if let Some(ref graph_addr) = graph_state_addr {
+                                info!("📨 BotsClient sending {} agents to GraphStateActor", converted_agents.len());

                                 // Send agents directly without conversion
                                 graph_addr.do_send(UpdateBotsGraph { agents: converted_agents.clone() });
@@ -247,4 +247,4 @@ impl BotsClient {
             }
         }
     }
-}
\ No newline at end of file
+}
diff --git a/src/services/database_service.rs b/src/services/database_service.rs
index 57711056..c4123809 100644
--- a/src/services/database_service.rs
+++ b/src/services/database_service.rs
@@ -8,24 +8,16 @@
 //! - Validation reports and inference results
 //! - Physics constraint generation

-use rusqlite::{Connection, OptionalExtension, params, Result as SqliteResult};
+use rusqlite::{Connection, OptionalExtension, params, Result as SqliteResult, Transaction};
+use std::collections::HashMap;
 use std::path::Path;
 use std::sync::{Arc, Mutex};
+use chrono::{DateTime, Utc};
 use serde_json::Value as JsonValue;

+use crate::models::metadata::Metadata;
 use crate::config::PhysicsSettings;

-/// Setting value types for database storage
-#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
-#[serde(untagged)]
-pub enum SettingValue {
-    String(String),
-    Integer(i64),
-    Float(f64),
-    Boolean(bool),
-    Json(JsonValue),
-}
-
 /// Main database service providing thread-safe access to SQLite
 pub struct DatabaseService {
     conn: Arc<Mutex<Connection>>,
@@ -280,36 +272,7 @@ impl DatabaseService {
         )?;

         // Also save physics settings to dedicated table for fast access
-        // Inline the physics save to avoid deadlock from double-locking
-        let physics = &settings.visualisation.graphs.logseq.physics;
-        conn.execute(
-            "INSERT INTO physics_settings (
-                profile_name, damping, dt, iterations, max_velocity, max_force,
-                repel_k, spring_k, mass_scale, boundary_damping, temperature, gravity,
-                bounds_size, enable_bounds, rest_length, repulsion_cutoff,
-                repulsion_softening_epsilon, center_gravity_k, grid_cell_size,
-                warmup_iterations, cooling_rate, constraint_ramp_frames,
-                constraint_max_force_per_node
-             ) VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10, ?11, ?12, ?13, ?14, ?15, ?16, ?17, ?18, ?19, ?20, ?21, ?22, ?23)
-             ON CONFLICT(profile_name) DO UPDATE SET
-                damping = excluded.damping,
-                dt = excluded.dt,
-                iterations = excluded.iterations,
-                max_velocity = excluded.max_velocity,
-                max_force = excluded.max_force,
-                repel_k = excluded.repel_k,
-                spring_k = excluded.spring_k,
-                updated_at = CURRENT_TIMESTAMP",
-            params![
-                "default", physics.damping, physics.dt, physics.iterations,
-                physics.max_velocity, physics.max_force, physics.repel_k, physics.spring_k,
-                physics.mass_scale, physics.boundary_damping, physics.temperature,
-                physics.gravity, physics.bounds_size, if physics.enable_bounds { 1 } else { 0 },
-                physics.rest_length, physics.repulsion_cutoff, physics.repulsion_softening_epsilon,
-                physics.center_gravity_k, physics.grid_cell_size, physics.warmup_iterations,
-                physics.cooling_rate, physics.constraint_ramp_frames, physics.constraint_max_force_per_node
-            ]
-        )?;
+        self.save_physics_settings("default", &settings.visualisation.graphs.logseq.physics)?;

         Ok(())
     }
@@ -338,3 +301,549 @@ impl DatabaseService {
         }
     }
 }
+
+// ================================================================
+// ONTOLOGY QUERIES
+// ================================================================
+
+impl DatabaseService {
+    /// Save ontology metadata
+    pub fn save_ontology(&self, ontology: &OntologyMetadata) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            "INSERT INTO ontologies (
+                ontology_id, source_path, source_type, base_iri, version_iri,
+                title, description, author, version, content_hash, axiom_count,
+                class_count, property_count
+             ) VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10, ?11, ?12, ?13)
+             ON CONFLICT(ontology_id) DO UPDATE SET
+                last_validated_at = CURRENT_TIMESTAMP,
+                updated_at = CURRENT_TIMESTAMP",
+            params![
+                ontology.ontology_id, ontology.source_path, ontology.source_type,
+                ontology.base_iri, ontology.version_iri, ontology.title,
+                ontology.description, ontology.author, ontology.version,
+                ontology.content_hash, ontology.axiom_count, ontology.class_count,
+                ontology.property_count
+            ]
+        )?;
+
+        Ok(())
+    }
+
+    /// Get ontology by ID
+    pub fn get_ontology(&self, ontology_id: &str) -> SqliteResult<Option<OntologyMetadata>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT ontology_id, source_path, source_type, base_iri, version_iri,
+                    title, description, author, version, content_hash, axiom_count,
+                    class_count, property_count, parsed_at, last_validated_at
+             FROM ontologies WHERE ontology_id = ?1"
+        )?;
+
+        stmt.query_row(params![ontology_id], |row| {
+            Ok(OntologyMetadata {
+                ontology_id: row.get(0)?,
+                source_path: row.get(1)?,
+                source_type: row.get(2)?,
+                base_iri: row.get(3)?,
+                version_iri: row.get(4)?,
+                title: row.get(5)?,
+                description: row.get(6)?,
+                author: row.get(7)?,
+                version: row.get(8)?,
+                content_hash: row.get(9)?,
+                axiom_count: row.get(10)?,
+                class_count: row.get(11)?,
+                property_count: row.get(12)?,
+                parsed_at: row.get(13)?,
+                last_validated_at: row.get(14)?,
+            })
+        }).optional()
+    }
+
+    /// Save OWL class definition
+    pub fn save_owl_class(&self, ontology_id: &str, class: &OwlClass) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            "INSERT INTO owl_classes (ontology_id, class_iri, label, comment, parent_class_iri, is_deprecated)
+             VALUES (?1, ?2, ?3, ?4, ?5, ?6)
+             ON CONFLICT(ontology_id, class_iri) DO UPDATE SET
+                label = excluded.label,
+                comment = excluded.comment,
+                parent_class_iri = excluded.parent_class_iri",
+            params![
+                ontology_id, class.class_iri, class.label, class.comment,
+                class.parent_class_iri, if class.is_deprecated { 1 } else { 0 }
+            ]
+        )?;
+
+        Ok(())
+    }
+
+    /// Get all classes for an ontology
+    pub fn get_owl_classes(&self, ontology_id: &str) -> SqliteResult<Vec<OwlClass>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT class_iri, label, comment, parent_class_iri, is_deprecated
+             FROM owl_classes WHERE ontology_id = ?1
+             ORDER BY class_iri"
+        )?;
+
+        let rows = stmt.query_map(params![ontology_id], |row| {
+            Ok(OwlClass {
+                class_iri: row.get(0)?,
+                label: row.get(1)?,
+                comment: row.get(2)?,
+                parent_class_iri: row.get(3)?,
+                is_deprecated: row.get::<_, i32>(4)? == 1,
+            })
+        })?;
+
+        rows.collect()
+    }
+
+    /// Get disjoint class pairs
+    pub fn get_disjoint_classes(&self, ontology_id: &str) -> SqliteResult<Vec<(String, String)>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT class_iri_1, class_iri_2 FROM owl_disjoint_classes WHERE ontology_id = ?1"
+        )?;
+
+        let rows = stmt.query_map(params![ontology_id], |row| {
+            Ok((row.get(0)?, row.get(1)?))
+        })?;
+
+        rows.collect()
+    }
+}
+
+// ================================================================
+// MAPPING CONFIGURATION QUERIES
+// ================================================================
+
+impl DatabaseService {
+    /// Get namespace IRI for prefix
+    pub fn get_namespace(&self, prefix: &str) -> SqliteResult<Option<String>> {
+        let conn = self.conn.lock().unwrap();
+        conn.query_row(
+            "SELECT namespace_iri FROM namespaces WHERE prefix = ?1",
+            params![prefix],
+            |row| row.get(0)
+        ).optional()
+    }
+
+    /// Save namespace
+    pub fn save_namespace(&self, prefix: &str, namespace_iri: &str, is_default: bool) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            "INSERT INTO namespaces (prefix, namespace_iri, is_default)
+             VALUES (?1, ?2, ?3)
+             ON CONFLICT(prefix) DO UPDATE SET
+                namespace_iri = excluded.namespace_iri,
+                is_default = excluded.is_default",
+            params![prefix, namespace_iri, if is_default { 1 } else { 0 }]
+        )?;
+
+        Ok(())
+    }
+
+    /// Get OWL class IRI for graph label
+    pub fn get_class_mapping(&self, graph_label: &str) -> SqliteResult<Option<String>> {
+        let conn = self.conn.lock().unwrap();
+        conn.query_row(
+            "SELECT owl_class_iri FROM class_mappings WHERE graph_label = ?1",
+            params![graph_label],
+            |row| row.get(0)
+        ).optional()
+    }
+
+    /// Get property mapping
+    pub fn get_property_mapping(&self, graph_property: &str) -> SqliteResult<Option<PropertyMapping>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT owl_property_iri, property_type, rdfs_domain, rdfs_range, inverse_property_iri
+             FROM property_mappings WHERE graph_property = ?1"
+        )?;
+
+        stmt.query_row(params![graph_property], |row| {
+            Ok(PropertyMapping {
+                owl_property_iri: row.get(0)?,
+                property_type: row.get(1)?,
+                rdfs_domain: row.get(2)?,
+                rdfs_range: row.get(3)?,
+                inverse_property_iri: row.get(4)?,
+            })
+        }).optional()
+    }
+}
+
+// ================================================================
+// FILE METADATA QUERIES
+// ================================================================
+
+impl DatabaseService {
+    /// Get file metadata
+    pub fn get_file_metadata(&self, file_name: &str) -> SqliteResult<Option<Metadata>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT file_name, file_size, sha1, file_blob_sha, node_id, node_size,
+                    hyperlink_count, perplexity_link, last_modified, last_content_change,
+                    last_commit, last_perplexity_process, change_count
+             FROM file_metadata WHERE file_name = ?1"
+        )?;
+
+        let result = stmt.query_row(params![file_name], |row| {
+            Ok(Metadata {
+                file_name: row.get(0)?,
+                file_size: row.get(1)?,
+                sha1: row.get(2)?,
+                file_blob_sha: row.get(3)?,
+                node_id: row.get(4)?,
+                node_size: row.get(5)?,
+                hyperlink_count: row.get(6)?,
+                perplexity_link: row.get(7)?,
+                last_modified: row.get(8)?,
+                last_content_change: row.get(9)?,
+                last_commit: row.get(10)?,
+                last_perplexity_process: row.get(11)?,
+                change_count: row.get(12)?,
+                topic_counts: HashMap::new(), // Will be populated separately
+            })
+        }).optional()?;
+
+        if let Some(mut metadata) = result {
+            metadata.topic_counts = self.get_file_topics(file_name)?;
+            Ok(Some(metadata))
+        } else {
+            Ok(None)
+        }
+    }
+
+    /// Get topic counts for a file
+    fn get_file_topics(&self, file_name: &str) -> SqliteResult<HashMap<String, usize>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT topic, count FROM file_topics WHERE file_name = ?1"
+        )?;
+
+        let rows = stmt.query_map(params![file_name], |row| {
+            Ok((row.get::<_, String>(0)?, row.get::<_, usize>(1)?))
+        })?;
+
+        let mut topics = HashMap::new();
+        for row in rows {
+            let (topic, count) = row?;
+            topics.insert(topic, count);
+        }
+
+        Ok(topics)
+    }
+
+    /// Get all file metadata
+    pub fn get_all_file_metadata(&self) -> SqliteResult<HashMap<String, Metadata>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT file_name FROM file_metadata ORDER BY file_name"
+        )?;
+
+        let file_names: Vec<String> = stmt.query_map([], |row| row.get(0))?.collect::<SqliteResult<_>>()?;
+
+        drop(stmt);
+        drop(conn);
+
+        let mut metadata_map = HashMap::new();
+        for file_name in file_names {
+            if let Some(metadata) = self.get_file_metadata(&file_name)? {
+                metadata_map.insert(file_name, metadata);
+            }
+        }
+
+        Ok(metadata_map)
+    }
+
+    /// Save file metadata
+    pub fn save_file_metadata(&self, metadata: &Metadata) -> SqliteResult<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            "INSERT INTO file_metadata (
+                file_name, file_path, file_size, sha1, file_blob_sha, node_id,
+                node_size, hyperlink_count, perplexity_link, last_modified,
+                last_content_change, last_commit, last_perplexity_process, change_count
+             ) VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10, ?11, ?12, ?13, ?14)
+             ON CONFLICT(file_name) DO UPDATE SET
+                file_size = excluded.file_size,
+                sha1 = excluded.sha1,
+                node_size = excluded.node_size,
+                hyperlink_count = excluded.hyperlink_count,
+                last_modified = excluded.last_modified,
+                updated_at = CURRENT_TIMESTAMP",
+            params![
+                metadata.file_name, format!("./markdown/{}", metadata.file_name),
+                metadata.file_size, metadata.sha1, metadata.file_blob_sha, metadata.node_id,
+                metadata.node_size, metadata.hyperlink_count, metadata.perplexity_link,
+                metadata.last_modified, metadata.last_content_change, metadata.last_commit,
+                metadata.last_perplexity_process, metadata.change_count
+            ]
+        )?;
+
+        // Save topic counts
+        for (topic, count) in &metadata.topic_counts {
+            conn.execute(
+                "INSERT INTO file_topics (file_name, topic, count) VALUES (?1, ?2, ?3)
+                 ON CONFLICT(file_name, topic) DO UPDATE SET count = excluded.count",
+                params![metadata.file_name, topic, count]
+            )?;
+        }
+
+        Ok(())
+    }
+}
+
+// ================================================================
+// CONSTRAINT QUERIES
+// ================================================================
+
+impl DatabaseService {
+    /// Get constraint groups by physics type
+    pub fn get_constraint_groups(&self, physics_type: &str) -> SqliteResult<Vec<ConstraintGroup>> {
+        let conn = self.conn.lock().unwrap();
+        let mut stmt = conn.prepare(
+            "SELECT id, group_name, kernel_name, physics_type, default_strength, enabled, batch_size
+             FROM constraint_groups
+             WHERE physics_type = ?1 AND enabled = 1"
+        )?;
+
+        let rows = stmt.query_map(params![physics_type], |row| {
+            Ok(ConstraintGroup {
+                id: row.get(0)?,
+                group_name: row.get(1)?,
+                kernel_name: row.get(2)?,
+                physics_type: row.get(3)?,
+                default_strength: row.get(4)?,
+                enabled: row.get::<_, i32>(5)? == 1,
+                batch_size: row.get(6)?,
+            })
+        })?;
+
+        rows.collect()
+    }
+}
+
+// ================================================================
+// TYPE DEFINITIONS
+// ================================================================
+
+#[derive(Debug, Clone)]
+pub enum SettingValue {
+    String(String),
+    Integer(i64),
+    Float(f64),
+    Boolean(bool),
+    Json(JsonValue),
+}
+
+#[derive(Debug, Clone)]
+pub struct OntologyMetadata {
+    pub ontology_id: String,
+    pub source_path: String,
+    pub source_type: String,
+    pub base_iri: String,
+    pub version_iri: Option<String>,
+    pub title: Option<String>,
+    pub description: Option<String>,
+    pub author: Option<String>,
+    pub version: Option<String>,
+    pub content_hash: String,
+    pub axiom_count: i32,
+    pub class_count: i32,
+    pub property_count: i32,
+    pub parsed_at: DateTime<Utc>,
+    pub last_validated_at: Option<DateTime<Utc>>,
+}
+
+#[derive(Debug, Clone)]
+pub struct OwlClass {
+    pub class_iri: String,
+    pub label: Option<String>,
+    pub comment: Option<String>,
+    pub parent_class_iri: Option<String>,
+    pub is_deprecated: bool,
+}
+
+#[derive(Debug, Clone)]
+pub struct PropertyMapping {
+    pub owl_property_iri: String,
+    pub property_type: String,
+    pub rdfs_domain: Option<String>,
+    pub rdfs_range: Option<String>,
+    pub inverse_property_iri: Option<String>,
+}
+
+#[derive(Debug, Clone)]
+pub struct ConstraintGroup {
+    pub id: i64,
+    pub group_name: String,
+    pub kernel_name: String,
+    pub physics_type: String,
+    pub default_strength: f64,
+    pub enabled: bool,
+    pub batch_size: i32,
+}
+
+// ================================================================
+// TESTS
+// ================================================================
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_database_creation() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+        let version = db.get_schema_version();
+        assert!(version.is_ok() || version.is_err()); // Schema not initialized yet
+    }
+
+    #[test]
+    fn test_setting_crud() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+
+        // Initialize schema first
+        let schema = include_str!("../../schema/ontology_db.sql");
+        db.execute_schema(schema).expect("Failed to initialize schema");
+
+        // Create setting
+        db.set_setting("test.key", SettingValue::String("value".to_string()), Some("Test setting"))
+            .expect("Failed to set setting");
+
+        // Read setting
+        let value = db.get_setting("test.key").expect("Failed to get setting");
+        assert!(value.is_some());
+
+        if let Some(SettingValue::String(s)) = value {
+            assert_eq!(s, "value");
+        } else {
+            panic!("Expected string value");
+        }
+    }
+
+    #[test]
+    fn test_to_camel_case() {
+        // Test snake_case to camelCase conversion
+        assert_eq!(DatabaseService::to_camel_case("spring_k"), "springK");
+        assert_eq!(DatabaseService::to_camel_case("max_velocity"), "maxVelocity");
+        assert_eq!(DatabaseService::to_camel_case("repulsion_cutoff"), "repulsionCutoff");
+        assert_eq!(DatabaseService::to_camel_case("center_gravity_k"), "centerGravityK");
+
+        // Test no conversion needed
+        assert_eq!(DatabaseService::to_camel_case("springK"), "springK");
+        assert_eq!(DatabaseService::to_camel_case("damping"), "damping");
+
+        // Test edge cases
+        assert_eq!(DatabaseService::to_camel_case("a_b"), "aB");
+        assert_eq!(DatabaseService::to_camel_case("a_b_c"), "aBC");
+        assert_eq!(DatabaseService::to_camel_case(""), "");
+    }
+
+    #[test]
+    fn test_camel_snake_fallback_lookup() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+        let schema = include_str!("../../schema/ontology_db.sql");
+        db.execute_schema(schema).expect("Failed to initialize schema");
+
+        // Store setting in camelCase format (preferred format)
+        db.set_setting("springK", SettingValue::Float(150.0), Some("Spring constant"))
+            .expect("Failed to set springK");
+
+        // Test direct camelCase lookup (exact match)
+        let value = db.get_setting("springK").expect("Failed to get springK");
+        assert!(value.is_some());
+        if let Some(SettingValue::Float(f)) = value {
+            assert_eq!(f, 150.0);
+        } else {
+            panic!("Expected float value for springK");
+        }
+
+        // Test snake_case lookup with camelCase fallback
+        let value = db.get_setting("spring_k").expect("Failed to get spring_k");
+        assert!(value.is_some());
+        if let Some(SettingValue::Float(f)) = value {
+            assert_eq!(f, 150.0);
+        } else {
+            panic!("Expected float value for spring_k");
+        }
+
+        // Test non-existent key returns None
+        let value = db.get_setting("nonexistent_key").expect("Failed to get nonexistent");
+        assert!(value.is_none());
+    }
+
+    #[test]
+    fn test_multiple_physics_settings_fallback() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+        let schema = include_str!("../../schema/ontology_db.sql");
+        db.execute_schema(schema).expect("Failed to initialize schema");
+
+        // Store multiple physics settings in camelCase
+        db.set_setting("repelK", SettingValue::Float(50.0), None).unwrap();
+        db.set_setting("maxVelocity", SettingValue::Float(10.0), None).unwrap();
+        db.set_setting("centerGravityK", SettingValue::Float(0.1), None).unwrap();
+
+        // Test snake_case lookups
+        assert_eq!(
+            match db.get_setting("repel_k").unwrap() {
+                Some(SettingValue::Float(f)) => f,
+                _ => panic!("Expected float"),
+            },
+            50.0
+        );
+
+        assert_eq!(
+            match db.get_setting("max_velocity").unwrap() {
+                Some(SettingValue::Float(f)) => f,
+                _ => panic!("Expected float"),
+            },
+            10.0
+        );
+
+        assert_eq!(
+            match db.get_setting("center_gravity_k").unwrap() {
+                Some(SettingValue::Float(f)) => f,
+                _ => panic!("Expected float"),
+            },
+            0.1
+        );
+    }
+
+    #[test]
+    fn test_exact_match_priority() {
+        let db = DatabaseService::new(":memory:").expect("Failed to create database");
+        let schema = include_str!("../../schema/ontology_db.sql");
+        db.execute_schema(schema).expect("Failed to initialize schema");
+
+        // Store both snake_case and camelCase versions
+        db.set_setting("spring_k", SettingValue::Float(100.0), None).unwrap();
+        db.set_setting("springK", SettingValue::Float(150.0), None).unwrap();
+
+        // Exact match should always take priority
+        let value1 = db.get_setting("spring_k").unwrap();
+        if let Some(SettingValue::Float(f)) = value1 {
+            assert_eq!(f, 100.0); // Gets exact match
+        } else {
+            panic!("Expected float");
+        }
+
+        let value2 = db.get_setting("springK").unwrap();
+        if let Some(SettingValue::Float(f)) = value2 {
+            assert_eq!(f, 150.0); // Gets exact match
+        } else {
+            panic!("Expected float");
+        }
+    }
+}
diff --git a/src/services/mod.rs b/src/services/mod.rs
index c353afe2..30e0bb2f 100755
--- a/src/services/mod.rs
+++ b/src/services/mod.rs
@@ -4,7 +4,6 @@ pub mod multi_mcp_agent_discovery;
 pub mod topology_visualization_engine;
 pub mod real_mcp_integration_bridge;
 pub mod bots_client;
-pub mod database_service;
 pub mod edge_generation;
 pub mod file_service;
 pub mod github;
@@ -12,9 +11,26 @@ pub mod management_api_client;
 // graph_service module removed - functionality moved to GraphServiceActor
 pub mod mcp_relay_manager;
 pub mod nostr_service;
-pub mod settings_service;
 #[cfg(feature = "ontology")]
 pub mod owl_validator;
+#[cfg(feature = "ontology")]
+pub mod ontology_downloader;
+#[cfg(feature = "ontology")]
+pub mod ontology_storage;
+#[cfg(feature = "ontology")]
+pub mod ontology_sync;
+#[cfg(feature = "ontology")]
+pub mod database_service;
+#[cfg(feature = "ontology")]
+pub mod ontology_init;
+#[cfg(feature = "ontology")]
+pub mod settings_migration;
+#[cfg(feature = "ontology")]
+pub mod settings_service;
+#[cfg(feature = "ontology")]
+pub mod settings_validator;
+#[cfg(feature = "ontology")]
+pub mod user_service;
 pub mod perplexity_service;
 pub mod ragflow_service;
 pub mod semantic_analyzer;
diff --git a/src/services/ontology_downloader.rs b/src/services/ontology_downloader.rs
new file mode 100644
index 00000000..5123f291
--- /dev/null
+++ b/src/services/ontology_downloader.rs
@@ -0,0 +1,847 @@
+use anyhow::{Context, Result, anyhow};
+use chrono::{DateTime, Utc};
+use log::{debug, info, warn, error};
+use regex::Regex;
+use reqwest::{Client, StatusCode};
+use serde::{Deserialize, Serialize};
+use std::collections::{HashMap, HashSet};
+use std::path::PathBuf;
+use std::sync::Arc;
+use std::time::Duration;
+use thiserror::Error;
+use tokio::sync::RwLock;
+use tokio::time::sleep;
+
+#[derive(Error, Debug)]
+pub enum DownloaderError {
+    #[error("GitHub API error: {0}")]
+    GitHubApi(String),
+
+    #[error("Network error: {0}")]
+    Network(#[from] reqwest::Error),
+
+    #[error("Parse error: {0}")]
+    Parse(String),
+
+    #[error("Database error: {0}")]
+    Database(String),
+
+    #[error("Rate limit exceeded, retry after: {0:?}")]
+    RateLimit(Duration),
+
+    #[error("Authentication error: {0}")]
+    Auth(String),
+
+    #[error("Configuration error: {0}")]
+    Config(String),
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct OntologyDownloaderConfig {
+    pub github_token: String,
+    pub repo_owner: String,
+    pub repo_name: String,
+    pub base_path: String,
+    pub max_retries: u32,
+    pub initial_retry_delay_ms: u64,
+    pub max_retry_delay_ms: u64,
+    pub request_timeout_secs: u64,
+    pub respect_rate_limits: bool,
+}
+
+impl Default for OntologyDownloaderConfig {
+    fn default() -> Self {
+        Self {
+            github_token: String::new(),
+            repo_owner: String::from("jjohare"),
+            repo_name: String::from("logseq"),
+            base_path: String::from("mainKnowledgeGraph/pages"),
+            max_retries: 3,
+            initial_retry_delay_ms: 1000,
+            max_retry_delay_ms: 30000,
+            request_timeout_secs: 30,
+            respect_rate_limits: true,
+        }
+    }
+}
+
+impl OntologyDownloaderConfig {
+    pub fn from_env() -> Result<Self> {
+        let github_token = std::env::var("GITHUB_TOKEN")
+            .or_else(|_| std::env::var("GH_TOKEN"))
+            .context("GITHUB_TOKEN or GH_TOKEN environment variable not set")?;
+
+        if github_token.is_empty() {
+            return Err(anyhow!("GitHub token cannot be empty"));
+        }
+
+        Ok(Self {
+            github_token,
+            ..Default::default()
+        })
+    }
+
+    pub fn with_token(token: String) -> Self {
+        Self {
+            github_token: token,
+            ..Default::default()
+        }
+    }
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct GitHubFile {
+    pub name: String,
+    pub path: String,
+    pub sha: String,
+    pub size: u64,
+    #[serde(rename = "type")]
+    pub file_type: String,
+    pub download_url: Option<String>,
+    pub url: String,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct OntologyBlock {
+    pub id: String,
+    pub source_file: String,
+    pub title: String,
+    pub properties: HashMap<String, Vec<String>>,
+    pub owl_content: Vec<String>,
+    pub classes: Vec<String>,
+    pub properties_list: Vec<String>,
+    pub relationships: Vec<OntologyRelationship>,
+    pub downloaded_at: DateTime<Utc>,
+    pub content_hash: String,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct OntologyRelationship {
+    pub subject: String,
+    pub predicate: String,
+    pub object: String,
+    pub relationship_type: RelationshipType,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
+pub enum RelationshipType {
+    SubClassOf,
+    ObjectProperty,
+    DataProperty,
+    DisjointWith,
+    EquivalentTo,
+    InverseOf,
+    Domain,
+    Range,
+    Other(String),
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct DownloadProgress {
+    pub total_files: usize,
+    pub processed_files: usize,
+    pub ontology_blocks_found: usize,
+    pub errors: Vec<String>,
+    pub started_at: DateTime<Utc>,
+    pub completed_at: Option<DateTime<Utc>>,
+    pub current_file: Option<String>,
+}
+
+impl DownloadProgress {
+    pub fn new(total_files: usize) -> Self {
+        Self {
+            total_files,
+            processed_files: 0,
+            ontology_blocks_found: 0,
+            errors: Vec::new(),
+            started_at: Utc::now(),
+            completed_at: None,
+            current_file: None,
+        }
+    }
+
+    pub fn percentage(&self) -> f64 {
+        if self.total_files == 0 {
+            return 0.0;
+        }
+        (self.processed_files as f64 / self.total_files as f64) * 100.0
+    }
+}
+
+pub struct OntologyDownloader {
+    config: OntologyDownloaderConfig,
+    client: Client,
+    progress: Arc<RwLock<DownloadProgress>>,
+    framework_files: HashSet<String>,
+}
+
+impl OntologyDownloader {
+    pub fn new(config: OntologyDownloaderConfig) -> Result<Self> {
+        if config.github_token.is_empty() {
+            return Err(DownloaderError::Config("GitHub token is required".to_string()).into());
+        }
+
+        let client = Client::builder()
+            .user_agent("ontology-downloader/1.0")
+            .timeout(Duration::from_secs(config.request_timeout_secs))
+            .build()?;
+
+        let mut framework_files = HashSet::new();
+        framework_files.insert("ETSI.md".to_string());
+        framework_files.insert("OntologyDefinition.md".to_string());
+        framework_files.insert("PropertySchema.md".to_string());
+
+        Ok(Self {
+            config,
+            client,
+            progress: Arc::new(RwLock::new(DownloadProgress::new(0))),
+            framework_files,
+        })
+    }
+
+    pub async fn get_progress(&self) -> DownloadProgress {
+        self.progress.read().await.clone()
+    }
+
+    pub async fn download_all(&self) -> Result<Vec<OntologyBlock>> {
+        info!("Starting ontology download from GitHub: {}/{}/{}",
+              self.config.repo_owner, self.config.repo_name, self.config.base_path);
+
+        let files = self.list_all_files().await?;
+
+        {
+            let mut progress = self.progress.write().await;
+            progress.total_files = files.len();
+            progress.started_at = Utc::now();
+        }
+
+        info!("Found {} files to process", files.len());
+
+        let mut all_blocks = Vec::new();
+
+        for file in &files {
+            {
+                let mut progress = self.progress.write().await;
+                progress.current_file = Some(file.name.clone());
+            }
+
+            match self.download_and_parse_file(file).await {
+                Ok(blocks) => {
+                    let block_count = blocks.len();
+                    if block_count > 0 {
+                        info!("Found {} ontology blocks in {}", block_count, file.name);
+                        all_blocks.extend(blocks);
+
+                        let mut progress = self.progress.write().await;
+                        progress.ontology_blocks_found += block_count;
+                    }
+                }
+                Err(e) => {
+                    warn!("Error processing file {}: {}", file.name, e);
+                    let mut progress = self.progress.write().await;
+                    progress.errors.push(format!("{}: {}", file.name, e));
+                }
+            }
+
+            {
+                let mut progress = self.progress.write().await;
+                progress.processed_files += 1;
+            }
+        }
+
+        {
+            let mut progress = self.progress.write().await;
+            progress.completed_at = Some(Utc::now());
+        }
+
+        info!("Download complete: {} ontology blocks from {} files",
+              all_blocks.len(), files.len());
+
+        Ok(all_blocks)
+    }
+
+    async fn list_all_files(&self) -> Result<Vec<GitHubFile>> {
+        info!("Listing files in {}", self.config.base_path);
+
+        let mut all_files = Vec::new();
+        let mut directories = vec![self.config.base_path.clone()];
+
+        while let Some(dir) = directories.pop() {
+            let files = self.list_directory(&dir).await?;
+
+            for file in files {
+                match file.file_type.as_str() {
+                    "file" => {
+                        if file.name.ends_with(".md") {
+                            all_files.push(file);
+                        }
+                    }
+                    "dir" => {
+                        directories.push(file.path.clone());
+                    }
+                    _ => {}
+                }
+            }
+        }
+
+        Ok(all_files)
+    }
+
+    async fn list_directory(&self, path: &str) -> Result<Vec<GitHubFile>> {
+        let url = format!(
+            "https://api.github.com/repos/{}/{}/contents/{}",
+            self.config.repo_owner,
+            self.config.repo_name,
+            path
+        );
+
+        let files = self.github_request::<Vec<GitHubFile>>(&url).await?;
+        Ok(files)
+    }
+
+    async fn download_and_parse_file(&self, file: &GitHubFile) -> Result<Vec<OntologyBlock>> {
+        debug!("Processing file: {}", file.name);
+
+        let content = if let Some(ref download_url) = file.download_url {
+            self.download_file_content(download_url).await?
+        } else {
+            self.get_file_content(&file.url).await?
+        };
+
+        if !self.should_process_file(&content) {
+            debug!("Skipping file {} - no public ontology blocks", file.name);
+            return Ok(Vec::new());
+        }
+
+        self.parse_ontology_file(&file.name, &file.path, &content)
+    }
+
+    fn should_process_file(&self, content: &str) -> bool {
+        let has_ontology_marker = content.contains("- ### OntologyBlock")
+            || content.contains("OntologyBlock");
+
+        if !has_ontology_marker {
+            return false;
+        }
+
+        let has_public_gate = content.contains("public:: true");
+
+        has_public_gate
+    }
+
+    fn parse_ontology_file(&self, filename: &str, filepath: &str, content: &str) -> Result<Vec<OntologyBlock>> {
+        let mut blocks = Vec::new();
+
+        let title = self.extract_title(filename, content);
+        let properties = self.extract_properties(content);
+        let owl_contents = self.extract_owl_blocks(content)?;
+
+        for (idx, owl_content) in owl_contents.iter().enumerate() {
+            let classes = self.extract_classes(owl_content);
+            let properties_list = self.extract_owl_properties(owl_content);
+            let relationships = self.extract_relationships(owl_content);
+
+            let block_id = format!("{}:block:{}", filepath, idx);
+            let content_hash = self.calculate_hash(&format!("{}{}", title, owl_content));
+
+            let block = OntologyBlock {
+                id: block_id,
+                source_file: filepath.to_string(),
+                title: title.clone(),
+                properties: properties.clone(),
+                owl_content: vec![owl_content.clone()],
+                classes,
+                properties_list,
+                relationships,
+                downloaded_at: Utc::now(),
+                content_hash,
+            };
+
+            blocks.push(block);
+        }
+
+        Ok(blocks)
+    }
+
+    fn extract_title(&self, filename: &str, content: &str) -> String {
+        let heading_re = Regex::new(r"^#\s+(.+)$").unwrap();
+        for line in content.lines() {
+            if let Some(cap) = heading_re.captures(line) {
+                return cap[1].trim().to_string();
+            }
+        }
+
+        filename.trim_end_matches(".md").to_string()
+    }
+
+    fn extract_properties(&self, content: &str) -> HashMap<String, Vec<String>> {
+        let mut properties = HashMap::new();
+        let property_re = Regex::new(r"^([a-zA-Z][a-zA-Z0-9-_]*)::\s*(.+)$").unwrap();
+
+        for line in content.lines() {
+            if let Some(cap) = property_re.captures(line.trim()) {
+                let key = cap[1].to_string();
+                let value = cap[2].to_string();
+
+                let values: Vec<String> = value
+                    .split(',')
+                    .map(|v| v.trim().to_string())
+                    .filter(|v| !v.is_empty())
+                    .collect();
+
+                properties.entry(key).or_insert_with(Vec::new).extend(values);
+            }
+        }
+
+        properties
+    }
+
+    fn extract_owl_blocks(&self, content: &str) -> Result<Vec<String>> {
+        let mut blocks = Vec::new();
+        let lines: Vec<&str> = content.lines().collect();
+        let mut i = 0;
+
+        while i < lines.len() {
+            let line = lines[i].trim();
+
+            let fence_match = if line.starts_with("```") {
+                Some(line)
+            } else if line.starts_with("- ```") {
+                Some(&line[2..])
+            } else {
+                None
+            };
+
+            if let Some(fence_line) = fence_match {
+                let language = fence_line.trim_start_matches("```").trim();
+
+                if language == "clojure" || language.is_empty() {
+                    i += 1;
+                    if i >= lines.len() {
+                        break;
+                    }
+
+                    let should_extract = if language == "clojure" {
+                        true
+                    } else if lines[i].trim().starts_with("owl:functional-syntax::") {
+                        i += 1;
+                        true
+                    } else {
+                        false
+                    };
+
+                    if should_extract {
+                        let mut block_lines = Vec::new();
+                        while i < lines.len() {
+                            let current_line = lines[i];
+                            if current_line.trim().starts_with("```") {
+                                break;
+                            }
+                            let trimmed = current_line.trim_start();
+                            if !trimmed.is_empty()
+                                && !trimmed.starts_with(";;")
+                                && !trimmed.starts_with("#")
+                                && trimmed != "|" {
+                                block_lines.push(trimmed);
+                            }
+                            i += 1;
+                        }
+
+                        let block_text = block_lines.join("\n");
+                        let is_owl = block_text.contains("Declaration(")
+                            || block_text.contains("SubClassOf(")
+                            || block_text.contains("EquivalentClasses(")
+                            || block_text.contains("DisjointClasses(")
+                            || block_text.contains("ObjectProperty(")
+                            || block_text.contains("DataProperty(");
+
+                        if is_owl && !block_lines.is_empty() {
+                            blocks.push(block_text);
+                        }
+                    }
+                }
+                i += 1;
+                continue;
+            }
+
+            if line.starts_with("owl:functional-syntax::") {
+                i += 1;
+                if i >= lines.len() {
+                    break;
+                }
+
+                if !lines[i].trim().starts_with('|') {
+                    i += 1;
+                    continue;
+                }
+
+                i += 1;
+
+                let mut block_lines = Vec::new();
+                let base_indent = if i < lines.len() {
+                    lines[i].len() - lines[i].trim_start().len()
+                } else {
+                    0
+                };
+
+                while i < lines.len() {
+                    let current_line = lines[i];
+                    let current_indent = current_line.len() - current_line.trim_start().len();
+
+                    if !current_line.trim().is_empty() && current_indent < base_indent {
+                        break;
+                    }
+
+                    if current_line.trim_start().starts_with('#')
+                        || current_line.trim().starts_with("```")
+                        || (current_line.contains("::") && !current_line.trim().starts_with("//"))
+                    {
+                        break;
+                    }
+
+                    if current_indent >= base_indent && !current_line.trim().is_empty() {
+                        let trimmed = if current_indent >= base_indent {
+                            &current_line[base_indent..]
+                        } else {
+                            current_line.trim_start()
+                        };
+                        block_lines.push(trimmed);
+                    }
+
+                    i += 1;
+                }
+
+                if !block_lines.is_empty() {
+                    blocks.push(block_lines.join("\n"));
+                }
+            } else {
+                i += 1;
+            }
+        }
+
+        Ok(blocks)
+    }
+
+    fn extract_classes(&self, owl_content: &str) -> Vec<String> {
+        let mut classes = Vec::new();
+        let class_re = Regex::new(r"Declaration\(Class\(([^)]+)\)\)").unwrap();
+
+        for cap in class_re.captures_iter(owl_content) {
+            classes.push(cap[1].to_string());
+        }
+
+        let subclass_re = Regex::new(r"SubClassOf\(([^\s]+)\s+[^)]+\)").unwrap();
+        for cap in subclass_re.captures_iter(owl_content) {
+            let class = cap[1].to_string();
+            if !classes.contains(&class) {
+                classes.push(class);
+            }
+        }
+
+        classes
+    }
+
+    fn extract_owl_properties(&self, owl_content: &str) -> Vec<String> {
+        let mut properties = Vec::new();
+
+        let obj_prop_re = Regex::new(r"Declaration\(ObjectProperty\(([^)]+)\)\)").unwrap();
+        for cap in obj_prop_re.captures_iter(owl_content) {
+            properties.push(cap[1].to_string());
+        }
+
+        let data_prop_re = Regex::new(r"Declaration\(DataProperty\(([^)]+)\)\)").unwrap();
+        for cap in data_prop_re.captures_iter(owl_content) {
+            properties.push(cap[1].to_string());
+        }
+
+        properties
+    }
+
+    fn extract_relationships(&self, owl_content: &str) -> Vec<OntologyRelationship> {
+        let mut relationships = Vec::new();
+
+        let subclass_re = Regex::new(r"SubClassOf\(([^\s]+)\s+([^)]+)\)").unwrap();
+        for cap in subclass_re.captures_iter(owl_content) {
+            relationships.push(OntologyRelationship {
+                subject: cap[1].to_string(),
+                predicate: "rdfs:subClassOf".to_string(),
+                object: cap[2].to_string(),
+                relationship_type: RelationshipType::SubClassOf,
+            });
+        }
+
+        let domain_re = Regex::new(r"ObjectPropertyDomain\(([^\s]+)\s+([^)]+)\)").unwrap();
+        for cap in domain_re.captures_iter(owl_content) {
+            relationships.push(OntologyRelationship {
+                subject: cap[1].to_string(),
+                predicate: "rdfs:domain".to_string(),
+                object: cap[2].to_string(),
+                relationship_type: RelationshipType::Domain,
+            });
+        }
+
+        let range_re = Regex::new(r"ObjectPropertyRange\(([^\s]+)\s+([^)]+)\)").unwrap();
+        for cap in range_re.captures_iter(owl_content) {
+            relationships.push(OntologyRelationship {
+                subject: cap[1].to_string(),
+                predicate: "rdfs:range".to_string(),
+                object: cap[2].to_string(),
+                relationship_type: RelationshipType::Range,
+            });
+        }
+
+        let disjoint_re = Regex::new(r"DisjointClasses\(([^\s]+)\s+([^)]+)\)").unwrap();
+        for cap in disjoint_re.captures_iter(owl_content) {
+            relationships.push(OntologyRelationship {
+                subject: cap[1].to_string(),
+                predicate: "owl:disjointWith".to_string(),
+                object: cap[2].to_string(),
+                relationship_type: RelationshipType::DisjointWith,
+            });
+        }
+
+        relationships
+    }
+
+    fn calculate_hash(&self, content: &str) -> String {
+        use blake3::Hasher;
+        let mut hasher = Hasher::new();
+        hasher.update(content.as_bytes());
+        hasher.finalize().to_hex().to_string()
+    }
+
+    async fn download_file_content(&self, url: &str) -> Result<String> {
+        let content = self.retry_with_backoff(|| async {
+            self.client
+                .get(url)
+                .header("Authorization", format!("token {}", self.config.github_token))
+                .send()
+                .await?
+                .error_for_status()?
+                .text()
+                .await
+                .map_err(|e| e.into())
+        })
+        .await?;
+
+        Ok(content)
+    }
+
+    async fn get_file_content(&self, api_url: &str) -> Result<String> {
+        #[derive(Deserialize)]
+        struct ContentResponse {
+            content: String,
+            encoding: String,
+        }
+
+        let response: ContentResponse = self.github_request(api_url).await?;
+
+        if response.encoding == "base64" {
+            let decoded = base64::decode(&response.content.replace('\n', ""))
+                .context("Failed to decode base64 content")?;
+            String::from_utf8(decoded).context("Invalid UTF-8 in file content")
+        } else {
+            Ok(response.content)
+        }
+    }
+
+    async fn github_request<T: for<'de> Deserialize<'de>>(&self, url: &str) -> Result<T> {
+        self.retry_with_backoff(|| async {
+            let response = self.client
+                .get(url)
+                .header("Authorization", format!("token {}", self.config.github_token))
+                .header("Accept", "application/vnd.github.v3+json")
+                .send()
+                .await?;
+
+            if response.status() == StatusCode::FORBIDDEN {
+                if let Some(retry_after) = response.headers().get("Retry-After") {
+                    if let Ok(retry_str) = retry_after.to_str() {
+                        if let Ok(seconds) = retry_str.parse::<u64>() {
+                            return Err(DownloaderError::RateLimit(Duration::from_secs(seconds)).into());
+                        }
+                    }
+                }
+
+                if let Some(remaining) = response.headers().get("X-RateLimit-Remaining") {
+                    if let Ok(remaining_str) = remaining.to_str() {
+                        if remaining_str == "0" {
+                            if let Some(reset) = response.headers().get("X-RateLimit-Reset") {
+                                if let Ok(reset_str) = reset.to_str() {
+                                    if let Ok(reset_timestamp) = reset_str.parse::<i64>() {
+                                        let now = Utc::now().timestamp();
+                                        let wait_seconds = (reset_timestamp - now).max(0) as u64;
+                                        return Err(DownloaderError::RateLimit(Duration::from_secs(wait_seconds)).into());
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+
+            if response.status() == StatusCode::UNAUTHORIZED {
+                return Err(DownloaderError::Auth("Invalid GitHub token".to_string()).into());
+            }
+
+            let response = response.error_for_status()?;
+            response.json::<T>().await.map_err(|e| e.into())
+        })
+        .await
+    }
+
+    async fn retry_with_backoff<F, Fut, T>(&self, mut operation: F) -> Result<T>
+    where
+        F: FnMut() -> Fut,
+        Fut: std::future::Future<Output = Result<T>>,
+    {
+        let mut attempt = 0;
+        let mut delay = Duration::from_millis(self.config.initial_retry_delay_ms);
+
+        loop {
+            match operation().await {
+                Ok(result) => return Ok(result),
+                Err(e) => {
+                    if let Some(rate_limit_err) = e.downcast_ref::<DownloaderError>() {
+                        if let DownloaderError::RateLimit(wait_duration) = rate_limit_err {
+                            if self.config.respect_rate_limits {
+                                warn!("Rate limit hit, waiting {:?}", wait_duration);
+                                sleep(*wait_duration).await;
+                                continue;
+                            }
+                        }
+                    }
+
+                    attempt += 1;
+                    if attempt >= self.config.max_retries {
+                        error!("Max retries ({}) exceeded", self.config.max_retries);
+                        return Err(e);
+                    }
+
+                    warn!("Request failed (attempt {}/{}): {}", attempt, self.config.max_retries, e);
+
+                    sleep(delay).await;
+
+                    delay = Duration::from_millis(
+                        (delay.as_millis() as u64 * 2).min(self.config.max_retry_delay_ms)
+                    );
+                }
+            }
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_extract_title() {
+        let downloader = create_test_downloader();
+
+        let content = "# Test Title\nSome content";
+        let title = downloader.extract_title("test.md", content);
+        assert_eq!(title, "Test Title");
+
+        let content_no_heading = "Some content without heading";
+        let title = downloader.extract_title("myfile.md", content_no_heading);
+        assert_eq!(title, "myfile");
+    }
+
+    #[test]
+    fn test_extract_properties() {
+        let downloader = create_test_downloader();
+
+        let content = r#"
+term-id:: 20067
+maturity:: mature
+tags:: ontology, test
+"#;
+
+        let props = downloader.extract_properties(content);
+        assert_eq!(props.get("term-id").unwrap()[0], "20067");
+        assert_eq!(props.get("maturity").unwrap()[0], "mature");
+        assert_eq!(props.get("tags").unwrap().len(), 2);
+    }
+
+    #[test]
+    fn test_extract_classes() {
+        let downloader = create_test_downloader();
+
+        let owl_content = r#"
+Declaration(Class(mv:Avatar))
+SubClassOf(mv:Avatar mv:VirtualEntity)
+"#;
+
+        let classes = downloader.extract_classes(owl_content);
+        assert!(classes.contains(&"mv:Avatar".to_string()));
+        assert!(classes.len() >= 1);
+    }
+
+    #[test]
+    fn test_extract_relationships() {
+        let downloader = create_test_downloader();
+
+        let owl_content = r#"
+SubClassOf(mv:Avatar mv:VirtualEntity)
+ObjectPropertyDomain(mv:hasProperty mv:Avatar)
+"#;
+
+        let relationships = downloader.extract_relationships(owl_content);
+        assert_eq!(relationships.len(), 2);
+        assert_eq!(relationships[0].relationship_type, RelationshipType::SubClassOf);
+        assert_eq!(relationships[1].relationship_type, RelationshipType::Domain);
+    }
+
+    #[test]
+    fn test_should_process_file() {
+        let downloader = create_test_downloader();
+
+        let content_with_public = r#"
+- ### OntologyBlock
+public:: true
+Some OWL content
+"#;
+        assert!(downloader.should_process_file(content_with_public));
+
+        let content_no_public = r#"
+- ### OntologyBlock
+Some OWL content
+"#;
+        assert!(!downloader.should_process_file(content_no_public));
+
+        let content_no_ontology = r#"
+public:: true
+Some regular content
+"#;
+        assert!(!downloader.should_process_file(content_no_ontology));
+    }
+
+    #[test]
+    fn test_config_from_token() {
+        let config = OntologyDownloaderConfig::with_token("test_token".to_string());
+        assert_eq!(config.github_token, "test_token");
+        assert_eq!(config.repo_owner, "jjohare");
+        assert_eq!(config.repo_name, "logseq");
+    }
+
+    #[test]
+    fn test_progress_percentage() {
+        let progress = DownloadProgress {
+            total_files: 100,
+            processed_files: 25,
+            ontology_blocks_found: 10,
+            errors: Vec::new(),
+            started_at: Utc::now(),
+            completed_at: None,
+            current_file: None,
+        };
+
+        assert_eq!(progress.percentage(), 25.0);
+    }
+
+    fn create_test_downloader() -> OntologyDownloader {
+        let config = OntologyDownloaderConfig::with_token("test_token".to_string());
+        OntologyDownloader::new(config).unwrap()
+    }
+}
diff --git a/src/services/ontology_init.rs b/src/services/ontology_init.rs
new file mode 100644
index 00000000..966d81dc
--- /dev/null
+++ b/src/services/ontology_init.rs
@@ -0,0 +1,176 @@
+//! Ontology System Initialization
+//!
+//! Handles complete initialization of the ontology system including:
+//! - SQLite database setup
+//! - GitHub repository scanning
+//! - Ontology data download and parsing
+//! - Database population
+//! - Integration with graph visualization
+
+use std::path::PathBuf;
+use std::sync::Arc;
+use tokio::time::Duration;
+use log::{info, error, warn, debug};
+
+#[cfg(feature = "ontology")]
+use crate::services::database_service::DatabaseService;
+#[cfg(feature = "ontology")]
+use crate::services::ontology_downloader::{OntologyDownloader, OntologyDownloaderConfig};
+#[cfg(feature = "ontology")]
+use crate::services::ontology_storage::OntologyStorage;
+#[cfg(feature = "ontology")]
+use crate::services::ontology_sync::OntologySync;
+
+/// Initialize the complete ontology system
+#[cfg(feature = "ontology")]
+pub async fn initialize_ontology_system() -> Result<Arc<DatabaseService>, String> {
+    info!("🔮 Initializing Ontology System");
+
+    // 1. Setup database path
+    let db_path = std::env::var("DATA_ROOT")
+        .unwrap_or_else(|_| "/app/data".to_string());
+    let db_file = PathBuf::from(db_path).join("ontology_db.sqlite3");
+
+    info!("📊 Database path: {}", db_file.display());
+
+    // 2. Initialize database with schema
+    let db_service = match DatabaseService::new(&db_file) {
+        Ok(service) => {
+            info!("✅ Database initialized successfully");
+            Arc::new(service)
+        }
+        Err(e) => {
+            error!("❌ Failed to initialize database: {}", e);
+            return Err(format!("Database initialization failed: {}", e));
+        }
+    };
+
+    // 3. Initialize database schema
+    match db_service.initialize_schema() {
+        Ok(_) => info!("✅ Database schema created/verified"),
+        Err(e) => {
+            error!("❌ Schema initialization failed: {}", e);
+            return Err(format!("Schema initialization failed: {}", e));
+        }
+    }
+
+    // 4. Run settings migration from YAML to SQLite (if not already done)
+    let migration_service = crate::services::settings_migration::SettingsMigration::new(Arc::clone(&db_service));
+    if !migration_service.is_migrated() {
+        info!("⚙️  Running settings migration from YAML to SQLite");
+        match migration_service.migrate_from_yaml_files() {
+            Ok(result) => {
+                info!("✅ Settings migration completed successfully");
+                info!("   📝 Settings migrated: {}", result.settings_migrated);
+                info!("   ⚡ Physics profiles: {}", result.physics_profiles_migrated);
+                info!("   🔧 Dev config params: {}", result.dev_config_params_migrated);
+                info!("   ⏱️  Duration: {:?}", result.duration);
+                if !result.errors.is_empty() {
+                    warn!("   ⚠️  Errors: {} (migration continues)", result.errors.len());
+                }
+            }
+            Err(e) => {
+                warn!("⚠️  Settings migration failed (continuing with defaults): {}", e);
+            }
+        }
+    } else {
+        info!("✅ Settings already migrated, skipping");
+    }
+
+    // 5. Initialize DevConfig from database
+    info!("🔧 Initializing DevConfig from database");
+    match crate::config::dev_config::DevConfig::initialize(Arc::clone(&db_service)) {
+        Ok(_) => {
+            info!("✅ DevConfig initialized successfully from database");
+        }
+        Err(e) => {
+            warn!("⚠️  DevConfig initialization failed (using defaults): {}", e);
+        }
+    }
+
+    // 6. Spawn background task for ontology download
+    let db_service_clone = Arc::clone(&db_service);
+    tokio::spawn(async move {
+        // Wait for server to fully start
+        info!("⏳ Waiting 10 seconds for server initialization before starting ontology sync");
+        tokio::time::sleep(Duration::from_secs(10)).await;
+
+        if let Err(e) = download_and_process_ontology(db_service_clone).await {
+            error!("❌ Ontology download/processing failed: {}", e);
+        }
+    });
+
+    Ok(db_service)
+}
+
+/// Download and process ontology data from GitHub
+#[cfg(feature = "ontology")]
+async fn download_and_process_ontology(db_service: Arc<DatabaseService>) -> Result<(), String> {
+    info!("🚀 Starting ontology download and processing");
+
+    // 1. Create downloader with configuration from environment
+    let github_token = std::env::var("GITHUB_TOKEN")
+        .map_err(|_| "GITHUB_TOKEN not set in environment".to_string())?;
+
+    let config = OntologyDownloaderConfig {
+        repo_owner: std::env::var("GITHUB_OWNER")
+            .unwrap_or_else(|_| "jjohare".to_string()),
+        repo_name: std::env::var("GITHUB_REPO")
+            .unwrap_or_else(|_| "logseq".to_string()),
+        base_path: std::env::var("GITHUB_BASE_PATH")
+            .unwrap_or_else(|_| "mainKnowledgeGraph/pages".to_string()),
+        github_token,
+        max_retries: 3,
+        initial_retry_delay_ms: 1000,
+        max_retry_delay_ms: 30000,
+        request_timeout_secs: 30,
+        respect_rate_limits: true,
+    };
+
+    // 2. Create storage (separate database for ontology blocks)
+    let ontology_db_path = std::env::var("DATA_ROOT")
+        .unwrap_or_else(|_| "/app/data".to_string());
+    let ontology_db_file = PathBuf::from(ontology_db_path).join("ontology_blocks.sqlite3");
+
+    let storage = OntologyStorage::new(&ontology_db_file)
+        .map_err(|e| format!("Failed to create storage: {}", e))?;
+
+    // 3. Create sync orchestrator
+    let sync = OntologySync::new(config, storage)
+        .map_err(|e| format!("Failed to create sync: {}", e))?;
+
+    // 4. Execute sync with progress tracking
+    info!("📥 Beginning ontology sync from GitHub");
+
+    match sync.sync().await {
+        Ok(result) => {
+            info!("✅ Ontology sync completed successfully!");
+            info!("   📥 Blocks downloaded: {}", result.blocks_downloaded);
+            info!("   📝 Blocks saved: {}", result.blocks_saved);
+            info!("   ⏱️  Duration: {} seconds", result.duration_seconds);
+
+            if !result.errors.is_empty() {
+                warn!("⚠️  {} errors occurred during sync:", result.errors.len());
+                for (idx, error) in result.errors.iter().enumerate().take(5) {
+                    warn!("   {}. {}", idx + 1, error);
+                }
+                if result.errors.len() > 5 {
+                    warn!("   ... and {} more errors", result.errors.len() - 5);
+                }
+            }
+
+            Ok(())
+        }
+        Err(e) => {
+            error!("❌ Ontology sync failed: {}", e);
+            Err(format!("Ontology sync failed: {}", e))
+        }
+    }
+}
+
+/// No-op initialization when ontology feature is disabled
+#[cfg(not(feature = "ontology"))]
+pub async fn initialize_ontology_system() -> Result<(), String> {
+    debug!("Ontology feature not enabled, skipping initialization");
+    Ok(())
+}
diff --git a/src/services/ontology_storage.rs b/src/services/ontology_storage.rs
new file mode 100644
index 00000000..5db8c0ab
--- /dev/null
+++ b/src/services/ontology_storage.rs
@@ -0,0 +1,754 @@
+use anyhow::{Context, Result};
+use chrono::{DateTime, Utc};
+use log::{debug, info};
+use rusqlite::{Connection, params, OptionalExtension};
+use std::path::Path;
+use std::sync::{Arc, Mutex};
+use thiserror::Error;
+
+use super::ontology_downloader::{OntologyBlock, OntologyRelationship, RelationshipType};
+
+#[derive(Error, Debug)]
+pub enum StorageError {
+    #[error("Database error: {0}")]
+    Database(#[from] rusqlite::Error),
+
+    #[error("Serialization error: {0}")]
+    Serialization(String),
+
+    #[error("Not found: {0}")]
+    NotFound(String),
+}
+
+pub struct OntologyStorage {
+    conn: Arc<Mutex<Connection>>,
+}
+
+impl OntologyStorage {
+    pub fn new<P: AsRef<Path>>(db_path: P) -> Result<Self> {
+        let conn = Connection::open(db_path)?;
+        let storage = Self {
+            conn: Arc::new(Mutex::new(conn)),
+        };
+
+        storage.initialize_schema()?;
+        Ok(storage)
+    }
+
+    pub fn in_memory() -> Result<Self> {
+        let conn = Connection::open_in_memory()?;
+        let storage = Self {
+            conn: Arc::new(Mutex::new(conn)),
+        };
+
+        storage.initialize_schema()?;
+        Ok(storage)
+    }
+
+    fn initialize_schema(&self) -> Result<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute_batch(
+            r#"
+            CREATE TABLE IF NOT EXISTS ontology_blocks (
+                id TEXT PRIMARY KEY,
+                source_file TEXT NOT NULL,
+                title TEXT NOT NULL,
+                content_hash TEXT NOT NULL,
+                downloaded_at TEXT NOT NULL,
+                created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
+                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_blocks_source_file
+                ON ontology_blocks(source_file);
+            CREATE INDEX IF NOT EXISTS idx_ontology_blocks_content_hash
+                ON ontology_blocks(content_hash);
+
+            CREATE TABLE IF NOT EXISTS ontology_properties (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                property_key TEXT NOT NULL,
+                property_value TEXT NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_properties_block_id
+                ON ontology_properties(block_id);
+            CREATE INDEX IF NOT EXISTS idx_ontology_properties_key
+                ON ontology_properties(property_key);
+
+            CREATE TABLE IF NOT EXISTS ontology_owl_content (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                content TEXT NOT NULL,
+                content_order INTEGER NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_owl_content_block_id
+                ON ontology_owl_content(block_id);
+
+            CREATE TABLE IF NOT EXISTS ontology_classes (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                class_name TEXT NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_classes_block_id
+                ON ontology_classes(block_id);
+            CREATE INDEX IF NOT EXISTS idx_ontology_classes_name
+                ON ontology_classes(class_name);
+
+            CREATE TABLE IF NOT EXISTS ontology_owl_properties (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                property_name TEXT NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_owl_properties_block_id
+                ON ontology_owl_properties(block_id);
+            CREATE INDEX IF NOT EXISTS idx_ontology_owl_properties_name
+                ON ontology_owl_properties(property_name);
+
+            CREATE TABLE IF NOT EXISTS ontology_relationships (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                block_id TEXT NOT NULL,
+                subject TEXT NOT NULL,
+                predicate TEXT NOT NULL,
+                object TEXT NOT NULL,
+                relationship_type TEXT NOT NULL,
+                FOREIGN KEY (block_id) REFERENCES ontology_blocks(id) ON DELETE CASCADE
+            );
+
+            CREATE INDEX IF NOT EXISTS idx_ontology_relationships_block_id
+                ON ontology_relationships(block_id);
+            CREATE INDEX IF NOT EXISTS idx_ontology_relationships_subject
+                ON ontology_relationships(subject);
+            CREATE INDEX IF NOT EXISTS idx_ontology_relationships_object
+                ON ontology_relationships(object);
+            CREATE INDEX IF NOT EXISTS idx_ontology_relationships_type
+                ON ontology_relationships(relationship_type);
+
+            CREATE TABLE IF NOT EXISTS sync_metadata (
+                key TEXT PRIMARY KEY,
+                value TEXT NOT NULL,
+                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
+            );
+            "#,
+        )?;
+
+        info!("Database schema initialized successfully");
+        Ok(())
+    }
+
+    pub fn save_block(&self, block: &OntologyBlock) -> Result<()> {
+        let conn = self.conn.lock().unwrap();
+
+        let tx = conn.unchecked_transaction()?;
+
+        tx.execute(
+            r#"
+            INSERT OR REPLACE INTO ontology_blocks
+                (id, source_file, title, content_hash, downloaded_at, updated_at)
+            VALUES (?1, ?2, ?3, ?4, ?5, ?6)
+            "#,
+            params![
+                &block.id,
+                &block.source_file,
+                &block.title,
+                &block.content_hash,
+                &block.downloaded_at.to_rfc3339(),
+                Utc::now().to_rfc3339(),
+            ],
+        )?;
+
+        tx.execute(
+            "DELETE FROM ontology_properties WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for (key, values) in &block.properties {
+            for value in values {
+                tx.execute(
+                    r#"
+                    INSERT INTO ontology_properties (block_id, property_key, property_value)
+                    VALUES (?1, ?2, ?3)
+                    "#,
+                    params![&block.id, key, value],
+                )?;
+            }
+        }
+
+        tx.execute(
+            "DELETE FROM ontology_owl_content WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for (idx, content) in block.owl_content.iter().enumerate() {
+            tx.execute(
+                r#"
+                INSERT INTO ontology_owl_content (block_id, content, content_order)
+                VALUES (?1, ?2, ?3)
+                "#,
+                params![&block.id, content, idx as i32],
+            )?;
+        }
+
+        tx.execute(
+            "DELETE FROM ontology_classes WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for class in &block.classes {
+            tx.execute(
+                r#"
+                INSERT INTO ontology_classes (block_id, class_name)
+                VALUES (?1, ?2)
+                "#,
+                params![&block.id, class],
+            )?;
+        }
+
+        tx.execute(
+            "DELETE FROM ontology_owl_properties WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for property in &block.properties_list {
+            tx.execute(
+                r#"
+                INSERT INTO ontology_owl_properties (block_id, property_name)
+                VALUES (?1, ?2)
+                "#,
+                params![&block.id, property],
+            )?;
+        }
+
+        tx.execute(
+            "DELETE FROM ontology_relationships WHERE block_id = ?1",
+            params![&block.id],
+        )?;
+
+        for rel in &block.relationships {
+            let rel_type = relationship_type_to_string(&rel.relationship_type);
+            tx.execute(
+                r#"
+                INSERT INTO ontology_relationships
+                    (block_id, subject, predicate, object, relationship_type)
+                VALUES (?1, ?2, ?3, ?4, ?5)
+                "#,
+                params![&block.id, &rel.subject, &rel.predicate, &rel.object, rel_type],
+            )?;
+        }
+
+        tx.commit()?;
+
+        debug!("Saved ontology block: {}", block.id);
+        Ok(())
+    }
+
+    pub fn save_blocks(&self, blocks: &[OntologyBlock]) -> Result<usize> {
+        let mut count = 0;
+        for block in blocks {
+            self.save_block(block)?;
+            count += 1;
+        }
+
+        info!("Saved {} ontology blocks to database", count);
+        Ok(count)
+    }
+
+    pub fn get_block(&self, id: &str) -> Result<Option<OntologyBlock>> {
+        let conn = self.conn.lock().unwrap();
+
+        let block_data: Option<(String, String, String, String, String)> = conn
+            .query_row(
+                r#"
+                SELECT id, source_file, title, content_hash, downloaded_at
+                FROM ontology_blocks
+                WHERE id = ?1
+                "#,
+                params![id],
+                |row| Ok((row.get(0)?, row.get(1)?, row.get(2)?, row.get(3)?, row.get(4)?)),
+            )
+            .optional()?;
+
+        if let Some((id, source_file, title, content_hash, downloaded_at)) = block_data {
+            let downloaded_at = DateTime::parse_from_rfc3339(&downloaded_at)?
+                .with_timezone(&Utc);
+
+            let properties = self.get_properties(&conn, &id)?;
+            let owl_content = self.get_owl_content(&conn, &id)?;
+            let classes = self.get_classes(&conn, &id)?;
+            let properties_list = self.get_owl_properties(&conn, &id)?;
+            let relationships = self.get_relationships(&conn, &id)?;
+
+            Ok(Some(OntologyBlock {
+                id,
+                source_file,
+                title,
+                properties,
+                owl_content,
+                classes,
+                properties_list,
+                relationships,
+                downloaded_at,
+                content_hash,
+            }))
+        } else {
+            Ok(None)
+        }
+    }
+
+    pub fn list_all_blocks(&self) -> Result<Vec<OntologyBlock>> {
+        let conn = self.conn.lock().unwrap();
+
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT id, source_file, title, content_hash, downloaded_at
+            FROM ontology_blocks
+            ORDER BY downloaded_at DESC
+            "#,
+        )?;
+
+        let block_ids: Vec<String> = stmt
+            .query_map([], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        drop(stmt);
+        drop(conn);
+
+        let mut blocks = Vec::new();
+        for id in block_ids {
+            if let Some(block) = self.get_block(&id)? {
+                blocks.push(block);
+            }
+        }
+
+        Ok(blocks)
+    }
+
+    pub fn search_by_class(&self, class_name: &str) -> Result<Vec<OntologyBlock>> {
+        let conn = self.conn.lock().unwrap();
+
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT DISTINCT block_id
+            FROM ontology_classes
+            WHERE class_name LIKE ?1
+            "#,
+        )?;
+
+        let pattern = format!("%{}%", class_name);
+        let block_ids: Vec<String> = stmt
+            .query_map(params![pattern], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        drop(stmt);
+        drop(conn);
+
+        let mut blocks = Vec::new();
+        for id in block_ids {
+            if let Some(block) = self.get_block(&id)? {
+                blocks.push(block);
+            }
+        }
+
+        Ok(blocks)
+    }
+
+    pub fn search_by_property(&self, property_key: &str) -> Result<Vec<OntologyBlock>> {
+        let conn = self.conn.lock().unwrap();
+
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT DISTINCT block_id
+            FROM ontology_properties
+            WHERE property_key = ?1
+            "#,
+        )?;
+
+        let block_ids: Vec<String> = stmt
+            .query_map(params![property_key], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        drop(stmt);
+        drop(conn);
+
+        let mut blocks = Vec::new();
+        for id in block_ids {
+            if let Some(block) = self.get_block(&id)? {
+                blocks.push(block);
+            }
+        }
+
+        Ok(blocks)
+    }
+
+    pub fn delete_block(&self, id: &str) -> Result<bool> {
+        let conn = self.conn.lock().unwrap();
+
+        let rows_affected = conn.execute(
+            "DELETE FROM ontology_blocks WHERE id = ?1",
+            params![id],
+        )?;
+
+        Ok(rows_affected > 0)
+    }
+
+    pub fn clear_all(&self) -> Result<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute_batch(
+            r#"
+            DELETE FROM ontology_relationships;
+            DELETE FROM ontology_owl_properties;
+            DELETE FROM ontology_classes;
+            DELETE FROM ontology_owl_content;
+            DELETE FROM ontology_properties;
+            DELETE FROM ontology_blocks;
+            DELETE FROM sync_metadata;
+            "#,
+        )?;
+
+        info!("Cleared all ontology data from database");
+        Ok(())
+    }
+
+    pub fn set_sync_metadata(&self, key: &str, value: &str) -> Result<()> {
+        let conn = self.conn.lock().unwrap();
+
+        conn.execute(
+            r#"
+            INSERT OR REPLACE INTO sync_metadata (key, value, updated_at)
+            VALUES (?1, ?2, ?3)
+            "#,
+            params![key, value, Utc::now().to_rfc3339()],
+        )?;
+
+        Ok(())
+    }
+
+    pub fn get_sync_metadata(&self, key: &str) -> Result<Option<String>> {
+        let conn = self.conn.lock().unwrap();
+
+        let value: Option<String> = conn
+            .query_row(
+                "SELECT value FROM sync_metadata WHERE key = ?1",
+                params![key],
+                |row| row.get(0),
+            )
+            .optional()?;
+
+        Ok(value)
+    }
+
+    pub fn get_statistics(&self) -> Result<DatabaseStatistics> {
+        let conn = self.conn.lock().unwrap();
+
+        let total_blocks: i64 = conn.query_row(
+            "SELECT COUNT(*) FROM ontology_blocks",
+            [],
+            |row| row.get(0),
+        )?;
+
+        let total_classes: i64 = conn.query_row(
+            "SELECT COUNT(DISTINCT class_name) FROM ontology_classes",
+            [],
+            |row| row.get(0),
+        )?;
+
+        let total_properties: i64 = conn.query_row(
+            "SELECT COUNT(DISTINCT property_name) FROM ontology_owl_properties",
+            [],
+            |row| row.get(0),
+        )?;
+
+        let total_relationships: i64 = conn.query_row(
+            "SELECT COUNT(*) FROM ontology_relationships",
+            [],
+            |row| row.get(0),
+        )?;
+
+        let last_sync: Option<String> = self.get_sync_metadata("last_sync_time")?;
+        let last_sync_time = last_sync
+            .and_then(|s| DateTime::parse_from_rfc3339(&s).ok())
+            .map(|dt| dt.with_timezone(&Utc));
+
+        Ok(DatabaseStatistics {
+            total_blocks: total_blocks as usize,
+            total_classes: total_classes as usize,
+            total_properties: total_properties as usize,
+            total_relationships: total_relationships as usize,
+            last_sync_time,
+        })
+    }
+
+    fn get_properties(
+        &self,
+        conn: &Connection,
+        block_id: &str,
+    ) -> Result<std::collections::HashMap<String, Vec<String>>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT property_key, property_value
+            FROM ontology_properties
+            WHERE block_id = ?1
+            "#,
+        )?;
+
+        let mut properties = std::collections::HashMap::new();
+        let rows = stmt.query_map(params![block_id], |row| {
+            Ok((row.get::<_, String>(0)?, row.get::<_, String>(1)?))
+        })?;
+
+        for row in rows {
+            let (key, value) = row?;
+            properties
+                .entry(key)
+                .or_insert_with(Vec::new)
+                .push(value);
+        }
+
+        Ok(properties)
+    }
+
+    fn get_owl_content(&self, conn: &Connection, block_id: &str) -> Result<Vec<String>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT content
+            FROM ontology_owl_content
+            WHERE block_id = ?1
+            ORDER BY content_order
+            "#,
+        )?;
+
+        let contents: Vec<String> = stmt
+            .query_map(params![block_id], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        Ok(contents)
+    }
+
+    fn get_classes(&self, conn: &Connection, block_id: &str) -> Result<Vec<String>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT class_name
+            FROM ontology_classes
+            WHERE block_id = ?1
+            "#,
+        )?;
+
+        let classes: Vec<String> = stmt
+            .query_map(params![block_id], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        Ok(classes)
+    }
+
+    fn get_owl_properties(&self, conn: &Connection, block_id: &str) -> Result<Vec<String>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT property_name
+            FROM ontology_owl_properties
+            WHERE block_id = ?1
+            "#,
+        )?;
+
+        let properties: Vec<String> = stmt
+            .query_map(params![block_id], |row| row.get(0))?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        Ok(properties)
+    }
+
+    fn get_relationships(
+        &self,
+        conn: &Connection,
+        block_id: &str,
+    ) -> Result<Vec<OntologyRelationship>> {
+        let mut stmt = conn.prepare(
+            r#"
+            SELECT subject, predicate, object, relationship_type
+            FROM ontology_relationships
+            WHERE block_id = ?1
+            "#,
+        )?;
+
+        let relationships: Vec<OntologyRelationship> = stmt
+            .query_map(params![block_id], |row| {
+                Ok(OntologyRelationship {
+                    subject: row.get(0)?,
+                    predicate: row.get(1)?,
+                    object: row.get(2)?,
+                    relationship_type: string_to_relationship_type(&row.get::<_, String>(3)?),
+                })
+            })?
+            .collect::<Result<Vec<_>, _>>()?;
+
+        Ok(relationships)
+    }
+}
+
+#[derive(Debug, Clone, serde::Serialize)]
+pub struct DatabaseStatistics {
+    pub total_blocks: usize,
+    pub total_classes: usize,
+    pub total_properties: usize,
+    pub total_relationships: usize,
+    pub last_sync_time: Option<DateTime<Utc>>,
+}
+
+fn relationship_type_to_string(rel_type: &RelationshipType) -> String {
+    match rel_type {
+        RelationshipType::SubClassOf => "SubClassOf".to_string(),
+        RelationshipType::ObjectProperty => "ObjectProperty".to_string(),
+        RelationshipType::DataProperty => "DataProperty".to_string(),
+        RelationshipType::DisjointWith => "DisjointWith".to_string(),
+        RelationshipType::EquivalentTo => "EquivalentTo".to_string(),
+        RelationshipType::InverseOf => "InverseOf".to_string(),
+        RelationshipType::Domain => "Domain".to_string(),
+        RelationshipType::Range => "Range".to_string(),
+        RelationshipType::Other(s) => s.clone(),
+    }
+}
+
+fn string_to_relationship_type(s: &str) -> RelationshipType {
+    match s {
+        "SubClassOf" => RelationshipType::SubClassOf,
+        "ObjectProperty" => RelationshipType::ObjectProperty,
+        "DataProperty" => RelationshipType::DataProperty,
+        "DisjointWith" => RelationshipType::DisjointWith,
+        "EquivalentTo" => RelationshipType::EquivalentTo,
+        "InverseOf" => RelationshipType::InverseOf,
+        "Domain" => RelationshipType::Domain,
+        "Range" => RelationshipType::Range,
+        _ => RelationshipType::Other(s.to_string()),
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use std::collections::HashMap;
+
+    #[test]
+    fn test_create_storage() {
+        let storage = OntologyStorage::in_memory().unwrap();
+        let stats = storage.get_statistics().unwrap();
+        assert_eq!(stats.total_blocks, 0);
+    }
+
+    #[test]
+    fn test_save_and_retrieve_block() {
+        let storage = OntologyStorage::in_memory().unwrap();
+
+        let mut properties = HashMap::new();
+        properties.insert("term-id".to_string(), vec!["123".to_string()]);
+
+        let block = OntologyBlock {
+            id: "test:block:1".to_string(),
+            source_file: "test.md".to_string(),
+            title: "Test Block".to_string(),
+            properties,
+            owl_content: vec!["Declaration(Class(test:Class))".to_string()],
+            classes: vec!["test:Class".to_string()],
+            properties_list: vec!["test:property".to_string()],
+            relationships: vec![OntologyRelationship {
+                subject: "test:A".to_string(),
+                predicate: "rdfs:subClassOf".to_string(),
+                object: "test:B".to_string(),
+                relationship_type: RelationshipType::SubClassOf,
+            }],
+            downloaded_at: Utc::now(),
+            content_hash: "abc123".to_string(),
+        };
+
+        storage.save_block(&block).unwrap();
+
+        let retrieved = storage.get_block("test:block:1").unwrap();
+        assert!(retrieved.is_some());
+
+        let retrieved_block = retrieved.unwrap();
+        assert_eq!(retrieved_block.id, "test:block:1");
+        assert_eq!(retrieved_block.title, "Test Block");
+        assert_eq!(retrieved_block.classes.len(), 1);
+        assert_eq!(retrieved_block.relationships.len(), 1);
+    }
+
+    #[test]
+    fn test_search_by_class() {
+        let storage = OntologyStorage::in_memory().unwrap();
+
+        let block = OntologyBlock {
+            id: "test:block:1".to_string(),
+            source_file: "test.md".to_string(),
+            title: "Test Block".to_string(),
+            properties: HashMap::new(),
+            owl_content: vec![],
+            classes: vec!["mv:Avatar".to_string()],
+            properties_list: vec![],
+            relationships: vec![],
+            downloaded_at: Utc::now(),
+            content_hash: "abc123".to_string(),
+        };
+
+        storage.save_block(&block).unwrap();
+
+        let results = storage.search_by_class("Avatar").unwrap();
+        assert_eq!(results.len(), 1);
+        assert_eq!(results[0].id, "test:block:1");
+    }
+
+    #[test]
+    fn test_statistics() {
+        let storage = OntologyStorage::in_memory().unwrap();
+
+        let block = OntologyBlock {
+            id: "test:block:1".to_string(),
+            source_file: "test.md".to_string(),
+            title: "Test Block".to_string(),
+            properties: HashMap::new(),
+            owl_content: vec![],
+            classes: vec!["test:Class1".to_string(), "test:Class2".to_string()],
+            properties_list: vec!["test:prop1".to_string()],
+            relationships: vec![],
+            downloaded_at: Utc::now(),
+            content_hash: "abc123".to_string(),
+        };
+
+        storage.save_block(&block).unwrap();
+
+        let stats = storage.get_statistics().unwrap();
+        assert_eq!(stats.total_blocks, 1);
+        assert_eq!(stats.total_classes, 2);
+        assert_eq!(stats.total_properties, 1);
+    }
+
+    #[test]
+    fn test_clear_all() {
+        let storage = OntologyStorage::in_memory().unwrap();
+
+        let block = OntologyBlock {
+            id: "test:block:1".to_string(),
+            source_file: "test.md".to_string(),
+            title: "Test Block".to_string(),
+            properties: HashMap::new(),
+            owl_content: vec![],
+            classes: vec![],
+            properties_list: vec![],
+            relationships: vec![],
+            downloaded_at: Utc::now(),
+            content_hash: "abc123".to_string(),
+        };
+
+        storage.save_block(&block).unwrap();
+        assert_eq!(storage.get_statistics().unwrap().total_blocks, 1);
+
+        storage.clear_all().unwrap();
+        assert_eq!(storage.get_statistics().unwrap().total_blocks, 0);
+    }
+}
diff --git a/src/services/ontology_sync.rs b/src/services/ontology_sync.rs
new file mode 100644
index 00000000..c4a699e7
--- /dev/null
+++ b/src/services/ontology_sync.rs
@@ -0,0 +1,93 @@
+use anyhow::Result;
+use chrono::Utc;
+use log::{info, warn};
+use std::sync::Arc;
+use tokio::sync::RwLock;
+
+use super::ontology_downloader::{OntologyDownloader, OntologyDownloaderConfig, DownloadProgress};
+use super::ontology_storage::{OntologyStorage, DatabaseStatistics};
+
+pub struct OntologySync {
+    downloader: OntologyDownloader,
+    storage: Arc<OntologyStorage>,
+}
+
+impl OntologySync {
+    pub fn new(
+        downloader_config: OntologyDownloaderConfig,
+        storage: OntologyStorage,
+    ) -> Result<Self> {
+        let downloader = OntologyDownloader::new(downloader_config)?;
+
+        Ok(Self {
+            downloader,
+            storage: Arc::new(storage),
+        })
+    }
+
+    pub async fn sync(&self) -> Result<SyncResult> {
+        info!("Starting ontology synchronization");
+
+        let start_time = Utc::now();
+
+        let blocks = self.downloader.download_all().await?;
+
+        let saved_count = self.storage.save_blocks(&blocks)?;
+
+        self.storage.set_sync_metadata("last_sync_time", &Utc::now().to_rfc3339())?;
+        self.storage.set_sync_metadata("last_sync_blocks", &saved_count.to_string())?;
+
+        let progress = self.downloader.get_progress().await;
+        let stats = self.storage.get_statistics()?;
+
+        let duration = Utc::now().signed_duration_since(start_time);
+
+        info!(
+            "Synchronization complete: {} blocks saved in {:?}",
+            saved_count, duration
+        );
+
+        Ok(SyncResult {
+            blocks_downloaded: blocks.len(),
+            blocks_saved: saved_count,
+            errors: progress.errors,
+            duration_seconds: duration.num_seconds() as u64,
+            statistics: stats,
+        })
+    }
+
+    pub async fn get_progress(&self) -> DownloadProgress {
+        self.downloader.get_progress().await
+    }
+
+    pub fn get_statistics(&self) -> Result<DatabaseStatistics> {
+        self.storage.get_statistics()
+    }
+
+    pub fn storage(&self) -> Arc<OntologyStorage> {
+        Arc::clone(&self.storage)
+    }
+}
+
+#[derive(Debug, Clone, serde::Serialize)]
+pub struct SyncResult {
+    pub blocks_downloaded: usize,
+    pub blocks_saved: usize,
+    pub errors: Vec<String>,
+    pub duration_seconds: u64,
+    pub statistics: DatabaseStatistics,
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[tokio::test]
+    async fn test_sync_creation() {
+        let config = OntologyDownloaderConfig::with_token("test_token".to_string());
+        let storage = OntologyStorage::in_memory().unwrap();
+        let sync = OntologySync::new(config, storage);
+
+        assert!(sync.is_ok());
+    }
+}
diff --git a/src/services/settings_migration.rs b/src/services/settings_migration.rs
new file mode 100644
index 00000000..b81030dd
--- /dev/null
+++ b/src/services/settings_migration.rs
@@ -0,0 +1,657 @@
+//! Settings Migration Service
+//!
+//! Migrates YAML-based settings to SQLite database with dual key format support.
+//! Supports both camelCase (client) and snake_case (server) key formats simultaneously.
+
+use std::path::Path;
+use std::collections::HashMap;
+use serde_yaml::Value as YamlValue;
+use log::{info, warn, error, debug};
+use rusqlite::{Connection, params, Result as SqliteResult};
+
+use crate::services::database_service::{DatabaseService, SettingValue};
+
+/// Settings migration service
+pub struct SettingsMigration {
+    db_service: std::sync::Arc<DatabaseService>,
+}
+
+impl SettingsMigration {
+    /// Create new migration service
+    pub fn new(db_service: std::sync::Arc<DatabaseService>) -> Self {
+        Self { db_service }
+    }
+
+    /// Execute complete migration from YAML files to database
+    pub fn migrate_from_yaml_files(&self) -> Result<MigrationResult, String> {
+        info!("Starting settings migration from YAML to SQLite");
+        let start_time = std::time::Instant::now();
+
+        let mut result = MigrationResult::default();
+
+        // Load and merge YAML files
+        let main_yaml_path = std::env::var("DATA_ROOT")
+            .unwrap_or_else(|_| "/app/data".to_string()) + "/settings.yaml";
+        let ontology_yaml_path = std::env::var("DATA_ROOT")
+            .unwrap_or_else(|_| "/app/data".to_string()) + "/settings_ontology_extension.yaml";
+
+        // Parse main settings
+        let main_settings = match self.load_yaml_file(&main_yaml_path) {
+            Ok(value) => {
+                info!("Loaded main settings from: {}", main_yaml_path);
+                value
+            }
+            Err(e) => {
+                error!("Failed to load main settings: {}", e);
+                return Err(format!("Failed to load main settings: {}", e));
+            }
+        };
+
+        // Parse ontology extension settings
+        let ontology_settings = match self.load_yaml_file(&ontology_yaml_path) {
+            Ok(value) => {
+                info!("Loaded ontology extension from: {}", ontology_yaml_path);
+                value
+            }
+            Err(e) => {
+                warn!("Failed to load ontology extension (continuing without it): {}", e);
+                YamlValue::Mapping(serde_yaml::Mapping::new())
+            }
+        };
+
+        // Merge settings
+        let merged = self.merge_yaml_values(vec![main_settings, ontology_settings]);
+
+        // Flatten YAML hierarchy into key-value pairs
+        let flattened = self.flatten_yaml(&merged, "");
+        info!("Flattened {} settings keys", flattened.len());
+
+        // Migrate each setting with dual key format
+        for (key, value) in flattened.iter() {
+            match self.migrate_setting(key, value) {
+                Ok(_) => {
+                    result.settings_migrated += 1;
+                }
+                Err(e) => {
+                    error!("Failed to migrate setting '{}': {}", key, e);
+                    result.errors.push(format!("Setting '{}': {}", key, e));
+                }
+            }
+        }
+
+        // Extract and migrate physics settings
+        match self.migrate_physics_profiles(&merged) {
+            Ok(count) => {
+                info!("Migrated {} physics profiles", count);
+                result.physics_profiles_migrated = count;
+            }
+            Err(e) => {
+                error!("Failed to migrate physics profiles: {}", e);
+                result.errors.push(format!("Physics profiles: {}", e));
+            }
+        }
+
+        // Migrate dev_config.toml
+        match self.migrate_dev_config_to_sqlite() {
+            Ok(count) => {
+                info!("Migrated {} dev_config parameters", count);
+                result.dev_config_params_migrated = count;
+            }
+            Err(e) => {
+                error!("Failed to migrate dev_config.toml: {}", e);
+                result.errors.push(format!("Dev config: {}", e));
+            }
+        }
+
+        result.duration = start_time.elapsed();
+        info!("Migration completed in {:?}", result.duration);
+        info!("  Settings migrated: {}", result.settings_migrated);
+        info!("  Physics profiles: {}", result.physics_profiles_migrated);
+        info!("  Dev config params: {}", result.dev_config_params_migrated);
+        if !result.errors.is_empty() {
+            warn!("  Errors encountered: {}", result.errors.len());
+        }
+
+        Ok(result)
+    }
+
+    /// Load YAML file from disk
+    fn load_yaml_file(&self, path: &str) -> Result<YamlValue, String> {
+        let contents = std::fs::read_to_string(path)
+            .map_err(|e| format!("Failed to read file: {}", e))?;
+
+        serde_yaml::from_str(&contents)
+            .map_err(|e| format!("Failed to parse YAML: {}", e))
+    }
+
+    /// Merge multiple YAML values (deep merge)
+    fn merge_yaml_values(&self, values: Vec<YamlValue>) -> YamlValue {
+        let mut result = YamlValue::Mapping(serde_yaml::Mapping::new());
+
+        for value in values {
+            if let YamlValue::Mapping(map) = value {
+                if let YamlValue::Mapping(result_map) = &mut result {
+                    for (k, v) in map {
+                        result_map.insert(k, v);
+                    }
+                }
+            }
+        }
+
+        result
+    }
+
+    /// Flatten nested YAML into hierarchical keys
+    fn flatten_yaml(&self, value: &YamlValue, prefix: &str) -> HashMap<String, YamlValue> {
+        let mut result = HashMap::new();
+
+        match value {
+            YamlValue::Mapping(map) => {
+                for (key, val) in map {
+                    if let Some(key_str) = key.as_str() {
+                        let new_prefix = if prefix.is_empty() {
+                            key_str.to_string()
+                        } else {
+                            format!("{}.{}", prefix, key_str)
+                        };
+
+                        match val {
+                            YamlValue::Mapping(_) => {
+                                // Recursively flatten nested objects
+                                let nested = self.flatten_yaml(val, &new_prefix);
+                                result.extend(nested);
+                            }
+                            YamlValue::Sequence(_) => {
+                                // Store arrays as JSON
+                                result.insert(new_prefix, val.clone());
+                            }
+                            _ => {
+                                // Store primitive values
+                                result.insert(new_prefix, val.clone());
+                            }
+                        }
+                    }
+                }
+            }
+            _ => {
+                // Non-mapping root value
+                if !prefix.is_empty() {
+                    result.insert(prefix.to_string(), value.clone());
+                }
+            }
+        }
+
+        result
+    }
+
+    /// Migrate a single setting to database
+    fn migrate_setting(&self, key: &str, value: &YamlValue) -> Result<(), String> {
+        // Convert YAML value to SettingValue
+        let setting_value = self.yaml_to_setting_value(value)?;
+
+        // Store with camelCase key only
+        self.db_service.set_setting(key, setting_value, None)
+            .map_err(|e| format!("Failed to store setting: {}", e))?;
+
+        debug!("Migrated: {}", key);
+        Ok(())
+    }
+
+    /// Convert YAML value to SettingValue
+    fn yaml_to_setting_value(&self, value: &YamlValue) -> Result<SettingValue, String> {
+        match value {
+            YamlValue::String(s) => Ok(SettingValue::String(s.clone())),
+            YamlValue::Number(n) => {
+                if let Some(i) = n.as_i64() {
+                    Ok(SettingValue::Integer(i))
+                } else if let Some(f) = n.as_f64() {
+                    Ok(SettingValue::Float(f))
+                } else {
+                    Err("Unsupported number type".to_string())
+                }
+            }
+            YamlValue::Bool(b) => Ok(SettingValue::Boolean(*b)),
+            YamlValue::Sequence(_) | YamlValue::Mapping(_) => {
+                // Convert to JSON for complex types
+                let json = serde_json::to_value(value)
+                    .map_err(|e| format!("Failed to convert to JSON: {}", e))?;
+                Ok(SettingValue::Json(json))
+            }
+            YamlValue::Null => Ok(SettingValue::String("".to_string())),
+            _ => Err("Unsupported YAML type".to_string()),
+        }
+    }
+
+    /// Convert hierarchical key to snake_case
+    fn to_snake_case_key(&self, key: &str) -> String {
+        key.split('.')
+            .map(|part| Self::to_snake_case_part(part))
+            .collect::<Vec<_>>()
+            .join(".")
+    }
+
+    /// Convert hierarchical key to camelCase
+    fn to_camel_case_key(&self, key: &str) -> String {
+        key.split('.')
+            .map(|part| Self::to_camel_case_part(part))
+            .collect::<Vec<_>>()
+            .join(".")
+    }
+
+    /// Convert a single part to snake_case
+    fn to_snake_case_part(s: &str) -> String {
+        let mut result = String::new();
+        let mut prev_is_upper = false;
+
+        for (i, ch) in s.chars().enumerate() {
+            if ch.is_uppercase() {
+                if i > 0 && !prev_is_upper {
+                    result.push('_');
+                }
+                result.push(ch.to_lowercase().next().unwrap());
+                prev_is_upper = true;
+            } else {
+                result.push(ch);
+                prev_is_upper = false;
+            }
+        }
+        result
+    }
+
+    /// Convert a single part to camelCase
+    fn to_camel_case_part(s: &str) -> String {
+        let mut result = String::new();
+        let mut capitalize_next = false;
+        let mut first = true;
+
+        for ch in s.chars() {
+            if ch == '_' {
+                capitalize_next = true;
+            } else if capitalize_next {
+                result.push(ch.to_uppercase().next().unwrap());
+                capitalize_next = false;
+                first = false;
+            } else {
+                if first {
+                    result.push(ch.to_lowercase().next().unwrap());
+                    first = false;
+                } else {
+                    result.push(ch);
+                }
+            }
+        }
+        result
+    }
+
+    /// Migrate physics settings profiles
+    fn migrate_physics_profiles(&self, yaml: &YamlValue) -> Result<usize, String> {
+        let mut count = 0;
+
+        // Extract physics settings from nested structure
+        // Path: visualisation.graphs.{profile}.physics
+        if let Some(vis) = yaml.get("visualisation") {
+            if let Some(graphs) = vis.get("graphs") {
+                if let YamlValue::Mapping(graphs_map) = graphs {
+                    for (profile_name, profile_config) in graphs_map {
+                        if let Some(profile_name_str) = profile_name.as_str() {
+                            if let Some(physics) = profile_config.get("physics") {
+                                match self.migrate_physics_profile(profile_name_str, physics) {
+                                    Ok(_) => {
+                                        count += 1;
+                                        debug!("Migrated physics profile: {}", profile_name_str);
+                                    }
+                                    Err(e) => {
+                                        warn!("Failed to migrate physics profile '{}': {}",
+                                              profile_name_str, e);
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        }
+
+        Ok(count)
+    }
+
+    /// Migrate a single physics profile
+    fn migrate_physics_profile(&self, profile_name: &str, physics: &YamlValue) -> Result<(), String> {
+        use crate::config::PhysicsSettings;
+
+        // Extract physics settings from YAML
+        let settings = PhysicsSettings {
+            damping: self.get_f32(physics, "damping").unwrap_or(0.95),
+            dt: self.get_f32(physics, "dt").unwrap_or(0.016),
+            iterations: self.get_u32(physics, "iterations").unwrap_or(100),
+            max_velocity: self.get_f32(physics, "maxVelocity").unwrap_or(1.0),
+            max_force: self.get_f32(physics, "maxForce").unwrap_or(100.0),
+            repel_k: self.get_f32(physics, "repelK").unwrap_or(50.0),
+            spring_k: self.get_f32(physics, "springK").unwrap_or(0.005),
+            mass_scale: self.get_f32(physics, "massScale").unwrap_or(1.0),
+            boundary_damping: self.get_f32(physics, "boundaryDamping").unwrap_or(0.95),
+            temperature: self.get_f32(physics, "temperature").unwrap_or(0.01),
+            gravity: self.get_f32(physics, "gravity").unwrap_or(0.0001),
+            bounds_size: self.get_f32(physics, "boundsSize").unwrap_or(500.0),
+            enable_bounds: self.get_bool(physics, "enableBounds").unwrap_or(false),
+            rest_length: self.get_f32(physics, "restLength").unwrap_or(50.0),
+            repulsion_cutoff: self.get_f32(physics, "repulsionCutoff").unwrap_or(50.0),
+            repulsion_softening_epsilon: self.get_f32(physics, "repulsionSofteningEpsilon").unwrap_or(0.0001),
+            center_gravity_k: self.get_f32(physics, "centerGravityK").unwrap_or(0.0),
+            grid_cell_size: self.get_f32(physics, "gridCellSize").unwrap_or(50.0),
+            warmup_iterations: self.get_u32(physics, "warmupIterations").unwrap_or(100),
+            cooling_rate: self.get_f32(physics, "coolingRate").unwrap_or(0.001),
+            constraint_ramp_frames: self.get_u32(physics, "constraintRampFrames").unwrap_or(60),
+            constraint_max_force_per_node: self.get_f32(physics, "constraintMaxForcePerNode").unwrap_or(50.0),
+            ..PhysicsSettings::default()
+        };
+
+        self.db_service.save_physics_settings(profile_name, &settings)
+            .map_err(|e| format!("Database error: {}", e))
+    }
+
+    /// Helper to get f32 from YAML
+    fn get_f32(&self, yaml: &YamlValue, key: &str) -> Option<f32> {
+        yaml.get(key)?.as_f64().map(|v| v as f32)
+    }
+
+    /// Helper to get u32 from YAML
+    fn get_u32(&self, yaml: &YamlValue, key: &str) -> Option<u32> {
+        yaml.get(key)?.as_i64().map(|v| v as u32)
+    }
+
+    /// Helper to get bool from YAML
+    fn get_bool(&self, yaml: &YamlValue, key: &str) -> Option<bool> {
+        yaml.get(key)?.as_bool()
+    }
+
+    /// Check if migration has been run
+    pub fn is_migrated(&self) -> bool {
+        // Check if migrated settings exist (use full settings key instead of version)
+        match self.db_service.get_setting("app_full_settings") {
+            Ok(Some(_)) => true,
+            _ => false,
+        }
+    }
+
+    /// Rollback migration (delete all settings)
+    pub fn rollback(&self) -> Result<(), String> {
+        warn!("Rolling back settings migration - this will delete all settings!");
+
+        // This would require direct database access
+        // For now, we'll just log a warning
+        warn!("Rollback not fully implemented - manual database cleanup required");
+
+        Ok(())
+    }
+
+    /// Migrate dev_config.toml to SQLite database
+    pub fn migrate_dev_config_to_sqlite(&self) -> Result<usize, String> {
+        info!("Starting dev_config.toml migration to SQLite");
+
+        let dev_config_path = "data/dev_config.toml";
+
+        // Check if file exists
+        if !std::path::Path::new(dev_config_path).exists() {
+            warn!("dev_config.toml not found, skipping migration");
+            return Ok(0);
+        }
+
+        // Load TOML file
+        let content = std::fs::read_to_string(dev_config_path)
+            .map_err(|e| format!("Failed to read dev_config.toml: {}", e))?;
+
+        let toml_value: toml::Value = toml::from_str(&content)
+            .map_err(|e| format!("Failed to parse dev_config.toml: {}", e))?;
+
+        let mut count = 0;
+
+        // Migrate each section
+        if let toml::Value::Table(table) = toml_value {
+            // Physics section (32 parameters)
+            if let Some(toml::Value::Table(physics)) = table.get("physics") {
+                count += self.migrate_toml_section("dev.physics", physics)?;
+            }
+
+            // CUDA section (11 parameters)
+            if let Some(toml::Value::Table(cuda)) = table.get("cuda") {
+                count += self.migrate_toml_section("dev.cuda", cuda)?;
+            }
+
+            // Network section (13 parameters)
+            if let Some(toml::Value::Table(network)) = table.get("network") {
+                count += self.migrate_toml_section("dev.network", network)?;
+            }
+
+            // Rendering section (with nested agent_colors)
+            if let Some(toml::Value::Table(rendering)) = table.get("rendering") {
+                // Separate agent_colors from other rendering params
+                let mut rendering_copy = rendering.clone();
+
+                // Handle nested agent_colors
+                if let Some(toml::Value::Table(agent_colors)) = rendering_copy.remove("agent_colors") {
+                    count += self.migrate_toml_section("dev.rendering.agent_colors", &agent_colors)?;
+                }
+
+                // Migrate remaining rendering params
+                count += self.migrate_toml_section("dev.rendering", &rendering_copy)?;
+            }
+
+            // Performance section (11 parameters)
+            if let Some(toml::Value::Table(performance)) = table.get("performance") {
+                count += self.migrate_toml_section("dev.performance", performance)?;
+            }
+
+            // Debug section (8 parameters)
+            if let Some(toml::Value::Table(debug)) = table.get("debug") {
+                count += self.migrate_toml_section("dev.debug", debug)?;
+            }
+        }
+
+        info!("Migrated {} dev_config parameters to database", count);
+        Ok(count)
+    }
+
+    /// Migrate a TOML section to database with hierarchical keys
+    fn migrate_toml_section(&self, prefix: &str, table: &toml::map::Map<String, toml::Value>) -> Result<usize, String> {
+        let mut count = 0;
+
+        for (key, value) in table {
+            let full_key = format!("{}.{}", prefix, key);
+
+            // Convert TOML value to SettingValue
+            let setting_value = self.toml_to_setting_value(value)?;
+
+            // Store with camelCase key only
+            self.db_service.set_setting(&full_key, setting_value, None)
+                .map_err(|e| format!("Failed to store key '{}': {}", full_key, e))?;
+
+            count += 1;
+            debug!("Migrated dev_config parameter: {}", full_key);
+        }
+
+        Ok(count)
+    }
+
+    /// Convert TOML value to SettingValue
+    fn toml_to_setting_value(&self, value: &toml::Value) -> Result<SettingValue, String> {
+        match value {
+            toml::Value::String(s) => Ok(SettingValue::String(s.clone())),
+            toml::Value::Integer(i) => Ok(SettingValue::Integer(*i)),
+            toml::Value::Float(f) => Ok(SettingValue::Float(*f)),
+            toml::Value::Boolean(b) => Ok(SettingValue::Boolean(*b)),
+            toml::Value::Array(_) | toml::Value::Table(_) => {
+                // Convert complex types to JSON
+                let json = serde_json::to_value(value)
+                    .map_err(|e| format!("Failed to convert TOML to JSON: {}", e))?;
+                Ok(SettingValue::Json(json))
+            }
+            toml::Value::Datetime(dt) => Ok(SettingValue::String(dt.to_string())),
+        }
+    }
+}
+
+/// Key format converter utility
+pub struct KeyFormatConverter;
+
+impl KeyFormatConverter {
+    /// Convert any key between camelCase and snake_case
+    pub fn to_snake_case(key: &str) -> String {
+        key.split('.')
+            .map(|part| Self::to_snake_case_part(part))
+            .collect::<Vec<_>>()
+            .join(".")
+    }
+
+    /// Convert any key to camelCase
+    pub fn to_camel_case(key: &str) -> String {
+        key.split('.')
+            .map(|part| Self::to_camel_case_part(part))
+            .collect::<Vec<_>>()
+            .join(".")
+    }
+
+    /// Get both key formats for a given key
+    pub fn both_formats(key: &str) -> (String, String) {
+        (Self::to_camel_case(key), Self::to_snake_case(key))
+    }
+
+    /// Convert a single part to snake_case
+    fn to_snake_case_part(s: &str) -> String {
+        let mut result = String::new();
+        let mut prev_is_upper = false;
+
+        for (i, ch) in s.chars().enumerate() {
+            if ch.is_uppercase() {
+                if i > 0 && !prev_is_upper {
+                    result.push('_');
+                }
+                result.push(ch.to_lowercase().next().unwrap());
+                prev_is_upper = true;
+            } else {
+                result.push(ch);
+                prev_is_upper = false;
+            }
+        }
+        result
+    }
+
+    /// Convert a single part to camelCase
+    fn to_camel_case_part(s: &str) -> String {
+        let mut result = String::new();
+        let mut capitalize_next = false;
+        let mut first = true;
+
+        for ch in s.chars() {
+            if ch == '_' {
+                capitalize_next = true;
+            } else if capitalize_next {
+                result.push(ch.to_uppercase().next().unwrap());
+                capitalize_next = false;
+                first = false;
+            } else {
+                if first {
+                    result.push(ch.to_lowercase().next().unwrap());
+                    first = false;
+                } else {
+                    result.push(ch);
+                }
+            }
+        }
+        result
+    }
+}
+
+/// Migration result statistics
+#[derive(Debug, Default)]
+pub struct MigrationResult {
+    pub settings_migrated: usize,
+    pub physics_profiles_migrated: usize,
+    pub dev_config_params_migrated: usize,
+    pub errors: Vec<String>,
+    pub duration: std::time::Duration,
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_key_conversion() {
+        assert_eq!(
+            KeyFormatConverter::to_snake_case("visualisation.graphs.logseq.nodes.baseColor"),
+            "visualisation.graphs.logseq.nodes.base_color"
+        );
+
+        assert_eq!(
+            KeyFormatConverter::to_camel_case("visualisation.graphs.logseq.nodes.base_color"),
+            "visualisation.graphs.logseq.nodes.baseColor"
+        );
+    }
+
+    #[test]
+    fn test_both_formats() {
+        let (camel, snake) = KeyFormatConverter::both_formats("visualisation.enableBounds");
+        assert_eq!(camel, "visualisation.enableBounds");
+        assert_eq!(snake, "visualisation.enable_bounds");
+    }
+
+    #[test]
+    fn test_yaml_flattening() {
+        let yaml_str = r#"
+        root:
+          nested:
+            value: 42
+            flag: true
+          array: [1, 2, 3]
+        "#;
+
+        let yaml: YamlValue = serde_yaml::from_str(yaml_str).unwrap();
+        let db = std::sync::Arc::new(
+            DatabaseService::new(":memory:").unwrap()
+        );
+        let migration = SettingsMigration::new(db);
+
+        let flattened = migration.flatten_yaml(&yaml, "");
+
+        assert!(flattened.contains_key("root.nested.value"));
+        assert!(flattened.contains_key("root.nested.flag"));
+        assert!(flattened.contains_key("root.array"));
+    }
+
+    #[test]
+    fn test_yaml_to_setting_value() {
+        let db = std::sync::Arc::new(
+            DatabaseService::new(":memory:").unwrap()
+        );
+        let migration = SettingsMigration::new(db);
+
+        // String
+        let yaml = YamlValue::String("test".to_string());
+        match migration.yaml_to_setting_value(&yaml).unwrap() {
+            SettingValue::String(s) => assert_eq!(s, "test"),
+            _ => panic!("Expected String"),
+        }
+
+        // Integer
+        let yaml = serde_yaml::to_value(42).unwrap();
+        match migration.yaml_to_setting_value(&yaml).unwrap() {
+            SettingValue::Integer(i) => assert_eq!(i, 42),
+            _ => panic!("Expected Integer"),
+        }
+
+        // Float
+        let yaml = serde_yaml::to_value(3.14).unwrap();
+        match migration.yaml_to_setting_value(&yaml).unwrap() {
+            SettingValue::Float(f) => assert!((f - 3.14).abs() < 0.001),
+            _ => panic!("Expected Float"),
+        }
+
+        // Boolean
+        let yaml = YamlValue::Bool(true);
+        match migration.yaml_to_setting_value(&yaml).unwrap() {
+            SettingValue::Boolean(b) => assert!(b),
+            _ => panic!("Expected Boolean"),
+        }
+    }
+}
diff --git a/src/services/settings_service.rs b/src/services/settings_service.rs
index 3e0fb1fd..eb9fa0c4 100644
--- a/src/services/settings_service.rs
+++ b/src/services/settings_service.rs
@@ -1,24 +1,21 @@
-// src/services/settings_service.rs
-//! Settings service providing high-level API over DatabaseService
-//!
-//! This service:
-//! - Provides async API for settings access
-//! - Handles validation before database writes
-//! - Manages in-memory cache with TTL
-//! - Broadcasts change notifications to listeners
-//! - Uses camelCase format (database handles snake_case fallback)
+// SQLite-backed Settings Service
+// Replaces file-based settings with database-backed configuration
+// Supports user-specific overrides and hierarchical settings

 use std::collections::HashMap;
 use std::sync::Arc;
 use tokio::sync::RwLock;
-use log::{debug, error, info};
+use serde_json::Value as JsonValue;
+use log::{info, error, debug, warn};

 use crate::services::database_service::{DatabaseService, SettingValue};
+use crate::services::settings_validator::{SettingsValidator, ValidationResult};
 use crate::config::{AppFullSettings, PhysicsSettings};

 #[derive(Clone)]
 pub struct SettingsService {
     db: Arc<DatabaseService>,
+    validator: Arc<SettingsValidator>,
     cache: Arc<RwLock<SettingsCache>>,
     change_listeners: Arc<RwLock<Vec<ChangeListener>>>,
 }
@@ -35,13 +32,22 @@ struct CachedSetting {
     timestamp: std::time::Instant,
 }

-type ChangeListener = Arc<dyn Fn(&str, &SettingValue) + Send + Sync>;
+type ChangeListener = Arc<dyn Fn(&str, &SettingValue, Option<&str>) + Send + Sync>;
+
+#[derive(Debug, Clone)]
+pub struct SettingsTreeNode {
+    pub key: String,
+    pub value: Option<SettingValue>,
+    pub children: HashMap<String, SettingsTreeNode>,
+}

 impl SettingsService {
-    /// Create new settings service
     pub fn new(db: Arc<DatabaseService>) -> Result<Self, String> {
+        let validator = Arc::new(SettingsValidator::new());
+
         Ok(Self {
             db,
+            validator,
             cache: Arc::new(RwLock::new(SettingsCache {
                 settings: HashMap::new(),
                 last_updated: std::time::Instant::now(),
@@ -52,6 +58,8 @@ impl SettingsService {

     /// Get setting by key (uses camelCase format, database handles snake_case fallback)
     pub async fn get_setting(&self, key: &str) -> Result<Option<SettingValue>, String> {
+        // Use key as-is (camelCase) - database service has smart lookup with fallback
+
         // Check cache first
         {
             let cache = self.cache.read().await;
@@ -82,12 +90,25 @@ impl SettingsService {
         }
     }

-    /// Set setting value
+    /// Set setting value with permission check
     pub async fn set_setting(
         &self,
         key: &str,
         value: SettingValue,
+        user_id: Option<&str>,
     ) -> Result<(), String> {
+        // Use key as-is (camelCase format) - no normalization needed
+
+        // Validate the setting
+        let validation = self.validator.validate_setting(key, &value)?;
+        if !validation.is_valid {
+            return Err(format!(
+                "Validation failed for {}: {}",
+                key,
+                validation.errors.join(", ")
+            ));
+        }
+
         // Store in database (camelCase format)
         self.db.set_setting(key, value.clone(), None)
             .map_err(|e| format!("Database error: {}", e))?;
@@ -96,102 +117,260 @@ impl SettingsService {
         {
             let mut cache = self.cache.write().await;
             cache.settings.remove(key);
+            cache.last_updated = std::time::Instant::now();
         }

         // Notify listeners
-        self.notify_change(key, &value).await;
+        self.notify_change(key, &value, user_id).await;

+        info!("Setting updated: {} by user {:?}", key, user_id);
         Ok(())
     }

-    /// Get batch of settings by keys
-    pub async fn get_settings_batch(&self, keys: &[String]) -> Result<HashMap<String, SettingValue>, String> {
-        let mut results = HashMap::new();
+    /// Get settings tree by prefix
+    pub async fn get_settings_tree(&self, prefix: &str) -> Result<SettingsTreeNode, String> {
+        // Use prefix as-is (camelCase format)
+        let all_settings = self.list_all_settings().await?;

-        for key in keys {
-            if let Some(value) = self.get_setting(key).await? {
-                results.insert(key.clone(), value);
+        let mut root = SettingsTreeNode {
+            key: prefix.to_string(),
+            value: None,
+            children: HashMap::new(),
+        };
+
+        // Build tree from flat settings
+        for (key, value) in all_settings {
+            if key.starts_with(prefix) {
+                self.insert_into_tree(&mut root, &key, value, prefix);
             }
         }

-        Ok(results)
+        Ok(root)
     }

-    /// Set batch of settings atomically
-    pub async fn set_settings_batch(&self, updates: HashMap<String, SettingValue>) -> Result<(), String> {
-        for (key, value) in updates {
-            self.set_setting(&key, value).await?;
+    fn insert_into_tree(
+        &self,
+        node: &mut SettingsTreeNode,
+        key: &str,
+        value: SettingValue,
+        prefix: &str,
+    ) {
+        let relative_key = if key.starts_with(prefix) {
+            &key[prefix.len()..].trim_start_matches('.')
+        } else {
+            key
+        };
+
+        let parts: Vec<&str> = relative_key.split('.').collect();
+        if parts.is_empty() {
+            return;
         }

-        Ok(())
+        let mut current = node;
+        for (i, part) in parts.iter().enumerate() {
+            if i == parts.len() - 1 {
+                // Leaf node
+                current.children.insert(
+                    part.to_string(),
+                    SettingsTreeNode {
+                        key: key.to_string(),
+                        value: Some(value.clone()),
+                        children: HashMap::new(),
+                    },
+                );
+            } else {
+                // Intermediate node
+                current = current
+                    .children
+                    .entry(part.to_string())
+                    .or_insert_with(|| SettingsTreeNode {
+                        key: format!("{}.{}", prefix, parts[..=i].join(".")),
+                        value: None,
+                        children: HashMap::new(),
+                    });
+            }
+        }
     }

-    /// Load complete settings from database
-    pub fn load_all_settings(&self) -> Result<Option<AppFullSettings>, String> {
-        self.db.load_all_settings()
+    /// Get physics profile
+    pub async fn get_physics_profile(&self, profile_name: &str) -> Result<PhysicsSettings, String> {
+        self.db
+            .get_physics_settings(profile_name)
             .map_err(|e| format!("Database error: {}", e))
     }

-    /// Save complete settings to database
-    /// IMPORTANT: This preserves separate graph configurations (logseq, visionflow)
-    /// DO NOT conflate graphs - each maintains its own settings
-    pub fn save_all_settings(&self, settings: &AppFullSettings) -> Result<(), String> {
-        self.db.save_all_settings(settings)
-            .map_err(|e| format!("Database error: {}", e))
+    /// Update physics profile with permission check
+    pub async fn update_physics_profile(
+        &self,
+        profile_name: &str,
+        params: PhysicsSettings,
+        user_id: Option<&str>,
+    ) -> Result<(), String> {
+        // Validate physics settings
+        let validation = self.validator.validate_physics_settings(&params)?;
+        if !validation.is_valid {
+            return Err(format!(
+                "Physics validation failed: {}",
+                validation.errors.join(", ")
+            ));
+        }
+
+        // Save to database
+        self.db
+            .save_physics_settings(profile_name, &params)
+            .map_err(|e| format!("Database error: {}", e))?;
+
+        info!(
+            "Physics profile {} updated by user {:?}",
+            profile_name, user_id
+        );
+        Ok(())
     }

-    /// Get physics settings for a specific graph profile
-    /// CRITICAL: Maintains separation between logseq and visionflow physics
-    pub fn get_physics_settings(&self, graph_name: &str) -> Result<PhysicsSettings, String> {
-        self.db.get_physics_settings(graph_name)
-            .map_err(|e| format!("Database error: {}", e))
+    /// List all settings (for power users)
+    pub async fn list_all_settings(&self) -> Result<HashMap<String, SettingValue>, String> {
+        // For now, return a simple implementation
+        // In production, this would query all settings from the database
+        let cache = self.cache.read().await;
+        let mut result = HashMap::new();
+
+        for (key, cached) in cache.settings.iter() {
+            result.insert(key.clone(), cached.value.clone());
+        }
+
+        Ok(result)
     }

-    /// Save physics settings for a specific graph profile
-    /// CRITICAL: Ensures graph-specific physics settings don't leak across graphs
-    pub fn save_physics_settings(&self, graph_name: &str, settings: &PhysicsSettings) -> Result<(), String> {
-        info!("Saving physics settings for graph: {}", graph_name);
-        self.db.save_physics_settings(graph_name, settings)
-            .map_err(|e| format!("Database error: {}", e))
+    /// Search settings by pattern
+    pub async fn search_settings(&self, pattern: &str) -> Result<Vec<(String, SettingValue)>, String> {
+        let all_settings = self.list_all_settings().await?;
+        let pattern_lower = pattern.to_lowercase();
+
+        let results: Vec<(String, SettingValue)> = all_settings
+            .into_iter()
+            .filter(|(key, _)| key.to_lowercase().contains(&pattern_lower))
+            .collect();
+
+        Ok(results)
     }

-    /// Clear all cached settings
-    pub async fn clear_cache(&self) {
-        let mut cache = self.cache.write().await;
-        cache.settings.clear();
-        cache.last_updated = std::time::Instant::now();
-        info!("Settings cache cleared");
+    /// Reset setting to default
+    pub async fn reset_to_default(&self, key: &str, user_id: Option<&str>) -> Result<(), String> {
+        // Use key as-is (camelCase format)
+
+        // Get default value from AppFullSettings
+        let defaults = AppFullSettings::default();
+        let default_value = self.extract_default_value(&defaults, key)?;
+
+        self.set_setting(key, default_value, user_id).await
     }

-    /// Add change listener for settings updates
-    pub async fn add_change_listener<F>(&self, listener: F)
+    /// Register change listener for WebSocket broadcasts
+    pub async fn register_change_listener<F>(&self, listener: F)
     where
-        F: Fn(&str, &SettingValue) + Send + Sync + 'static,
+        F: Fn(&str, &SettingValue, Option<&str>) + Send + Sync + 'static,
     {
         let mut listeners = self.change_listeners.write().await;
         listeners.push(Arc::new(listener));
     }

     /// Notify all listeners of a setting change
-    async fn notify_change(&self, key: &str, value: &SettingValue) {
+    async fn notify_change(&self, key: &str, value: &SettingValue, user_id: Option<&str>) {
         let listeners = self.change_listeners.read().await;
         for listener in listeners.iter() {
-            listener(key, value);
+            listener(key, value, user_id);
         }
     }

-    /// Get cache statistics
-    pub async fn get_cache_stats(&self) -> CacheStats {
-        let cache = self.cache.read().await;
-        CacheStats {
-            entries: cache.settings.len(),
-            last_updated: cache.last_updated.elapsed().as_secs(),
+    /// Extract default value from AppFullSettings
+    fn extract_default_value(
+        &self,
+        defaults: &AppFullSettings,
+        key: &str,
+    ) -> Result<SettingValue, String> {
+        // Convert settings to JSON and extract the value
+        let json = serde_json::to_value(defaults)
+            .map_err(|e| format!("Failed to serialize defaults: {}", e))?;
+
+        let parts: Vec<&str> = key.split('.').collect();
+        let mut current = &json;
+
+        for part in parts {
+            match current.get(part) {
+                Some(v) => current = v,
+                None => {
+                    return Err(format!("Key not found in defaults: {}", key));
+                }
+            }
+        }
+
+        // Convert JSON value to SettingValue
+        match current {
+            JsonValue::String(s) => Ok(SettingValue::String(s.clone())),
+            JsonValue::Number(n) => {
+                if let Some(i) = n.as_i64() {
+                    Ok(SettingValue::Integer(i))
+                } else if let Some(f) = n.as_f64() {
+                    Ok(SettingValue::Float(f))
+                } else {
+                    Err("Invalid number type".to_string())
+                }
+            }
+            JsonValue::Bool(b) => Ok(SettingValue::Boolean(*b)),
+            JsonValue::Object(_) | JsonValue::Array(_) => {
+                Ok(SettingValue::Json(current.clone()))
+            }
+            JsonValue::Null => Err("Cannot reset to null value".to_string()),
         }
     }
+
+    /// Clear cache
+    pub async fn clear_cache(&self) {
+        let mut cache = self.cache.write().await;
+        cache.settings.clear();
+        cache.last_updated = std::time::Instant::now();
+        info!("Settings cache cleared");
+    }
+
+    /// Warm cache with common settings
+    pub async fn warm_cache(&self) {
+        let common_keys = vec![
+            "visualisation.graphs.logseq.physics",
+            "visualisation.rendering",
+            "system",
+        ];
+
+        for key in common_keys {
+            if let Err(e) = self.get_setting(key).await {
+                warn!("Failed to warm cache for {}: {}", key, e);
+            }
+        }
+
+        info!("Settings cache warmed");
+    }
 }

-#[derive(Debug)]
-pub struct CacheStats {
-    pub entries: usize,
-    pub last_updated: u64,
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[tokio::test]
+    async fn test_camel_case_keys() {
+        // Verify that settings service uses camelCase keys directly
+        let db = Arc::new(DatabaseService::new(":memory:").unwrap());
+        db.initialize_schema().unwrap();
+
+        let service = SettingsService::new(db).unwrap();
+
+        // Set a camelCase setting
+        service.set_setting(
+            "testSetting",
+            SettingValue::String("test_value".to_string()),
+            None
+        ).await.unwrap();
+
+        // Should retrieve with same camelCase key
+        let value = service.get_setting("testSetting").await.unwrap();
+        assert!(value.is_some());
+    }
 }
diff --git a/src/services/settings_validator.rs b/src/services/settings_validator.rs
new file mode 100644
index 00000000..b93d1826
--- /dev/null
+++ b/src/services/settings_validator.rs
@@ -0,0 +1,590 @@
+// Settings Validation Layer
+// Schema-based validation for all settings categories
+
+use std::collections::HashMap;
+use serde_json::Value as JsonValue;
+
+use crate::services::database_service::SettingValue;
+use crate::config::PhysicsSettings;
+
+#[derive(Debug, Clone)]
+pub struct ValidationResult {
+    pub is_valid: bool,
+    pub errors: Vec<String>,
+    pub warnings: Vec<String>,
+}
+
+impl ValidationResult {
+    pub fn valid() -> Self {
+        Self {
+            is_valid: true,
+            errors: Vec::new(),
+            warnings: Vec::new(),
+        }
+    }
+
+    pub fn invalid(error: String) -> Self {
+        Self {
+            is_valid: false,
+            errors: vec![error],
+            warnings: Vec::new(),
+        }
+    }
+
+    pub fn with_warning(mut self, warning: String) -> Self {
+        self.warnings.push(warning);
+        self
+    }
+}
+
+pub struct SettingsValidator {
+    rules: HashMap<String, ValidationRule>,
+}
+
+#[derive(Clone)]
+struct ValidationRule {
+    value_type: ValueType,
+    min: Option<f64>,
+    max: Option<f64>,
+    allowed_values: Option<Vec<String>>,
+    pattern: Option<String>,
+    required: bool,
+}
+
+#[derive(Clone, PartialEq)]
+enum ValueType {
+    Float,
+    Integer,
+    Boolean,
+    String,
+    Object,
+    Array,
+}
+
+impl SettingsValidator {
+    pub fn new() -> Self {
+        let mut rules = HashMap::new();
+
+        // Visualization settings rules
+        Self::register_visualization_rules(&mut rules);
+
+        // Physics settings rules
+        Self::register_physics_rules(&mut rules);
+
+        // System settings rules
+        Self::register_system_rules(&mut rules);
+
+        // Ontology settings rules
+        Self::register_ontology_rules(&mut rules);
+
+        Self { rules }
+    }
+
+    fn register_visualization_rules(rules: &mut HashMap<String, ValidationRule>) {
+        // Rendering settings
+        rules.insert(
+            "visualisation.rendering.ambient_light_intensity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(2.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.rendering.environment_intensity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(2.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.rendering.enable_shadows".to_string(),
+            ValidationRule {
+                value_type: ValueType::Boolean,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        // Glow settings
+        rules.insert(
+            "visualisation.glow.opacity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.glow.intensity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(5.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        // Node settings
+        rules.insert(
+            "visualisation.graphs.logseq.nodes.base_size".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.1),
+                max: Some(100.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+    }
+
+    fn register_physics_rules(rules: &mut HashMap<String, ValidationRule>) {
+        rules.insert(
+            "visualisation.graphs.logseq.physics.damping".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.dt".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.001),
+                max: Some(0.1),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.iterations".to_string(),
+            ValidationRule {
+                value_type: ValueType::Integer,
+                min: Some(1.0),
+                max: Some(10000.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.max_velocity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.1),
+                max: Some(100.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.max_force".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.1),
+                max: Some(1000.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.repel_k".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1000.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.spring_k".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(10.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.gravity".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.temperature".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(0.0),
+                max: Some(1.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.bounds_size".to_string(),
+            ValidationRule {
+                value_type: ValueType::Float,
+                min: Some(100.0),
+                max: Some(10000.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "visualisation.graphs.logseq.physics.enabled".to_string(),
+            ValidationRule {
+                value_type: ValueType::Boolean,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+    }
+
+    fn register_system_rules(rules: &mut HashMap<String, ValidationRule>) {
+        rules.insert(
+            "system.port".to_string(),
+            ValidationRule {
+                value_type: ValueType::Integer,
+                min: Some(1024.0),
+                max: Some(65535.0),
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "system.host".to_string(),
+            ValidationRule {
+                value_type: ValueType::String,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+
+        rules.insert(
+            "system.persist_settings".to_string(),
+            ValidationRule {
+                value_type: ValueType::Boolean,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: true,
+            },
+        );
+    }
+
+    fn register_ontology_rules(rules: &mut HashMap<String, ValidationRule>) {
+        rules.insert(
+            "ontology.reasoner.engine".to_string(),
+            ValidationRule {
+                value_type: ValueType::String,
+                min: None,
+                max: None,
+                allowed_values: Some(vec![
+                    "horned-owl".to_string(),
+                    "whelk".to_string(),
+                    "custom".to_string(),
+                ]),
+                pattern: None,
+                required: false,
+            },
+        );
+
+        rules.insert(
+            "ontology.gpu.enabled".to_string(),
+            ValidationRule {
+                value_type: ValueType::Boolean,
+                min: None,
+                max: None,
+                allowed_values: None,
+                pattern: None,
+                required: false,
+            },
+        );
+
+        rules.insert(
+            "ontology.validation.mode".to_string(),
+            ValidationRule {
+                value_type: ValueType::String,
+                min: None,
+                max: None,
+                allowed_values: Some(vec![
+                    "quick".to_string(),
+                    "full".to_string(),
+                    "incremental".to_string(),
+                ]),
+                pattern: None,
+                required: false,
+            },
+        );
+    }
+
+    /// Validate a single setting
+    pub fn validate_setting(&self, key: &str, value: &SettingValue) -> Result<ValidationResult, String> {
+        // Find matching rule
+        let rule = match self.rules.get(key) {
+            Some(r) => r,
+            None => {
+                // No explicit rule, use basic type checking
+                return Ok(ValidationResult::valid().with_warning(format!(
+                    "No validation rule found for key: {}",
+                    key
+                )));
+            }
+        };
+
+        // Type validation
+        if !self.validate_type(value, &rule.value_type) {
+            return Ok(ValidationResult::invalid(format!(
+                "Type mismatch for {}: expected {:?}, got {:?}",
+                key, rule.value_type, value
+            )));
+        }
+
+        // Range validation for numeric types
+        if let Some(min) = rule.min {
+            if let Some(num) = self.extract_number(value) {
+                if num < min {
+                    return Ok(ValidationResult::invalid(format!(
+                        "Value {} is below minimum {} for {}",
+                        num, min, key
+                    )));
+                }
+            }
+        }
+
+        if let Some(max) = rule.max {
+            if let Some(num) = self.extract_number(value) {
+                if num > max {
+                    return Ok(ValidationResult::invalid(format!(
+                        "Value {} is above maximum {} for {}",
+                        num, max, key
+                    )));
+                }
+            }
+        }
+
+        // Allowed values validation
+        if let Some(allowed) = &rule.allowed_values {
+            if let SettingValue::String(s) = value {
+                if !allowed.contains(s) {
+                    return Ok(ValidationResult::invalid(format!(
+                        "Value '{}' is not in allowed values for {}: {:?}",
+                        s, key, allowed
+                    )));
+                }
+            }
+        }
+
+        Ok(ValidationResult::valid())
+    }
+
+    /// Validate physics settings
+    pub fn validate_physics_settings(&self, settings: &PhysicsSettings) -> Result<ValidationResult, String> {
+        let mut result = ValidationResult::valid();
+
+        // Validate damping
+        if settings.damping < 0.0 || settings.damping > 1.0 {
+            result.is_valid = false;
+            result
+                .errors
+                .push(format!("Damping must be between 0.0 and 1.0, got {}", settings.damping));
+        }
+
+        // Validate dt
+        if settings.dt < 0.001 || settings.dt > 0.1 {
+            result.is_valid = false;
+            result
+                .errors
+                .push(format!("dt must be between 0.001 and 0.1, got {}", settings.dt));
+        }
+
+        // Validate iterations
+        if settings.iterations < 1 || settings.iterations > 10000 {
+            result.is_valid = false;
+            result.errors.push(format!(
+                "Iterations must be between 1 and 10000, got {}",
+                settings.iterations
+            ));
+        }
+
+        // Validate max_velocity
+        if settings.max_velocity <= 0.0 {
+            result.is_valid = false;
+            result.errors.push(format!(
+                "Max velocity must be positive, got {}",
+                settings.max_velocity
+            ));
+        }
+
+        // Validate repel_k and spring_k (must be non-negative)
+        if settings.repel_k < 0.0 {
+            result.is_valid = false;
+            result
+                .errors
+                .push(format!("Repel_k must be non-negative, got {}", settings.repel_k));
+        }
+
+        if settings.spring_k < 0.0 {
+            result.is_valid = false;
+            result.errors.push(format!(
+                "Spring_k must be non-negative, got {}",
+                settings.spring_k
+            ));
+        }
+
+        // Cross-field validation: temperature and cooling_rate
+        if settings.temperature > 0.5 && settings.cooling_rate < 0.0001 {
+            result.warnings.push(
+                "High temperature with low cooling rate may cause instability".to_string(),
+            );
+        }
+
+        // Constraint validation
+        if settings.constraint_max_force_per_node <= 0.0 {
+            result.is_valid = false;
+            result.errors.push(format!(
+                "Constraint max force must be positive, got {}",
+                settings.constraint_max_force_per_node
+            ));
+        }
+
+        Ok(result)
+    }
+
+    fn validate_type(&self, value: &SettingValue, expected: &ValueType) -> bool {
+        match (value, expected) {
+            (SettingValue::Float(_), ValueType::Float) => true,
+            (SettingValue::Integer(_), ValueType::Integer) => true,
+            (SettingValue::Boolean(_), ValueType::Boolean) => true,
+            (SettingValue::String(_), ValueType::String) => true,
+            (SettingValue::Json(v), ValueType::Object) => v.is_object(),
+            (SettingValue::Json(v), ValueType::Array) => v.is_array(),
+            _ => false,
+        }
+    }
+
+    fn extract_number(&self, value: &SettingValue) -> Option<f64> {
+        match value {
+            SettingValue::Float(f) => Some(*f),
+            SettingValue::Integer(i) => Some(*i as f64),
+            _ => None,
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_validate_physics_damping() {
+        let validator = SettingsValidator::new();
+
+        // Valid damping
+        let result = validator
+            .validate_setting(
+                "visualisation.graphs.logseq.physics.damping",
+                &SettingValue::Float(0.95),
+            )
+            .unwrap();
+        assert!(result.is_valid);
+
+        // Invalid damping (too high)
+        let result = validator
+            .validate_setting(
+                "visualisation.graphs.logseq.physics.damping",
+                &SettingValue::Float(1.5),
+            )
+            .unwrap();
+        assert!(!result.is_valid);
+
+        // Invalid damping (negative)
+        let result = validator
+            .validate_setting(
+                "visualisation.graphs.logseq.physics.damping",
+                &SettingValue::Float(-0.1),
+            )
+            .unwrap();
+        assert!(!result.is_valid);
+    }
+
+    #[test]
+    fn test_validate_port() {
+        let validator = SettingsValidator::new();
+
+        // Valid port
+        let result = validator
+            .validate_setting("system.port", &SettingValue::Integer(8080))
+            .unwrap();
+        assert!(result.is_valid);
+
+        // Invalid port (too low)
+        let result = validator
+            .validate_setting("system.port", &SettingValue::Integer(80))
+            .unwrap();
+        assert!(!result.is_valid);
+
+        // Invalid port (too high)
+        let result = validator
+            .validate_setting("system.port", &SettingValue::Integer(70000))
+            .unwrap();
+        assert!(!result.is_valid);
+    }
+}
diff --git a/src/services/user_service.rs b/src/services/user_service.rs
new file mode 100644
index 00000000..364ed57d
--- /dev/null
+++ b/src/services/user_service.rs
@@ -0,0 +1,471 @@
+use rusqlite::{params, Connection, Result as SqliteResult};
+use serde::{Deserialize, Serialize};
+use serde_json::Value as JsonValue;
+use std::sync::Arc;
+use tokio::sync::RwLock;
+use chrono::Utc;
+use thiserror::Error;
+use log::{debug, error, info, warn};
+
+#[derive(Debug, Error)]
+pub enum UserServiceError {
+    #[error("Database error: {0}")]
+    DatabaseError(String),
+    #[error("User not found")]
+    UserNotFound,
+    #[error("Invalid setting value")]
+    InvalidSettingValue,
+    #[error("Permission denied")]
+    PermissionDenied,
+    #[error("User already exists")]
+    UserAlreadyExists,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct User {
+    pub id: i64,
+    pub nostr_pubkey: String,
+    pub username: Option<String>,
+    pub is_power_user: bool,
+    pub created_at: i64,
+    pub last_seen: i64,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+#[serde(tag = "type", content = "value")]
+pub enum SettingValue {
+    String(String),
+    Integer(i64),
+    Float(f64),
+    Boolean(bool),
+    Json(JsonValue),
+}
+
+impl SettingValue {
+    fn value_type(&self) -> &'static str {
+        match self {
+            SettingValue::String(_) => "string",
+            SettingValue::Integer(_) => "integer",
+            SettingValue::Float(_) => "float",
+            SettingValue::Boolean(_) => "boolean",
+            SettingValue::Json(_) => "json",
+        }
+    }
+
+    fn to_json(&self) -> String {
+        match self {
+            SettingValue::String(s) => s.clone(),
+            SettingValue::Integer(i) => i.to_string(),
+            SettingValue::Float(f) => f.to_string(),
+            SettingValue::Boolean(b) => b.to_string(),
+            SettingValue::Json(v) => serde_json::to_string(v).unwrap_or_default(),
+        }
+    }
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct UserSetting {
+    pub id: i64,
+    pub user_id: i64,
+    pub key: String,
+    pub value: SettingValue,
+    pub created_at: i64,
+    pub updated_at: i64,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct AuditLogEntry {
+    pub id: i64,
+    pub user_id: Option<i64>,
+    pub key: String,
+    pub old_value: Option<String>,
+    pub new_value: Option<String>,
+    pub action: String,
+    pub timestamp: i64,
+}
+
+pub struct UserService {
+    db_path: String,
+    conn: Arc<RwLock<Connection>>,
+}
+
+impl UserService {
+    pub async fn new(db_path: String) -> Result<Self, UserServiceError> {
+        let conn = Connection::open(&db_path)
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        Ok(Self {
+            db_path,
+            conn: Arc::new(RwLock::new(conn)),
+        })
+    }
+
+    pub async fn get_user_by_nostr_pubkey(&self, pubkey: &str) -> Result<User, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare("SELECT id, nostr_pubkey, username, is_power_user, strftime('%s', created_at) as created_at, strftime('%s', last_seen) as last_seen FROM users WHERE nostr_pubkey = ?1")
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let user = stmt
+            .query_row(params![pubkey], |row| {
+                Ok(User {
+                    id: row.get(0)?,
+                    nostr_pubkey: row.get(1)?,
+                    username: row.get(2)?,
+                    is_power_user: row.get::<_, i64>(3)? == 1,
+                    created_at: row.get::<_, String>(4)?.parse().unwrap_or(0),
+                    last_seen: row.get::<_, String>(5)?.parse().unwrap_or(0),
+                })
+            })
+            .map_err(|_| UserServiceError::UserNotFound)?;
+
+        Ok(user)
+    }
+
+    pub async fn get_user_by_id(&self, user_id: i64) -> Result<User, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare("SELECT id, nostr_pubkey, username, is_power_user, strftime('%s', created_at) as created_at, strftime('%s', last_seen) as last_seen FROM users WHERE id = ?1")
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let user = stmt
+            .query_row(params![user_id], |row| {
+                Ok(User {
+                    id: row.get(0)?,
+                    nostr_pubkey: row.get(1)?,
+                    username: row.get(2)?,
+                    is_power_user: row.get::<_, i64>(3)? == 1,
+                    created_at: row.get::<_, String>(4)?.parse().unwrap_or(0),
+                    last_seen: row.get::<_, String>(5)?.parse().unwrap_or(0),
+                })
+            })
+            .map_err(|_| UserServiceError::UserNotFound)?;
+
+        Ok(user)
+    }
+
+    pub async fn create_or_update_user(
+        &self,
+        pubkey: &str,
+        username: Option<String>,
+    ) -> Result<User, UserServiceError> {
+        let mut conn = self.conn.write().await;
+        let now = Utc::now().timestamp();
+
+        let tx = conn.transaction()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let result = tx.execute(
+            "INSERT INTO users (nostr_pubkey, username, created_at, last_seen)
+             VALUES (?1, ?2, datetime('now'), datetime('now'))
+             ON CONFLICT(nostr_pubkey) DO UPDATE SET
+             username = COALESCE(?2, username),
+             last_seen = datetime('now')",
+            params![pubkey, username],
+        );
+
+        match result {
+            Ok(_) => {
+                let user: User = tx.query_row(
+                    "SELECT id, nostr_pubkey, username, is_power_user, strftime('%s', created_at) as created_at, strftime('%s', last_seen) as last_seen FROM users WHERE nostr_pubkey = ?1",
+                    params![pubkey],
+                    |row| {
+                        Ok(User {
+                            id: row.get(0)?,
+                            nostr_pubkey: row.get(1)?,
+                            username: row.get(2)?,
+                            is_power_user: row.get::<_, i64>(3)? == 1,
+                            created_at: row.get::<_, String>(4)?.parse().unwrap_or(0),
+                            last_seen: row.get::<_, String>(5)?.parse().unwrap_or(0),
+                        })
+                    }
+                ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+                tx.commit().map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+                info!("Created/updated user: pubkey={}, id={}", pubkey, user.id);
+                Ok(user)
+            }
+            Err(e) => {
+                error!("Failed to create/update user {}: {}", pubkey, e);
+                Err(UserServiceError::DatabaseError(e.to_string()))
+            }
+        }
+    }
+
+    pub async fn is_power_user(&self, user_id: i64) -> Result<bool, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare("SELECT is_power_user FROM users WHERE id = ?1")
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let is_power = stmt
+            .query_row(params![user_id], |row| {
+                let val: i64 = row.get(0)?;
+                Ok(val == 1)
+            })
+            .map_err(|_| UserServiceError::UserNotFound)?;
+
+        Ok(is_power)
+    }
+
+    pub async fn grant_power_user(&self, user_id: i64) -> Result<(), UserServiceError> {
+        let mut conn = self.conn.write().await;
+        conn.execute(
+            "UPDATE users SET is_power_user = 1 WHERE id = ?1",
+            params![user_id],
+        )
+        .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        info!("Granted power user to user_id={}", user_id);
+        Ok(())
+    }
+
+    pub async fn revoke_power_user(&self, user_id: i64) -> Result<(), UserServiceError> {
+        let mut conn = self.conn.write().await;
+        conn.execute(
+            "UPDATE users SET is_power_user = 0 WHERE id = ?1",
+            params![user_id],
+        )
+        .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        info!("Revoked power user from user_id={}", user_id);
+        Ok(())
+    }
+
+    pub async fn get_user_settings(&self, user_id: i64) -> Result<Vec<UserSetting>, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare(
+                "SELECT id, user_id, key, value_type, value_text, value_integer, value_float, value_boolean, value_json,
+                 strftime('%s', created_at) as created_at, strftime('%s', updated_at) as updated_at
+                 FROM user_settings WHERE user_id = ?1"
+            )
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let settings = stmt
+            .query_map(params![user_id], |row| {
+                let value_type: String = row.get(3)?;
+                let value = match value_type.as_str() {
+                    "string" => SettingValue::String(row.get(4)?),
+                    "integer" => SettingValue::Integer(row.get(5)?),
+                    "float" => SettingValue::Float(row.get(6)?),
+                    "boolean" => SettingValue::Boolean(row.get::<_, i64>(7)? == 1),
+                    "json" => {
+                        let json_str: String = row.get(8)?;
+                        let json_val: JsonValue = serde_json::from_str(&json_str).unwrap_or(JsonValue::Null);
+                        SettingValue::Json(json_val)
+                    }
+                    _ => SettingValue::String(String::new()),
+                };
+
+                Ok(UserSetting {
+                    id: row.get(0)?,
+                    user_id: row.get(1)?,
+                    key: row.get(2)?,
+                    value,
+                    created_at: row.get::<_, String>(9)?.parse().unwrap_or(0),
+                    updated_at: row.get::<_, String>(10)?.parse().unwrap_or(0),
+                })
+            })
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?
+            .collect::<Result<Vec<_>, _>>()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        Ok(settings)
+    }
+
+    pub async fn set_user_setting(
+        &self,
+        user_id: i64,
+        key: &str,
+        value: SettingValue,
+    ) -> Result<(), UserServiceError> {
+        let mut conn = self.conn.write().await;
+        let tx = conn.transaction()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let old_value: Option<String> = tx
+            .query_row(
+                "SELECT value_text, value_integer, value_float, value_boolean, value_json, value_type FROM user_settings WHERE user_id = ?1 AND key = ?2",
+                params![user_id, key],
+                |row| {
+                    let value_type: String = row.get(5)?;
+                    let val = match value_type.as_str() {
+                        "string" => row.get::<_, String>(0).ok(),
+                        "integer" => row.get::<_, i64>(1).ok().map(|v| v.to_string()),
+                        "float" => row.get::<_, f64>(2).ok().map(|v| v.to_string()),
+                        "boolean" => row.get::<_, i64>(3).ok().map(|v| (v == 1).to_string()),
+                        "json" => row.get::<_, String>(4).ok(),
+                        _ => None,
+                    };
+                    Ok(val)
+                }
+            )
+            .ok()
+            .flatten();
+
+        let (value_text, value_integer, value_float, value_boolean, value_json) = match &value {
+            SettingValue::String(s) => (Some(s.clone()), None, None, None, None),
+            SettingValue::Integer(i) => (None, Some(*i), None, None, None),
+            SettingValue::Float(f) => (None, None, Some(*f), None, None),
+            SettingValue::Boolean(b) => (None, None, None, Some(if *b { 1 } else { 0 }), None),
+            SettingValue::Json(j) => (None, None, None, None, Some(serde_json::to_string(j).unwrap_or_default())),
+        };
+
+        let action = if old_value.is_some() { "update" } else { "create" };
+
+        tx.execute(
+            "INSERT INTO user_settings (user_id, key, value_type, value_text, value_integer, value_float, value_boolean, value_json, created_at, updated_at)
+             VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, datetime('now'), datetime('now'))
+             ON CONFLICT(user_id, key) DO UPDATE SET
+             value_type = ?3,
+             value_text = ?4,
+             value_integer = ?5,
+             value_float = ?6,
+             value_boolean = ?7,
+             value_json = ?8,
+             updated_at = datetime('now')",
+            params![user_id, key, value.value_type(), value_text, value_integer, value_float, value_boolean, value_json],
+        ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        tx.execute(
+            "INSERT INTO settings_audit_log (user_id, key, old_value, new_value, action, timestamp)
+             VALUES (?1, ?2, ?3, ?4, ?5, datetime('now'))",
+            params![user_id, key, old_value, value.to_json(), action],
+        ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        tx.commit().map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        info!("Set user setting: user_id={}, key={}, action={}", user_id, key, action);
+        Ok(())
+    }
+
+    pub async fn delete_user_setting(&self, user_id: i64, key: &str) -> Result<(), UserServiceError> {
+        let mut conn = self.conn.write().await;
+        let tx = conn.transaction()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let old_value: Option<String> = tx
+            .query_row(
+                "SELECT value_text, value_integer, value_float, value_boolean, value_json, value_type FROM user_settings WHERE user_id = ?1 AND key = ?2",
+                params![user_id, key],
+                |row| {
+                    let value_type: String = row.get(5)?;
+                    let val = match value_type.as_str() {
+                        "string" => row.get::<_, String>(0).ok(),
+                        "integer" => row.get::<_, i64>(1).ok().map(|v| v.to_string()),
+                        "float" => row.get::<_, f64>(2).ok().map(|v| v.to_string()),
+                        "boolean" => row.get::<_, i64>(3).ok().map(|v| (v == 1).to_string()),
+                        "json" => row.get::<_, String>(4).ok(),
+                        _ => None,
+                    };
+                    Ok(val)
+                }
+            )
+            .ok()
+            .flatten();
+
+        tx.execute(
+            "DELETE FROM user_settings WHERE user_id = ?1 AND key = ?2",
+            params![user_id, key],
+        ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        tx.execute(
+            "INSERT INTO settings_audit_log (user_id, key, old_value, new_value, action, timestamp)
+             VALUES (?1, ?2, ?3, NULL, 'delete', datetime('now'))",
+            params![user_id, key, old_value],
+        ).map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        tx.commit().map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        info!("Deleted user setting: user_id={}, key={}", user_id, key);
+        Ok(())
+    }
+
+    pub async fn get_audit_log(
+        &self,
+        key: Option<String>,
+        user_id: Option<i64>,
+        limit: i64,
+    ) -> Result<Vec<AuditLogEntry>, UserServiceError> {
+        let conn = self.conn.read().await;
+
+        let query = match (key.as_ref(), user_id) {
+            (Some(_), Some(_)) => {
+                "SELECT id, user_id, key, old_value, new_value, action, strftime('%s', timestamp) as timestamp
+                 FROM settings_audit_log WHERE key = ?1 AND user_id = ?2 ORDER BY timestamp DESC LIMIT ?3"
+            }
+            (Some(_), None) => {
+                "SELECT id, user_id, key, old_value, new_value, action, strftime('%s', timestamp) as timestamp
+                 FROM settings_audit_log WHERE key = ?1 ORDER BY timestamp DESC LIMIT ?2"
+            }
+            (None, Some(_)) => {
+                "SELECT id, user_id, key, old_value, new_value, action, strftime('%s', timestamp) as timestamp
+                 FROM settings_audit_log WHERE user_id = ?1 ORDER BY timestamp DESC LIMIT ?2"
+            }
+            (None, None) => {
+                "SELECT id, user_id, key, old_value, new_value, action, strftime('%s', timestamp) as timestamp
+                 FROM settings_audit_log ORDER BY timestamp DESC LIMIT ?1"
+            }
+        };
+
+        let mut stmt = conn.prepare(query)
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let entries = match (key, user_id) {
+            (Some(k), Some(u)) => {
+                stmt.query_map(params![k, u, limit], Self::map_audit_row)
+            }
+            (Some(k), None) => {
+                stmt.query_map(params![k, limit], Self::map_audit_row)
+            }
+            (None, Some(u)) => {
+                stmt.query_map(params![u, limit], Self::map_audit_row)
+            }
+            (None, None) => {
+                stmt.query_map(params![limit], Self::map_audit_row)
+            }
+        }
+        .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?
+        .collect::<Result<Vec<_>, _>>()
+        .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        Ok(entries)
+    }
+
+    fn map_audit_row(row: &rusqlite::Row) -> rusqlite::Result<AuditLogEntry> {
+        Ok(AuditLogEntry {
+            id: row.get(0)?,
+            user_id: row.get(1)?,
+            key: row.get(2)?,
+            old_value: row.get(3)?,
+            new_value: row.get(4)?,
+            action: row.get(5)?,
+            timestamp: row.get::<_, String>(6)?.parse().unwrap_or(0),
+        })
+    }
+
+    pub async fn list_all_users(&self) -> Result<Vec<User>, UserServiceError> {
+        let conn = self.conn.read().await;
+        let mut stmt = conn
+            .prepare("SELECT id, nostr_pubkey, username, is_power_user, strftime('%s', created_at) as created_at, strftime('%s', last_seen) as last_seen FROM users ORDER BY created_at DESC")
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        let users = stmt
+            .query_map([], |row| {
+                Ok(User {
+                    id: row.get(0)?,
+                    nostr_pubkey: row.get(1)?,
+                    username: row.get(2)?,
+                    is_power_user: row.get::<_, i64>(3)? == 1,
+                    created_at: row.get::<_, String>(4)?.parse().unwrap_or(0),
+                    last_seen: row.get::<_, String>(5)?.parse().unwrap_or(0),
+                })
+            })
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?
+            .collect::<Result<Vec<_>, _>>()
+            .map_err(|e| UserServiceError::DatabaseError(e.to_string()))?;
+
+        Ok(users)
+    }
+}
diff --git a/src/utils/binary_protocol.rs b/src/utils/binary_protocol.rs
index b58ac71d..627e0667 100644
--- a/src/utils/binary_protocol.rs
+++ b/src/utils/binary_protocol.rs
@@ -1,7 +1,7 @@
 use crate::utils::socket_flow_messages::BinaryNodeData;
 use crate::types::vec3::Vec3Data;
 use crate::models::constraints::{Constraint, AdvancedParams};
-use log::{trace, debug};
+use log::{trace, debug, warn};
 use serde::{Serialize, Deserialize};
 use serde_json;

@@ -21,30 +21,11 @@ const ONTOLOGY_PROPERTY_FLAG: u32 = 0x10000000;   // Bit 28: OWL Property

 const NODE_ID_MASK: u32 = 0x3FFFFFFF;        // Mask to extract actual node ID (bits 0-29)

-// Node type flag constants for u16 (wire format v1 - DEPRECATED)
-// BUG: These constants truncate node IDs > 16383, causing collisions
-// FIXED: Use PROTOCOL_V2 with full u32 IDs for node_id > 16383
-const WIRE_V1_AGENT_FLAG: u16 = 0x8000;         // Bit 15 indicates agent node
-const WIRE_V1_KNOWLEDGE_FLAG: u16 = 0x4000;     // Bit 14 indicates knowledge graph node
-const WIRE_V1_NODE_ID_MASK: u16 = 0x3FFF;       // Mask to extract actual node ID (bits 0-13)
-
 // Node type flag constants for u32 (wire format v2)
 const WIRE_V2_AGENT_FLAG: u32 = 0x80000000;     // Bit 31 indicates agent node
 const WIRE_V2_KNOWLEDGE_FLAG: u32 = 0x40000000; // Bit 30 indicates knowledge graph node
 const WIRE_V2_NODE_ID_MASK: u32 = 0x3FFFFFFF;   // Mask to extract actual node ID (bits 0-29)

-/// Wire format v1 struct (LEGACY - 34 bytes)
-/// BUG: Truncates node IDs to 14 bits (max 16383), causing collisions
-/// DEPRECATED: Use WireNodeDataItemV2 for new implementations
-pub struct WireNodeDataItemV1 {
-    pub id: u16,                // 2 bytes - TRUNCATED to 14 bits + 2 flag bits
-    pub position: Vec3Data,     // 12 bytes
-    pub velocity: Vec3Data,     // 12 bytes
-    pub sssp_distance: f32,     // 4 bytes - SSSP distance from source
-    pub sssp_parent: i32,       // 4 bytes - Parent node for path reconstruction
-    // Total: 34 bytes
-}
-
 /// Wire format v2 struct (FIXED - 38 bytes)
 /// FIXES: Uses full 32-bit node IDs (30 bits + 2 flag bits)
 /// Supports node IDs up to 1,073,741,823 (2^30 - 1)
@@ -61,12 +42,11 @@ pub struct WireNodeDataItemV2 {
 pub type WireNodeDataItem = WireNodeDataItemV2;

 // Constants for wire format sizes
-const WIRE_V1_ID_SIZE: usize = 2;  // u16 (LEGACY)
 const WIRE_V2_ID_SIZE: usize = 4;  // u32 (FIXED)
 const WIRE_VEC3_SIZE: usize = 12;  // 3 * f32
 const WIRE_F32_SIZE: usize = 4;    // f32
 const WIRE_I32_SIZE: usize = 4;    // i32
-const WIRE_V1_ITEM_SIZE: usize = WIRE_V1_ID_SIZE + WIRE_VEC3_SIZE + WIRE_VEC3_SIZE + WIRE_F32_SIZE + WIRE_I32_SIZE; // 34 bytes (2+12+12+4+4)
+const WIRE_V1_ITEM_SIZE: usize = 34; // V1 legacy size (2+12+12+4+4) - kept for decoder only
 const WIRE_V2_ITEM_SIZE: usize = WIRE_V2_ID_SIZE + WIRE_VEC3_SIZE + WIRE_VEC3_SIZE + WIRE_F32_SIZE + WIRE_I32_SIZE; // 36 bytes (4+12+12+4+4) NOT 38!

 // Backwards compatibility alias - DEPRECATED
@@ -191,40 +171,6 @@ pub fn is_ontology_node(node_id: u32) -> bool {
     (node_id & ONTOLOGY_TYPE_MASK) != 0
 }

-/// Convert u32 node ID with flags to u16 wire format (V1 - LEGACY)
-/// BUG: Truncates node IDs to 14 bits! Use to_wire_id_v2 instead.
-/// DEPRECATED: Only kept for backwards compatibility with old clients
-#[deprecated(note = "Use to_wire_id_v2 for full 32-bit node ID support")]
-pub fn to_wire_id_v1(node_id: u32) -> u16 {
-    let actual_id = get_actual_node_id(node_id);
-    let wire_id = (actual_id & 0x3FFF) as u16; // BUG: Truncates to 14 bits!
-
-    // Preserve node type flags
-    if is_agent_node(node_id) {
-        wire_id | WIRE_V1_AGENT_FLAG
-    } else if is_knowledge_node(node_id) {
-        wire_id | WIRE_V1_KNOWLEDGE_FLAG
-    } else {
-        wire_id
-    }
-}
-
-/// Convert u16 wire ID back to u32 preserving flags (V1 - LEGACY)
-/// DEPRECATED: Only kept for backwards compatibility with old clients
-#[deprecated(note = "Use from_wire_id_v2 for full 32-bit node ID support")]
-pub fn from_wire_id_v1(wire_id: u16) -> u32 {
-    let actual_id = (wire_id & WIRE_V1_NODE_ID_MASK) as u32;
-
-    // Restore node type flags
-    if (wire_id & WIRE_V1_AGENT_FLAG) != 0 {
-        actual_id | AGENT_NODE_FLAG
-    } else if (wire_id & WIRE_V1_KNOWLEDGE_FLAG) != 0 {
-        actual_id | KNOWLEDGE_NODE_FLAG
-    } else {
-        actual_id
-    }
-}
-
 /// Convert u32 node ID with flags to u32 wire format (V2 - FIXED)
 /// FIXED: Preserves full 32-bit node ID without truncation
 pub fn to_wire_id_v2(node_id: u32) -> u32 {
@@ -291,8 +237,8 @@ pub fn encode_node_data_extended(
     ontology_individual_ids: &[u32],
     ontology_property_ids: &[u32]
 ) -> Vec<u8> {
-    // Always use V2 protocol to prevent truncation bugs (V1 is only for backwards compat)
-    let use_v2 = true; // Force V2 for all new encoding
+    // Always use V2 protocol to prevent truncation bugs
+    // V1 encoding removed - decoder kept for backward compatibility only
     let item_size = WIRE_V2_ITEM_SIZE;
     let protocol_version = PROTOCOL_V2;

@@ -317,6 +263,24 @@ pub fn encode_node_data_extended(
     for (node_id, node) in nodes {
         // Check node type and set the appropriate flag
         // Priority: Agent > Knowledge > Ontology types > None
+
+        // Validate exclusive type assignment in debug mode
+        #[cfg(debug_assertions)]
+        {
+            let type_count = [
+                agent_node_ids.contains(node_id),
+                knowledge_node_ids.contains(node_id),
+                ontology_class_ids.contains(node_id),
+                ontology_individual_ids.contains(node_id),
+                ontology_property_ids.contains(node_id),
+            ].iter().filter(|&&x| x).count();
+
+            if type_count > 1 {
+                warn!("Node {} appears in {} type lists - using priority order (Agent > Knowledge > Ontology)",
+                      node_id, type_count);
+            }
+        }
+
         let flagged_id = if agent_node_ids.contains(node_id) {
             set_agent_flag(*node_id)
         } else if knowledge_node_ids.contains(node_id) {
@@ -340,16 +304,10 @@ pub fn encode_node_data_extended(
                 agent_node_ids.contains(node_id));
         }

-        if use_v2 {
-            // V2: Write u32 ID (4 bytes) - NO TRUNCATION
-            let wire_id = to_wire_id_v2(flagged_id);
-            buffer.extend_from_slice(&wire_id.to_le_bytes());
-        } else {
-            // V1: Write u16 ID (2 bytes) - TRUNCATES (legacy support only)
-            #[allow(deprecated)]
-            let wire_id = to_wire_id_v1(flagged_id);
-            buffer.extend_from_slice(&wire_id.to_le_bytes());
-        }
+        // V2: Write u32 ID (4 bytes) - NO TRUNCATION
+        // Note: use_v2 is always true (line 295), V1 encoding removed as dead code
+        let wire_id = to_wire_id_v2(flagged_id);
+        buffer.extend_from_slice(&wire_id.to_le_bytes());

         // Write position (12 bytes = 3 * f32)
         buffer.extend_from_slice(&node.x.to_le_bytes());
@@ -456,9 +414,16 @@ fn decode_node_data_v1(data: &[u8]) -> Result<Vec<(u32, BinaryNodeData)>, String
         cursor += 4;
         let _sssp_parent = i32::from_le_bytes([chunk[cursor], chunk[cursor + 1], chunk[cursor + 2], chunk[cursor + 3]]);

-        // Convert V1 wire ID back to u32 with flags
-        #[allow(deprecated)]
-        let full_node_id = from_wire_id_v1(wire_id);
+        // Convert V1 wire ID back to u32 with flags (inlined for backward compatibility)
+        // V1 constants: AGENT_FLAG=0x8000, KNOWLEDGE_FLAG=0x4000, ID_MASK=0x3FFF
+        let actual_id = (wire_id & 0x3FFF) as u32;
+        let full_node_id = if (wire_id & 0x8000) != 0 {
+            actual_id | AGENT_NODE_FLAG
+        } else if (wire_id & 0x4000) != 0 {
+            actual_id | KNOWLEDGE_NODE_FLAG
+        } else {
+            actual_id
+        };

         if samples_logged < max_samples {
             let is_agent = is_agent_node(full_node_id);
@@ -883,23 +848,15 @@ mod tests {
     }

     #[test]
-    fn test_v1_backwards_compatibility() {
-        // Test that small node IDs can use V1 if needed
-        let small_nodes = vec![
-            (100u32, BinaryNodeData {
-                node_id: 100,
-                x: 1.0, y: 2.0, z: 3.0,
-                vx: 0.1, vy: 0.2, vz: 0.3,
-            }),
-        ];
-
-        // V1 not needed, but should decode correctly if received
+    fn test_v1_backwards_compatibility_decoder() {
+        // Test V1 decoder for backward compatibility with old clients
+        // V1 encoding removed, but decoder kept to support legacy data
         let mut v1_encoded = vec![PROTOCOL_V1];
-        #[allow(deprecated)]
-        {
-            let wire_id = to_wire_id_v1(100);
-            v1_encoded.extend_from_slice(&wire_id.to_le_bytes());
-        }
+
+        // Manually construct V1 wire format (u16 ID with flags)
+        let wire_id: u16 = 100; // Small node ID without flags
+        v1_encoded.extend_from_slice(&wire_id.to_le_bytes());
+
         // Add position
         v1_encoded.extend_from_slice(&1.0f32.to_le_bytes());
         v1_encoded.extend_from_slice(&2.0f32.to_le_bytes());
